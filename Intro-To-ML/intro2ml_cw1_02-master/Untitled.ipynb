{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76ea2fe-fb4b-4d66-8f4c-ecb39d1c36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c7c97-d8c3-41e4-aa58-fba864282d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b78b3be-fd24-492b-a3c4-6f3b92433eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    \"\"\" Read in the dataset from the specified filepath\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The filepath to the dataset file\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x, y, classes), each being a numpy array. \n",
    "               - x is a numpy array with shape (N, K), \n",
    "                   where N is the number of instances\n",
    "                   K is the number of features/attributes\n",
    "               - y is a numpy array with shape (N, ), and each element should be \n",
    "                   an integer from 0 to C-1 where C is the number of classes \n",
    "               - classes : a numpy array with shape (C, ), which contains the \n",
    "                   unique class labels corresponding to the integers in y\n",
    "    \"\"\"\n",
    "\n",
    "    x = []\n",
    "    y_labels = []\n",
    "    for line in open(filepath):\n",
    "        if line.strip() != \"\": # handle empty rows in file\n",
    "            row = line.strip().split(\",\")\n",
    "            x.append(list(map(float, row[:-1]))) \n",
    "            y_labels.append(row[-1])\n",
    "    \n",
    "\n",
    "    x = np.array(x, dtype=int)\n",
    "    y = np.array(y_labels)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f43d80-04c0-4a90-b8a1-57cfafdfdaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given\n",
    "        test set proportion.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
    "        test_proportion (float): the desired proportion of test examples\n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
    "               - y_test (np.ndarray): Test labels, shape (N_train, )\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    indices = np.arange(len(x))\n",
    "    random_generator.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    x_row, x_col = x.shape\n",
    "    partition = 1- int(x_row * test_proportion)\n",
    "    x_train = x[:partition, :]\n",
    "    x_test = x[partition:, :]\n",
    "    y_train = y[:partition]\n",
    "    y_test = y[partition:]\n",
    "    print(x_test.shape)\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06d3a3b-a300-428d-8a4e-a759148491a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q' 'C' 'Q' 'Q' 'G' 'C' 'E' 'C' 'O' 'E' 'A' 'A' 'G' 'A' 'A' 'A' 'G' 'G'\n",
      " 'E' 'E' 'E' 'E' 'O' 'C' 'Q' 'Q' 'Q' 'C' 'O' 'Q' 'G' 'Q' 'G' 'O' 'Q' 'Q'\n",
      " 'G' 'O' 'Q' 'A' 'C' 'Q' 'O' 'Q' 'A' 'G' 'O' 'E' 'E' 'A' 'Q' 'G' 'A' 'C'\n",
      " 'A' 'G' 'A' 'C' 'O' 'C' 'O' 'E' 'G' 'Q' 'O' 'E' 'C' 'A' 'A' 'G' 'A' 'C'\n",
      " 'C' 'Q' 'O' 'O' 'O' 'A' 'E' 'C' 'A' 'Q' 'O' 'Q' 'C' 'A' 'A' 'Q' 'C' 'E'\n",
      " 'A' 'A' 'C' 'Q' 'A' 'Q' 'O' 'Q' 'O' 'G' 'C' 'G' 'C' 'C' 'O' 'Q' 'A' 'Q'\n",
      " 'Q' 'C' 'G' 'Q' 'C' 'Q' 'O' 'Q' 'O' 'G' 'Q' 'E' 'O' 'Q' 'E' 'Q' 'O' 'E'\n",
      " 'O' 'C' 'O' 'A' 'G' 'Q' 'E' 'G' 'O' 'Q' 'E' 'O' 'C' 'O' 'G' 'G' 'Q' 'Q'\n",
      " 'E' 'E' 'A' 'C' 'A' 'A' 'A' 'A' 'C' 'O' 'Q' 'G' 'O' 'O' 'E' 'C' 'C' 'Q'\n",
      " 'Q' 'C' 'C' 'O' 'C' 'A' 'G' 'A' 'C' 'A' 'C' 'G' 'E' 'Q' 'E' 'C' 'O' 'A'\n",
      " 'C' 'E' 'C' 'G' 'G' 'Q' 'G' 'Q' 'A' 'C' 'C' 'E' 'G' 'E' 'A' 'E' 'Q' 'O'\n",
      " 'O' 'O']\n"
     ]
    }
   ],
   "source": [
    "(x_test, y_test) = read_dataset(\"data/test.txt\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38834672-4aab-46f8-91a6-600c9623899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_val, y_val) = read_dataset(\"data/validation.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d108024f-c9d6-4c09-82b8-1f73ad2b2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_full, y_full) = read_dataset(\"data/train_full.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b7aee109-de9f-40d2-ba6f-36ec0daa2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_noisy, y_noisy) = read_dataset(\"data/train_noisy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb3d165-4f38-4e9d-9c53-51dd8b00f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_toy, y_toy) = read_dataset(\"data/toy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc8c7b5-095e-4f6d-8041-e6575365346c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \n",
    "    # if no class_labels are given, we obtain the set of unique class labels from\n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int64)\n",
    "\n",
    "    # for each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        # get predictions where the ground truth is the current class label\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # quick way to get the counts per label\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # convert the counts to a dictionary\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # fill up the confusion matrix for the current row\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9c4057-5a53-49cf-8040-76e43c4e8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def recall(y_gold, y_prediction):\n",
    "\n",
    "    confusion = confusion_matrix(y_gold, y_prediction)\n",
    "    r = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[c, :]) > 0:\n",
    "            r[c] = confusion[c, c] / np.sum(confusion[c, :])\n",
    "\n",
    "    macro_r = 0.\n",
    "    if len(r) > 0:\n",
    "        macro_r = np.mean(r)\n",
    "    return (r, macro_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c48aef1-2386-4029-98e5-8ca8e898d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_gold, y_prediction):\n",
    "\n",
    "    confusion = confusion_matrix(y_gold, y_prediction)\n",
    "    p = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[:, c]) > 0:\n",
    "            p[c] = confusion[c, c] / np.sum(confusion[:, c])\n",
    "\n",
    "    macro_p = 0.\n",
    "    if len(p) > 0:\n",
    "        macro_p = np.mean(p)\n",
    "    \n",
    "    return (p, macro_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5a8f6c-4cf9-4f73-bf82-4ef8f27c2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_gold, y_prediction):\n",
    "\n",
    "    (precisions, macro_p) = precision(y_gold, y_prediction)\n",
    "    (recalls, macro_r) = recall(y_gold, y_prediction)\n",
    "\n",
    "    # just to make sure they are of the same length\n",
    "    assert len(precisions) == len(recalls)\n",
    "\n",
    "    f = np.zeros((len(precisions), ))\n",
    "    for c, (p, r) in enumerate(zip(precisions, recalls)):\n",
    "        if p + r > 0:\n",
    "            f[c] = 2 * p * r / (p + r)\n",
    "\n",
    "    macro_f = 0.\n",
    "    if len(f) > 0:\n",
    "        macro_f = np.mean(f)\n",
    "    \n",
    "    return (f, macro_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d0ac4f8-1efc-4698-9024-69db43ba7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_score(y_gold, y_prediction, b):\n",
    "\n",
    "    (precisions, macro_p) = precision(y_gold, y_prediction)\n",
    "    (recalls, macro_r) = recall(y_gold, y_prediction)\n",
    "\n",
    "    # just to make sure they are of the same length\n",
    "    assert len(precisions) == len(recalls)\n",
    "\n",
    "    f = np.zeros((len(precisions), ))\n",
    "    for c, (p, r) in enumerate(zip(precisions, recalls)):\n",
    "        if p + r > 0:\n",
    "            f[c] = (1+b**2) * p * r / ((b**2) * p + r)\n",
    "\n",
    "    macro_f = 0.\n",
    "    if len(f) > 0:\n",
    "        macro_f = np.mean(f)\n",
    "    \n",
    "    return (f, macro_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "539c438f-ed3f-419d-87dd-59563de30d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForestClassifier(object):\n",
    "\n",
    "    def __init__(self, treesNum, num_layers_to_prune=0, feature_proportion=1):\n",
    "        self.is_trained = False\n",
    "        self.roots = []\n",
    "        self.treesNum = treesNum\n",
    "        self.prune_layer_num = num_layers_to_prune\n",
    "        self.feature_proportion = feature_proportion\n",
    "\n",
    "    def fit(self, x, y):\n",
    "\n",
    "        features_num = int(x.shape[1] * self.feature_proportion)\n",
    "        \n",
    "        # for treesNum\n",
    "        for i in range(self.treesNum):\n",
    "            # bagged samples from x\n",
    "            bagged_ints = np.random.randint(0, len(x), len(x), dtype=int)\n",
    "            x_bagged = x[bagged_ints, :]\n",
    "            y_bagged = y[bagged_ints]\n",
    "\n",
    "            features = np.arange(0,x.shape[1])\n",
    "            np.random.shuffle(features)\n",
    "            feature_list = features[:features_num]\n",
    "            x_bagged = x_bagged[:, feature_list]\n",
    "            \n",
    "            # train a tree on this subset\n",
    "            tree = DecisionTreeClassifier(self.prune_layer_num)\n",
    "            tree.fit(x_bagged, y_bagged)\n",
    "            \n",
    "            # add to roots\n",
    "            self.roots.append((tree, feature_list))\n",
    "\n",
    "    def predict(self, x, num_of_trees):\n",
    "\n",
    "        predictions = np.zeros((num_of_trees, len(x)), dtype='str')\n",
    "        # for each tree in roots\n",
    "        new_forest = random.sample(self.roots, k=num_of_trees)\n",
    "        for i, (tree, feature_list) in enumerate(new_forest):\n",
    "            predictions[i] = tree.predict(x[:, feature_list])\n",
    "            \n",
    "        # then find mode of predictions\n",
    "        mode, count = np.unique(predictions, return_counts=True, axis=0)\n",
    "        modePredictions = mode[np.argmax(count)]\n",
    "        return modePredictions\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b75cbe-6b6d-43a2-889a-dd2553ec176b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e7c4086-4647-4bc3-9611-05dd15abd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForestClassifier(object):\n",
    "\n",
    "    def __init__(self, treesNum, num_layers_to_prune=0, feature_proportion=1):\n",
    "        self.is_trained = False\n",
    "        self.roots = []\n",
    "        self.treesNum = treesNum\n",
    "        self.prune_layer_num = num_layers_to_prune\n",
    "        self.feature_proportion = feature_proportion\n",
    "\n",
    "    def fit(self, x, y):\n",
    "\n",
    "        features_num = int(x.shape[1] * self.feature_proportion)\n",
    "        \n",
    "        # for treesNum\n",
    "        for i in range(self.treesNum):\n",
    "            # bagged samples from x\n",
    "            bagged_ints = np.random.randint(0, len(x), len(x), dtype=int)\n",
    "            x_bagged = x[bagged_ints, :]\n",
    "            y_bagged = y[bagged_ints]\n",
    "\n",
    "            features = np.arange(0,x.shape[1])\n",
    "            np.random.shuffle(features)\n",
    "            feature_list = features[:features_num]\n",
    "            x_bagged = x_bagged[:, feature_list]\n",
    "            \n",
    "            # train a tree on this subset\n",
    "            tree = DecisionTreeClassifier(self.prune_layer_num)\n",
    "            tree.fit(x_bagged, y_bagged)\n",
    "            \n",
    "            # add to roots\n",
    "            self.roots.append((tree, feature_list))\n",
    "\n",
    "    def predict(self, x, num_of_trees):\n",
    "\n",
    "        predictions = np.zeros((num_of_trees, len(x)), dtype='str')\n",
    "        # for each tree in roots\n",
    "        new_forest = random.sample(self.roots, k=num_of_trees)\n",
    "        for i, (tree, feature_list) in enumerate(new_forest):\n",
    "            predictions[i] = tree.predict(x[:, feature_list])\n",
    "            \n",
    "        # then find mode of predictions\n",
    "        mode, count = np.unique(predictions, return_counts=True, axis=0)\n",
    "        modePredictions = mode[np.argmax(count)]\n",
    "        return modePredictions\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "31ab90fb-c774-45ba-8680-3afe901c48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y):\n",
    "    return np.sum(x==y)*100/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13fba0d8-381c-4fa0-9071-afc993cd27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, split_val=None, column=None, label=None):\n",
    "        self.left = self.right = None\n",
    "        self.split_val = split_val\n",
    "        self.column = column\n",
    "        self.label = label\n",
    "        \n",
    "    def add_child(self, child): \n",
    "        if self.left == None:\n",
    "            self.left = child\n",
    "        elif self.right == None:\n",
    "            self.right = child\n",
    "        else:\n",
    "            print(\"no children node free\")\n",
    "            exit()\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.label != None:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def max_depth(self, d=0):\n",
    "\n",
    "        if self.left == None and self.right == None:\n",
    "            return 0\n",
    "        \n",
    "        left_depth = self.left.max_depth(d) + 1\n",
    "        right_depth = self.right.max_depth(d) + 1\n",
    "\n",
    "        if right_depth >= left_depth:\n",
    "            return right_depth\n",
    "        else:\n",
    "            return left_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a2cfcb29-47ef-47ea-bd6f-24b4f7c2f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    \"\"\" Basic decision tree classifier\n",
    "    \n",
    "    Attributes:\n",
    "    is_trained (bool): Keeps track of whether the classifier has been trained\n",
    "    \n",
    "    Methods:\n",
    "    fit(x, y): Constructs a decision tree from data X and label y\n",
    "    predict(x): Predicts the class label of samples X\n",
    "    prune(x_val, y_val): Post-prunes the decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_trained = False\n",
    "        self.root = None\n",
    "\n",
    "    \n",
    "    def calculate_entropy(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    def calculate_info_gain(self, x, y, x_val, sort_col):\n",
    "\n",
    "        # Calculate total entropy for overall data\n",
    "        total_entropy = self.calculate_entropy(y)\n",
    "\n",
    "        # Calculate entropy for left and right of split\n",
    "        left_entropy = self.calculate_entropy(y[x[:, sort_col] < x_val])\n",
    "        right_entropy = self.calculate_entropy(y[x[:, sort_col] >= x_val])\n",
    "\n",
    "        # Calculate info gain\n",
    "        info_gain = total_entropy - ((len(y[x[:, sort_col] < x_val])/len(y) * left_entropy) \n",
    "                                    + (len(y[x[:, sort_col] >= x_val])/len(y) * right_entropy))\n",
    "        \n",
    "        return info_gain\n",
    "\n",
    "        \n",
    "    def find_best_node(self, x, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        # To keep track of info gain\n",
    "        max_gain = value_to_split_on = column_to_split_on = None\n",
    "                \n",
    "        # loop through each column \n",
    "        for i in range(x.shape[1]):\n",
    "            # sort by that column\n",
    "            index_list = x[:, i].argsort()\n",
    "            x = x[index_list]\n",
    "            y = y[index_list]\n",
    "\n",
    "            starting_label = y[0]\n",
    "            starting_val = x[:, i][0]\n",
    "\n",
    "            # loop through the column\n",
    "            for x_val, y_val in zip(x[:, i], y):\n",
    "                if (y_val != starting_label) and (x_val != starting_val):\n",
    "\n",
    "                    # calculate information gain\n",
    "                    info_gain = self.calculate_info_gain(x, y, x_val, i)\n",
    "                    \n",
    "                    # update the max information gain\n",
    "                    if max_gain is None or info_gain > max_gain:\n",
    "                        max_gain = info_gain\n",
    "                        value_to_split_on = x_val\n",
    "                        column_to_split_on = i\n",
    "\n",
    "                    # Update starting label\n",
    "                    starting_label = y_val\n",
    "                    starting_val = x_val\n",
    "                    \n",
    "        return Node(value_to_split_on, column_to_split_on)\n",
    "\n",
    "\n",
    "    def split_dataset(self, x, y, node):\n",
    "        # Simplified dataset splitting\n",
    "        left_mask = x[:, node.column] < node.split_val\n",
    "        right_mask = ~left_mask  # Inverse of left_mask\n",
    "        return (x[left_mask], y[left_mask]), (x[right_mask], y[right_mask])\n",
    "\n",
    "\n",
    "    \n",
    "    def induce_decision_tree(self, x, y):\n",
    "        # check y count is 1 or node column returns -1\n",
    "        if (len(np.unique(y)) <= 1 or x.shape[0] == 1):\n",
    "            leaf_node = Node(label=y[0])\n",
    "            return leaf_node\n",
    "\n",
    "        else:\n",
    "            # find best node\n",
    "            parent = self.find_best_node(x, y)\n",
    "            \n",
    "            # get left and right datasets\n",
    "            if parent.split_val is None:\n",
    "                label_set, count = np.unique(y, return_counts=True)\n",
    "                label = label_set[np.argmax(count)]\n",
    "                leaf_node = Node(label=label)\n",
    "                return leaf_node\n",
    "                 \n",
    "            child_data = self.split_dataset(x, y, parent)\n",
    "            \n",
    "            for i in child_data: \n",
    "                child_node = self.induce_decision_tree(i[0], i[1])\n",
    "                parent.add_child(child_node)\n",
    "        \n",
    "            return parent\n",
    "    \n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\" Constructs a decision tree classifier from data\n",
    "        \n",
    "        Args:\n",
    "        x (numpy.ndarray): Instances, numpy array of shape (N, K) \n",
    "                           N is the number of instances\n",
    "                           K is the number of attributes\n",
    "        y (numpy.ndarray): Class labels, numpy array of shape (N, )\n",
    "                           Each element in y is a str \n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure that x and y have the same number of instances\n",
    "        assert x.shape[0] == len(y), \\\n",
    "            \"Training failed. x and y must have the same number of instances.\"\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 2.1: COMPLETE THIS METHOD **\n",
    "        #######################################################################    \n",
    "        self.root = self.induce_decision_tree(x, y)\n",
    "        \n",
    "        # set a flag so that we know that the classifier has been trained\n",
    "        self.is_trained = True\n",
    "\n",
    "    \n",
    "    def classify_instance(self, instance, node):\n",
    "\n",
    "        if node.label != None:\n",
    "            return node.label\n",
    "\n",
    "        if instance[node.column] >= node.split_val:\n",
    "            return self.classify_instance(instance, node.right)\n",
    "        \n",
    "        return self.classify_instance(instance, node.left)\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Predicts a set of samples using the trained DecisionTreeClassifier.\n",
    "        \n",
    "        Assumes that the DecisionTreeClassifier has already been trained.\n",
    "        \n",
    "        Args:\n",
    "        x (numpy.ndarray): Instances, numpy array of shape (M, K) \n",
    "                           M is the number of test instances\n",
    "                           K is the number of attributes\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: A numpy array of shape (M, ) containing the predicted\n",
    "                       class label for each instance in x\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sure that the classifier has been trained before predicting\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"DecisionTreeClassifier has not yet been trained.\")\n",
    "        \n",
    "        # set up an empty (M, ) numpy array to store the predicted labels \n",
    "        # feel free to change this if needed\n",
    "        predictions = np.zeros((x.shape[0],), dtype=object)\n",
    "        #######################################################################\n",
    "        #                 ** TASK 2.2: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        for i in range(len(x)):\n",
    "            label = self.classify_instance(x[i], self.root)\n",
    "            predictions[i] = label\n",
    "    \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fdbb9704-30d9-48d9-b86b-65ae716cba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDecisionTreeClassifier(object):\n",
    "    \"\"\" Basic decision tree classifier\n",
    "    \n",
    "    Attributes:\n",
    "    is_trained (bool): Keeps track of whether the classifier has been trained\n",
    "    \n",
    "    Methods:\n",
    "    fit(x, y): Constructs a decision tree from data X and label y\n",
    "    predict(x): Predicts the class label of samples X\n",
    "    prune(x_val, y_val): Post-prunes the decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers_to_prune=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.is_trained = False\n",
    "        self.root = None\n",
    "        self.num_layers_to_prune = num_layers_to_prune\n",
    "        self.max_depth = 0\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    \n",
    "    def calculate_entropy(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    def calculate_info_gain(self, x, y, x_val, sort_col):\n",
    "\n",
    "        # Calculate total entropy for overall data\n",
    "        total_entropy = self.calculate_entropy(y)\n",
    "\n",
    "        # Calculate entropy for left and right of split\n",
    "        left_entropy = self.calculate_entropy(y[x[:, sort_col] < x_val])\n",
    "        right_entropy = self.calculate_entropy(y[x[:, sort_col] >= x_val])\n",
    "\n",
    "        # Calculate info gain\n",
    "        info_gain = total_entropy - ((len(y[x[:, sort_col] < x_val])/len(y) * left_entropy) \n",
    "                                    + (len(y[x[:, sort_col] >= x_val])/len(y) * right_entropy))\n",
    "        \n",
    "        return info_gain\n",
    "\n",
    "        \n",
    "    def find_best_node(self, x, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        # To keep track of info gain\n",
    "        max_gain = value_to_split_on = column_to_split_on = None\n",
    "                \n",
    "        # loop through each column \n",
    "        for i in range(x.shape[1]):\n",
    "            # sort by that column\n",
    "            index_list = x[:, i].argsort()\n",
    "            x = x[index_list]\n",
    "            y = y[index_list]\n",
    "\n",
    "            starting_label = y[0]\n",
    "            starting_val = x[:, i][0]\n",
    "\n",
    "            # loop through the column\n",
    "            for x_val, y_val in zip(x[:, i], y):\n",
    "                if (y_val != starting_label) and (x_val != starting_val):\n",
    "\n",
    "                    # calculate information gain\n",
    "                    info_gain = self.calculate_info_gain(x, y, x_val, i)\n",
    "                    \n",
    "                    # update the max information gain\n",
    "                    if max_gain is None or info_gain > max_gain:\n",
    "                        max_gain = info_gain\n",
    "                        value_to_split_on = x_val\n",
    "                        column_to_split_on = i\n",
    "\n",
    "                    # Update starting label\n",
    "                    starting_label = y_val\n",
    "                    starting_val = x_val\n",
    "                    \n",
    "        return Node(value_to_split_on, column_to_split_on)\n",
    "\n",
    "\n",
    "    def split_dataset(self, x, y, node):\n",
    "        # Simplified dataset splitting\n",
    "        left_mask = x[:, node.column] < node.split_val\n",
    "        right_mask = ~left_mask  # Inverse of left_mask\n",
    "        return (x[left_mask], y[left_mask]), (x[right_mask], y[right_mask])\n",
    "\n",
    "\n",
    "        \n",
    "    def induce_decision_tree(self, x, y, depth=0):\n",
    "        if len(np.unique(y)) <= 1 or x.shape[0] < self.min_samples_split or depth == self.max_depth:\n",
    "            label_set, count = np.unique(y, return_counts=True)\n",
    "            leaf_node = Node(label=label_set[np.argmax(count)])\n",
    "            return leaf_node\n",
    "        else:\n",
    "            parent = self.find_best_node(x, y)\n",
    "            if parent.split_val is None or x.shape[0] <= self.min_samples_leaf:\n",
    "                label_set, count = np.unique(y, return_counts=True)\n",
    "                leaf_node = Node(label=label_set[np.argmax(count)])\n",
    "                return leaf_node\n",
    "            \n",
    "            (x_left, y_left), (x_right, y_right) = self.split_dataset(x, y, parent)\n",
    "            \n",
    "            if len(y_left) >= self.min_samples_leaf and len(y_right) >= self.min_samples_leaf:\n",
    "                parent.left = self.induce_decision_tree(x_left, y_left, depth + 1)\n",
    "                parent.right = self.induce_decision_tree(x_right, y_right, depth + 1)\n",
    "            else:\n",
    "                label_set, count = np.unique(y, return_counts=True)\n",
    "                return Node(label=label_set[np.argmax(count)])\n",
    "            \n",
    "            return parent\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, x, y):\n",
    "\n",
    "        \n",
    "        # Make sure that x and y have the same number of instances\n",
    "        assert x.shape[0] == len(y), \\\n",
    "            \"Training failed. x and y must have the same number of instances.\"\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 2.1: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        self.max_depth = np.inf\n",
    "        self.root = self.induce_decision_tree(x, y)\n",
    "        self.max_depth = self.get_max_depth_from_tree() - self.num_layers_to_prune\n",
    "\n",
    "        self.root = self.induce_decision_tree(x, y)\n",
    "        \n",
    "        # set a flag so that we know that the classifier has been trained\n",
    "        self.is_trained = True\n",
    "\n",
    "    \n",
    "    def classify_instance(self, instance, node):\n",
    "\n",
    "        if node.label != None:\n",
    "            return node.label\n",
    "\n",
    "        if instance[node.column] >= node.split_val:\n",
    "            return self.classify_instance(instance, node.right)\n",
    "        \n",
    "        return self.classify_instance(instance, node.left)\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "\n",
    "        \n",
    "        # make sure that the classifier has been trained before predicting\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"DecisionTreeClassifier has not yet been trained.\")\n",
    "        \n",
    "        # set up an empty (M, ) numpy array to store the predicted labels \n",
    "        # feel free to change this if needed\n",
    "        predictions = np.zeros((x.shape[0],), dtype=object)\n",
    "        #######################################################################\n",
    "        #                 ** TASK 2.2: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        for i in range(len(x)):\n",
    "            label = self.classify_instance(x[i], self.root)\n",
    "            predictions[i] = label\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "    def get_max_depth_from_tree(self):\n",
    "        return self.root.max_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9d50f8-3158-4a06-952b-fab42252f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTreeClassifier(8);\n",
    "t.fit(x_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa453db1-5afc-4c8e-a509-3f43a6ab36f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "93.0\n"
     ]
    }
   ],
   "source": [
    "print(t.get_max_depth_from_tree())\n",
    "\n",
    "predict = t.predict(x_val)\n",
    "\n",
    "print(accuracy(predict, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ec3206d-5a2b-4820-a5d4-25feea10d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForestClassifier(object):\n",
    "\n",
    "    def __init__(self, treesNum, num_layers_to_prune=0, feature_proportion=1):\n",
    "        self.is_trained = False\n",
    "        self.roots = []\n",
    "        self.treesNum = treesNum\n",
    "        self.prune_layer_num = num_layers_to_prune\n",
    "        self.feature_proportion = feature_proportion\n",
    "\n",
    "    def fit(self, x, y):\n",
    "\n",
    "        features_num = int(x.shape[1] * self.feature_proportion)\n",
    "        \n",
    "        # for treesNum\n",
    "        for i in range(self.treesNum):\n",
    "            # bagged samples from x\n",
    "            bagged_ints = np.random.randint(0, len(x), len(x), dtype=int)\n",
    "            x_bagged = x[bagged_ints, :]\n",
    "            y_bagged = y[bagged_ints]\n",
    "\n",
    "            features = np.arange(0,x.shape[1])\n",
    "            np.random.shuffle(features)\n",
    "            feature_list = features[:features_num]\n",
    "            x_bagged = x_bagged[:, feature_list]\n",
    "            \n",
    "            # train a tree on this subset\n",
    "            tree = DecisionTreeClassifier(self.prune_layer_num)\n",
    "            tree.fit(x_bagged, y_bagged)\n",
    "            \n",
    "            # add to roots\n",
    "            self.roots.append((tree, feature_list))\n",
    "\n",
    "    def predict(self, x, num_of_trees):\n",
    "\n",
    "        predictions = np.zeros((num_of_trees, len(x)), dtype='str')\n",
    "        # for each tree in roots\n",
    "        new_forest = random.sample(self.roots, k=num_of_trees)\n",
    "        for i, (tree, feature_list) in enumerate(new_forest):\n",
    "            predictions[i] = tree.predict(x[:, feature_list])\n",
    "            \n",
    "        # then find mode of predictions\n",
    "        mode, count = np.unique(predictions, return_counts=True, axis=0)\n",
    "        modePredictions = mode[np.argmax(count)]\n",
    "        return modePredictions\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a37eb88-b693-4aef-b478-c508976f8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RandomForestClassifier(10, 8, 0.4);\n",
    "test.fit(x_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb404e4c-4e3f-4f83-b7e7-ef8d199ce4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DecisionTreeClassifier object at 0x106bb2690>\n",
      "<__main__.DecisionTreeClassifier object at 0x37d3ad790>\n",
      "<__main__.DecisionTreeClassifier object at 0x37d171b50>\n",
      "<__main__.DecisionTreeClassifier object at 0x17c1b36d0>\n",
      "<__main__.DecisionTreeClassifier object at 0x37d0e8050>\n",
      "<__main__.DecisionTreeClassifier object at 0x37d077150>\n",
      "<__main__.DecisionTreeClassifier object at 0x37c9c3e50>\n",
      "<__main__.DecisionTreeClassifier object at 0x37d0dc290>\n",
      "['A' 'A' 'A' 'E' 'Q' 'C' 'Q' 'Q' 'O' 'A' 'O' 'Q' 'C' 'O' 'A' 'O' 'C' 'C'\n",
      " 'C' 'A' 'A' 'C' 'A' 'E' 'O' 'G' 'A' 'C' 'E' 'E' 'O' 'A' 'C' 'E' 'E' 'Q'\n",
      " 'E' 'C' 'A' 'O' 'Q' 'O' 'O' 'Q' 'G' 'E' 'G' 'A' 'C' 'E' 'Q' 'O' 'O' 'O'\n",
      " 'C' 'C' 'C' 'A' 'O' 'E' 'E' 'E' 'O' 'E' 'Q' 'O' 'G' 'C' 'O' 'E' 'O' 'Q'\n",
      " 'Q' 'A' 'C' 'G' 'E' 'E' 'C' 'O' 'O' 'C' 'Q' 'A' 'E' 'C' 'O' 'G' 'A' 'A'\n",
      " 'G' 'E' 'Q' 'O' 'A' 'A' 'A' 'A' 'O' 'G']\n"
     ]
    }
   ],
   "source": [
    "thirty_predict = test.predict(x_val, 8)\n",
    "print(thirty_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d226586-3faa-4303-97de-f97504b29bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  1  0  1  1]\n",
      " [ 0 13  0  0  0  0]\n",
      " [ 0  2 12  3  0  1]\n",
      " [ 1  0  1 11  0  1]\n",
      " [ 3  1  1  0 14  1]\n",
      " [ 1  0  0  0  1 14]]\n"
     ]
    }
   ],
   "source": [
    "confusion_thirty = confusion_matrix(y_val, thirty_predict)\n",
    "print(confusion_thirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f49b6290-326e-4c59-a0ab-d176b45f6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(thirty_predict, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc54bfe-8a7a-4a8c-90c6-1e69614c1feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'C' 'Q' 'A' 'G' 'C' 'C' 'C' 'Q' 'E' 'A' 'A' 'G' 'A' 'A' 'A' 'G' 'Q'\n",
      " 'E' 'E' 'E' 'G' 'O' 'C' 'Q' 'Q' 'Q' 'C' 'G' 'O' 'O' 'G' 'G' 'O' 'Q' 'A'\n",
      " 'O' 'O' 'A' 'A' 'C' 'G' 'O' 'G' 'A' 'Q' 'E' 'E' 'E' 'G' 'Q' 'G' 'A' 'C'\n",
      " 'G' 'G' 'G' 'C' 'G' 'G' 'O' 'E' 'G' 'Q' 'O' 'E' 'G' 'A' 'A' 'G' 'Q' 'C'\n",
      " 'O' 'C' 'O' 'O' 'O' 'A' 'G' 'C' 'C' 'Q' 'O' 'C' 'C' 'A' 'A' 'Q' 'C' 'E'\n",
      " 'A' 'A' 'C' 'Q' 'A' 'Q' 'A' 'Q' 'O' 'G' 'C' 'Q' 'G' 'C' 'O' 'Q' 'A' 'Q'\n",
      " 'Q' 'C' 'G' 'Q' 'C' 'Q' 'G' 'Q' 'O' 'G' 'Q' 'E' 'O' 'Q' 'E' 'Q' 'O' 'E'\n",
      " 'O' 'G' 'O' 'Q' 'G' 'Q' 'E' 'Q' 'E' 'G' 'E' 'O' 'G' 'O' 'G' 'G' 'Q' 'Q'\n",
      " 'G' 'E' 'A' 'C' 'A' 'G' 'A' 'Q' 'C' 'O' 'Q' 'G' 'O' 'O' 'E' 'C' 'C' 'Q'\n",
      " 'G' 'C' 'C' 'O' 'C' 'A' 'A' 'A' 'O' 'C' 'C' 'G' 'E' 'Q' 'E' 'G' 'O' 'A'\n",
      " 'C' 'E' 'G' 'G' 'G' 'Q' 'G' 'Q' 'A' 'C' 'C' 'O' 'G' 'E' 'A' 'E' 'Q' 'O'\n",
      " 'A' 'O']\n"
     ]
    }
   ],
   "source": [
    "test_predict = test.predict(x_test)\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f769069-938c-418c-92ed-3457f999de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(test_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d277a12-e725-4baf-baa2-ebb2f99b913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((10, len(x_val)), dtype='str')\n",
    "\n",
    "for i in range(1,101, 10):\n",
    "    test = RandomForestClassifier(i)\n",
    "    test.fit(x_full, y_full)\n",
    "\n",
    "    predictions[i//10] = (test.predict(x_val))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "046b762f-8f24-4504-9c38-ce3f3cd9241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 88.0\n",
      "1: 82.0\n",
      "2: 85.0\n",
      "3: 86.0\n",
      "4: 84.0\n",
      "5: 82.0\n",
      "6: 91.0\n",
      "7: 85.0\n",
      "8: 92.0\n",
      "9: 89.0\n"
     ]
    }
   ],
   "source": [
    "for i, list in enumerate(predictions):\n",
    "    print(f\"{i}: {accuracy(list, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "03fe09ff-793e-4d88-a2e7-ca896b18c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 5 0.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# create random forest\u001b[39;00m\n\u001b[1;32m     23\u001b[0m tempForest \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m100\u001b[39m, j, k)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtempForest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# save to list\u001b[39;00m\n\u001b[1;32m     27\u001b[0m config \u001b[38;5;241m=\u001b[39m (j, k, tempForest)\n",
      "Cell \u001b[0;32mIn[110], line 30\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# train a tree on this subset\u001b[39;00m\n\u001b[1;32m     29\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_layer_num)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_bagged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bagged\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# add to roots\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroots\u001b[38;5;241m.\u001b[39mappend((tree, feature_list))\n",
      "Cell \u001b[0;32mIn[137], line 128\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x, y)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_depth_from_tree() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers_to_prune\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# set a flag so that we know that the classifier has been trained\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[137], line 104\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m (x_left, y_left), (x_right, y_right) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dataset(x, y, parent)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[0;32m--> 104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[137], line 105\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m    104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     label_set, count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[137], line 105\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m    104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     label_set, count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping similar frames: DecisionTreeClassifier.induce_decision_tree at line 105 (1 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[137], line 105\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m    104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     label_set, count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[137], line 104\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m (x_left, y_left), (x_right, y_right) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dataset(x, y, parent)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[0;32m--> 104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[137], line 104\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m (x_left, y_left), (x_right, y_right) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dataset(x, y, parent)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[0;32m--> 104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[137], line 105\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m    104\u001b[0m     parent\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minduce_decision_tree(x_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     parent\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minduce_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     label_set, count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[137], line 95\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.induce_decision_tree\u001b[0;34m(self, x, y, depth)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf_node\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent\u001b[38;5;241m.\u001b[39msplit_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m     97\u001b[0m         label_set, count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[137], line 66\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.find_best_node\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_val, y_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x[:, i], y):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (y_val \u001b[38;5;241m!=\u001b[39m starting_label) \u001b[38;5;129;01mand\u001b[39;00m (x_val \u001b[38;5;241m!=\u001b[39m starting_val):\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# calculate information gain\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m         info_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_info_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# update the max information gain\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_gain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m info_gain \u001b[38;5;241m>\u001b[39m max_gain:\n",
      "Cell \u001b[0;32mIn[137], line 32\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.calculate_info_gain\u001b[0;34m(self, x, y, x_val, sort_col)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_info_gain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, x_val, sort_col):\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Calculate total entropy for overall data\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     total_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Calculate entropy for left and right of split\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     left_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_entropy(y[x[:, sort_col] \u001b[38;5;241m<\u001b[39m x_val])\n",
      "Cell \u001b[0;32mIn[137], line 25\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.calculate_entropy\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m values, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 25\u001b[0m entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m entropy\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/ComputingMSc/Intro2ML/intro2ml_cw1_02/venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ComputingMSc/Intro2ML/intro2ml_cw1_02/venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save each configuration\n",
    "    # 6x? array\n",
    "    # 6th index = forest\n",
    "\n",
    "# ranges of configurations\n",
    "    # num of trees\n",
    "    # pruning layers (0-10)\n",
    "    # min split sample size (2-7)\n",
    "    # min leaf sample size (1-6)\n",
    "    # proportions of features (0.3 - 0.9, steps of 0.05)\n",
    "\n",
    "# create array for configs\n",
    "configurations = []\n",
    "\n",
    "# pruning range\n",
    "for j in range(5,9): #3,4,5,6\n",
    "    \n",
    "    # feature range\n",
    "    for k in np.arange(0.33, 0.67, 0.33): #0.33, 0.66\n",
    "        print(100,j,k)\n",
    "\n",
    "        # create random forest\n",
    "        tempForest = RandomForestClassifier(100, j, k)\n",
    "        tempForest.fit(x_full, y_full)\n",
    "        \n",
    "        # save to list\n",
    "        config = (j, k, tempForest)\n",
    "        configurations.append(config)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "10ffc467-8de9-46a6-9f83-f0f406331858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.33, 25) (accuracy is: 74.7) and (recall is: 0.7385150176307415) and (precision is: 0.7385150176307415)\n",
      "(1, 0.33, 50) (accuracy is: 76.8) and (recall is: 0.7665191736108101) and (precision is: 0.7665191736108101)\n",
      "(1, 0.33, 75) (accuracy is: 74.8) and (recall is: 0.7485071938191374) and (precision is: 0.7485071938191374)\n",
      "(1, 0.33, 100) (accuracy is: 74.0) and (recall is: 0.7444608707698157) and (precision is: 0.7444608707698157)\n",
      "(1, 0.66, 25) (accuracy is: 86.5) and (recall is: 0.8654944817213852) and (precision is: 0.8654944817213852)\n",
      "(1, 0.66, 50) (accuracy is: 86.3) and (recall is: 0.8655792394368851) and (precision is: 0.8655792394368851)\n",
      "(1, 0.66, 75) (accuracy is: 86.4) and (recall is: 0.8672571898737447) and (precision is: 0.8672571898737447)\n",
      "(1, 0.66, 100) (accuracy is: 86.0) and (recall is: 0.86361429196184) and (precision is: 0.86361429196184)\n",
      "(2, 0.33, 25) (accuracy is: 72.7) and (recall is: 0.7255831197683572) and (precision is: 0.7255831197683572)\n",
      "(2, 0.33, 50) (accuracy is: 68.6) and (recall is: 0.6771166078549227) and (precision is: 0.6771166078549227)\n",
      "(2, 0.33, 75) (accuracy is: 65.4) and (recall is: 0.6418317799062792) and (precision is: 0.6418317799062792)\n",
      "(2, 0.33, 100) (accuracy is: 64.0) and (recall is: 0.6271208381228676) and (precision is: 0.6271208381228676)\n",
      "(2, 0.66, 25) (accuracy is: 85.5) and (recall is: 0.8536909936897608) and (precision is: 0.8536909936897608)\n",
      "(2, 0.66, 50) (accuracy is: 85.0) and (recall is: 0.8494050866073785) and (precision is: 0.8494050866073785)\n",
      "(2, 0.66, 75) (accuracy is: 84.4) and (recall is: 0.8450403938336507) and (precision is: 0.8450403938336507)\n",
      "(2, 0.66, 100) (accuracy is: 84.0) and (recall is: 0.842236329697645) and (precision is: 0.842236329697645)\n",
      "(3, 0.33, 25) (accuracy is: 70.0) and (recall is: 0.6933325875249168) and (precision is: 0.6933325875249168)\n",
      "(3, 0.33, 50) (accuracy is: 70.2) and (recall is: 0.6928822298893819) and (precision is: 0.6928822298893819)\n",
      "(3, 0.33, 75) (accuracy is: 67.4) and (recall is: 0.6610556261985907) and (precision is: 0.6610556261985907)\n",
      "(3, 0.33, 100) (accuracy is: 67.0) and (recall is: 0.6518254989315182) and (precision is: 0.6518254989315182)\n",
      "(3, 0.66, 25) (accuracy is: 88.8) and (recall is: 0.8918767475319687) and (precision is: 0.8918767475319687)\n",
      "(3, 0.66, 50) (accuracy is: 89.4) and (recall is: 0.8987185686080522) and (precision is: 0.8987185686080522)\n",
      "(3, 0.66, 75) (accuracy is: 89.4) and (recall is: 0.8982349629844677) and (precision is: 0.8982349629844677)\n",
      "(3, 0.66, 100) (accuracy is: 91.0) and (recall is: 0.9173490475945278) and (precision is: 0.9173490475945278)\n",
      "(4, 0.33, 25) (accuracy is: 72.8) and (recall is: 0.721529455135337) and (precision is: 0.721529455135337)\n",
      "(4, 0.33, 50) (accuracy is: 73.6) and (recall is: 0.7318408094695183) and (precision is: 0.7318408094695183)\n",
      "(4, 0.33, 75) (accuracy is: 70.0) and (recall is: 0.6925517155702294) and (precision is: 0.6925517155702294)\n",
      "(4, 0.33, 100) (accuracy is: 70.0) and (recall is: 0.6925517155702294) and (precision is: 0.6925517155702294)\n",
      "(4, 0.66, 25) (accuracy is: 85.4) and (recall is: 0.8562526683388683) and (precision is: 0.8562526683388683)\n",
      "(4, 0.66, 50) (accuracy is: 85.2) and (recall is: 0.8596259183725957) and (precision is: 0.8596259183725957)\n",
      "(4, 0.66, 75) (accuracy is: 85.2) and (recall is: 0.8593290879920706) and (precision is: 0.8593290879920706)\n",
      "(4, 0.66, 100) (accuracy is: 85.0) and (recall is: 0.8566919093095526) and (precision is: 0.8566919093095526)\n"
     ]
    }
   ],
   "source": [
    "# 100\n",
    "    # 1-7 0.33\n",
    "    # 1-7 0.66\n",
    "\n",
    "# 1 0.33\n",
    "\n",
    "for (j, k, tempForest) in configurations:\n",
    "    for i in range(25,101,25):\n",
    "        accuracies = np.zeros(10)\n",
    "        f1_scores = np.zeros(10)\n",
    "        recalls = np.zeros(10)\n",
    "        precisions = np.zeros(10)\n",
    "        for m in range(0,10):\n",
    "            temp_prediction = tempForest.predict(x_val, i)\n",
    "            accuracies[m] = accuracy(temp_prediction, y_val)\n",
    "            recalls[m] = fb_score(y_val, temp_prediction, 0.2)[1]\n",
    "            precisions[m] = fb_score(y_val, temp_prediction, 0.2)[1]\n",
    "            f1_scores[m] = fb_score(y_val, temp_prediction, 0.2)[1]\n",
    "\n",
    "        ave_accuracy = np.sum(accuracies)/10\n",
    "        ave_f1 = np.sum(f1_scores)/10\n",
    "        ave_recall = np.sum(recalls)/10\n",
    "        ave_precision = np.sum(precisions)/10\n",
    "            \n",
    "        print(f\"({j}, {k}, {i}) (accuracy is: {ave_accuracy}) and (recall is: {ave_recall}) and (precision is: {ave_precision})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "003d1519-5084-4813-b933-931e15aca4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "67.0\n",
      "79.0\n",
      "79.0\n",
      "83.0\n",
      "83.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "['O' 'C' 'E' 'Q' 'G' 'C' 'E' 'C' 'O' 'E' 'A' 'A' 'G' 'A' 'A' 'A' 'G' 'G'\n",
      " 'E' 'E' 'E' 'E' 'O' 'C' 'C' 'Q' 'Q' 'C' 'O' 'Q' 'G' 'G' 'G' 'O' 'Q' 'G'\n",
      " 'G' 'O' 'Q' 'C' 'C' 'Q' 'O' 'E' 'A' 'E' 'O' 'E' 'E' 'A' 'G' 'G' 'A' 'C'\n",
      " 'A' 'G' 'A' 'C' 'Q' 'G' 'O' 'E' 'G' 'Q' 'O' 'E' 'G' 'A' 'A' 'A' 'A' 'C'\n",
      " 'C' 'G' 'G' 'O' 'O' 'A' 'E' 'C' 'A' 'Q' 'E' 'Q' 'C' 'A' 'A' 'Q' 'C' 'E'\n",
      " 'A' 'A' 'C' 'Q' 'A' 'Q' 'O' 'Q' 'O' 'Q' 'C' 'G' 'C' 'C' 'O' 'Q' 'A' 'G'\n",
      " 'Q' 'C' 'G' 'Q' 'C' 'Q' 'G' 'Q' 'O' 'G' 'Q' 'E' 'O' 'Q' 'E' 'Q' 'O' 'E'\n",
      " 'O' 'G' 'O' 'A' 'G' 'O' 'E' 'G' 'Q' 'Q' 'E' 'E' 'E' 'O' 'G' 'C' 'Q' 'Q'\n",
      " 'E' 'G' 'A' 'C' 'A' 'Q' 'A' 'A' 'C' 'G' 'Q' 'O' 'O' 'O' 'E' 'C' 'C' 'Q'\n",
      " 'Q' 'C' 'C' 'O' 'C' 'A' 'G' 'A' 'C' 'A' 'C' 'G' 'E' 'Q' 'E' 'C' 'O' 'A'\n",
      " 'C' 'C' 'C' 'G' 'G' 'Q' 'G' 'Q' 'A' 'C' 'C' 'E' 'G' 'E' 'A' 'E' 'Q' 'C'\n",
      " 'O' 'O']\n",
      "84.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "494739b5-6428-49a5-861f-0342623adcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 1\n",
      "0 2 2\n",
      "0 2 3\n",
      "0 2 4\n",
      "0 3 1\n",
      "0 3 2\n",
      "0 3 3\n",
      "0 3 4\n",
      "0 4 1\n",
      "0 4 2\n",
      "0 4 3\n",
      "0 4 4\n",
      "0 5 1\n",
      "0 5 2\n",
      "0 5 3\n",
      "0 5 4\n",
      "0 6 1\n",
      "0 6 2\n",
      "0 6 3\n",
      "0 6 4\n",
      "0 7 1\n",
      "0 7 2\n",
      "0 7 3\n",
      "0 7 4\n",
      "0 8 1\n",
      "0 8 2\n",
      "0 8 3\n",
      "0 8 4\n",
      "0 9 1\n",
      "0 9 2\n",
      "0 9 3\n",
      "0 9 4\n",
      "1 2 1\n",
      "1 2 2\n",
      "1 2 3\n",
      "1 2 4\n",
      "1 3 1\n",
      "1 3 2\n",
      "1 3 3\n",
      "1 3 4\n",
      "1 4 1\n",
      "1 4 2\n",
      "1 4 3\n",
      "1 4 4\n",
      "1 5 1\n",
      "1 5 2\n",
      "1 5 3\n",
      "1 5 4\n",
      "1 6 1\n",
      "1 6 2\n",
      "1 6 3\n",
      "1 6 4\n",
      "1 7 1\n",
      "1 7 2\n",
      "1 7 3\n",
      "1 7 4\n",
      "1 8 1\n",
      "1 8 2\n",
      "1 8 3\n",
      "1 8 4\n",
      "1 9 1\n",
      "1 9 2\n",
      "1 9 3\n",
      "1 9 4\n",
      "2 2 1\n",
      "2 2 2\n",
      "2 2 3\n",
      "2 2 4\n",
      "2 3 1\n",
      "2 3 2\n",
      "2 3 3\n",
      "2 3 4\n",
      "2 4 1\n",
      "2 4 2\n",
      "2 4 3\n",
      "2 4 4\n",
      "2 5 1\n",
      "2 5 2\n",
      "2 5 3\n",
      "2 5 4\n",
      "2 6 1\n",
      "2 6 2\n",
      "2 6 3\n",
      "2 6 4\n",
      "2 7 1\n",
      "2 7 2\n",
      "2 7 3\n",
      "2 7 4\n",
      "2 8 1\n",
      "2 8 2\n",
      "2 8 3\n",
      "2 8 4\n",
      "2 9 1\n",
      "2 9 2\n",
      "2 9 3\n",
      "2 9 4\n",
      "3 2 1\n",
      "3 2 2\n",
      "3 2 3\n",
      "3 2 4\n",
      "3 3 1\n",
      "3 3 2\n",
      "3 3 3\n",
      "3 3 4\n",
      "3 4 1\n",
      "3 4 2\n",
      "3 4 3\n",
      "3 4 4\n",
      "3 5 1\n",
      "3 5 2\n",
      "3 5 3\n",
      "3 5 4\n",
      "3 6 1\n",
      "3 6 2\n",
      "3 6 3\n",
      "3 6 4\n",
      "3 7 1\n",
      "3 7 2\n",
      "3 7 3\n",
      "3 7 4\n",
      "3 8 1\n",
      "3 8 2\n",
      "3 8 3\n",
      "3 8 4\n",
      "3 9 1\n",
      "3 9 2\n",
      "3 9 3\n",
      "3 9 4\n",
      "4 2 1\n",
      "4 2 2\n",
      "4 2 3\n",
      "4 2 4\n",
      "4 3 1\n",
      "4 3 2\n",
      "4 3 3\n",
      "4 3 4\n",
      "4 4 1\n",
      "4 4 2\n",
      "4 4 3\n",
      "4 4 4\n",
      "4 5 1\n",
      "4 5 2\n",
      "4 5 3\n",
      "4 5 4\n",
      "4 6 1\n",
      "4 6 2\n",
      "4 6 3\n",
      "4 6 4\n",
      "4 7 1\n",
      "4 7 2\n",
      "4 7 3\n",
      "4 7 4\n",
      "4 8 1\n",
      "4 8 2\n",
      "4 8 3\n",
      "4 8 4\n",
      "4 9 1\n",
      "4 9 2\n",
      "4 9 3\n",
      "4 9 4\n",
      "5 2 1\n",
      "5 2 2\n",
      "5 2 3\n",
      "5 2 4\n",
      "5 3 1\n",
      "5 3 2\n",
      "5 3 3\n",
      "5 3 4\n",
      "5 4 1\n",
      "5 4 2\n",
      "5 4 3\n",
      "5 4 4\n",
      "5 5 1\n",
      "5 5 2\n",
      "5 5 3\n",
      "5 5 4\n",
      "5 6 1\n",
      "5 6 2\n",
      "5 6 3\n",
      "5 6 4\n",
      "5 7 1\n",
      "5 7 2\n",
      "5 7 3\n",
      "5 7 4\n",
      "5 8 1\n",
      "5 8 2\n",
      "5 8 3\n",
      "5 8 4\n",
      "5 9 1\n",
      "5 9 2\n",
      "5 9 3\n",
      "5 9 4\n",
      "6 2 1\n",
      "6 2 2\n",
      "6 2 3\n",
      "6 2 4\n",
      "6 3 1\n",
      "6 3 2\n",
      "6 3 3\n",
      "6 3 4\n",
      "6 4 1\n",
      "6 4 2\n",
      "6 4 3\n",
      "6 4 4\n",
      "6 5 1\n",
      "6 5 2\n",
      "6 5 3\n",
      "6 5 4\n",
      "6 6 1\n",
      "6 6 2\n",
      "6 6 3\n",
      "6 6 4\n",
      "6 7 1\n",
      "6 7 2\n",
      "6 7 3\n",
      "6 7 4\n",
      "6 8 1\n",
      "6 8 2\n",
      "6 8 3\n",
      "6 8 4\n",
      "6 9 1\n",
      "6 9 2\n",
      "6 9 3\n",
      "6 9 4\n",
      "7 2 1\n",
      "7 2 2\n",
      "7 2 3\n",
      "7 2 4\n",
      "7 3 1\n",
      "7 3 2\n",
      "7 3 3\n",
      "7 3 4\n",
      "7 4 1\n",
      "7 4 2\n",
      "7 4 3\n",
      "7 4 4\n",
      "7 5 1\n",
      "7 5 2\n",
      "7 5 3\n",
      "7 5 4\n",
      "7 6 1\n",
      "7 6 2\n",
      "7 6 3\n",
      "7 6 4\n",
      "7 7 1\n",
      "7 7 2\n",
      "7 7 3\n",
      "7 7 4\n",
      "7 8 1\n",
      "7 8 2\n",
      "7 8 3\n",
      "7 8 4\n",
      "7 9 1\n",
      "7 9 2\n",
      "7 9 3\n",
      "7 9 4\n",
      "8 2 1\n",
      "8 2 2\n",
      "8 2 3\n",
      "8 2 4\n",
      "8 3 1\n",
      "8 3 2\n",
      "8 3 3\n",
      "8 3 4\n",
      "8 4 1\n",
      "8 4 2\n",
      "8 4 3\n",
      "8 4 4\n",
      "8 5 1\n",
      "8 5 2\n",
      "8 5 3\n",
      "8 5 4\n",
      "8 6 1\n",
      "8 6 2\n",
      "8 6 3\n",
      "8 6 4\n",
      "8 7 1\n",
      "8 7 2\n",
      "8 7 3\n",
      "8 7 4\n",
      "8 8 1\n",
      "8 8 2\n",
      "8 8 3\n",
      "8 8 4\n",
      "8 9 1\n",
      "8 9 2\n",
      "8 9 3\n",
      "8 9 4\n",
      "9 2 1\n",
      "9 2 2\n",
      "9 2 3\n",
      "9 2 4\n",
      "9 3 1\n",
      "9 3 2\n",
      "9 3 3\n",
      "9 3 4\n",
      "9 4 1\n",
      "9 4 2\n",
      "9 4 3\n",
      "9 4 4\n",
      "9 5 1\n",
      "9 5 2\n",
      "9 5 3\n",
      "9 5 4\n",
      "9 6 1\n",
      "9 6 2\n",
      "9 6 3\n",
      "9 6 4\n",
      "9 7 1\n",
      "9 7 2\n",
      "9 7 3\n",
      "9 7 4\n",
      "9 8 1\n",
      "9 8 2\n",
      "9 8 3\n",
      "9 8 4\n",
      "9 9 1\n",
      "9 9 2\n",
      "9 9 3\n",
      "9 9 4\n"
     ]
    }
   ],
   "source": [
    "# save each configuration\n",
    "    # 6x? array\n",
    "    # 6th index = forest\n",
    "\n",
    "# ranges of configurations\n",
    "    # num of trees\n",
    "    # pruning layers (0-10)\n",
    "    # min split sample size (2-7)\n",
    "    # min leaf sample size (1-6)\n",
    "    # proportions of features (0.3 - 0.9, steps of 0.05)\n",
    "\n",
    "# create array for configs\n",
    "configurations = []\n",
    "tempTree = None\n",
    "\n",
    "# pruning range\n",
    "for i in range(0,10):\n",
    "    for j in range(2,10):\n",
    "        for k in range(1,5):\n",
    "\n",
    "            print (i, j, k)\n",
    "            overall_acc = 0\n",
    "            \n",
    "            for n in range(0,5):\n",
    "                tempTree = DecisionTreeClassifier(i,j,k)\n",
    "                tempTree.fit(x_full, y_full)\n",
    "\n",
    "                temp_prediction = tempTree.predict(x_val)\n",
    "                temp_acc = accuracy(temp_prediction, y_val)\n",
    "                overall_acc += temp_acc\n",
    "\n",
    "            overall_acc /= 5\n",
    "            \n",
    "            # save to list\n",
    "            config = (i, j, k, tempTree, overall_acc)\n",
    "            configurations.append(config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d3db0013-b06c-45f8-b604-8545afba8de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((6, 2, 2)) accuracy is: 93.0\n",
      "((6, 3, 2)) accuracy is: 93.0\n",
      "((6, 4, 2)) accuracy is: 93.0\n",
      "((6, 5, 2)) accuracy is: 93.0\n",
      "((7, 2, 2)) accuracy is: 93.0\n",
      "((7, 3, 2)) accuracy is: 93.0\n",
      "((7, 4, 2)) accuracy is: 93.0\n",
      "((7, 5, 2)) accuracy is: 93.0\n",
      "((8, 2, 1)) accuracy is: 93.0\n",
      "((8, 3, 1)) accuracy is: 93.0\n",
      "((8, 4, 1)) accuracy is: 93.0\n",
      "((8, 5, 1)) accuracy is: 93.0\n"
     ]
    }
   ],
   "source": [
    "for (i,j,k, tree, acc) in configurations:\n",
    "    \n",
    "    # accuracies = np.zeros(10)\n",
    "    # f1_scores = np.zeros(10)\n",
    "    \n",
    "    # temp_prediction = tree.predict(x_test)\n",
    "    \n",
    "    # ave_accuracy = accuracy(temp_prediction, y_test)\n",
    "\n",
    "    # ave_f1 = fb_score(y_test, temp_prediction, 1)[1]\n",
    "\n",
    "    if acc > 92:\n",
    "        print(f\"({i, j, k}) accuracy is: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0226e470-31c1-42eb-8030-5f512a16fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(x_train, y_train, x_test, x_val, y_val):\n",
    "    \"\"\" Interface to train and test the new/improved decision tree.\n",
    "    \n",
    "    This function is an interface for training and testing the new/improved\n",
    "    decision tree classifier. \n",
    "\n",
    "    x_train and y_train should be used to train your classifier, while \n",
    "    x_test should be used to test your classifier. \n",
    "    x_val and y_val may optionally be used as the validation dataset. \n",
    "    You can just ignore x_val and y_val if you do not need a validation dataset.\n",
    "\n",
    "    Args:\n",
    "    x_train (numpy.ndarray): Training instances, numpy array of shape (N, K) \n",
    "                       N is the number of instances\n",
    "                       K is the number of attributes\n",
    "    y_train (numpy.ndarray): Class labels, numpy array of shape (N, )\n",
    "                       Each element in y is a str \n",
    "    x_test (numpy.ndarray): Test instances, numpy array of shape (M, K) \n",
    "                            M is the number of test instances\n",
    "                            K is the number of attributes\n",
    "    x_val (numpy.ndarray): Validation instances, numpy array of shape (L, K) \n",
    "                       L is the number of validation instances\n",
    "                       K is the number of attributes\n",
    "    y_val (numpy.ndarray): Class labels of validation set, numpy array of shape (L, )\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: A numpy array of shape (M, ) containing the predicted class label for each instance in x_test\n",
    "    \"\"\"\n",
    "\n",
    "    #######################################################################\n",
    "    #                 ** TASK 4.1: COMPLETE THIS FUNCTION **\n",
    "    #######################################################################\n",
    "       \n",
    "\n",
    "    # TODO: Train new classifier\n",
    "\n",
    "    # set up an empty (M, ) numpy array to store the predicted labels \n",
    "    # feel free to change this if needed\n",
    "    predictions = np.zeros((x_test.shape[0],), dtype=object)\n",
    "        \n",
    "    # TODO: Make predictions on x_test using new classifier        \n",
    "\n",
    "    prev_accuracy = 0\n",
    "    prev_std = 100\n",
    "    accuracies = np.zeros(3, dtype=np.float64)\n",
    "    best_model = None\n",
    "\n",
    "    # pruning range\n",
    "    for i in range(7,9):\n",
    "        \n",
    "        # min split sample size\n",
    "        for j in range(4,6):\n",
    "\n",
    "            # min leaf sample size\n",
    "            for k in range(1,3):\n",
    "\n",
    "                # average accuracy\n",
    "                for n in range(0,1):\n",
    "                    \n",
    "                    tempTree = NewDecisionTreeClassifier(i,j,k)\n",
    "                    tempTree.fit(x_train, y_train)\n",
    "\n",
    "                    temp_prediction = tempTree.predict(x_val)\n",
    "                    accuracies[n] = accuracy(temp_prediction, y_val)\n",
    "                \n",
    "                overall_acc = np.mean(accuracies)\n",
    "                overall_std = np.std(accuracies)\n",
    "                \n",
    "                if overall_acc > prev_accuracy:\n",
    "                    prev_accuracy = overall_acc\n",
    "                    best_model = tempTree\n",
    "                    prev_std = overall_std\n",
    "\n",
    "                if np.isclose(overall_acc, prev_accuracy, atol=0.5) and overall_std < prev_std:\n",
    "                    prev_accuracy = overall_acc\n",
    "                    prev_std = overall_std\n",
    "                    best_model = tempTree\n",
    "\n",
    "    print(f\" val std: {prev_std}\")\n",
    "    print(f\" val acc: {prev_accuracy}\")\n",
    "    best_prediction = best_model.predict(x_test)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "    print(confusion_matrix(y_test, best_prediction))\n",
    "    print(recall(y_test, best_prediction))\n",
    "    print(precision(y_test, best_prediction))\n",
    "    print(f1_score(y_test, best_prediction))\n",
    "    print(accuracy(y_test, best_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "79d275c4-e9a3-4f6c-bd42-f4f8cb111160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val std: 0.0\n",
      " val acc: 93.0\n",
      "[[33  0  0  1  0  0]\n",
      " [ 0 34  2  1  0  0]\n",
      " [ 0  1 23  0  1  1]\n",
      " [ 1  1  0 21  1  3]\n",
      " [ 0  1  0  1 32  0]\n",
      " [ 0  1  2  0  6 33]]\n",
      "(array([0.97058824, 0.91891892, 0.88461538, 0.77777778, 0.94117647,\n",
      "       0.78571429]), 0.8797985121514533)\n",
      "(array([0.97058824, 0.89473684, 0.85185185, 0.875     , 0.8       ,\n",
      "       0.89189189]), 0.8806781368571874)\n",
      "(array([0.97058824, 0.90666667, 0.86792453, 0.82352941, 0.86486486,\n",
      "       0.83544304]), 0.8781694574778208)\n",
      "88.0\n"
     ]
    }
   ],
   "source": [
    "train_and_predict(x_full, y_full, x_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "389a75e7-0cf9-48eb-8719-2e1b73474ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val std: 0.0\n",
      " val acc: 87.0\n",
      "[[31  0  0  0  1  2]\n",
      " [ 1 31  1  3  1  0]\n",
      " [ 0  0 25  1  0  0]\n",
      " [ 0  1  2 16  1  7]\n",
      " [ 0  2  0  1 28  3]\n",
      " [ 1  0  3  3  7 28]]\n",
      "(array([0.91176471, 0.83783784, 0.96153846, 0.59259259, 0.82352941,\n",
      "       0.66666667]), 0.7989882793804363)\n",
      "(array([0.93939394, 0.91176471, 0.80645161, 0.66666667, 0.73684211,\n",
      "       0.7       ]), 0.793519838351557)\n",
      "(array([0.92537313, 0.87323944, 0.87719298, 0.62745098, 0.77777778,\n",
      "       0.68292683]), 0.7939935234737407)\n",
      "79.5\n"
     ]
    }
   ],
   "source": [
    "train_and_predict(x_noisy, y_noisy, x_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a7b714ad-5ca4-4ad4-96c6-2d018da09b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tree = DecisionTreeClassifier()\n",
    "base_tree.fit(x_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b28b875e-9b4e-418d-aac9-dff0831214a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tree_noisy = DecisionTreeClassifier()\n",
    "base_tree_noisy.fit(x_noisy, y_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1c77614b-f614-4207-911d-311515621c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val std: 0.0\n",
      " val acc: 93.0\n"
     ]
    }
   ],
   "source": [
    "new_tree = train_and_predict(x_full, y_full, x_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "074fba9c-dbe6-4ed6-89e0-05ec70b682eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val std: 0.0\n",
      " val acc: 87.0\n"
     ]
    }
   ],
   "source": [
    "new_tree_noisy = train_and_predict(x_noisy, y_noisy, x_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "03f37a06-340f-43c0-ba0f-71697befcc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_t_test(model1, model2, x_test):\n",
    "\n",
    "    paired_accuracies = []\n",
    "\n",
    "    # split the test set into 10\n",
    "    \n",
    "    # generate a random permutation of indices from 0 to n_instances\n",
    "    seed = 12312\n",
    "    rg = default_rng(seed)\n",
    "\n",
    "    shuffled_indices = rg.permutation(len(x_test))\n",
    "\n",
    "    # split shuffled indices into almost equal sized splits\n",
    "    split_indices = np.array_split(shuffled_indices, 10)\n",
    "\n",
    "    # per split\n",
    "    for k in split_indices:\n",
    "        \n",
    "        # predict for both models\n",
    "        model1_predictions = model1.predict(x_test[k]) \n",
    "        model2_predictions = model2.predict(x_test[k]) \n",
    "        \n",
    "        # obtain accuracy\n",
    "        model1_acc = accuracy(model1_predictions, y_test[k])\n",
    "        model2_acc = accuracy(model2_predictions, y_test[k])\n",
    "        \n",
    "        # pair in a tuple, and add to list\n",
    "        paired_accuracies.append((model1_acc, model2_acc))\n",
    "\n",
    "    # t test formulas here\n",
    "    diff = np.zeros(10, dtype=np.float64)\n",
    "    for i, (a, b) in enumerate(paired_accuracies):\n",
    "        diff[i] = b-a\n",
    "    mean = np.mean(diff)\n",
    "    sd = np.std(diff)\n",
    "    t = (np.sqrt(10) * mean) / sd\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b4dd575a-05e9-4766-9018-c97025eb5d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3801311186847085\n",
      "0.8624393618641034\n"
     ]
    }
   ],
   "source": [
    "paired_t_test(base_tree, new_tree, x_test)\n",
    "paired_t_test(base_tree_noisy, new_tree_noisy, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913596b-9c39-47be-a783-218954551325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
