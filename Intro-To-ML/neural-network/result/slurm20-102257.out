testing
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5364160865689594
Iteration 10, Training loss = 0.2527124651010908
Iteration 20, Training loss = 0.24142727828595886
Iteration 30, Training loss = 0.23340182333787762
Iteration 40, Training loss = 0.2262246102527614
Iteration 50, Training loss = 0.2202645466272825
Iteration 60, Training loss = 0.2154962877961683
Iteration 70, Training loss = 0.21088358625480974
Iteration 80, Training loss = 0.21104298145047978
Iteration 90, Training loss = 0.20793760382992016
Model training time: 51.44895029067993
Device: cuda
Iteration 0, Training loss = 0.5587788819905921
Iteration 10, Training loss = 0.2543029324861906
Iteration 20, Training loss = 0.24044126512135489
Iteration 30, Training loss = 0.23284617836697627
Iteration 40, Training loss = 0.2282810674272207
Iteration 50, Training loss = 0.22397464456050217
Iteration 60, Training loss = 0.2205562085436418
Iteration 70, Training loss = 0.21766618207169214
Iteration 80, Training loss = 0.2156448480887361
Iteration 90, Training loss = 0.21249775056328093
Model training time: 51.55121040344238
Device: cuda
Iteration 0, Training loss = 0.5236094760331923
Iteration 10, Training loss = 0.2554949859188775
Iteration 20, Training loss = 0.24389362746520424
Iteration 30, Training loss = 0.2363381250862925
Iteration 40, Training loss = 0.23130363565993656
Iteration 50, Training loss = 0.22747648455040217
Iteration 60, Training loss = 0.22214010686187421
Iteration 70, Training loss = 0.21853541166997417
Iteration 80, Training loss = 0.21633237359490579
Iteration 90, Training loss = 0.21310761597088693
Model training time: 50.457417726516724
Device: cuda
Iteration 0, Training loss = 0.5755224914332856
Iteration 10, Training loss = 0.2514391346286631
Iteration 20, Training loss = 0.2410673124233112
Iteration 30, Training loss = 0.23163730663768314
Iteration 40, Training loss = 0.22623442939638225
Iteration 50, Training loss = 0.222082445267182
Iteration 60, Training loss = 0.21828243363711794
Iteration 70, Training loss = 0.2141438352697265
Iteration 80, Training loss = 0.2116896383301975
Iteration 90, Training loss = 0.2094462482443589
Model training time: 51.013771057128906
Device: cuda
Iteration 0, Training loss = 0.6005015733672112
Iteration 10, Training loss = 0.2556411662560687
Iteration 20, Training loss = 0.24244143536030235
Iteration 30, Training loss = 0.2318950216973665
Iteration 40, Training loss = 0.2274664170751416
Iteration 50, Training loss = 0.2239994375671082
Iteration 60, Training loss = 0.22060920885306293
Iteration 70, Training loss = 0.2197944511621108
Iteration 80, Training loss = 0.2184883449001116
Iteration 90, Training loss = 0.21647346436472262
Model training time: 50.992509603500366
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9765815004598142
Iteration 10, Training loss = 0.38118157387862195
Iteration 20, Training loss = 0.33764500944868414
Iteration 30, Training loss = 0.3197019034162272
Iteration 40, Training loss = 0.31025395349353624
Iteration 50, Training loss = 0.30379435221594703
Iteration 60, Training loss = 0.2981033785579857
Iteration 70, Training loss = 0.2925321332587168
Iteration 80, Training loss = 0.2872672734407767
Iteration 90, Training loss = 0.2827602424918162
Model training time: 43.00045084953308
Device: cuda
Iteration 0, Training loss = 0.9708052415391724
Iteration 10, Training loss = 0.34157148898369466
Iteration 20, Training loss = 0.3194556472593012
Iteration 30, Training loss = 0.30827336246035003
Iteration 40, Training loss = 0.29978014747037146
Iteration 50, Training loss = 0.2912955252367994
Iteration 60, Training loss = 0.2849945737360176
Iteration 70, Training loss = 0.279703108652452
Iteration 80, Training loss = 0.2751303803599631
Iteration 90, Training loss = 0.27112839405242234
Model training time: 43.49880766868591
Device: cuda
Iteration 0, Training loss = 0.986026918267511
Iteration 10, Training loss = 0.36212492299426263
Iteration 20, Training loss = 0.3261934899461183
Iteration 30, Training loss = 0.3121946892243321
Iteration 40, Training loss = 0.3043505902837321
Iteration 50, Training loss = 0.29812858715545176
Iteration 60, Training loss = 0.2920443690393219
Iteration 70, Training loss = 0.2864421096610099
Iteration 80, Training loss = 0.28133417465712773
Iteration 90, Training loss = 0.2773138925830042
Model training time: 43.41233205795288
Device: cuda
Iteration 0, Training loss = 0.9791628986813543
Iteration 10, Training loss = 0.38297604764056264
Iteration 20, Training loss = 0.32079036858555193
Iteration 30, Training loss = 0.3006189055831034
Iteration 40, Training loss = 0.2892589263672113
Iteration 50, Training loss = 0.2814789348198484
Iteration 60, Training loss = 0.27506910863424905
Iteration 70, Training loss = 0.27031586560946114
Iteration 80, Training loss = 0.2661121764714146
Iteration 90, Training loss = 0.26274771737633834
Model training time: 43.575831174850464
Device: cuda
Iteration 0, Training loss = 1.0717559544861173
Iteration 10, Training loss = 0.5091314011278222
Iteration 20, Training loss = 0.35683288466699475
Iteration 30, Training loss = 0.32035843017747845
Iteration 40, Training loss = 0.3044904984407506
Iteration 50, Training loss = 0.2942892397611828
Iteration 60, Training loss = 0.2859535269781983
Iteration 70, Training loss = 0.27902826672244013
Iteration 80, Training loss = 0.27374133754512586
Iteration 90, Training loss = 0.2689183270671466
Model training time: 43.80391836166382
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5806295037738636
Iteration 10, Training loss = 0.2536309417562681
Iteration 20, Training loss = 0.2445922889555049
Iteration 30, Training loss = 0.24051391598531754
Iteration 40, Training loss = 0.2354524107627372
Iteration 50, Training loss = 0.23309823170701183
Iteration 60, Training loss = 0.23115385730568203
Iteration 70, Training loss = 0.22946553215492724
Iteration 80, Training loss = 0.2284275132792914
Iteration 90, Training loss = 0.22685463916844087
Model training time: 50.55763220787048
Device: cuda
Iteration 0, Training loss = 0.5633144331396925
Iteration 10, Training loss = 0.26835981556203115
Iteration 20, Training loss = 0.2590203364233128
Iteration 30, Training loss = 0.25315608168088494
Iteration 40, Training loss = 0.24969803577979022
Iteration 50, Training loss = 0.24669016637900262
Iteration 60, Training loss = 0.24217930937650417
Iteration 70, Training loss = 0.23993501349489857
Iteration 80, Training loss = 0.23783067622860177
Iteration 90, Training loss = 0.2361250242565648
Model training time: 50.710986375808716
Device: cuda
Iteration 0, Training loss = 0.5618234499567357
Iteration 10, Training loss = 0.26496447509123106
Iteration 20, Training loss = 0.2537906683855138
Iteration 30, Training loss = 0.24531941682316777
Iteration 40, Training loss = 0.24086720665993472
Iteration 50, Training loss = 0.23800226252246423
Iteration 60, Training loss = 0.23644923507157017
Iteration 70, Training loss = 0.23428793702518103
Iteration 80, Training loss = 0.2323329033956978
Iteration 90, Training loss = 0.23115590889430795
Model training time: 51.194928884506226
Device: cuda
Iteration 0, Training loss = 0.7748680243913544
Iteration 10, Training loss = 0.26376325509017085
Iteration 20, Training loss = 0.2529351251249452
Iteration 30, Training loss = 0.2440702976888952
Iteration 40, Training loss = 0.2382148691874728
Iteration 50, Training loss = 0.23335097968289698
Iteration 60, Training loss = 0.22860738575530687
Iteration 70, Training loss = 0.22492064535617828
Iteration 80, Training loss = 0.22112216286307096
Iteration 90, Training loss = 0.21830108578679924
Model training time: 50.7958722114563
Device: cuda
Iteration 0, Training loss = 0.9516409722136527
Iteration 10, Training loss = 0.26457513933753274
Iteration 20, Training loss = 0.253985538284946
Iteration 30, Training loss = 0.25015346014975925
Iteration 40, Training loss = 0.24605163446566672
Iteration 50, Training loss = 0.24262056392380746
Iteration 60, Training loss = 0.23930409117703288
Iteration 70, Training loss = 0.236922141994388
Iteration 80, Training loss = 0.2361613349317061
Iteration 90, Training loss = 0.23402657739583574
Model training time: 51.208808183670044
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0637958907041942
Iteration 10, Training loss = 0.4619079355971288
Iteration 20, Training loss = 0.35538710033200843
Iteration 30, Training loss = 0.33829577342938566
Iteration 40, Training loss = 0.33115566944022445
Iteration 50, Training loss = 0.3255437448729037
Iteration 60, Training loss = 0.32039288005658556
Iteration 70, Training loss = 0.31558579368827994
Iteration 80, Training loss = 0.3112205434004273
Iteration 90, Training loss = 0.30677624198339754
Model training time: 42.34089136123657
Device: cuda
Iteration 0, Training loss = 1.043103380223452
Iteration 10, Training loss = 0.4691247652864341
Iteration 20, Training loss = 0.3449391103902106
Iteration 30, Training loss = 0.3188610050926486
Iteration 40, Training loss = 0.3063104659787968
Iteration 50, Training loss = 0.29733188778667125
Iteration 60, Training loss = 0.29007914108528643
Iteration 70, Training loss = 0.2838830886539478
Iteration 80, Training loss = 0.27861118071304397
Iteration 90, Training loss = 0.27425868491716593
Model training time: 43.07382369041443
Device: cuda
Iteration 0, Training loss = 1.040499444590931
Iteration 10, Training loss = 0.3549720001458833
Iteration 20, Training loss = 0.3259619710015327
Iteration 30, Training loss = 0.3128034655392603
Iteration 40, Training loss = 0.3028092078918695
Iteration 50, Training loss = 0.29421402945140374
Iteration 60, Training loss = 0.28672722506970527
Iteration 70, Training loss = 0.28072989688730704
Iteration 80, Training loss = 0.2765971074020603
Iteration 90, Training loss = 0.27297942823849924
Model training time: 43.44190812110901
Device: cuda
Iteration 0, Training loss = 1.0473375866690213
Iteration 10, Training loss = 0.5399869016166461
Iteration 20, Training loss = 0.3474042809074497
Iteration 30, Training loss = 0.3220579193918526
Iteration 40, Training loss = 0.30915248139646384
Iteration 50, Training loss = 0.30098990154324085
Iteration 60, Training loss = 0.29493907707366757
Iteration 70, Training loss = 0.2900989077462122
Iteration 80, Training loss = 0.28584120263848406
Iteration 90, Training loss = 0.2822325527667999
Model training time: 43.219624280929565
Device: cuda
Iteration 0, Training loss = 1.0772516056354051
Iteration 10, Training loss = 0.392838683313088
Iteration 20, Training loss = 0.3279467080392791
Iteration 30, Training loss = 0.30951794772690777
Iteration 40, Training loss = 0.29913847027549445
Iteration 50, Training loss = 0.2917390095011374
Iteration 60, Training loss = 0.28583990450560615
Iteration 70, Training loss = 0.2808239416875504
Iteration 80, Training loss = 0.27633438865512105
Iteration 90, Training loss = 0.27235977064827166
Model training time: 43.37281560897827
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5371615267730798
Iteration 10, Training loss = 0.2600430393045809
Iteration 20, Training loss = 0.24783700232824749
Iteration 30, Training loss = 0.2416901920378641
Iteration 40, Training loss = 0.23761233562923806
Iteration 50, Training loss = 0.2326023935117098
Iteration 60, Training loss = 0.2304878149623588
Iteration 70, Training loss = 0.227349021983926
Iteration 80, Training loss = 0.22606960296197895
Iteration 90, Training loss = 0.2241398467170413
Model training time: 50.439963579177856
Device: cuda
Iteration 0, Training loss = 0.5995996853305121
Iteration 10, Training loss = 0.2594748795140742
Iteration 20, Training loss = 0.24901157148525155
Iteration 30, Training loss = 0.24191790204901095
Iteration 40, Training loss = 0.23741803386814658
Iteration 50, Training loss = 0.23365210184317814
Iteration 60, Training loss = 0.23079913636313223
Iteration 70, Training loss = 0.2255909690232023
Iteration 80, Training loss = 0.22068506417926806
Iteration 90, Training loss = 0.2189632423456587
Model training time: 51.062854290008545
Device: cuda
Iteration 0, Training loss = 0.5156646754060473
Iteration 10, Training loss = 0.25618394644655845
Iteration 20, Training loss = 0.24513345105307444
Iteration 30, Training loss = 0.23888500402828106
Iteration 40, Training loss = 0.23251629884176625
Iteration 50, Training loss = 0.22829811424738558
Iteration 60, Training loss = 0.2245302174249371
Iteration 70, Training loss = 0.22264491337335715
Iteration 80, Training loss = 0.2181808999931264
Iteration 90, Training loss = 0.21864642781040858
Model training time: 50.25110483169556
Device: cuda
Iteration 0, Training loss = 0.6017640348857598
Iteration 10, Training loss = 0.2606476230533302
Iteration 20, Training loss = 0.24712378701271792
Iteration 30, Training loss = 0.23828212712095379
Iteration 40, Training loss = 0.23409226700916128
Iteration 50, Training loss = 0.23106648619613693
Iteration 60, Training loss = 0.22834891384832795
Iteration 70, Training loss = 0.22525245625949655
Iteration 80, Training loss = 0.2238778273956106
Iteration 90, Training loss = 0.22191452332400236
Model training time: 50.405702352523804
Device: cuda
Iteration 0, Training loss = 0.4865977589861821
Iteration 10, Training loss = 0.2549925867747741
Iteration 20, Training loss = 0.24475816170037804
Iteration 30, Training loss = 0.23945846604341167
Iteration 40, Training loss = 0.23249005546148407
Iteration 50, Training loss = 0.22639394397272325
Iteration 60, Training loss = 0.21863805177028472
Iteration 70, Training loss = 0.21449813596705836
Iteration 80, Training loss = 0.21233779960481075
Iteration 90, Training loss = 0.2093450225186117
Model training time: 52.14856433868408
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0078913601201036
Iteration 10, Training loss = 0.3830255976095615
Iteration 20, Training loss = 0.32980304137413496
Iteration 30, Training loss = 0.31668792805717877
Iteration 40, Training loss = 0.308611396600923
Iteration 50, Training loss = 0.3009319760572824
Iteration 60, Training loss = 0.2945801755886967
Iteration 70, Training loss = 0.28882322651638537
Iteration 80, Training loss = 0.28393698320442196
Iteration 90, Training loss = 0.2794985179642788
Model training time: 43.672030210494995
Device: cuda
Iteration 0, Training loss = 0.9404938721772256
Iteration 10, Training loss = 0.3640937448522081
Iteration 20, Training loss = 0.32772774996177334
Iteration 30, Training loss = 0.3139061275102902
Iteration 40, Training loss = 0.3034143648843211
Iteration 50, Training loss = 0.2948850765102712
Iteration 60, Training loss = 0.2872533200908515
Iteration 70, Training loss = 0.28060898036847104
Iteration 80, Training loss = 0.2742445372027
Iteration 90, Training loss = 0.26955214328612886
Model training time: 42.73088049888611
Device: cuda
Iteration 0, Training loss = 0.9808993035020898
Iteration 10, Training loss = 0.38578652700558125
Iteration 20, Training loss = 0.338752022107779
Iteration 30, Training loss = 0.3240883527706957
Iteration 40, Training loss = 0.31355534849314076
Iteration 50, Training loss = 0.3050816439579243
Iteration 60, Training loss = 0.29754691874071704
Iteration 70, Training loss = 0.291354103000343
Iteration 80, Training loss = 0.28605009263638437
Iteration 90, Training loss = 0.2810597722993636
Model training time: 43.389098167419434
Device: cuda
Iteration 0, Training loss = 1.013437413851805
Iteration 10, Training loss = 0.39982019559667414
Iteration 20, Training loss = 0.33930416331236357
Iteration 30, Training loss = 0.32305509914569647
Iteration 40, Training loss = 0.31186801029175304
Iteration 50, Training loss = 0.302694750773849
Iteration 60, Training loss = 0.295001801653578
Iteration 70, Training loss = 0.28888988015005146
Iteration 80, Training loss = 0.28379495716123837
Iteration 90, Training loss = 0.27985735587577265
Model training time: 43.122413873672485
Device: cuda
Iteration 0, Training loss = 0.9994772003724558
Iteration 10, Training loss = 0.39703105574081365
Iteration 20, Training loss = 0.33916616457859483
Iteration 30, Training loss = 0.3222225580644088
Iteration 40, Training loss = 0.3124117430741504
Iteration 50, Training loss = 0.30424218982025153
Iteration 60, Training loss = 0.29728744195246526
Iteration 70, Training loss = 0.29121687539264596
Iteration 80, Training loss = 0.28571813349718045
Iteration 90, Training loss = 0.28095711993175326
Model training time: 42.716474771499634
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.602076921440643
Iteration 10, Training loss = 0.2534539097013543
Iteration 20, Training loss = 0.23859140984984634
Iteration 30, Training loss = 0.23130941353106904
Iteration 40, Training loss = 0.2265767782619877
Iteration 50, Training loss = 0.22192673603429056
Iteration 60, Training loss = 0.2174672210187658
Iteration 70, Training loss = 0.2148174798120598
Iteration 80, Training loss = 0.21248012103735678
Iteration 90, Training loss = 0.20964806828169788
Iteration 100, Training loss = 0.20644929634348533
Iteration 110, Training loss = 0.20641393486870402
Iteration 120, Training loss = 0.2051415398771047
Iteration 130, Training loss = 0.20430692021011151
Iteration 140, Training loss = 0.2044824257348698
Iteration 150, Training loss = 0.2029953551259947
Iteration 160, Training loss = 0.2030256901288148
Iteration 170, Training loss = 0.20141978815342265
Iteration 180, Training loss = 0.2005370253872929
Iteration 190, Training loss = 0.20040914969243667
Model training time: 102.31202507019043
Device: cuda
Iteration 0, Training loss = 0.6021631859585679
Iteration 10, Training loss = 0.2546282801732024
Iteration 20, Training loss = 0.2417326493968975
Iteration 30, Training loss = 0.23142548625246953
Iteration 40, Training loss = 0.22498828331437007
Iteration 50, Training loss = 0.22148431178191383
Iteration 60, Training loss = 0.21859667835346722
Iteration 70, Training loss = 0.21460669641056015
Iteration 80, Training loss = 0.21231314646346228
Iteration 90, Training loss = 0.21003444319631517
Iteration 100, Training loss = 0.2069965707674875
Iteration 110, Training loss = 0.20657315988264996
Iteration 120, Training loss = 0.20328947710500214
Iteration 130, Training loss = 0.20170272635995043
Iteration 140, Training loss = 0.19911445440954503
Iteration 150, Training loss = 0.19880421054673542
Iteration 160, Training loss = 0.19885612582251178
Iteration 170, Training loss = 0.19765493347699647
Iteration 180, Training loss = 0.1968037838580822
Iteration 190, Training loss = 0.19644436742614313
Model training time: 103.78314971923828
Device: cuda
Iteration 0, Training loss = 0.5140370291276648
Iteration 10, Training loss = 0.2504274950116945
Iteration 20, Training loss = 0.2344572143967446
Iteration 30, Training loss = 0.22541874845582116
Iteration 40, Training loss = 0.21933768666180226
Iteration 50, Training loss = 0.2150560064336001
Iteration 60, Training loss = 0.2102731775411249
Iteration 70, Training loss = 0.20764247788066725
Iteration 80, Training loss = 0.20491183176636696
Iteration 90, Training loss = 0.20297366211114148
Iteration 100, Training loss = 0.20193100207401823
Iteration 110, Training loss = 0.19927490203129467
Iteration 120, Training loss = 0.19919201871492961
Iteration 130, Training loss = 0.19735934995746207
Iteration 140, Training loss = 0.19673917912506017
Iteration 150, Training loss = 0.19527094245642496
Iteration 160, Training loss = 0.19347565135857672
Iteration 170, Training loss = 0.1937019906703554
Iteration 180, Training loss = 0.194003705025513
Iteration 190, Training loss = 0.19420183576264624
Model training time: 101.94900822639465
Device: cuda
Iteration 0, Training loss = 0.5323483434305064
Iteration 10, Training loss = 0.24870082928142884
Iteration 20, Training loss = 0.2372111808192932
Iteration 30, Training loss = 0.22903660232016307
Iteration 40, Training loss = 0.22530074464495475
Iteration 50, Training loss = 0.22025578956121972
Iteration 60, Training loss = 0.21539647902472545
Iteration 70, Training loss = 0.2130705929264318
Iteration 80, Training loss = 0.2102941453240397
Iteration 90, Training loss = 0.20839467971218123
Iteration 100, Training loss = 0.20676865987028684
Iteration 110, Training loss = 0.20688992996697853
Iteration 120, Training loss = 0.20434959875036382
Iteration 130, Training loss = 0.20402560890911567
Iteration 140, Training loss = 0.20380884141054628
Iteration 150, Training loss = 0.2040526471743428
Iteration 160, Training loss = 0.20243436464313733
Iteration 170, Training loss = 0.20184767917339796
Iteration 180, Training loss = 0.2018423687224671
Iteration 190, Training loss = 0.20110302763939192
Model training time: 111.7851197719574
Device: cuda
Iteration 0, Training loss = 0.5144311051463966
Iteration 10, Training loss = 0.25394085424864266
Iteration 20, Training loss = 0.24179677708529965
Iteration 30, Training loss = 0.2372209993134688
Iteration 40, Training loss = 0.23244711946127777
Iteration 50, Training loss = 0.22900685103335044
Iteration 60, Training loss = 0.22672898170806594
Iteration 70, Training loss = 0.2230119605077381
Iteration 80, Training loss = 0.21783750020779363
Iteration 90, Training loss = 0.21527393352574067
Iteration 100, Training loss = 0.21206263538459022
Iteration 110, Training loss = 0.20973975703877917
Iteration 120, Training loss = 0.20710717893902383
Iteration 130, Training loss = 0.20491710150970674
Iteration 140, Training loss = 0.20358288268272293
Iteration 150, Training loss = 0.20277918768780573
Iteration 160, Training loss = 0.20207602979015496
Iteration 170, Training loss = 0.20061335394262692
Iteration 180, Training loss = 0.19857967886667852
Iteration 190, Training loss = 0.2001921897506021
Model training time: 105.59512305259705
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.978223095072961
Iteration 10, Training loss = 0.3738796340784207
Iteration 20, Training loss = 0.3284362948907778
Iteration 30, Training loss = 0.3143441066614941
Iteration 40, Training loss = 0.30482916214636396
Iteration 50, Training loss = 0.2968130729569361
Iteration 60, Training loss = 0.2903956798137533
Iteration 70, Training loss = 0.2847714860163647
Iteration 80, Training loss = 0.2800260934681061
Iteration 90, Training loss = 0.2754745003169732
Iteration 100, Training loss = 0.2717199150752502
Iteration 110, Training loss = 0.2682271130004172
Iteration 120, Training loss = 0.2653312252847969
Iteration 130, Training loss = 0.2631134642671442
Iteration 140, Training loss = 0.2614535966991801
Iteration 150, Training loss = 0.25974696432634936
Iteration 160, Training loss = 0.2587692510273497
Iteration 170, Training loss = 0.25762693883431736
Iteration 180, Training loss = 0.2568029251139043
Iteration 190, Training loss = 0.25582699116869645
Model training time: 86.93712711334229
Device: cuda
Iteration 0, Training loss = 1.0407182523038139
Iteration 10, Training loss = 0.35392961569906145
Iteration 20, Training loss = 0.3285178785498725
Iteration 30, Training loss = 0.31403511710302595
Iteration 40, Training loss = 0.30252153538257676
Iteration 50, Training loss = 0.29335414008496746
Iteration 60, Training loss = 0.28577658959797453
Iteration 70, Training loss = 0.2800338175734077
Iteration 80, Training loss = 0.2756344259470773
Iteration 90, Training loss = 0.27254242974967124
Iteration 100, Training loss = 0.2701647511658599
Iteration 110, Training loss = 0.26794262935766006
Iteration 120, Training loss = 0.266196145380669
Iteration 130, Training loss = 0.26424291015537255
Iteration 140, Training loss = 0.2626570856996945
Iteration 150, Training loss = 0.260911495909636
Iteration 160, Training loss = 0.25926910616800225
Iteration 170, Training loss = 0.2575498329741614
Iteration 180, Training loss = 0.25611684074485563
Iteration 190, Training loss = 0.2549607096551117
Model training time: 86.92783570289612
Device: cuda
Iteration 0, Training loss = 1.0231486915387484
Iteration 10, Training loss = 0.36151464216717794
Iteration 20, Training loss = 0.31815936898059477
Iteration 30, Training loss = 0.30615776023189323
Iteration 40, Training loss = 0.29607910433779616
Iteration 50, Training loss = 0.28804528177433963
Iteration 60, Training loss = 0.28130374783898093
Iteration 70, Training loss = 0.27562557009317107
Iteration 80, Training loss = 0.271387038919308
Iteration 90, Training loss = 0.26760444214644213
Iteration 100, Training loss = 0.2643410202485597
Iteration 110, Training loss = 0.2612912839066607
Iteration 120, Training loss = 0.25872686402488854
Iteration 130, Training loss = 0.25621321497904476
Iteration 140, Training loss = 0.2539828128878199
Iteration 150, Training loss = 0.2520902308727869
Iteration 160, Training loss = 0.25035842923903
Iteration 170, Training loss = 0.24884523563177188
Iteration 180, Training loss = 0.2473809088041361
Iteration 190, Training loss = 0.2460046058687546
Model training time: 86.75271081924438
Device: cuda
Iteration 0, Training loss = 0.9902973664297607
Iteration 10, Training loss = 0.36218008305200944
Iteration 20, Training loss = 0.32148137751777295
Iteration 30, Training loss = 0.3066746685248022
Iteration 40, Training loss = 0.2963066084657685
Iteration 50, Training loss = 0.28806338013661686
Iteration 60, Training loss = 0.2819118237452126
Iteration 70, Training loss = 0.27712849368844134
Iteration 80, Training loss = 0.2734927282855817
Iteration 90, Training loss = 0.27007092259481513
Iteration 100, Training loss = 0.267171706553305
Iteration 110, Training loss = 0.2643696917333701
Iteration 120, Training loss = 0.26198137690455225
Iteration 130, Training loss = 0.2596670131742521
Iteration 140, Training loss = 0.2578958681362882
Iteration 150, Training loss = 0.25622815074124
Iteration 160, Training loss = 0.2546944665941286
Iteration 170, Training loss = 0.25357564274429123
Iteration 180, Training loss = 0.252419400716665
Iteration 190, Training loss = 0.2513907707361707
Model training time: 87.08603763580322
Device: cuda
Iteration 0, Training loss = 0.9969321449645784
Iteration 10, Training loss = 0.4074964889530408
Iteration 20, Training loss = 0.3415778540165026
Iteration 30, Training loss = 0.32190952790995775
Iteration 40, Training loss = 0.3088873526663238
Iteration 50, Training loss = 0.29804857989849826
Iteration 60, Training loss = 0.2882804623275057
Iteration 70, Training loss = 0.28032323718070984
Iteration 80, Training loss = 0.27468786890151714
Iteration 90, Training loss = 0.27055888604959044
Iteration 100, Training loss = 0.26752132570628107
Iteration 110, Training loss = 0.265277371207392
Iteration 120, Training loss = 0.2632576002588284
Iteration 130, Training loss = 0.2617380630053562
Iteration 140, Training loss = 0.2600861921383307
Iteration 150, Training loss = 0.2587997932843954
Iteration 160, Training loss = 0.25764156629040513
Iteration 170, Training loss = 0.25645055872620454
Iteration 180, Training loss = 0.2555300321330747
Iteration 190, Training loss = 0.25453717615139687
Model training time: 86.86946702003479
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6683833592596123
Iteration 10, Training loss = 0.2598954713734241
Iteration 20, Training loss = 0.247063258802631
Iteration 30, Training loss = 0.24210722090509557
Iteration 40, Training loss = 0.23816576726355795
Iteration 50, Training loss = 0.23406540324627342
Iteration 60, Training loss = 0.23196683910655053
Iteration 70, Training loss = 0.22836875697241568
Iteration 80, Training loss = 0.22666655297754174
Iteration 90, Training loss = 0.22503877220986543
Iteration 100, Training loss = 0.2224494371806738
Iteration 110, Training loss = 0.22221560901612694
Iteration 120, Training loss = 0.2218443087367544
Iteration 130, Training loss = 0.22147873619494657
Iteration 140, Training loss = 0.22071880964476606
Iteration 150, Training loss = 0.22002674297364225
Iteration 160, Training loss = 0.2188375956462313
Iteration 170, Training loss = 0.2177141344954979
Iteration 180, Training loss = 0.21667103601064866
Iteration 190, Training loss = 0.2160145154461878
Model training time: 103.25352096557617
Device: cuda
Iteration 0, Training loss = 0.6783997626122782
Iteration 10, Training loss = 0.26129492794413833
Iteration 20, Training loss = 0.2480602673735226
Iteration 30, Training loss = 0.2405744215369802
Iteration 40, Training loss = 0.2371836444836552
Iteration 50, Training loss = 0.2335133965219626
Iteration 60, Training loss = 0.2295336246526559
Iteration 70, Training loss = 0.2260688259187391
Iteration 80, Training loss = 0.2234144656677656
Iteration 90, Training loss = 0.22190578061789634
Iteration 100, Training loss = 0.21991387867429643
Iteration 110, Training loss = 0.21887272321247014
Iteration 120, Training loss = 0.21834751721877452
Iteration 130, Training loss = 0.21759075104071787
Iteration 140, Training loss = 0.21670512897447292
Iteration 150, Training loss = 0.21509346520171904
Iteration 160, Training loss = 0.2147105790620566
Iteration 170, Training loss = 0.2149670150212168
Iteration 180, Training loss = 0.2142671701153312
Iteration 190, Training loss = 0.21349352526029722
Model training time: 102.40866184234619
Device: cuda
Iteration 0, Training loss = 0.6339748766607003
Iteration 10, Training loss = 0.2610480907682068
Iteration 20, Training loss = 0.2487158851819812
Iteration 30, Training loss = 0.24404859059370748
Iteration 40, Training loss = 0.24068293606830857
Iteration 50, Training loss = 0.23860800805666257
Iteration 60, Training loss = 0.23729971089100432
Iteration 70, Training loss = 0.23627008061143445
Iteration 80, Training loss = 0.2345590797358939
Iteration 90, Training loss = 0.23261449743991325
Iteration 100, Training loss = 0.2319360542636518
Iteration 110, Training loss = 0.23210820045150798
Iteration 120, Training loss = 0.2306436373430505
Iteration 130, Training loss = 0.2305810040192223
Iteration 140, Training loss = 0.22992757974828415
Iteration 150, Training loss = 0.22914222020827252
Iteration 160, Training loss = 0.22854365926172773
Iteration 170, Training loss = 0.22875175400486003
Iteration 180, Training loss = 0.2270621968564052
Iteration 190, Training loss = 0.22677752050880082
Model training time: 103.0247220993042
Device: cuda
Iteration 0, Training loss = 0.5630698662801459
Iteration 10, Training loss = 0.26011500004273064
Iteration 20, Training loss = 0.24760101084342592
Iteration 30, Training loss = 0.23913075320296368
Iteration 40, Training loss = 0.2298438505745396
Iteration 50, Training loss = 0.22403836948458855
Iteration 60, Training loss = 0.2203613263318094
Iteration 70, Training loss = 0.21683781175506606
Iteration 80, Training loss = 0.2153377451778324
Iteration 90, Training loss = 0.2140134526980414
Iteration 100, Training loss = 0.21055965154424996
Iteration 110, Training loss = 0.21014373383279575
Iteration 120, Training loss = 0.20962137666691014
Iteration 130, Training loss = 0.20978206217505452
Iteration 140, Training loss = 0.20811298584281387
Iteration 150, Training loss = 0.20855691133306042
Iteration 160, Training loss = 0.20732551105951857
Iteration 170, Training loss = 0.2075852957118943
Iteration 180, Training loss = 0.2073179960214774
Iteration 190, Training loss = 0.2074515392719689
Model training time: 101.18292236328125
Device: cuda
Iteration 0, Training loss = 0.5829390657041899
Iteration 10, Training loss = 0.25469259686822177
Iteration 20, Training loss = 0.2449222702451826
Iteration 30, Training loss = 0.2389308554244388
Iteration 40, Training loss = 0.2316682997901561
Iteration 50, Training loss = 0.2257156860507285
Iteration 60, Training loss = 0.2222637439520971
Iteration 70, Training loss = 0.21971473059014893
Iteration 80, Training loss = 0.2173306251228866
Iteration 90, Training loss = 0.21608692277718977
Iteration 100, Training loss = 0.21477247384933815
Iteration 110, Training loss = 0.2134376336746008
Iteration 120, Training loss = 0.21319884481246765
Iteration 130, Training loss = 0.2121894669428143
Iteration 140, Training loss = 0.21153964028303618
Iteration 150, Training loss = 0.2108489757011358
Iteration 160, Training loss = 0.20959820872574106
Iteration 170, Training loss = 0.2082404033630437
Iteration 180, Training loss = 0.20866551971825215
Iteration 190, Training loss = 0.2075086902042278
Model training time: 101.61696815490723
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9950587254459575
Iteration 10, Training loss = 0.5810734256200005
Iteration 20, Training loss = 0.4254386151890489
Iteration 30, Training loss = 0.3714483683752956
Iteration 40, Training loss = 0.33920296330284555
Iteration 50, Training loss = 0.3185744983556773
Iteration 60, Training loss = 0.3042936062047903
Iteration 70, Training loss = 0.29438395599764716
Iteration 80, Training loss = 0.28636276718346315
Iteration 90, Training loss = 0.2800134269579271
Iteration 100, Training loss = 0.2752922209606621
Iteration 110, Training loss = 0.2715740194115743
Iteration 120, Training loss = 0.2684979558387911
Iteration 130, Training loss = 0.2658797963082646
Iteration 140, Training loss = 0.2637051657996801
Iteration 150, Training loss = 0.26204656816784466
Iteration 160, Training loss = 0.26054275324500503
Iteration 170, Training loss = 0.2588624454150766
Iteration 180, Training loss = 0.2574350997246351
Iteration 190, Training loss = 0.25628809054488133
Model training time: 85.98195838928223
Device: cuda
Iteration 0, Training loss = 1.0304229581327184
Iteration 10, Training loss = 0.5566650485804814
Iteration 20, Training loss = 0.399928192429912
Iteration 30, Training loss = 0.3499161768479151
Iteration 40, Training loss = 0.32398157934804805
Iteration 50, Training loss = 0.3072232528077777
Iteration 60, Training loss = 0.2948675861911393
Iteration 70, Training loss = 0.28676208478486565
Iteration 80, Training loss = 0.28024461347672897
Iteration 90, Training loss = 0.2762412231677092
Iteration 100, Training loss = 0.27310015594915965
Iteration 110, Training loss = 0.27112254691614657
Iteration 120, Training loss = 0.2691575719571287
Iteration 130, Training loss = 0.26781667588553765
Iteration 140, Training loss = 0.2665768633382372
Iteration 150, Training loss = 0.26546172077878044
Iteration 160, Training loss = 0.26421275265857613
Iteration 170, Training loss = 0.2639088256523626
Iteration 180, Training loss = 0.2628227416380843
Iteration 190, Training loss = 0.2618782070012127
Model training time: 86.19603872299194
Device: cuda
Iteration 0, Training loss = 1.0277474285182306
Iteration 10, Training loss = 0.5561151257727395
Iteration 20, Training loss = 0.3970767757505828
Iteration 30, Training loss = 0.34784548107273067
Iteration 40, Training loss = 0.3229237082560905
Iteration 50, Training loss = 0.3069824259232089
Iteration 60, Training loss = 0.2956384701279982
Iteration 70, Training loss = 0.2876454012481987
Iteration 80, Training loss = 0.2821147283291124
Iteration 90, Training loss = 0.27766577899456024
Iteration 100, Training loss = 0.2743947790985246
Iteration 110, Training loss = 0.2714278502051536
Iteration 120, Training loss = 0.268940390437192
Iteration 130, Training loss = 0.2667735921845886
Iteration 140, Training loss = 0.2648105973030551
Iteration 150, Training loss = 0.2630264884803543
Iteration 160, Training loss = 0.2611301640008033
Iteration 170, Training loss = 0.2594209088251608
Iteration 180, Training loss = 0.2582235374548822
Iteration 190, Training loss = 0.2570026893051427
Model training time: 91.27958512306213
Device: cuda
Iteration 0, Training loss = 1.016624220924285
Iteration 10, Training loss = 0.38369086727536045
Iteration 20, Training loss = 0.3304011785486131
Iteration 30, Training loss = 0.31292901043813975
Iteration 40, Training loss = 0.30179114791296296
Iteration 50, Training loss = 0.29300698361948097
Iteration 60, Training loss = 0.28604495296250243
Iteration 70, Training loss = 0.28063621045457826
Iteration 80, Training loss = 0.2765925202978725
Iteration 90, Training loss = 0.2733252338217188
Iteration 100, Training loss = 0.27045118173733174
Iteration 110, Training loss = 0.268364953504059
Iteration 120, Training loss = 0.26597467891096205
Iteration 130, Training loss = 0.26402708813187287
Iteration 140, Training loss = 0.26233419070593095
Iteration 150, Training loss = 0.26091999964693846
Iteration 160, Training loss = 0.2597628425960102
Iteration 170, Training loss = 0.258388252924343
Iteration 180, Training loss = 0.2572300494005836
Iteration 190, Training loss = 0.2560814645132343
Model training time: 87.0484402179718
Device: cuda
Iteration 0, Training loss = 1.0291064202641171
Iteration 10, Training loss = 0.38362048555517314
Iteration 20, Training loss = 0.32215200810181316
Iteration 30, Training loss = 0.30771678737556385
Iteration 40, Training loss = 0.29542946128161135
Iteration 50, Training loss = 0.2868838321606992
Iteration 60, Training loss = 0.28048741631660856
Iteration 70, Training loss = 0.2751127887363873
Iteration 80, Training loss = 0.2708998504692359
Iteration 90, Training loss = 0.26760634828638513
Iteration 100, Training loss = 0.2649687795243598
Iteration 110, Training loss = 0.2626218356679196
Iteration 120, Training loss = 0.260432397779844
Iteration 130, Training loss = 0.25865014121116797
Iteration 140, Training loss = 0.2570315456383165
Iteration 150, Training loss = 0.25556636863268606
Iteration 160, Training loss = 0.2542133230815211
Iteration 170, Training loss = 0.2531258104860638
Iteration 180, Training loss = 0.252209069617724
Iteration 190, Training loss = 0.2512408628266025
Model training time: 87.0187463760376
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5091779584349212
Iteration 10, Training loss = 0.2696730606708919
Iteration 20, Training loss = 0.2566187096890468
Iteration 30, Training loss = 0.2504571976081511
Iteration 40, Training loss = 0.2463063822278965
Iteration 50, Training loss = 0.24284418156375032
Iteration 60, Training loss = 0.24076374836680964
Iteration 70, Training loss = 0.23699098769530258
Iteration 80, Training loss = 0.23358940482861193
Iteration 90, Training loss = 0.2308121354434161
Iteration 100, Training loss = 0.22710211643583839
Iteration 110, Training loss = 0.22486294960246825
Iteration 120, Training loss = 0.22345889279433948
Iteration 130, Training loss = 0.2198526743916709
Iteration 140, Training loss = 0.21756782953697312
Iteration 150, Training loss = 0.21570310701072648
Iteration 160, Training loss = 0.21430497362163395
Iteration 170, Training loss = 0.21348262697313947
Iteration 180, Training loss = 0.21161634232352777
Iteration 190, Training loss = 0.21080205735658042
Model training time: 101.02456855773926
Device: cuda
Iteration 0, Training loss = 0.6133322191512613
Iteration 10, Training loss = 0.25866185608556713
Iteration 20, Training loss = 0.25064144225804624
Iteration 30, Training loss = 0.24480440768866504
Iteration 40, Training loss = 0.24069016923807723
Iteration 50, Training loss = 0.23723449695124754
Iteration 60, Training loss = 0.23520737126938657
Iteration 70, Training loss = 0.2338559359275977
Iteration 80, Training loss = 0.23144725919419282
Iteration 90, Training loss = 0.2285136855759863
Iteration 100, Training loss = 0.2265103130305939
Iteration 110, Training loss = 0.22279303993569738
Iteration 120, Training loss = 0.22046707335128912
Iteration 130, Training loss = 0.21901032406178284
Iteration 140, Training loss = 0.2171867016170706
Iteration 150, Training loss = 0.21818208910939768
Iteration 160, Training loss = 0.21556042825699429
Iteration 170, Training loss = 0.2149384928276406
Iteration 180, Training loss = 0.2146334511388012
Iteration 190, Training loss = 0.21379366448514397
Model training time: 104.66394233703613
Device: cuda
Iteration 0, Training loss = 0.5835946600965379
Iteration 10, Training loss = 0.2571813012800263
Iteration 20, Training loss = 0.24626271635943406
Iteration 30, Training loss = 0.23802806942139643
Iteration 40, Training loss = 0.23329571874464972
Iteration 50, Training loss = 0.22878757981497785
Iteration 60, Training loss = 0.2245692228289551
Iteration 70, Training loss = 0.22112322435432427
Iteration 80, Training loss = 0.21918377489500993
Iteration 90, Training loss = 0.21613687634035114
Iteration 100, Training loss = 0.21497753713523504
Iteration 110, Training loss = 0.21216043586375927
Iteration 120, Training loss = 0.21021251145921666
Iteration 130, Training loss = 0.2094897217958372
Iteration 140, Training loss = 0.20857349045340143
Iteration 150, Training loss = 0.20744016276758173
Iteration 160, Training loss = 0.20734771940018304
Iteration 170, Training loss = 0.20595248590153586
Iteration 180, Training loss = 0.20538511434818008
Iteration 190, Training loss = 0.20453983766295142
Model training time: 102.93548107147217
Device: cuda
Iteration 0, Training loss = 0.578150075298822
Iteration 10, Training loss = 0.25882412982116887
Iteration 20, Training loss = 0.2447939338139991
Iteration 30, Training loss = 0.23621141958359368
Iteration 40, Training loss = 0.2286835295163979
Iteration 50, Training loss = 0.22290330252116297
Iteration 60, Training loss = 0.21854048505533694
Iteration 70, Training loss = 0.21506766520314297
Iteration 80, Training loss = 0.21209959660546254
Iteration 90, Training loss = 0.21055419129696076
Iteration 100, Training loss = 0.2089414700649264
Iteration 110, Training loss = 0.20808154785640304
Iteration 120, Training loss = 0.20694504767548086
Iteration 130, Training loss = 0.20719834842852183
Iteration 140, Training loss = 0.20602141497498852
Iteration 150, Training loss = 0.20588421137872678
Iteration 160, Training loss = 0.20516300095881157
Iteration 170, Training loss = 0.20436389562076288
Iteration 180, Training loss = 0.20477861013561127
Iteration 190, Training loss = 0.20434116201416633
Model training time: 112.4720504283905
Device: cuda
Iteration 0, Training loss = 0.5746089290403569
Iteration 10, Training loss = 0.26036468090503034
Iteration 20, Training loss = 0.2475716711786411
Iteration 30, Training loss = 0.24176694813565539
Iteration 40, Training loss = 0.23772204257818458
Iteration 50, Training loss = 0.23533543708032903
Iteration 60, Training loss = 0.23105685438139964
Iteration 70, Training loss = 0.22830606678604504
Iteration 80, Training loss = 0.22333858401196632
Iteration 90, Training loss = 0.2212862525559222
Iteration 100, Training loss = 0.2195777160005044
Iteration 110, Training loss = 0.2168518783610323
Iteration 120, Training loss = 0.2161868020134457
Iteration 130, Training loss = 0.21501902955388041
Iteration 140, Training loss = 0.21304021511434354
Iteration 150, Training loss = 0.2128657060551297
Iteration 160, Training loss = 0.21142095085639354
Iteration 170, Training loss = 0.21177754877698912
Iteration 180, Training loss = 0.21056851181799216
Iteration 190, Training loss = 0.2107473020295254
Model training time: 103.40471172332764
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0065619086093534
Iteration 10, Training loss = 0.4363388534608245
Iteration 20, Training loss = 0.33738432353763836
Iteration 30, Training loss = 0.31909368892558837
Iteration 40, Training loss = 0.30956054141533
Iteration 50, Training loss = 0.3028779641587567
Iteration 60, Training loss = 0.2978981626885278
Iteration 70, Training loss = 0.2929377841963895
Iteration 80, Training loss = 0.2888217077164327
Iteration 90, Training loss = 0.285064368123148
Iteration 100, Training loss = 0.28193738205092295
Iteration 110, Training loss = 0.2789719776401508
Iteration 120, Training loss = 0.276302893102602
Iteration 130, Training loss = 0.27418402843989126
Iteration 140, Training loss = 0.2723244536604922
Iteration 150, Training loss = 0.27100291364706747
Iteration 160, Training loss = 0.2695117806587323
Iteration 170, Training loss = 0.2684168310127882
Iteration 180, Training loss = 0.2672285300118005
Iteration 190, Training loss = 0.26609439841194243
Model training time: 88.34974575042725
Device: cuda
Iteration 0, Training loss = 1.0660994737690932
Iteration 10, Training loss = 0.8858118829369256
Iteration 20, Training loss = 0.35774913736896424
Iteration 30, Training loss = 0.32811654028967563
Iteration 40, Training loss = 0.3161550144198154
Iteration 50, Training loss = 0.3076209985711961
Iteration 60, Training loss = 0.2994577438180441
Iteration 70, Training loss = 0.29252799032098153
Iteration 80, Training loss = 0.28634134268717387
Iteration 90, Training loss = 0.28124941335246867
Iteration 100, Training loss = 0.27672120140409934
Iteration 110, Training loss = 0.2729228893723384
Iteration 120, Training loss = 0.2697190661569629
Iteration 130, Training loss = 0.26714669895013365
Iteration 140, Training loss = 0.26502789600276483
Iteration 150, Training loss = 0.262971868851283
Iteration 160, Training loss = 0.2614291907258819
Iteration 170, Training loss = 0.2602353989987772
Iteration 180, Training loss = 0.2586693401486764
Iteration 190, Training loss = 0.2574653377255862
Model training time: 86.24593377113342
Device: cuda
Iteration 0, Training loss = 0.9930512791107122
Iteration 10, Training loss = 0.35569377299443283
Iteration 20, Training loss = 0.32198860252740596
Iteration 30, Training loss = 0.3112026836912511
Iteration 40, Training loss = 0.3044783334286103
Iteration 50, Training loss = 0.29877005420065966
Iteration 60, Training loss = 0.2941091446877969
Iteration 70, Training loss = 0.28976322346103106
Iteration 80, Training loss = 0.28533162048523997
Iteration 90, Training loss = 0.2810845338259136
Iteration 100, Training loss = 0.27701069856599225
Iteration 110, Training loss = 0.2734877641964478
Iteration 120, Training loss = 0.27039544123786413
Iteration 130, Training loss = 0.2676242272804782
Iteration 140, Training loss = 0.26532339504931224
Iteration 150, Training loss = 0.26317038902106354
Iteration 160, Training loss = 0.2611872577277569
Iteration 170, Training loss = 0.2594423682110939
Iteration 180, Training loss = 0.2578773558428443
Iteration 190, Training loss = 0.256242827158286
Model training time: 85.62039279937744
Device: cuda
Iteration 0, Training loss = 1.0344221469276464
Iteration 10, Training loss = 0.39551764704990616
Iteration 20, Training loss = 0.3247877151026564
Iteration 30, Training loss = 0.3102507910738557
Iteration 40, Training loss = 0.29974465853293353
Iteration 50, Training loss = 0.29113768436213094
Iteration 60, Training loss = 0.28414300569394024
Iteration 70, Training loss = 0.2784548275190751
Iteration 80, Training loss = 0.2734983665169584
Iteration 90, Training loss = 0.2694725549574626
Iteration 100, Training loss = 0.26627642904875065
Iteration 110, Training loss = 0.2636793047983479
Iteration 120, Training loss = 0.26135345333713594
Iteration 130, Training loss = 0.2590843708621965
Iteration 140, Training loss = 0.2574653497583641
Iteration 150, Training loss = 0.25593001168158097
Iteration 160, Training loss = 0.2546089872104492
Iteration 170, Training loss = 0.2532269147565232
Iteration 180, Training loss = 0.25243647427232735
Iteration 190, Training loss = 0.2513188687096785
Model training time: 86.58769583702087
Device: cuda
Iteration 0, Training loss = 0.9992626554740832
Iteration 10, Training loss = 0.40149620370195216
Iteration 20, Training loss = 0.3308699675463591
Iteration 30, Training loss = 0.3140917099813861
Iteration 40, Training loss = 0.30324911802429844
Iteration 50, Training loss = 0.2943733342839788
Iteration 60, Training loss = 0.28631114956108766
Iteration 70, Training loss = 0.27988404022940133
Iteration 80, Training loss = 0.27471928022459113
Iteration 90, Training loss = 0.2708450015138194
Iteration 100, Training loss = 0.26765923281414455
Iteration 110, Training loss = 0.26533840022999206
Iteration 120, Training loss = 0.26287963472252607
Iteration 130, Training loss = 0.26078770511736304
Iteration 140, Training loss = 0.25884616741184463
Iteration 150, Training loss = 0.2570389018869862
Iteration 160, Training loss = 0.25555038201390395
Iteration 170, Training loss = 0.25421830672617396
Iteration 180, Training loss = 0.25290469352615946
Iteration 190, Training loss = 0.2519313053974372
Model training time: 85.75120043754578
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.4812907689672694
Iteration 10, Training loss = 0.24636163267579841
Iteration 20, Training loss = 0.23782769417430819
Iteration 30, Training loss = 0.2312031607290157
Iteration 40, Training loss = 0.2263786859413469
Iteration 50, Training loss = 0.2230137870365136
Iteration 60, Training loss = 0.21918879754246004
Iteration 70, Training loss = 0.21809555899541258
Iteration 80, Training loss = 0.2145904686852171
Iteration 90, Training loss = 0.2120988312432177
Iteration 100, Training loss = 0.21176175049733884
Iteration 110, Training loss = 0.2091252583442098
Iteration 120, Training loss = 0.20859725952833963
Iteration 130, Training loss = 0.2080862619361635
Iteration 140, Training loss = 0.2060562232216103
Iteration 150, Training loss = 0.20529757075679217
Iteration 160, Training loss = 0.2032835651234189
Iteration 170, Training loss = 0.20443468699660197
Iteration 180, Training loss = 0.20372130706473354
Iteration 190, Training loss = 0.2022081143414426
Iteration 200, Training loss = 0.20069974422743475
Iteration 210, Training loss = 0.2011082676796301
Iteration 220, Training loss = 0.20027836022524223
Iteration 230, Training loss = 0.2008343629773535
Iteration 240, Training loss = 0.1992121046774324
Iteration 250, Training loss = 0.19937870954558
Iteration 260, Training loss = 0.19896179895857055
Iteration 270, Training loss = 0.1984518941573023
Iteration 280, Training loss = 0.19900547621938564
Iteration 290, Training loss = 0.19931929504395107
Model training time: 154.2697777748108
Device: cuda
Iteration 0, Training loss = 0.5892912838382525
Iteration 10, Training loss = 0.2531936909218677
Iteration 20, Training loss = 0.24047550986595362
Iteration 30, Training loss = 0.2331191561845544
Iteration 40, Training loss = 0.22782016613184683
Iteration 50, Training loss = 0.22266468425567734
Iteration 60, Training loss = 0.21946353748548983
Iteration 70, Training loss = 0.21575585233169375
Iteration 80, Training loss = 0.2137606725024542
Iteration 90, Training loss = 0.2125205865206499
Iteration 100, Training loss = 0.21025098439928405
Iteration 110, Training loss = 0.2085005910322972
Iteration 120, Training loss = 0.20791955579063218
Iteration 130, Training loss = 0.20584817062026076
Iteration 140, Training loss = 0.20502241063168494
Iteration 150, Training loss = 0.2037592605568162
Iteration 160, Training loss = 0.20342023211551635
Iteration 170, Training loss = 0.20288201533348163
Iteration 180, Training loss = 0.2027452323252006
Iteration 190, Training loss = 0.2017162964830387
Iteration 200, Training loss = 0.20239822935430246
Iteration 210, Training loss = 0.20149667305786154
Iteration 220, Training loss = 0.20041644701296712
Iteration 230, Training loss = 0.20168281901294036
Iteration 240, Training loss = 0.20013540561962648
Iteration 250, Training loss = 0.2001755063762532
Iteration 260, Training loss = 0.1996660197095057
Iteration 270, Training loss = 0.20004653479660395
Iteration 280, Training loss = 0.1980740317023984
Iteration 290, Training loss = 0.19913415524197547
Model training time: 154.33642387390137
Device: cuda
Iteration 0, Training loss = 0.5142466513570804
Iteration 10, Training loss = 0.256598134252263
Iteration 20, Training loss = 0.24520590282597785
Iteration 30, Training loss = 0.23570082755195604
Iteration 40, Training loss = 0.2286555548773551
Iteration 50, Training loss = 0.2223551348302757
Iteration 60, Training loss = 0.21595236673482104
Iteration 70, Training loss = 0.21328681182413933
Iteration 80, Training loss = 0.21034268141875256
Iteration 90, Training loss = 0.20827637519912917
Iteration 100, Training loss = 0.20708113893433575
Iteration 110, Training loss = 0.20377417488731714
Iteration 120, Training loss = 0.20209765828442341
Iteration 130, Training loss = 0.2003500324834057
Iteration 140, Training loss = 0.20064366668390593
Iteration 150, Training loss = 0.19858094791883996
Iteration 160, Training loss = 0.19688769047470994
Iteration 170, Training loss = 0.19538361425838516
Iteration 180, Training loss = 0.19666415791209904
Iteration 190, Training loss = 0.19555639331262856
Iteration 200, Training loss = 0.19460094483799276
Iteration 210, Training loss = 0.19475941496199903
Iteration 220, Training loss = 0.1944487236945306
Iteration 230, Training loss = 0.19456736770510386
Iteration 240, Training loss = 0.19467299631808052
Iteration 250, Training loss = 0.19420270964539368
Iteration 260, Training loss = 0.19338107647568203
Iteration 270, Training loss = 0.192715771875139
Iteration 280, Training loss = 0.19371524656114797
Iteration 290, Training loss = 0.19321686233287863
Model training time: 154.85072946548462
Device: cuda
Iteration 0, Training loss = 0.5216050801785171
Iteration 10, Training loss = 0.255216696234217
Iteration 20, Training loss = 0.24200403986196542
Iteration 30, Training loss = 0.2356313582843499
Iteration 40, Training loss = 0.23146138412395342
Iteration 50, Training loss = 0.228673960454311
Iteration 60, Training loss = 0.2251731988773219
Iteration 70, Training loss = 0.2225878061704139
Iteration 80, Training loss = 0.21899585546722714
Iteration 90, Training loss = 0.21788613498210907
Iteration 100, Training loss = 0.2165517874747587
Iteration 110, Training loss = 0.21398210050154828
Iteration 120, Training loss = 0.21261333288133288
Iteration 130, Training loss = 0.21134137348711635
Iteration 140, Training loss = 0.21020150114274774
Iteration 150, Training loss = 0.20816771188169067
Iteration 160, Training loss = 0.2070649312245355
Iteration 170, Training loss = 0.20639515803454864
Iteration 180, Training loss = 0.2061347880605924
Iteration 190, Training loss = 0.20373810466929151
Iteration 200, Training loss = 0.2039869647139354
Iteration 210, Training loss = 0.2041245086736454
Iteration 220, Training loss = 0.20300065845106763
Iteration 230, Training loss = 0.20295613471950805
Iteration 240, Training loss = 0.20188533689727506
Iteration 250, Training loss = 0.2011002354576426
Iteration 260, Training loss = 0.20096033160319918
Iteration 270, Training loss = 0.2003901017329162
Iteration 280, Training loss = 0.2001869315349161
Iteration 290, Training loss = 0.19863187160893156
Model training time: 155.41342544555664
Device: cuda
Iteration 0, Training loss = 0.48360139335399677
Iteration 10, Training loss = 0.25192654062486447
Iteration 20, Training loss = 0.24078623249621714
Iteration 30, Training loss = 0.23502507773352016
Iteration 40, Training loss = 0.22878300616873956
Iteration 50, Training loss = 0.22337335387817595
Iteration 60, Training loss = 0.21851292795549004
Iteration 70, Training loss = 0.21518495148521358
Iteration 80, Training loss = 0.21259692165161737
Iteration 90, Training loss = 0.21011039077983065
Iteration 100, Training loss = 0.20944633586535152
Iteration 110, Training loss = 0.20691488455917875
Iteration 120, Training loss = 0.2064634341178304
Iteration 130, Training loss = 0.20505844736719825
Iteration 140, Training loss = 0.20509480422511517
Iteration 150, Training loss = 0.20364539202564277
Iteration 160, Training loss = 0.20389338274267627
Iteration 170, Training loss = 0.20260725028758764
Iteration 180, Training loss = 0.1998558933903028
Iteration 190, Training loss = 0.20080703888404167
Iteration 200, Training loss = 0.19935884664949147
Iteration 210, Training loss = 0.19830115062362777
Iteration 220, Training loss = 0.19708214422403755
Iteration 230, Training loss = 0.1975253068235394
Iteration 240, Training loss = 0.19744813356539528
Iteration 250, Training loss = 0.19807002116129993
Iteration 260, Training loss = 0.1964872070919659
Iteration 270, Training loss = 0.19708313654936832
Iteration 280, Training loss = 0.19617996778853292
Iteration 290, Training loss = 0.1963103731900237
Model training time: 155.33016276359558
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0180248359791015
Iteration 10, Training loss = 0.37569197113952685
Iteration 20, Training loss = 0.32458213909774947
Iteration 30, Training loss = 0.307251061026467
Iteration 40, Training loss = 0.2953857710228705
Iteration 50, Training loss = 0.28637022861520833
Iteration 60, Training loss = 0.2796582107862895
Iteration 70, Training loss = 0.2744507275648036
Iteration 80, Training loss = 0.2703040061072056
Iteration 90, Training loss = 0.26705367073019826
Iteration 100, Training loss = 0.26450648882198563
Iteration 110, Training loss = 0.26233498526615323
Iteration 120, Training loss = 0.26040387056208697
Iteration 130, Training loss = 0.25877116106396436
Iteration 140, Training loss = 0.2575889792807454
Iteration 150, Training loss = 0.2565678702103601
Iteration 160, Training loss = 0.2556870475447495
Iteration 170, Training loss = 0.2548539567801912
Iteration 180, Training loss = 0.2538718872728417
Iteration 190, Training loss = 0.25321814111156554
Iteration 200, Training loss = 0.2525798016865952
Iteration 210, Training loss = 0.2517108102904105
Iteration 220, Training loss = 0.25111593946489813
Iteration 230, Training loss = 0.25057249934780107
Iteration 240, Training loss = 0.25008965372028996
Iteration 250, Training loss = 0.24923346280041386
Iteration 260, Training loss = 0.24842946070807898
Iteration 270, Training loss = 0.24780200897168017
Iteration 280, Training loss = 0.24716576116642133
Iteration 290, Training loss = 0.24652556764733127
Model training time: 132.55036544799805
Device: cuda
Iteration 0, Training loss = 1.0349136162323755
Iteration 10, Training loss = 0.6219427143257409
Iteration 20, Training loss = 0.3422762688077968
Iteration 30, Training loss = 0.32181868754489656
Iteration 40, Training loss = 0.3106887159783384
Iteration 50, Training loss = 0.30107769714645843
Iteration 60, Training loss = 0.2922359653376494
Iteration 70, Training loss = 0.2847648599606738
Iteration 80, Training loss = 0.27850808325893367
Iteration 90, Training loss = 0.27419772556561245
Iteration 100, Training loss = 0.27109426425963856
Iteration 110, Training loss = 0.2685394172374042
Iteration 120, Training loss = 0.26650163022571266
Iteration 130, Training loss = 0.26472371271822703
Iteration 140, Training loss = 0.2632017486500971
Iteration 150, Training loss = 0.2617698758840561
Iteration 160, Training loss = 0.26052225368507836
Iteration 170, Training loss = 0.2593226713865779
Iteration 180, Training loss = 0.258383870629941
Iteration 190, Training loss = 0.25741930812885916
Iteration 200, Training loss = 0.2566859114004105
Iteration 210, Training loss = 0.25586031555912975
Iteration 220, Training loss = 0.2547663275251666
Iteration 230, Training loss = 0.254377863110555
Iteration 240, Training loss = 0.2535969282141898
Iteration 250, Training loss = 0.2527623226906716
Iteration 260, Training loss = 0.2519589470604719
Iteration 270, Training loss = 0.251131857693484
Iteration 280, Training loss = 0.25046839844660956
Iteration 290, Training loss = 0.24953142333478096
Model training time: 129.93074798583984
Device: cuda
Iteration 0, Training loss = 0.993835740242397
Iteration 10, Training loss = 0.3829126240049667
Iteration 20, Training loss = 0.3273924253069171
Iteration 30, Training loss = 0.3109820653790423
Iteration 40, Training loss = 0.3019704321178339
Iteration 50, Training loss = 0.2945560415886505
Iteration 60, Training loss = 0.2883323271359716
Iteration 70, Training loss = 0.2832508633053043
Iteration 80, Training loss = 0.2788934971027744
Iteration 90, Training loss = 0.27519591646968017
Iteration 100, Training loss = 0.2718183072802518
Iteration 110, Training loss = 0.26899392805434313
Iteration 120, Training loss = 0.2667204244084855
Iteration 130, Training loss = 0.2645947828022966
Iteration 140, Training loss = 0.2627068331541796
Iteration 150, Training loss = 0.26105592367363323
Iteration 160, Training loss = 0.25966503501804344
Iteration 170, Training loss = 0.2583511718447358
Iteration 180, Training loss = 0.2572912203089377
Iteration 190, Training loss = 0.2563770856464746
Iteration 200, Training loss = 0.2554219508719502
Iteration 210, Training loss = 0.25438925167478027
Iteration 220, Training loss = 0.25355158572155106
Iteration 230, Training loss = 0.25302174124532983
Iteration 240, Training loss = 0.2521166252851775
Iteration 250, Training loss = 0.25160080086232384
Iteration 260, Training loss = 0.2507273980729805
Iteration 270, Training loss = 0.24995758732641002
Iteration 280, Training loss = 0.24933765081747392
Iteration 290, Training loss = 0.24859888924705204
Model training time: 129.96751189231873
Device: cuda
Iteration 0, Training loss = 0.994950224426699
Iteration 10, Training loss = 0.3732074508871928
Iteration 20, Training loss = 0.32593273353417906
Iteration 30, Training loss = 0.31096406203421784
Iteration 40, Training loss = 0.30187561512715017
Iteration 50, Training loss = 0.29527490319048233
Iteration 60, Training loss = 0.289567160205893
Iteration 70, Training loss = 0.2845912580953383
Iteration 80, Training loss = 0.2800526677948799
Iteration 90, Training loss = 0.27572090209396355
Iteration 100, Training loss = 0.2720959051613658
Iteration 110, Training loss = 0.26887494839782
Iteration 120, Training loss = 0.26620470083005204
Iteration 130, Training loss = 0.2636049944573684
Iteration 140, Training loss = 0.26152084038996526
Iteration 150, Training loss = 0.2596548858750386
Iteration 160, Training loss = 0.2579378559283425
Iteration 170, Training loss = 0.256119234690366
Iteration 180, Training loss = 0.254480418304915
Iteration 190, Training loss = 0.25301685095843623
Iteration 200, Training loss = 0.2517012423738729
Iteration 210, Training loss = 0.2501262587474853
Iteration 220, Training loss = 0.24884868420425973
Iteration 230, Training loss = 0.24767061317515432
Iteration 240, Training loss = 0.24610895649067716
Iteration 250, Training loss = 0.24514806514141346
Iteration 260, Training loss = 0.244303323202214
Iteration 270, Training loss = 0.2428471327333127
Iteration 280, Training loss = 0.24197368320194917
Iteration 290, Training loss = 0.24069769231300378
Model training time: 139.8425304889679
Device: cuda
Iteration 0, Training loss = 0.9589881720900825
Iteration 10, Training loss = 0.35776829712327396
Iteration 20, Training loss = 0.3134524384220345
Iteration 30, Training loss = 0.2981838196443876
Iteration 40, Training loss = 0.28857891265185637
Iteration 50, Training loss = 0.281675821192761
Iteration 60, Training loss = 0.27585456513102924
Iteration 70, Training loss = 0.2709372958966664
Iteration 80, Training loss = 0.2668479066567617
Iteration 90, Training loss = 0.2633947278339118
Iteration 100, Training loss = 0.2604554551529538
Iteration 110, Training loss = 0.2579108268094698
Iteration 120, Training loss = 0.25556141525939935
Iteration 130, Training loss = 0.2538366782463203
Iteration 140, Training loss = 0.2520930314973249
Iteration 150, Training loss = 0.25077261113658655
Iteration 160, Training loss = 0.24947065535310395
Iteration 170, Training loss = 0.24821763300758587
Iteration 180, Training loss = 0.2470977695401731
Iteration 190, Training loss = 0.2459242628034899
Iteration 200, Training loss = 0.24473868741611013
Iteration 210, Training loss = 0.24355815580119233
Iteration 220, Training loss = 0.24244560714423224
Iteration 230, Training loss = 0.24128196280242167
Iteration 240, Training loss = 0.24031297727228654
Iteration 250, Training loss = 0.2389112445401799
Iteration 260, Training loss = 0.23818298757581388
Iteration 270, Training loss = 0.237238846845546
Iteration 280, Training loss = 0.23628690086539664
Iteration 290, Training loss = 0.23525999240198378
Model training time: 134.40015625953674
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6694278579214294
Iteration 10, Training loss = 0.2697929772424352
Iteration 20, Training loss = 0.2524372402028368
Iteration 30, Training loss = 0.24686669689979737
Iteration 40, Training loss = 0.24278920818544183
Iteration 50, Training loss = 0.2394745277182242
Iteration 60, Training loss = 0.23670823967702165
Iteration 70, Training loss = 0.23398444788940884
Iteration 80, Training loss = 0.23103696813234118
Iteration 90, Training loss = 0.22831575652769345
Iteration 100, Training loss = 0.2269245417693914
Iteration 110, Training loss = 0.22425461589351983
Iteration 120, Training loss = 0.22296886435612928
Iteration 130, Training loss = 0.2219204547077634
Iteration 140, Training loss = 0.22108728562190805
Iteration 150, Training loss = 0.21962743306636234
Iteration 160, Training loss = 0.21905474178944026
Iteration 170, Training loss = 0.21873022579829282
Iteration 180, Training loss = 0.21792434388658902
Iteration 190, Training loss = 0.2174552202260812
Iteration 200, Training loss = 0.21543910714276768
Iteration 210, Training loss = 0.21505858500197206
Iteration 220, Training loss = 0.2150559164944486
Iteration 230, Training loss = 0.21414092984095612
Iteration 240, Training loss = 0.21469311082406425
Iteration 250, Training loss = 0.21292921825882596
Iteration 260, Training loss = 0.21281369376846435
Iteration 270, Training loss = 0.21311599066701986
Iteration 280, Training loss = 0.2122341352571298
Iteration 290, Training loss = 0.2123986826736038
Model training time: 152.8680384159088
Device: cuda
Iteration 0, Training loss = 0.659193475223338
Iteration 10, Training loss = 0.26159161273344955
Iteration 20, Training loss = 0.2549481927518695
Iteration 30, Training loss = 0.25034165429289634
Iteration 40, Training loss = 0.24773912889472507
Iteration 50, Training loss = 0.24317025983160404
Iteration 60, Training loss = 0.23888513666845984
Iteration 70, Training loss = 0.23361582406424436
Iteration 80, Training loss = 0.228457228296389
Iteration 90, Training loss = 0.22620885379919128
Iteration 100, Training loss = 0.22432575933184232
Iteration 110, Training loss = 0.22332330557718405
Iteration 120, Training loss = 0.22033359294364874
Iteration 130, Training loss = 0.21868067591949467
Iteration 140, Training loss = 0.21745653633596534
Iteration 150, Training loss = 0.2156445457719573
Iteration 160, Training loss = 0.21525683952157204
Iteration 170, Training loss = 0.21485833342910968
Iteration 180, Training loss = 0.21265156479602
Iteration 190, Training loss = 0.212308291499176
Iteration 200, Training loss = 0.21131360069927524
Iteration 210, Training loss = 0.211310014254966
Iteration 220, Training loss = 0.2096543908660406
Iteration 230, Training loss = 0.21059601523036242
Iteration 240, Training loss = 0.21020073444553206
Iteration 250, Training loss = 0.20852850069073153
Iteration 260, Training loss = 0.20885033182998258
Iteration 270, Training loss = 0.2085834632720122
Iteration 280, Training loss = 0.2076824831778576
Iteration 290, Training loss = 0.20769669649531708
Model training time: 157.03964614868164
Device: cuda
Iteration 0, Training loss = 0.5482230838433305
Iteration 10, Training loss = 0.2637906341337696
Iteration 20, Training loss = 0.2529462703817116
Iteration 30, Training loss = 0.24977014122697688
Iteration 40, Training loss = 0.24576166822387865
Iteration 50, Training loss = 0.2429566812536907
Iteration 60, Training loss = 0.23855601988065328
Iteration 70, Training loss = 0.23630394156995177
Iteration 80, Training loss = 0.2334496758012304
Iteration 90, Training loss = 0.23040629490380138
Iteration 100, Training loss = 0.22812275705773374
Iteration 110, Training loss = 0.22505815066487392
Iteration 120, Training loss = 0.22256063375865576
Iteration 130, Training loss = 0.2217698048159805
Iteration 140, Training loss = 0.22093079433963606
Iteration 150, Training loss = 0.2196797545431024
Iteration 160, Training loss = 0.21879999840194608
Iteration 170, Training loss = 0.21857114583614665
Iteration 180, Training loss = 0.2183289968613851
Iteration 190, Training loss = 0.21725710259511455
Iteration 200, Training loss = 0.21641808511919317
Iteration 210, Training loss = 0.216311232446181
Iteration 220, Training loss = 0.21668272434799205
Iteration 230, Training loss = 0.2153095526408774
Iteration 240, Training loss = 0.2148922887917148
Iteration 250, Training loss = 0.21421504381493853
Iteration 260, Training loss = 0.21429486059500288
Iteration 270, Training loss = 0.21414553897216304
Iteration 280, Training loss = 0.21376465766106623
Iteration 290, Training loss = 0.21446652364066957
Model training time: 158.8598337173462
Device: cuda
Iteration 0, Training loss = 0.5441506578616311
Iteration 10, Training loss = 0.2602180966940544
Iteration 20, Training loss = 0.24940805230966204
Iteration 30, Training loss = 0.24412357182970348
Iteration 40, Training loss = 0.23927236595540588
Iteration 50, Training loss = 0.23593104129192616
Iteration 60, Training loss = 0.2324411515642598
Iteration 70, Training loss = 0.22807114585765048
Iteration 80, Training loss = 0.22521529664211065
Iteration 90, Training loss = 0.22312433146319147
Iteration 100, Training loss = 0.22187945354035346
Iteration 110, Training loss = 0.21934258537344437
Iteration 120, Training loss = 0.21856407587558535
Iteration 130, Training loss = 0.21755287677193957
Iteration 140, Training loss = 0.21674363610963845
Iteration 150, Training loss = 0.21630306816129938
Iteration 160, Training loss = 0.21522494364015704
Iteration 170, Training loss = 0.21608813635590002
Iteration 180, Training loss = 0.21420167096085468
Iteration 190, Training loss = 0.21427319325127844
Iteration 200, Training loss = 0.21418352847237854
Iteration 210, Training loss = 0.21411855080442335
Iteration 220, Training loss = 0.21287548646655557
Iteration 230, Training loss = 0.21396605001162675
Iteration 240, Training loss = 0.21251827859957917
Iteration 250, Training loss = 0.21261462380717222
Iteration 260, Training loss = 0.21221151773887742
Iteration 270, Training loss = 0.2124012282916478
Iteration 280, Training loss = 0.21220794309282418
Iteration 290, Training loss = 0.2114374340933547
Model training time: 153.9446461200714
Device: cuda
Iteration 0, Training loss = 0.6069875664052894
Iteration 10, Training loss = 0.261496687700327
Iteration 20, Training loss = 0.2495444446044453
Iteration 30, Training loss = 0.24243208973445268
Iteration 40, Training loss = 0.23795275741029018
Iteration 50, Training loss = 0.2361957387362785
Iteration 60, Training loss = 0.2324485625647748
Iteration 70, Training loss = 0.22993151230182832
Iteration 80, Training loss = 0.22918424716548538
Iteration 90, Training loss = 0.22756218432153397
Iteration 100, Training loss = 0.2260964061460541
Iteration 110, Training loss = 0.22418584904608657
Iteration 120, Training loss = 0.22257082028986466
Iteration 130, Training loss = 0.22137121825544367
Iteration 140, Training loss = 0.22148630342313222
Iteration 150, Training loss = 0.22031138031844943
Iteration 160, Training loss = 0.2181063969112193
Iteration 170, Training loss = 0.2168391928906591
Iteration 180, Training loss = 0.21580796564318078
Iteration 190, Training loss = 0.21404193673670724
Iteration 200, Training loss = 0.21324617069295762
Iteration 210, Training loss = 0.21250979001599998
Iteration 220, Training loss = 0.21114267033612757
Iteration 230, Training loss = 0.20970968339258475
Iteration 240, Training loss = 0.20953973889639532
Iteration 250, Training loss = 0.20985529368819972
Iteration 260, Training loss = 0.21031164020732876
Iteration 270, Training loss = 0.2100567579738453
Iteration 280, Training loss = 0.2082490257954915
Iteration 290, Training loss = 0.20957184443137547
Model training time: 151.7184739112854
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0396670484081019
Iteration 10, Training loss = 0.38263228393135
Iteration 20, Training loss = 0.33354192990223375
Iteration 30, Training loss = 0.3173397299236016
Iteration 40, Training loss = 0.3088565430543036
Iteration 50, Training loss = 0.30286546722595686
Iteration 60, Training loss = 0.2982777495108563
Iteration 70, Training loss = 0.2941957418299472
Iteration 80, Training loss = 0.2899789314798235
Iteration 90, Training loss = 0.28603874244355115
Iteration 100, Training loss = 0.2822012766365855
Iteration 110, Training loss = 0.2785923202808486
Iteration 120, Training loss = 0.2757481365457863
Iteration 130, Training loss = 0.27317191947242536
Iteration 140, Training loss = 0.27095997349762746
Iteration 150, Training loss = 0.2690195569511476
Iteration 160, Training loss = 0.2670423433220704
Iteration 170, Training loss = 0.2654022075253884
Iteration 180, Training loss = 0.26377307134736827
Iteration 190, Training loss = 0.2623644761073676
Iteration 200, Training loss = 0.2613051693715137
Iteration 210, Training loss = 0.2601589844460637
Iteration 220, Training loss = 0.25895548332257073
Iteration 230, Training loss = 0.25790855044963573
Iteration 240, Training loss = 0.2567909298595447
Iteration 250, Training loss = 0.2558524875967035
Iteration 260, Training loss = 0.2548244602563306
Iteration 270, Training loss = 0.2540437627682963
Iteration 280, Training loss = 0.25315370973533347
Iteration 290, Training loss = 0.25239950595265725
Model training time: 130.17677187919617
Device: cuda
Iteration 0, Training loss = 1.0834626769038147
Iteration 10, Training loss = 0.35238507909578504
Iteration 20, Training loss = 0.3207260031640963
Iteration 30, Training loss = 0.31029038403643244
Iteration 40, Training loss = 0.30125535059422615
Iteration 50, Training loss = 0.2934739134899063
Iteration 60, Training loss = 0.2870179916084823
Iteration 70, Training loss = 0.2821787854444605
Iteration 80, Training loss = 0.27778548234960937
Iteration 90, Training loss = 0.2746769557492785
Iteration 100, Training loss = 0.2724387569411615
Iteration 110, Training loss = 0.27085463736954957
Iteration 120, Training loss = 0.2695351178254977
Iteration 130, Training loss = 0.2681433495503938
Iteration 140, Training loss = 0.2674864450825906
Iteration 150, Training loss = 0.2662822475913939
Iteration 160, Training loss = 0.2653822018866389
Iteration 170, Training loss = 0.26467579368673283
Iteration 180, Training loss = 0.2634850246132719
Iteration 190, Training loss = 0.26247778329231547
Iteration 200, Training loss = 0.2616632089523946
Iteration 210, Training loss = 0.2609699686885457
Iteration 220, Training loss = 0.26028463682813735
Iteration 230, Training loss = 0.25952312532478616
Iteration 240, Training loss = 0.25882032904187646
Iteration 250, Training loss = 0.258101362726013
Iteration 260, Training loss = 0.25758118092219995
Iteration 270, Training loss = 0.2569053381068077
Iteration 280, Training loss = 0.2560290966282168
Iteration 290, Training loss = 0.25551676077285634
Model training time: 131.98960971832275
Device: cuda
Iteration 0, Training loss = 0.9536462826676865
Iteration 10, Training loss = 0.3886465752095922
Iteration 20, Training loss = 0.32950066805751793
Iteration 30, Training loss = 0.3122316682959296
Iteration 40, Training loss = 0.3012316584695338
Iteration 50, Training loss = 0.29308371054635496
Iteration 60, Training loss = 0.28669066651392794
Iteration 70, Training loss = 0.2817064430790144
Iteration 80, Training loss = 0.2771697814867225
Iteration 90, Training loss = 0.2734361454014628
Iteration 100, Training loss = 0.27035850073610035
Iteration 110, Training loss = 0.2679370700429052
Iteration 120, Training loss = 0.2658347561233846
Iteration 130, Training loss = 0.2638226103299178
Iteration 140, Training loss = 0.2618795995922989
Iteration 150, Training loss = 0.26028147347036923
Iteration 160, Training loss = 0.25881057909123834
Iteration 170, Training loss = 0.2573829166068003
Iteration 180, Training loss = 0.2561458328301335
Iteration 190, Training loss = 0.2551535273796132
Iteration 200, Training loss = 0.2540632328311698
Iteration 210, Training loss = 0.253165169395777
Iteration 220, Training loss = 0.25237503723572874
Iteration 230, Training loss = 0.251305422865161
Iteration 240, Training loss = 0.2505280053644434
Iteration 250, Training loss = 0.24982421881277972
Iteration 260, Training loss = 0.24869683208101886
Iteration 270, Training loss = 0.24825595427511968
Iteration 280, Training loss = 0.24749940830411404
Iteration 290, Training loss = 0.24665731314308129
Model training time: 130.54866194725037
Device: cuda
Iteration 0, Training loss = 0.9724826130826595
Iteration 10, Training loss = 0.36520434739226004
Iteration 20, Training loss = 0.33165260527021373
Iteration 30, Training loss = 0.31824561493520875
Iteration 40, Training loss = 0.3094480158558192
Iteration 50, Training loss = 0.30248271105653146
Iteration 60, Training loss = 0.2965143747609695
Iteration 70, Training loss = 0.291333735566451
Iteration 80, Training loss = 0.2864385550304995
Iteration 90, Training loss = 0.2822680923970502
Iteration 100, Training loss = 0.27863898927449604
Iteration 110, Training loss = 0.27568141374619765
Iteration 120, Training loss = 0.2727648385284022
Iteration 130, Training loss = 0.27047354107402427
Iteration 140, Training loss = 0.26850873702735645
Iteration 150, Training loss = 0.2666626474650951
Iteration 160, Training loss = 0.2649955216786186
Iteration 170, Training loss = 0.2632668744817773
Iteration 180, Training loss = 0.26167470285303657
Iteration 190, Training loss = 0.26036949863445386
Iteration 200, Training loss = 0.25900173416365724
Iteration 210, Training loss = 0.257851001659981
Iteration 220, Training loss = 0.25686648387019917
Iteration 230, Training loss = 0.2555733402654276
Iteration 240, Training loss = 0.2546229631827184
Iteration 250, Training loss = 0.2535168181103598
Iteration 260, Training loss = 0.2526156831415456
Iteration 270, Training loss = 0.2518630118693336
Iteration 280, Training loss = 0.2510750251337345
Iteration 290, Training loss = 0.2501519741467933
Model training time: 132.2110002040863
Device: cuda
Iteration 0, Training loss = 1.0271261190819683
Iteration 10, Training loss = 0.49320834788514106
Iteration 20, Training loss = 0.33279006962481766
Iteration 30, Training loss = 0.3106078548090799
Iteration 40, Training loss = 0.29934786725563806
Iteration 50, Training loss = 0.2911677404191823
Iteration 60, Training loss = 0.2838968731120193
Iteration 70, Training loss = 0.27730787287468484
Iteration 80, Training loss = 0.27169858583093554
Iteration 90, Training loss = 0.26730010405940524
Iteration 100, Training loss = 0.26434199733898656
Iteration 110, Training loss = 0.2619420408156247
Iteration 120, Training loss = 0.25989733842036916
Iteration 130, Training loss = 0.25810623876119065
Iteration 140, Training loss = 0.256740035051728
Iteration 150, Training loss = 0.25570722369896587
Iteration 160, Training loss = 0.25450450416482967
Iteration 170, Training loss = 0.25346740113332256
Iteration 180, Training loss = 0.2524740941118963
Iteration 190, Training loss = 0.2517772490118087
Iteration 200, Training loss = 0.25093798539612544
Iteration 210, Training loss = 0.25022781274076233
Iteration 220, Training loss = 0.24969690289465624
Iteration 230, Training loss = 0.24902156477942883
Iteration 240, Training loss = 0.2483437504375818
Iteration 250, Training loss = 0.2480244425105125
Iteration 260, Training loss = 0.24721757753709325
Iteration 270, Training loss = 0.24663314888068896
Iteration 280, Training loss = 0.24608615033563055
Iteration 290, Training loss = 0.24559750333969588
Model training time: 130.0115201473236
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.571154668349908
Iteration 10, Training loss = 0.2592661553592428
Iteration 20, Training loss = 0.2490697732921374
Iteration 30, Training loss = 0.2436670654297741
Iteration 40, Training loss = 0.2382662079633004
Iteration 50, Training loss = 0.2340972125800845
Iteration 60, Training loss = 0.23206613162311457
Iteration 70, Training loss = 0.23058979267358204
Iteration 80, Training loss = 0.22830795478662047
Iteration 90, Training loss = 0.2262442297962907
Iteration 100, Training loss = 0.22611009426072204
Iteration 110, Training loss = 0.22214572077513317
Iteration 120, Training loss = 0.22220047648103128
Iteration 130, Training loss = 0.22019091399147495
Iteration 140, Training loss = 0.21968691837556714
Iteration 150, Training loss = 0.218896256051687
Iteration 160, Training loss = 0.21703907279139859
Iteration 170, Training loss = 0.21685391524693867
Iteration 180, Training loss = 0.21518776232047462
Iteration 190, Training loss = 0.2148700564576552
Iteration 200, Training loss = 0.21427886402707988
Iteration 210, Training loss = 0.21346357403453844
Iteration 220, Training loss = 0.2127173878342707
Iteration 230, Training loss = 0.21197087246446575
Iteration 240, Training loss = 0.21130535844487947
Iteration 250, Training loss = 0.21167313302111684
Iteration 260, Training loss = 0.21131751421144454
Iteration 270, Training loss = 0.21053280868087207
Iteration 280, Training loss = 0.2108373716924583
Iteration 290, Training loss = 0.20957206698364553
Model training time: 152.70265674591064
Device: cuda
Iteration 0, Training loss = 0.571912181272778
Iteration 10, Training loss = 0.25227573099061307
Iteration 20, Training loss = 0.23884954067177114
Iteration 30, Training loss = 0.23133781858997252
Iteration 40, Training loss = 0.22463556375471788
Iteration 50, Training loss = 0.22106393883072434
Iteration 60, Training loss = 0.21873395547234695
Iteration 70, Training loss = 0.21625153785323403
Iteration 80, Training loss = 0.21599752865461114
Iteration 90, Training loss = 0.21315060824733092
Iteration 100, Training loss = 0.21361110646268647
Iteration 110, Training loss = 0.2126789566481229
Iteration 120, Training loss = 0.21089150832366135
Iteration 130, Training loss = 0.2117918307372069
Iteration 140, Training loss = 0.21072796558099854
Iteration 150, Training loss = 0.2094986131888325
Iteration 160, Training loss = 0.20944043349916652
Iteration 170, Training loss = 0.2093817352364629
Iteration 180, Training loss = 0.2084245589397144
Iteration 190, Training loss = 0.20751991278106305
Iteration 200, Training loss = 0.20768383402404428
Iteration 210, Training loss = 0.20721677664586188
Iteration 220, Training loss = 0.20687635695746678
Iteration 230, Training loss = 0.20637193227470932
Iteration 240, Training loss = 0.2065830934549215
Iteration 250, Training loss = 0.20544856601816402
Iteration 260, Training loss = 0.2049781697606059
Iteration 270, Training loss = 0.20518114977035914
Iteration 280, Training loss = 0.20455481606303347
Iteration 290, Training loss = 0.20517816490180268
Model training time: 163.3709692955017
Device: cuda
Iteration 0, Training loss = 0.5253541189049693
Iteration 10, Training loss = 0.2609246917447801
Iteration 20, Training loss = 0.24888522401632465
Iteration 30, Training loss = 0.2404699473671128
Iteration 40, Training loss = 0.23363204378878233
Iteration 50, Training loss = 0.23144678058910023
Iteration 60, Training loss = 0.2287830024344292
Iteration 70, Training loss = 0.22657966900607865
Iteration 80, Training loss = 0.22512111260801482
Iteration 90, Training loss = 0.2229007075723378
Iteration 100, Training loss = 0.22159581306699402
Iteration 110, Training loss = 0.22067463767802745
Iteration 120, Training loss = 0.21953240340038882
Iteration 130, Training loss = 0.21804668003869115
Iteration 140, Training loss = 0.21644496867212198
Iteration 150, Training loss = 0.21558615078216026
Iteration 160, Training loss = 0.2150376592976995
Iteration 170, Training loss = 0.21369654648276565
Iteration 180, Training loss = 0.2131014162038631
Iteration 190, Training loss = 0.2120732810555734
Iteration 200, Training loss = 0.21066781524884498
Iteration 210, Training loss = 0.20988527573554913
Iteration 220, Training loss = 0.2096193753922534
Iteration 230, Training loss = 0.20911597718482444
Iteration 240, Training loss = 0.20827523564816097
Iteration 250, Training loss = 0.20763171319883614
Iteration 260, Training loss = 0.20636059233769954
Iteration 270, Training loss = 0.2063675724673502
Iteration 280, Training loss = 0.20573125090495148
Iteration 290, Training loss = 0.20471894861025325
Model training time: 156.86847805976868
Device: cuda
Iteration 0, Training loss = 0.6029559902792693
Iteration 10, Training loss = 0.2524927323616446
Iteration 20, Training loss = 0.23767394014153873
Iteration 30, Training loss = 0.2313507153375241
Iteration 40, Training loss = 0.2248405102487049
Iteration 50, Training loss = 0.22000434262267612
Iteration 60, Training loss = 0.2166451338236615
Iteration 70, Training loss = 0.2133210255667459
Iteration 80, Training loss = 0.21079253521691513
Iteration 90, Training loss = 0.21047817015474704
Iteration 100, Training loss = 0.20907813670524097
Iteration 110, Training loss = 0.20705235915452458
Iteration 120, Training loss = 0.20522036499932372
Iteration 130, Training loss = 0.20406755417152408
Iteration 140, Training loss = 0.20309764731305563
Iteration 150, Training loss = 0.20185627733282835
Iteration 160, Training loss = 0.20122629536663073
Iteration 170, Training loss = 0.20051907193249421
Iteration 180, Training loss = 0.1995009191280127
Iteration 190, Training loss = 0.19877380098398892
Iteration 200, Training loss = 0.19859549219181116
Iteration 210, Training loss = 0.19707365948300962
Iteration 220, Training loss = 0.19789626140860034
Iteration 230, Training loss = 0.19663556448700353
Iteration 240, Training loss = 0.19708553756661623
Iteration 250, Training loss = 0.19626186499439774
Iteration 260, Training loss = 0.19555771120848725
Iteration 270, Training loss = 0.1955431603418424
Iteration 280, Training loss = 0.19480631044968855
Iteration 290, Training loss = 0.19425446841387714
Model training time: 154.47178149223328
Device: cuda
Iteration 0, Training loss = 0.5315210302574582
Iteration 10, Training loss = 0.2575550066176396
Iteration 20, Training loss = 0.24662996346884145
Iteration 30, Training loss = 0.24161339774908222
Iteration 40, Training loss = 0.23768659771282505
Iteration 50, Training loss = 0.23441454142187756
Iteration 60, Training loss = 0.23066131213170277
Iteration 70, Training loss = 0.22859827965750532
Iteration 80, Training loss = 0.22680267529250922
Iteration 90, Training loss = 0.224369404697678
Iteration 100, Training loss = 0.2235664123845158
Iteration 110, Training loss = 0.22159394914099437
Iteration 120, Training loss = 0.2203327972108169
Iteration 130, Training loss = 0.21959589170991076
Iteration 140, Training loss = 0.21858639571337665
Iteration 150, Training loss = 0.218127286607359
Iteration 160, Training loss = 0.2168309073399112
Iteration 170, Training loss = 0.21638943717677425
Iteration 180, Training loss = 0.21663609974212567
Iteration 190, Training loss = 0.21507138619988653
Iteration 200, Training loss = 0.21566921561848165
Iteration 210, Training loss = 0.21444117536736748
Iteration 220, Training loss = 0.2139681992245067
Iteration 230, Training loss = 0.2139362453349999
Iteration 240, Training loss = 0.21373056629379494
Iteration 250, Training loss = 0.21264046245676266
Iteration 260, Training loss = 0.21217777074898703
Iteration 270, Training loss = 0.21173492213823894
Iteration 280, Training loss = 0.21051831593055992
Iteration 290, Training loss = 0.2093607972919508
Model training time: 154.45133924484253
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.038087262801339
Iteration 10, Training loss = 0.46356248126768895
Iteration 20, Training loss = 0.3484715006830617
Iteration 30, Training loss = 0.3228923994822306
Iteration 40, Training loss = 0.31095240106377703
Iteration 50, Training loss = 0.3016623233514894
Iteration 60, Training loss = 0.29454938506315176
Iteration 70, Training loss = 0.28812291844777277
Iteration 80, Training loss = 0.28267604240710165
Iteration 90, Training loss = 0.2779812628434876
Iteration 100, Training loss = 0.27375352770523065
Iteration 110, Training loss = 0.27020821633769004
Iteration 120, Training loss = 0.26728881722356734
Iteration 130, Training loss = 0.26471111062941194
Iteration 140, Training loss = 0.26256347451241774
Iteration 150, Training loss = 0.260998861841659
Iteration 160, Training loss = 0.2594492408034299
Iteration 170, Training loss = 0.25800322863943065
Iteration 180, Training loss = 0.25683288808379856
Iteration 190, Training loss = 0.25582347656927157
Iteration 200, Training loss = 0.25487826156630644
Iteration 210, Training loss = 0.2540413381184562
Iteration 220, Training loss = 0.25352183783530613
Iteration 230, Training loss = 0.25263046765515074
Iteration 240, Training loss = 0.2521432817216791
Iteration 250, Training loss = 0.25150159521808635
Iteration 260, Training loss = 0.2511640613758824
Iteration 270, Training loss = 0.25069113874911686
Iteration 280, Training loss = 0.2502889922723066
Iteration 290, Training loss = 0.24967919158372695
Model training time: 129.97363710403442
Device: cuda
Iteration 0, Training loss = 1.0413125831336143
Iteration 10, Training loss = 0.3833007790491021
Iteration 20, Training loss = 0.32947033046593677
Iteration 30, Training loss = 0.3127360409779641
Iteration 40, Training loss = 0.3031607811625587
Iteration 50, Training loss = 0.2953649269141816
Iteration 60, Training loss = 0.28918116904127683
Iteration 70, Training loss = 0.28428261691014356
Iteration 80, Training loss = 0.27945670115673515
Iteration 90, Training loss = 0.2751432894108948
Iteration 100, Training loss = 0.27147491186063455
Iteration 110, Training loss = 0.2686272197139177
Iteration 120, Training loss = 0.2663939662151418
Iteration 130, Training loss = 0.2646658673124798
Iteration 140, Training loss = 0.2635165086922576
Iteration 150, Training loss = 0.2622636280520921
Iteration 160, Training loss = 0.2609193988559321
Iteration 170, Training loss = 0.2597525229645988
Iteration 180, Training loss = 0.2588854502101787
Iteration 190, Training loss = 0.25789517313963567
Iteration 200, Training loss = 0.25726489829165594
Iteration 210, Training loss = 0.25661579395033257
Iteration 220, Training loss = 0.25567655360944047
Iteration 230, Training loss = 0.25495915163588007
Iteration 240, Training loss = 0.25449092271252927
Iteration 250, Training loss = 0.25378284367031395
Iteration 260, Training loss = 0.2532037950276463
Iteration 270, Training loss = 0.25254426121747814
Iteration 280, Training loss = 0.25221720904328343
Iteration 290, Training loss = 0.2516444832194804
Model training time: 129.91819548606873
Device: cuda
Iteration 0, Training loss = 0.9724441592110271
Iteration 10, Training loss = 0.38676742644705436
Iteration 20, Training loss = 0.3278376966642698
Iteration 30, Training loss = 0.3130097506482145
Iteration 40, Training loss = 0.300633701588282
Iteration 50, Training loss = 0.2914255075427291
Iteration 60, Training loss = 0.28409795966405266
Iteration 70, Training loss = 0.2787878564570198
Iteration 80, Training loss = 0.27433508107794974
Iteration 90, Training loss = 0.27071715519734213
Iteration 100, Training loss = 0.2675823872552657
Iteration 110, Training loss = 0.2650110657365212
Iteration 120, Training loss = 0.26273584820456425
Iteration 130, Training loss = 0.26081313636632
Iteration 140, Training loss = 0.25889175451985286
Iteration 150, Training loss = 0.2569985231677787
Iteration 160, Training loss = 0.255309685063492
Iteration 170, Training loss = 0.253782304901332
Iteration 180, Training loss = 0.2520941142669313
Iteration 190, Training loss = 0.2508149449206149
Iteration 200, Training loss = 0.24960876357252315
Iteration 210, Training loss = 0.24859826919818906
Iteration 220, Training loss = 0.2475540119135351
Iteration 230, Training loss = 0.24674262956109522
Iteration 240, Training loss = 0.24590431563285592
Iteration 250, Training loss = 0.2450584591553517
Iteration 260, Training loss = 0.24442656008080188
Iteration 270, Training loss = 0.24309079576346834
Iteration 280, Training loss = 0.2422416217120017
Iteration 290, Training loss = 0.2413721934015035
Model training time: 129.91621208190918
Device: cuda
Iteration 0, Training loss = 1.0141476220282168
Iteration 10, Training loss = 0.39282087270919114
Iteration 20, Training loss = 0.32772373260149945
Iteration 30, Training loss = 0.31120355780852044
Iteration 40, Training loss = 0.3004130253491621
Iteration 50, Training loss = 0.29314171392966704
Iteration 60, Training loss = 0.28753425559755097
Iteration 70, Training loss = 0.28294469356609026
Iteration 80, Training loss = 0.27892049199948876
Iteration 90, Training loss = 0.27544376804306203
Iteration 100, Training loss = 0.27226146285311653
Iteration 110, Training loss = 0.26971431996574124
Iteration 120, Training loss = 0.2671188528832165
Iteration 130, Training loss = 0.26496408778732106
Iteration 140, Training loss = 0.2629207302669636
Iteration 150, Training loss = 0.2613840683951938
Iteration 160, Training loss = 0.26005470061778446
Iteration 170, Training loss = 0.2584447455536078
Iteration 180, Training loss = 0.2571608804021996
Iteration 190, Training loss = 0.2559816555523699
Iteration 200, Training loss = 0.2548052207826125
Iteration 210, Training loss = 0.25381678108297306
Iteration 220, Training loss = 0.25266506928797206
Iteration 230, Training loss = 0.2514560231645806
Iteration 240, Training loss = 0.2503328414827802
Iteration 250, Training loss = 0.24924574466760452
Iteration 260, Training loss = 0.24847012940460486
Iteration 270, Training loss = 0.24739191368150076
Iteration 280, Training loss = 0.2465536546043276
Iteration 290, Training loss = 0.2454976183937335
Model training time: 128.1078701019287
Device: cuda
Iteration 0, Training loss = 1.0087541976073175
Iteration 10, Training loss = 0.544145281104141
Iteration 20, Training loss = 0.38578243571678605
Iteration 30, Training loss = 0.3375013337874239
Iteration 40, Training loss = 0.313851063552405
Iteration 50, Training loss = 0.30037967662257087
Iteration 60, Training loss = 0.2913000734283907
Iteration 70, Training loss = 0.2842773157769485
Iteration 80, Training loss = 0.2786714611958361
Iteration 90, Training loss = 0.2744007637187586
Iteration 100, Training loss = 0.27080117855175934
Iteration 110, Training loss = 0.26765457455961816
Iteration 120, Training loss = 0.2650192160493092
Iteration 130, Training loss = 0.26228735002420717
Iteration 140, Training loss = 0.25982301043612616
Iteration 150, Training loss = 0.2577064165047237
Iteration 160, Training loss = 0.2560988481339184
Iteration 170, Training loss = 0.25472715944412544
Iteration 180, Training loss = 0.2535680927715059
Iteration 190, Training loss = 0.2522630122542093
Iteration 200, Training loss = 0.25117979328583284
Iteration 210, Training loss = 0.2502385100606856
Iteration 220, Training loss = 0.24884925338892902
Iteration 230, Training loss = 0.24810666230126097
Iteration 240, Training loss = 0.24690107980485979
Iteration 250, Training loss = 0.24575018043884642
Iteration 260, Training loss = 0.24494414266027492
Iteration 270, Training loss = 0.2440500063303019
Iteration 280, Training loss = 0.24338323835815703
Iteration 290, Training loss = 0.24254738792019376
Model training time: 129.40499091148376
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.42287579644245904
Iteration 10, Training loss = 0.24846308943578752
Iteration 20, Training loss = 0.2386147104221741
Iteration 30, Training loss = 0.22772606649857746
Iteration 40, Training loss = 0.22095665047012863
Iteration 50, Training loss = 0.21859157432691526
Iteration 60, Training loss = 0.21582371153424496
Iteration 70, Training loss = 0.21396147894512943
Iteration 80, Training loss = 0.2120267238386751
Iteration 90, Training loss = 0.21090839722940188
Model training time: 51.77448225021362
Device: cuda
Iteration 0, Training loss = 0.4779748970746417
Iteration 10, Training loss = 0.2466711455879431
Iteration 20, Training loss = 0.23479037250213994
Iteration 30, Training loss = 0.22654860624569959
Iteration 40, Training loss = 0.22175716119874764
Iteration 50, Training loss = 0.21928206824000754
Iteration 60, Training loss = 0.2151059563346117
Iteration 70, Training loss = 0.2118717793907438
Iteration 80, Training loss = 0.21120845609441508
Iteration 90, Training loss = 0.20794856456304578
Model training time: 51.537179708480835
Device: cuda
Iteration 0, Training loss = 0.4056598323309393
Iteration 10, Training loss = 0.2448941283198592
Iteration 20, Training loss = 0.23478903456367822
Iteration 30, Training loss = 0.2243379407154157
Iteration 40, Training loss = 0.219449578885882
Iteration 50, Training loss = 0.21459636351964087
Iteration 60, Training loss = 0.21212989746730496
Iteration 70, Training loss = 0.20877479419076125
Iteration 80, Training loss = 0.20975861370058382
Iteration 90, Training loss = 0.2067481657945648
Model training time: 51.1693320274353
Device: cuda
Iteration 0, Training loss = 0.43032316481085725
Iteration 10, Training loss = 0.24077442636573573
Iteration 20, Training loss = 0.22529682273509716
Iteration 30, Training loss = 0.21564190822132565
Iteration 40, Training loss = 0.2108484634782298
Iteration 50, Training loss = 0.20791908597109104
Iteration 60, Training loss = 0.2049168130838409
Iteration 70, Training loss = 0.20308770690356848
Iteration 80, Training loss = 0.20409478615184673
Iteration 90, Training loss = 0.2003990534611822
Model training time: 51.05983090400696
Device: cuda
Iteration 0, Training loss = 0.41410659536827565
Iteration 10, Training loss = 0.24301129809555938
Iteration 20, Training loss = 0.2284837533645422
Iteration 30, Training loss = 0.22079173061482554
Iteration 40, Training loss = 0.21362662646405345
Iteration 50, Training loss = 0.2095973943393975
Iteration 60, Training loss = 0.20720674167894568
Iteration 70, Training loss = 0.20461338925339986
Iteration 80, Training loss = 0.20308666110724283
Iteration 90, Training loss = 0.2009174133454772
Model training time: 51.128315687179565
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0043650949982696
Iteration 10, Training loss = 0.33737807100318534
Iteration 20, Training loss = 0.3065969002304585
Iteration 30, Training loss = 0.28742381569548325
Iteration 40, Training loss = 0.2759201089709492
Iteration 50, Training loss = 0.2687491197454727
Iteration 60, Training loss = 0.26281087037365314
Iteration 70, Training loss = 0.2585237814027807
Iteration 80, Training loss = 0.25561909148465056
Iteration 90, Training loss = 0.2538097449386668
Model training time: 44.07800102233887
Device: cuda
Iteration 0, Training loss = 0.9813986992143257
Iteration 10, Training loss = 0.3307112645871991
Iteration 20, Training loss = 0.3070027513235591
Iteration 30, Training loss = 0.28938100809912415
Iteration 40, Training loss = 0.27612421327872655
Iteration 50, Training loss = 0.2670407160016151
Iteration 60, Training loss = 0.2615734625418307
Iteration 70, Training loss = 0.25740578928488794
Iteration 80, Training loss = 0.2544427993583333
Iteration 90, Training loss = 0.2524454029362658
Model training time: 43.89880061149597
Device: cuda
Iteration 0, Training loss = 0.9734210879260057
Iteration 10, Training loss = 0.3244394743521912
Iteration 20, Training loss = 0.2977456024354076
Iteration 30, Training loss = 0.2841929127676435
Iteration 40, Training loss = 0.27684675867851943
Iteration 50, Training loss = 0.27106318640867677
Iteration 60, Training loss = 0.26657183015534147
Iteration 70, Training loss = 0.26250239685358207
Iteration 80, Training loss = 0.259775204155549
Iteration 90, Training loss = 0.25750299452941583
Model training time: 43.88978433609009
Device: cuda
Iteration 0, Training loss = 0.9404951515122707
Iteration 10, Training loss = 0.3135633759976299
Iteration 20, Training loss = 0.29353060342425585
Iteration 30, Training loss = 0.28212165513208937
Iteration 40, Training loss = 0.2738865556569711
Iteration 50, Training loss = 0.26736853969876473
Iteration 60, Training loss = 0.26282762801026605
Iteration 70, Training loss = 0.25939945750317334
Iteration 80, Training loss = 0.2560987545777175
Iteration 90, Training loss = 0.25398980421138156
Model training time: 43.4752995967865
Device: cuda
Iteration 0, Training loss = 0.9841279590245309
Iteration 10, Training loss = 0.3231217346129348
Iteration 20, Training loss = 0.29655736268071803
Iteration 30, Training loss = 0.2813346044455833
Iteration 40, Training loss = 0.27028275531948914
Iteration 50, Training loss = 0.2632124927842011
Iteration 60, Training loss = 0.25895880431514096
Iteration 70, Training loss = 0.2557284006122815
Iteration 80, Training loss = 0.2528455100827298
Iteration 90, Training loss = 0.2506964659864042
Model training time: 42.79436898231506
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.44749778130296936
Iteration 10, Training loss = 0.2533256800678394
Iteration 20, Training loss = 0.24576816808874324
Iteration 30, Training loss = 0.24223432110816456
Iteration 40, Training loss = 0.23923861142473418
Iteration 50, Training loss = 0.23658161981991938
Iteration 60, Training loss = 0.23662755037624092
Iteration 70, Training loss = 0.23348595398606747
Iteration 80, Training loss = 0.23138754268246758
Iteration 90, Training loss = 0.2284859708232972
Model training time: 50.48264694213867
Device: cuda
Iteration 0, Training loss = 0.528392627979595
Iteration 10, Training loss = 0.2593464470010693
Iteration 20, Training loss = 0.2503030214539885
Iteration 30, Training loss = 0.2458172041120021
Iteration 40, Training loss = 0.24360079760268583
Iteration 50, Training loss = 0.2382743844343011
Iteration 60, Training loss = 0.23599143432719366
Iteration 70, Training loss = 0.23338066654834563
Iteration 80, Training loss = 0.23039352975010005
Iteration 90, Training loss = 0.2281314879295035
Model training time: 50.361414194107056
Device: cuda
Iteration 0, Training loss = 0.46175750940316523
Iteration 10, Training loss = 0.24844088022919603
Iteration 20, Training loss = 0.24245568772565942
Iteration 30, Training loss = 0.23631691357197543
Iteration 40, Training loss = 0.2335551759683624
Iteration 50, Training loss = 0.23342068460893978
Iteration 60, Training loss = 0.2300594604098335
Iteration 70, Training loss = 0.22632854535598443
Iteration 80, Training loss = 0.22341704962112136
Iteration 90, Training loss = 0.22214757297503746
Model training time: 50.65013122558594
Device: cuda
Iteration 0, Training loss = 0.42437331847503745
Iteration 10, Training loss = 0.25097844129108055
Iteration 20, Training loss = 0.23888270482962126
Iteration 30, Training loss = 0.22851578981478046
Iteration 40, Training loss = 0.22336543902538303
Iteration 50, Training loss = 0.220207747757507
Iteration 60, Training loss = 0.2183237753032772
Iteration 70, Training loss = 0.2167321949583855
Iteration 80, Training loss = 0.21570660942403225
Iteration 90, Training loss = 0.21402582506362808
Model training time: 52.38939166069031
Device: cuda
Iteration 0, Training loss = 0.47768955625431303
Iteration 10, Training loss = 0.24865413583436255
Iteration 20, Training loss = 0.23334319048874602
Iteration 30, Training loss = 0.22825226909311863
Iteration 40, Training loss = 0.225187431452638
Iteration 50, Training loss = 0.2225234800494179
Iteration 60, Training loss = 0.22193118073858015
Iteration 70, Training loss = 0.2204757849471621
Iteration 80, Training loss = 0.21983748943190887
Iteration 90, Training loss = 0.21888293354765267
Model training time: 56.96995258331299
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.060542560085546
Iteration 10, Training loss = 0.3376677798662001
Iteration 20, Training loss = 0.3059613330528753
Iteration 30, Training loss = 0.2908004912531982
Iteration 40, Training loss = 0.28052348138417227
Iteration 50, Training loss = 0.2734409118788294
Iteration 60, Training loss = 0.2686858365343789
Iteration 70, Training loss = 0.26545960417346576
Iteration 80, Training loss = 0.2627269975281801
Iteration 90, Training loss = 0.26046750016203807
Model training time: 47.76173210144043
Device: cuda
Iteration 0, Training loss = 0.990636938492842
Iteration 10, Training loss = 0.3453183412984843
Iteration 20, Training loss = 0.31674448160801905
Iteration 30, Training loss = 0.30318797291335414
Iteration 40, Training loss = 0.2925874830446867
Iteration 50, Training loss = 0.28440757977976927
Iteration 60, Training loss = 0.27767171945251506
Iteration 70, Training loss = 0.2715010923890456
Iteration 80, Training loss = 0.26712573813973556
Iteration 90, Training loss = 0.2639748992592313
Model training time: 43.63641929626465
Device: cuda
Iteration 0, Training loss = 1.003940369618141
Iteration 10, Training loss = 0.3233272348853058
Iteration 20, Training loss = 0.3026928022936816
Iteration 30, Training loss = 0.29075075859163346
Iteration 40, Training loss = 0.28032480072022636
Iteration 50, Training loss = 0.2718740713762313
Iteration 60, Training loss = 0.2656342875365772
Iteration 70, Training loss = 0.26097454855863467
Iteration 80, Training loss = 0.25762952985039056
Iteration 90, Training loss = 0.25521275090824896
Model training time: 43.694618463516235
Device: cuda
Iteration 0, Training loss = 0.953413230743593
Iteration 10, Training loss = 0.334461443541125
Iteration 20, Training loss = 0.30179433026700564
Iteration 30, Training loss = 0.2892959185810557
Iteration 40, Training loss = 0.28239941405759306
Iteration 50, Training loss = 0.27841451798383027
Iteration 60, Training loss = 0.27523756543169875
Iteration 70, Training loss = 0.2725265969953006
Iteration 80, Training loss = 0.2699077401770229
Iteration 90, Training loss = 0.2677842423897101
Model training time: 43.3411431312561
Device: cuda
Iteration 0, Training loss = 0.9945618335617656
Iteration 10, Training loss = 0.32570191553949446
Iteration 20, Training loss = 0.30278757596997313
Iteration 30, Training loss = 0.2905539625457355
Iteration 40, Training loss = 0.28197565566540916
Iteration 50, Training loss = 0.27524609152976304
Iteration 60, Training loss = 0.2697069444141145
Iteration 70, Training loss = 0.26523264661683876
Iteration 80, Training loss = 0.26007826417035107
Iteration 90, Training loss = 0.2569357203838323
Model training time: 44.21191334724426
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.43609578378913477
Iteration 10, Training loss = 0.24761935817776812
Iteration 20, Training loss = 0.2382785339296297
Iteration 30, Training loss = 0.23261813512725923
Iteration 40, Training loss = 0.2287616361356532
Iteration 50, Training loss = 0.2246587707618247
Iteration 60, Training loss = 0.22213585370786254
Iteration 70, Training loss = 0.22127998105225494
Iteration 80, Training loss = 0.22000199137819304
Iteration 90, Training loss = 0.21817819301499003
Model training time: 52.554516077041626
Device: cuda
Iteration 0, Training loss = 0.43026573874832064
Iteration 10, Training loss = 0.24663220393780358
Iteration 20, Training loss = 0.23026369163004307
Iteration 30, Training loss = 0.22366253990165835
Iteration 40, Training loss = 0.22127171328007164
Iteration 50, Training loss = 0.21840410850960176
Iteration 60, Training loss = 0.21663295919612302
Iteration 70, Training loss = 0.21419223291533335
Iteration 80, Training loss = 0.21333040882434165
Iteration 90, Training loss = 0.21197906644342887
Model training time: 50.75775456428528
Device: cuda
Iteration 0, Training loss = 0.439059649573833
Iteration 10, Training loss = 0.25082037276780056
Iteration 20, Training loss = 0.240034948610509
Iteration 30, Training loss = 0.23156928756335168
Iteration 40, Training loss = 0.22860058017309584
Iteration 50, Training loss = 0.2251938438859315
Iteration 60, Training loss = 0.2194388825401267
Iteration 70, Training loss = 0.21918155569538078
Iteration 80, Training loss = 0.2146382466788442
Iteration 90, Training loss = 0.2133350864088853
Model training time: 51.194271087646484
Device: cuda
Iteration 0, Training loss = 0.45144049747515536
Iteration 10, Training loss = 0.2513904309049068
Iteration 20, Training loss = 0.23862710789964506
Iteration 30, Training loss = 0.22964514868636107
Iteration 40, Training loss = 0.2234662182574988
Iteration 50, Training loss = 0.21767636508507243
Iteration 60, Training loss = 0.2158758198305712
Iteration 70, Training loss = 0.21274822899850748
Iteration 80, Training loss = 0.20974138297916325
Iteration 90, Training loss = 0.20740460469128144
Model training time: 52.39717483520508
Device: cuda
Iteration 0, Training loss = 0.4596054942299754
Iteration 10, Training loss = 0.2527764230587725
Iteration 20, Training loss = 0.24024179992750827
Iteration 30, Training loss = 0.2346620015421156
Iteration 40, Training loss = 0.22928242939219926
Iteration 50, Training loss = 0.22625058602550704
Iteration 60, Training loss = 0.22191116320235388
Iteration 70, Training loss = 0.21821693603409403
Iteration 80, Training loss = 0.21789598798708534
Iteration 90, Training loss = 0.21680468668119382
Model training time: 52.1815299987793
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0192866280638855
Iteration 10, Training loss = 0.33096226088287756
Iteration 20, Training loss = 0.30053602409781327
Iteration 30, Training loss = 0.2854908233549058
Iteration 40, Training loss = 0.27684925195307764
Iteration 50, Training loss = 0.2705180972958881
Iteration 60, Training loss = 0.2658635190085984
Iteration 70, Training loss = 0.26266354442400447
Iteration 80, Training loss = 0.2599958507547078
Iteration 90, Training loss = 0.25820128762188027
Model training time: 44.54759430885315
Device: cuda
Iteration 0, Training loss = 1.0576546063723344
Iteration 10, Training loss = 0.4130630142824413
Iteration 20, Training loss = 0.3133499236105429
Iteration 30, Training loss = 0.28984725774777714
Iteration 40, Training loss = 0.2792330169396308
Iteration 50, Training loss = 0.2720937539460295
Iteration 60, Training loss = 0.26708476405960596
Iteration 70, Training loss = 0.2636976557767997
Iteration 80, Training loss = 0.26145925735445924
Iteration 90, Training loss = 0.2588062751361735
Model training time: 42.993908166885376
Device: cuda
Iteration 0, Training loss = 1.0155845826387984
Iteration 10, Training loss = 0.4022519621303526
Iteration 20, Training loss = 0.3337090790776883
Iteration 30, Training loss = 0.3097070574616116
Iteration 40, Training loss = 0.29494743197073947
Iteration 50, Training loss = 0.2821923256028651
Iteration 60, Training loss = 0.27413694878034384
Iteration 70, Training loss = 0.26756702309370617
Iteration 80, Training loss = 0.262773493841543
Iteration 90, Training loss = 0.2592400461254917
Model training time: 42.9579017162323
Device: cuda
Iteration 0, Training loss = 0.9752134844552518
Iteration 10, Training loss = 0.330970346458599
Iteration 20, Training loss = 0.3075068941200039
Iteration 30, Training loss = 0.2939911453869672
Iteration 40, Training loss = 0.28443872667037257
Iteration 50, Training loss = 0.2763547820487842
Iteration 60, Training loss = 0.2696340009750523
Iteration 70, Training loss = 0.2643292957948426
Iteration 80, Training loss = 0.26014018877799516
Iteration 90, Training loss = 0.25697345869304483
Model training time: 42.51439547538757
Device: cuda
Iteration 0, Training loss = 0.9359185148670944
Iteration 10, Training loss = 0.3353908829228641
Iteration 20, Training loss = 0.3027259927540657
Iteration 30, Training loss = 0.2865132227287454
Iteration 40, Training loss = 0.2770234283249257
Iteration 50, Training loss = 0.27102928874748095
Iteration 60, Training loss = 0.2666135054198939
Iteration 70, Training loss = 0.26287247687939
Iteration 80, Training loss = 0.2601289774221312
Iteration 90, Training loss = 0.2584879181503384
Model training time: 43.76303672790527
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.4455924521563417
Iteration 10, Training loss = 0.24085368187173514
Iteration 20, Training loss = 0.23076469290631735
Iteration 30, Training loss = 0.2229325392127903
Iteration 40, Training loss = 0.215643328007933
Iteration 50, Training loss = 0.21183147459103754
Iteration 60, Training loss = 0.20994385732360382
Iteration 70, Training loss = 0.20693069914712167
Iteration 80, Training loss = 0.2047188906923622
Iteration 90, Training loss = 0.20241936364777152
Iteration 100, Training loss = 0.2012495896700075
Iteration 110, Training loss = 0.20187903027395215
Iteration 120, Training loss = 0.20035826353487032
Iteration 130, Training loss = 0.1978268782466145
Iteration 140, Training loss = 0.19896309156214353
Iteration 150, Training loss = 0.19794844414759202
Iteration 160, Training loss = 0.19597260056843768
Iteration 170, Training loss = 0.19691451881125965
Iteration 180, Training loss = 0.19572900252791062
Iteration 190, Training loss = 0.19533973400983914
Model training time: 101.61311864852905
Device: cuda
Iteration 0, Training loss = 0.4183600128562918
Iteration 10, Training loss = 0.24901948840364127
Iteration 20, Training loss = 0.23824162019042358
Iteration 30, Training loss = 0.2302826354888969
Iteration 40, Training loss = 0.22702610561258857
Iteration 50, Training loss = 0.22363418024078408
Iteration 60, Training loss = 0.22128306197557265
Iteration 70, Training loss = 0.21660884607754666
Iteration 80, Training loss = 0.21580613902420456
Iteration 90, Training loss = 0.21397982256320142
Iteration 100, Training loss = 0.21216495658020706
Iteration 110, Training loss = 0.2119923211345517
Iteration 120, Training loss = 0.2089812213658709
Iteration 130, Training loss = 0.20859018419181463
Iteration 140, Training loss = 0.20667592689539271
Iteration 150, Training loss = 0.20672719650529775
Iteration 160, Training loss = 0.2054085914884439
Iteration 170, Training loss = 0.2038956748244982
Iteration 180, Training loss = 0.20447742350886577
Iteration 190, Training loss = 0.20412501023482468
Model training time: 101.8392972946167
Device: cuda
Iteration 0, Training loss = 0.44041135257727876
Iteration 10, Training loss = 0.240848948894921
Iteration 20, Training loss = 0.22618061859728927
Iteration 30, Training loss = 0.22205340827095885
Iteration 40, Training loss = 0.21521394402149516
Iteration 50, Training loss = 0.21193924141240467
Iteration 60, Training loss = 0.20996165206119166
Iteration 70, Training loss = 0.2066884668719826
Iteration 80, Training loss = 0.20592116492351667
Iteration 90, Training loss = 0.2049239495494031
Iteration 100, Training loss = 0.20275806796175516
Iteration 110, Training loss = 0.20369700928721532
Iteration 120, Training loss = 0.20277763964477521
Iteration 130, Training loss = 0.20042784029469074
Iteration 140, Training loss = 0.19872503239688516
Iteration 150, Training loss = 0.199609302336093
Iteration 160, Training loss = 0.20051168364452393
Iteration 170, Training loss = 0.1969998933050159
Iteration 180, Training loss = 0.19611980425099196
Iteration 190, Training loss = 0.19671335890347963
Model training time: 101.86912822723389
Device: cuda
Iteration 0, Training loss = 0.4815855887120919
Iteration 10, Training loss = 0.24335378492318688
Iteration 20, Training loss = 0.22487771269339915
Iteration 30, Training loss = 0.21778627490448893
Iteration 40, Training loss = 0.21177156049460533
Iteration 50, Training loss = 0.20747125825278695
Iteration 60, Training loss = 0.2050661891428235
Iteration 70, Training loss = 0.20331432293315777
Iteration 80, Training loss = 0.20063676309859782
Iteration 90, Training loss = 0.20091249007217532
Iteration 100, Training loss = 0.20059143266579718
Iteration 110, Training loss = 0.19991263542459606
Iteration 120, Training loss = 0.19933908012386672
Iteration 130, Training loss = 0.19818672139116408
Iteration 140, Training loss = 0.19778407411086069
Iteration 150, Training loss = 0.19755363397606926
Iteration 160, Training loss = 0.1970544842682941
Iteration 170, Training loss = 0.19616059800181493
Iteration 180, Training loss = 0.19582494727130664
Iteration 190, Training loss = 0.19407758266455324
Model training time: 102.08241629600525
Device: cuda
Iteration 0, Training loss = 0.4599771682741278
Iteration 10, Training loss = 0.24817890863297348
Iteration 20, Training loss = 0.2335487352030185
Iteration 30, Training loss = 0.22388407033270555
Iteration 40, Training loss = 0.21711705931739714
Iteration 50, Training loss = 0.21316540354894378
Iteration 60, Training loss = 0.20942725920395758
Iteration 70, Training loss = 0.20925720690887142
Iteration 80, Training loss = 0.20649892604091266
Iteration 90, Training loss = 0.20602395272752852
Iteration 100, Training loss = 0.204509492783151
Iteration 110, Training loss = 0.2039614586434699
Iteration 120, Training loss = 0.20317654804152957
Iteration 130, Training loss = 0.20226830955748118
Iteration 140, Training loss = 0.20049800635394405
Iteration 150, Training loss = 0.1998162054914539
Iteration 160, Training loss = 0.20021152423275296
Iteration 170, Training loss = 0.19850402429122613
Iteration 180, Training loss = 0.19773670698157522
Iteration 190, Training loss = 0.19666024115270334
Model training time: 103.21795153617859
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9593456111866394
Iteration 10, Training loss = 0.3130660343004197
Iteration 20, Training loss = 0.29195176295881986
Iteration 30, Training loss = 0.2803017525746516
Iteration 40, Training loss = 0.27347911589442964
Iteration 50, Training loss = 0.2689159498448522
Iteration 60, Training loss = 0.2652728248739358
Iteration 70, Training loss = 0.26241932381801397
Iteration 80, Training loss = 0.2597707561499126
Iteration 90, Training loss = 0.2572300081878684
Iteration 100, Training loss = 0.25523512952083827
Iteration 110, Training loss = 0.25311664304274334
Iteration 120, Training loss = 0.251234667280973
Iteration 130, Training loss = 0.24975847847958166
Iteration 140, Training loss = 0.24799273424662344
Iteration 150, Training loss = 0.24612445718006706
Iteration 160, Training loss = 0.24436824545083843
Iteration 170, Training loss = 0.2433121635355326
Iteration 180, Training loss = 0.2417229345253247
Iteration 190, Training loss = 0.24080376048317545
Model training time: 86.60575580596924
Device: cuda
Iteration 0, Training loss = 0.947467181425695
Iteration 10, Training loss = 0.33335317517957735
Iteration 20, Training loss = 0.309419579479221
Iteration 30, Training loss = 0.2939012696826718
Iteration 40, Training loss = 0.2820448570007562
Iteration 50, Training loss = 0.2726916301697853
Iteration 60, Training loss = 0.26570889449364915
Iteration 70, Training loss = 0.2608223208034587
Iteration 80, Training loss = 0.25757083237243333
Iteration 90, Training loss = 0.2551836804385335
Iteration 100, Training loss = 0.2533103899971625
Iteration 110, Training loss = 0.25178867251979814
Iteration 120, Training loss = 0.250680079011666
Iteration 130, Training loss = 0.2493758388946045
Iteration 140, Training loss = 0.2481897743576664
Iteration 150, Training loss = 0.2472282758131443
Iteration 160, Training loss = 0.24637346043641573
Iteration 170, Training loss = 0.24533776186641135
Iteration 180, Training loss = 0.244987015068964
Iteration 190, Training loss = 0.24396883063108524
Model training time: 87.77166748046875
Device: cuda
Iteration 0, Training loss = 0.9582628913710828
Iteration 10, Training loss = 0.3228241230652061
Iteration 20, Training loss = 0.2945109667306085
Iteration 30, Training loss = 0.2807100308251727
Iteration 40, Training loss = 0.2714653035792687
Iteration 50, Training loss = 0.265010677169801
Iteration 60, Training loss = 0.2605635586936595
Iteration 70, Training loss = 0.25749863630056957
Iteration 80, Training loss = 0.25451027760710615
Iteration 90, Training loss = 0.2514380641083163
Iteration 100, Training loss = 0.2490972304315313
Iteration 110, Training loss = 0.24641100336181915
Iteration 120, Training loss = 0.24487003307004818
Iteration 130, Training loss = 0.2431333418619835
Iteration 140, Training loss = 0.24130693032038703
Iteration 150, Training loss = 0.23970954984282178
Iteration 160, Training loss = 0.23820295136142297
Iteration 170, Training loss = 0.23638424002193364
Iteration 180, Training loss = 0.23502743582423893
Iteration 190, Training loss = 0.23321727711986975
Model training time: 88.4837098121643
Device: cuda
Iteration 0, Training loss = 1.0107686116966728
Iteration 10, Training loss = 0.331584847687809
Iteration 20, Training loss = 0.30386876551204384
Iteration 30, Training loss = 0.28916107902876115
Iteration 40, Training loss = 0.2768914861194158
Iteration 50, Training loss = 0.2681182583582026
Iteration 60, Training loss = 0.26275763671493413
Iteration 70, Training loss = 0.25920895393261323
Iteration 80, Training loss = 0.25649465535729044
Iteration 90, Training loss = 0.2543555274605751
Iteration 100, Training loss = 0.2521413480614923
Iteration 110, Training loss = 0.25008195853334364
Iteration 120, Training loss = 0.24779947349219864
Iteration 130, Training loss = 0.246275738440905
Iteration 140, Training loss = 0.24482790160626533
Iteration 150, Training loss = 0.24332419012562703
Iteration 160, Training loss = 0.241706467082512
Iteration 170, Training loss = 0.2402167487844428
Iteration 180, Training loss = 0.23861518909942728
Iteration 190, Training loss = 0.23733751483027063
Model training time: 86.02315664291382
Device: cuda
Iteration 0, Training loss = 0.9890811610019813
Iteration 10, Training loss = 0.3849534790322509
Iteration 20, Training loss = 0.30762923373581424
Iteration 30, Training loss = 0.27999265504590537
Iteration 40, Training loss = 0.2694526535366407
Iteration 50, Training loss = 0.2631535375666676
Iteration 60, Training loss = 0.2593661043026257
Iteration 70, Training loss = 0.2563664480213969
Iteration 80, Training loss = 0.2542604922743167
Iteration 90, Training loss = 0.2519906527626601
Iteration 100, Training loss = 0.2498241125202352
Iteration 110, Training loss = 0.24796892376052843
Iteration 120, Training loss = 0.24573008192942158
Iteration 130, Training loss = 0.24344255638108125
Iteration 140, Training loss = 0.2410330593441647
Iteration 150, Training loss = 0.23901800603829054
Iteration 160, Training loss = 0.23684876258811996
Iteration 170, Training loss = 0.23507436382929292
Iteration 180, Training loss = 0.23340268250888543
Iteration 190, Training loss = 0.23198569845633704
Model training time: 87.9608473777771
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.46874085669439586
Iteration 10, Training loss = 0.2516760674032403
Iteration 20, Training loss = 0.24089515477996185
Iteration 30, Training loss = 0.23528859971799226
Iteration 40, Training loss = 0.2316754470798063
Iteration 50, Training loss = 0.2289585677171735
Iteration 60, Training loss = 0.2291017012179042
Iteration 70, Training loss = 0.22690027347849298
Iteration 80, Training loss = 0.22484142594851247
Iteration 90, Training loss = 0.22444265852648468
Iteration 100, Training loss = 0.22338313336955434
Iteration 110, Training loss = 0.2230014708479438
Iteration 120, Training loss = 0.22321338566430546
Iteration 130, Training loss = 0.22167813512733428
Iteration 140, Training loss = 0.22041761621688527
Iteration 150, Training loss = 0.22047176692842282
Iteration 160, Training loss = 0.2208762652811213
Iteration 170, Training loss = 0.2208622069555824
Iteration 180, Training loss = 0.22106448898303885
Iteration 190, Training loss = 0.2203739458293661
Model training time: 104.35274028778076
Device: cuda
Iteration 0, Training loss = 0.6603659894001686
Iteration 10, Training loss = 0.26127144488670634
Iteration 20, Training loss = 0.251953594188136
Iteration 30, Training loss = 0.24662169927762725
Iteration 40, Training loss = 0.24364768926586425
Iteration 50, Training loss = 0.24205927422707652
Iteration 60, Training loss = 0.23984886763603866
Iteration 70, Training loss = 0.23782629940036423
Iteration 80, Training loss = 0.23463283580238536
Iteration 90, Training loss = 0.23264882083053162
Iteration 100, Training loss = 0.23006537343774522
Iteration 110, Training loss = 0.2280066044365234
Iteration 120, Training loss = 0.2260030311944987
Iteration 130, Training loss = 0.22408233889655974
Iteration 140, Training loss = 0.22485080123236334
Iteration 150, Training loss = 0.22373593974741263
Iteration 160, Training loss = 0.22346136368107564
Iteration 170, Training loss = 0.2234411833686055
Iteration 180, Training loss = 0.22422959846748855
Iteration 190, Training loss = 0.22280740983261035
Model training time: 108.90446853637695
Device: cuda
Iteration 0, Training loss = 0.48358631837598925
Iteration 10, Training loss = 0.2554686284256472
Iteration 20, Training loss = 0.24288574057759732
Iteration 30, Training loss = 0.23816625975956351
Iteration 40, Training loss = 0.23478082045191137
Iteration 50, Training loss = 0.2312929394224221
Iteration 60, Training loss = 0.23028711630054016
Iteration 70, Training loss = 0.22874739030082925
Iteration 80, Training loss = 0.22424421911342093
Iteration 90, Training loss = 0.222384158566991
Iteration 100, Training loss = 0.22234668719782957
Iteration 110, Training loss = 0.2198160895423797
Iteration 120, Training loss = 0.21905462322057592
Iteration 130, Training loss = 0.21669950256841233
Iteration 140, Training loss = 0.21799726998005017
Iteration 150, Training loss = 0.21693868904439936
Iteration 160, Training loss = 0.21610216298490112
Iteration 170, Training loss = 0.21403499518591035
Iteration 180, Training loss = 0.21305801436196806
Iteration 190, Training loss = 0.21164362126235234
Model training time: 109.45147800445557
Device: cuda
Iteration 0, Training loss = 0.6417036749801394
Iteration 10, Training loss = 0.2674341494213005
Iteration 20, Training loss = 0.25136064652344503
Iteration 30, Training loss = 0.2463042738037883
Iteration 40, Training loss = 0.24325580014587891
Iteration 50, Training loss = 0.242464994240471
Iteration 60, Training loss = 0.23701102936022506
Iteration 70, Training loss = 0.2363847164351484
Iteration 80, Training loss = 0.23597411547063626
Iteration 90, Training loss = 0.23473989716381485
Iteration 100, Training loss = 0.2337482134308711
Iteration 110, Training loss = 0.23325871246851096
Iteration 120, Training loss = 0.2331741120098001
Iteration 130, Training loss = 0.2310810480943315
Iteration 140, Training loss = 0.23138881137382608
Iteration 150, Training loss = 0.23094441705624646
Iteration 160, Training loss = 0.22984782426863837
Iteration 170, Training loss = 0.22977848186836405
Iteration 180, Training loss = 0.22937191839692955
Iteration 190, Training loss = 0.23002426230662093
Model training time: 101.07923102378845
Device: cuda
Iteration 0, Training loss = 0.6581233129735143
Iteration 10, Training loss = 0.2550203039448429
Iteration 20, Training loss = 0.24514682630649778
Iteration 30, Training loss = 0.2416704632612464
Iteration 40, Training loss = 0.24001816578985127
Iteration 50, Training loss = 0.2362946516159949
Iteration 60, Training loss = 0.2326395319680036
Iteration 70, Training loss = 0.2310216180781764
Iteration 80, Training loss = 0.22741686558175028
Iteration 90, Training loss = 0.22800480507620888
Iteration 100, Training loss = 0.2252788377100417
Iteration 110, Training loss = 0.22442497641353285
Iteration 120, Training loss = 0.2232775622685654
Iteration 130, Training loss = 0.22377781869424168
Iteration 140, Training loss = 0.2222048113929879
Iteration 150, Training loss = 0.22146034616859717
Iteration 160, Training loss = 0.22118147085567363
Iteration 170, Training loss = 0.2214998943284695
Iteration 180, Training loss = 0.219965682705508
Iteration 190, Training loss = 0.21967833705661372
Model training time: 100.98342847824097
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0468308860684135
Iteration 10, Training loss = 0.3274055872716857
Iteration 20, Training loss = 0.3047008017037452
Iteration 30, Training loss = 0.29044645073988246
Iteration 40, Training loss = 0.2806147607381638
Iteration 50, Training loss = 0.27466509004121253
Iteration 60, Training loss = 0.2705325530458593
Iteration 70, Training loss = 0.267635459021708
Iteration 80, Training loss = 0.2656028192066396
Iteration 90, Training loss = 0.2639418045739862
Iteration 100, Training loss = 0.2624379076767199
Iteration 110, Training loss = 0.26065979934056216
Iteration 120, Training loss = 0.2595793152403889
Iteration 130, Training loss = 0.25808385658567234
Iteration 140, Training loss = 0.2569676381659854
Iteration 150, Training loss = 0.25563695775142015
Iteration 160, Training loss = 0.2545542642293773
Iteration 170, Training loss = 0.2536445510986354
Iteration 180, Training loss = 0.2523532079726674
Iteration 190, Training loss = 0.2518718257077381
Model training time: 87.32514023780823
Device: cuda
Iteration 0, Training loss = 1.0123349993194275
Iteration 10, Training loss = 0.3246958981883728
Iteration 20, Training loss = 0.3006429393160141
Iteration 30, Training loss = 0.2861452463644469
Iteration 40, Training loss = 0.27623088463022405
Iteration 50, Training loss = 0.2696917397635324
Iteration 60, Training loss = 0.2643690370293853
Iteration 70, Training loss = 0.2610352284141949
Iteration 80, Training loss = 0.25873158900991766
Iteration 90, Training loss = 0.2567315584738664
Iteration 100, Training loss = 0.2554327495594579
Iteration 110, Training loss = 0.25381620022632884
Iteration 120, Training loss = 0.2523434979809543
Iteration 130, Training loss = 0.25073086561468266
Iteration 140, Training loss = 0.24977728450269446
Iteration 150, Training loss = 0.24820471458479798
Iteration 160, Training loss = 0.24738804748863633
Iteration 170, Training loss = 0.246335897188787
Iteration 180, Training loss = 0.24514920296052756
Iteration 190, Training loss = 0.24465858690963818
Model training time: 86.4905195236206
Device: cuda
Iteration 0, Training loss = 0.953773467145301
Iteration 10, Training loss = 0.3353499089509754
Iteration 20, Training loss = 0.3132149752851837
Iteration 30, Training loss = 0.29956547360298996
Iteration 40, Training loss = 0.28586594706297497
Iteration 50, Training loss = 0.2749167631490756
Iteration 60, Training loss = 0.2696469617741449
Iteration 70, Training loss = 0.2670503297671856
Iteration 80, Training loss = 0.26434870129203103
Iteration 90, Training loss = 0.2620625636984592
Iteration 100, Training loss = 0.2601645238715569
Iteration 110, Training loss = 0.2580277227388456
Iteration 120, Training loss = 0.2565964479315079
Iteration 130, Training loss = 0.25503838672945345
Iteration 140, Training loss = 0.25399438445129346
Iteration 150, Training loss = 0.2530451245227103
Iteration 160, Training loss = 0.25201667225570135
Iteration 170, Training loss = 0.2515354304333143
Iteration 180, Training loss = 0.2509339894134254
Iteration 190, Training loss = 0.25004932601861746
Model training time: 88.72793364524841
Device: cuda
Iteration 0, Training loss = 0.9970558490649263
Iteration 10, Training loss = 0.33185345708385794
Iteration 20, Training loss = 0.30128245798929554
Iteration 30, Training loss = 0.28410995598278094
Iteration 40, Training loss = 0.27481235374333496
Iteration 50, Training loss = 0.2689741666972204
Iteration 60, Training loss = 0.26441687502454037
Iteration 70, Training loss = 0.260588296551537
Iteration 80, Training loss = 0.25800958728602663
Iteration 90, Training loss = 0.2557929539579456
Iteration 100, Training loss = 0.2536785346940701
Iteration 110, Training loss = 0.2513942808886995
Iteration 120, Training loss = 0.24951793098276523
Iteration 130, Training loss = 0.24694612981259967
Iteration 140, Training loss = 0.24481477276320607
Iteration 150, Training loss = 0.2428408771861552
Iteration 160, Training loss = 0.24132389654619643
Iteration 170, Training loss = 0.23938807790093214
Iteration 180, Training loss = 0.2377464798330972
Iteration 190, Training loss = 0.23646131751395888
Model training time: 89.54827737808228
Device: cuda
Iteration 0, Training loss = 1.0157725551226526
Iteration 10, Training loss = 0.3623704072313505
Iteration 20, Training loss = 0.2918491986451657
Iteration 30, Training loss = 0.27977962717955107
Iteration 40, Training loss = 0.27159908781906017
Iteration 50, Training loss = 0.26595070418737127
Iteration 60, Training loss = 0.2621626192230289
Iteration 70, Training loss = 0.2591598961529374
Iteration 80, Training loss = 0.2566569206681436
Iteration 90, Training loss = 0.25441433404533975
Iteration 100, Training loss = 0.25246328658471673
Iteration 110, Training loss = 0.2507535262488857
Iteration 120, Training loss = 0.24917388297888038
Iteration 130, Training loss = 0.24763906530620977
Iteration 140, Training loss = 0.24633141800075697
Iteration 150, Training loss = 0.24555001151691627
Iteration 160, Training loss = 0.2444084848910931
Iteration 170, Training loss = 0.24330384405126873
Iteration 180, Training loss = 0.2425959328545785
Iteration 190, Training loss = 0.2417269143440533
Model training time: 86.76459288597107
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.4167328994297231
Iteration 10, Training loss = 0.249089620213676
Iteration 20, Training loss = 0.23672607127062922
Iteration 30, Training loss = 0.2286973614110352
Iteration 40, Training loss = 0.2227314349092525
Iteration 50, Training loss = 0.2186616116246213
Iteration 60, Training loss = 0.21606537847628604
Iteration 70, Training loss = 0.21378871146364015
Iteration 80, Training loss = 0.21424253777427188
Iteration 90, Training loss = 0.2123246492849568
Iteration 100, Training loss = 0.21056446918347269
Iteration 110, Training loss = 0.2100600598799403
Iteration 120, Training loss = 0.20937338407370137
Iteration 130, Training loss = 0.20906884526297198
Iteration 140, Training loss = 0.20687392678734176
Iteration 150, Training loss = 0.2063551473162942
Iteration 160, Training loss = 0.2058844648247481
Iteration 170, Training loss = 0.2058507745260188
Iteration 180, Training loss = 0.20522681857317182
Iteration 190, Training loss = 0.20458223582324336
Model training time: 101.72046947479248
Device: cuda
Iteration 0, Training loss = 0.47678707983822854
Iteration 10, Training loss = 0.2478702163580832
Iteration 20, Training loss = 0.23869589505126343
Iteration 30, Training loss = 0.23134304951091655
Iteration 40, Training loss = 0.22578075890751786
Iteration 50, Training loss = 0.22156049960606322
Iteration 60, Training loss = 0.21665976299356318
Iteration 70, Training loss = 0.21268595807416676
Iteration 80, Training loss = 0.21061625825038258
Iteration 90, Training loss = 0.2095480804617988
Iteration 100, Training loss = 0.21043923106630835
Iteration 110, Training loss = 0.2081416600336463
Iteration 120, Training loss = 0.205718799749024
Iteration 130, Training loss = 0.2047289761175688
Iteration 140, Training loss = 0.2044418623747606
Iteration 150, Training loss = 0.20279807116177845
Iteration 160, Training loss = 0.20353972550129776
Iteration 170, Training loss = 0.20210266640053534
Iteration 180, Training loss = 0.20053866904310105
Iteration 190, Training loss = 0.20051419077391486
Model training time: 103.87214541435242
Device: cuda
Iteration 0, Training loss = 0.44743245006617854
Iteration 10, Training loss = 0.24522437932777058
Iteration 20, Training loss = 0.23190615254005567
Iteration 30, Training loss = 0.22509089092437637
Iteration 40, Training loss = 0.2220296022678403
Iteration 50, Training loss = 0.22010138278182137
Iteration 60, Training loss = 0.2167374450109051
Iteration 70, Training loss = 0.2138659059369968
Iteration 80, Training loss = 0.21355647495417848
Iteration 90, Training loss = 0.21087479239586768
Iteration 100, Training loss = 0.20748725885268274
Iteration 110, Training loss = 0.2050036010997636
Iteration 120, Training loss = 0.2041927304290253
Iteration 130, Training loss = 0.20395889544530296
Iteration 140, Training loss = 0.20218328049194437
Iteration 150, Training loss = 0.2011664411725057
Iteration 160, Training loss = 0.20074516799093448
Iteration 170, Training loss = 0.20216795987027897
Iteration 180, Training loss = 0.20045592428408293
Iteration 190, Training loss = 0.19886414817653902
Model training time: 103.5119047164917
Device: cuda
Iteration 0, Training loss = 0.47050457252933675
Iteration 10, Training loss = 0.24628292976655336
Iteration 20, Training loss = 0.22863256160990666
Iteration 30, Training loss = 0.22095858457229905
Iteration 40, Training loss = 0.21579075041824622
Iteration 50, Training loss = 0.21084909243892527
Iteration 60, Training loss = 0.20887441345046276
Iteration 70, Training loss = 0.206855559220975
Iteration 80, Training loss = 0.20508612508346613
Iteration 90, Training loss = 0.20450665113531938
Iteration 100, Training loss = 0.20313728386062688
Iteration 110, Training loss = 0.20273086682106334
Iteration 120, Training loss = 0.2012660339222116
Iteration 130, Training loss = 0.2023717961465764
Iteration 140, Training loss = 0.20112816250677837
Iteration 150, Training loss = 0.20007429739176216
Iteration 160, Training loss = 0.20123055432956963
Iteration 170, Training loss = 0.19977535135123978
Iteration 180, Training loss = 0.19994529269040354
Iteration 190, Training loss = 0.20118775192566993
Model training time: 102.72738790512085
Device: cuda
Iteration 0, Training loss = 0.4248315774211006
Iteration 10, Training loss = 0.24619312219087205
Iteration 20, Training loss = 0.23406660278072947
Iteration 30, Training loss = 0.22270884683251957
Iteration 40, Training loss = 0.21752850317168468
Iteration 50, Training loss = 0.21338506523727216
Iteration 60, Training loss = 0.2116423438215227
Iteration 70, Training loss = 0.2086907429552973
Iteration 80, Training loss = 0.20780355595142444
Iteration 90, Training loss = 0.20666243448925653
Iteration 100, Training loss = 0.20394254530854433
Iteration 110, Training loss = 0.20446089831954342
Iteration 120, Training loss = 0.20424095528610683
Iteration 130, Training loss = 0.2038318369149729
Iteration 140, Training loss = 0.20238233634584174
Iteration 150, Training loss = 0.2033747532483885
Iteration 160, Training loss = 0.20206863784905496
Iteration 170, Training loss = 0.20055453601963005
Iteration 180, Training loss = 0.20178735755005126
Iteration 190, Training loss = 0.20126807457850862
Model training time: 102.27408289909363
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9897389440790504
Iteration 10, Training loss = 0.33350246854252735
Iteration 20, Training loss = 0.3069321194767375
Iteration 30, Training loss = 0.29324637916272833
Iteration 40, Training loss = 0.2834128793374101
Iteration 50, Training loss = 0.2753379496477418
Iteration 60, Training loss = 0.2689330799563745
Iteration 70, Training loss = 0.2647609450013239
Iteration 80, Training loss = 0.2617515578829925
Iteration 90, Training loss = 0.2592850013412806
Iteration 100, Training loss = 0.2570822981995763
Iteration 110, Training loss = 0.2555130256145976
Iteration 120, Training loss = 0.25387346340438066
Iteration 130, Training loss = 0.2529209576565186
Iteration 140, Training loss = 0.25142931962525583
Iteration 150, Training loss = 0.25049134827338465
Iteration 160, Training loss = 0.24940655972204254
Iteration 170, Training loss = 0.24844604626875236
Iteration 180, Training loss = 0.24748598601100807
Iteration 190, Training loss = 0.24707755219128172
Model training time: 86.97436833381653
Device: cuda
Iteration 0, Training loss = 0.9678151557001017
Iteration 10, Training loss = 0.3248426907720635
Iteration 20, Training loss = 0.2993927028472141
Iteration 30, Training loss = 0.286851117148382
Iteration 40, Training loss = 0.27781938835437303
Iteration 50, Training loss = 0.2704602516916993
Iteration 60, Training loss = 0.26526005572434197
Iteration 70, Training loss = 0.26177677993840803
Iteration 80, Training loss = 0.2591438540065837
Iteration 90, Training loss = 0.2574531182141627
Iteration 100, Training loss = 0.2553889151148444
Iteration 110, Training loss = 0.2534346294027841
Iteration 120, Training loss = 0.25163076555505215
Iteration 130, Training loss = 0.250162764229151
Iteration 140, Training loss = 0.24885707135711396
Iteration 150, Training loss = 0.2475452953115214
Iteration 160, Training loss = 0.24644323000030424
Iteration 170, Training loss = 0.24542311315602885
Iteration 180, Training loss = 0.24417280615025513
Iteration 190, Training loss = 0.2431147556580585
Model training time: 86.63952088356018
Device: cuda
Iteration 0, Training loss = 0.9613345337549075
Iteration 10, Training loss = 0.318784043752108
Iteration 20, Training loss = 0.2983908437318721
Iteration 30, Training loss = 0.2871323685019703
Iteration 40, Training loss = 0.2785441120658025
Iteration 50, Training loss = 0.27146705290523626
Iteration 60, Training loss = 0.2668991785938457
Iteration 70, Training loss = 0.26311306826788344
Iteration 80, Training loss = 0.2601082801963169
Iteration 90, Training loss = 0.2571919366140054
Iteration 100, Training loss = 0.25482448136330227
Iteration 110, Training loss = 0.2529224193413667
Iteration 120, Training loss = 0.25131826037789085
Iteration 130, Training loss = 0.24990005720830713
Iteration 140, Training loss = 0.2483082460036578
Iteration 150, Training loss = 0.24728062688474506
Iteration 160, Training loss = 0.24562831933792798
Iteration 170, Training loss = 0.24483583753896973
Iteration 180, Training loss = 0.24355335919678067
Iteration 190, Training loss = 0.2430537351684478
Model training time: 87.25047588348389
Device: cuda
Iteration 0, Training loss = 0.9964081287816997
Iteration 10, Training loss = 0.3828738476837518
Iteration 20, Training loss = 0.3130461667770335
Iteration 30, Training loss = 0.29510513456768334
Iteration 40, Training loss = 0.2830979898932771
Iteration 50, Training loss = 0.2747564030581179
Iteration 60, Training loss = 0.26820639452331
Iteration 70, Training loss = 0.26419970015239774
Iteration 80, Training loss = 0.2611539542638938
Iteration 90, Training loss = 0.25818283846894996
Iteration 100, Training loss = 0.25588435721166486
Iteration 110, Training loss = 0.2540657817235005
Iteration 120, Training loss = 0.2523539969584844
Iteration 130, Training loss = 0.2507392977326026
Iteration 140, Training loss = 0.24944336824209293
Iteration 150, Training loss = 0.24717494883274627
Iteration 160, Training loss = 0.2453938969670283
Iteration 170, Training loss = 0.2433050463458672
Iteration 180, Training loss = 0.24182260812194814
Iteration 190, Training loss = 0.24044957919357476
Model training time: 86.90329051017761
Device: cuda
Iteration 0, Training loss = 0.9630538746173387
Iteration 10, Training loss = 0.3241201951089552
Iteration 20, Training loss = 0.30403205124575633
Iteration 30, Training loss = 0.2915666681369338
Iteration 40, Training loss = 0.2788211000636761
Iteration 50, Training loss = 0.2686521868259797
Iteration 60, Training loss = 0.26249455869847294
Iteration 70, Training loss = 0.25880025233396897
Iteration 80, Training loss = 0.25625917084569216
Iteration 90, Training loss = 0.2545825250753479
Iteration 100, Training loss = 0.252661178551488
Iteration 110, Training loss = 0.2508971979954341
Iteration 120, Training loss = 0.24934242456501968
Iteration 130, Training loss = 0.24812842911835445
Iteration 140, Training loss = 0.24668478143128586
Iteration 150, Training loss = 0.24536930183377162
Iteration 160, Training loss = 0.2442183764809269
Iteration 170, Training loss = 0.24340360727134108
Iteration 180, Training loss = 0.2423882710298672
Iteration 190, Training loss = 0.24135040152664625
Model training time: 86.16754102706909
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.4233659175168227
Iteration 10, Training loss = 0.24985638075027858
Iteration 20, Training loss = 0.236552177912603
Iteration 30, Training loss = 0.22919419654994552
Iteration 40, Training loss = 0.22045283116959485
Iteration 50, Training loss = 0.21551655048896845
Iteration 60, Training loss = 0.2132805489346421
Iteration 70, Training loss = 0.2084781672471805
Iteration 80, Training loss = 0.20814223944707874
Iteration 90, Training loss = 0.205004381778093
Iteration 100, Training loss = 0.20360059962217802
Iteration 110, Training loss = 0.2033184065909709
Iteration 120, Training loss = 0.20280125754328385
Iteration 130, Training loss = 0.20375671966529066
Iteration 140, Training loss = 0.20149648792085578
Iteration 150, Training loss = 0.20254849757070115
Iteration 160, Training loss = 0.2005333251367181
Iteration 170, Training loss = 0.20081693604227704
Iteration 180, Training loss = 0.19873081135042645
Iteration 190, Training loss = 0.19933565761939087
Iteration 200, Training loss = 0.1990068815558355
Iteration 210, Training loss = 0.19815923533341498
Iteration 220, Training loss = 0.19703198326232935
Iteration 230, Training loss = 0.1963832101471199
Iteration 240, Training loss = 0.19694311077961332
Iteration 250, Training loss = 0.19597927231080017
Iteration 260, Training loss = 0.19510238659150952
Iteration 270, Training loss = 0.19572404953346703
Iteration 280, Training loss = 0.1951785250487974
Iteration 290, Training loss = 0.19610078493020436
Model training time: 155.96575045585632
Device: cuda
Iteration 0, Training loss = 0.40889498692447857
Iteration 10, Training loss = 0.24435745249107732
Iteration 20, Training loss = 0.23364192061577235
Iteration 30, Training loss = 0.22570351833256624
Iteration 40, Training loss = 0.22169453187081195
Iteration 50, Training loss = 0.21662516076541696
Iteration 60, Training loss = 0.2161337341075948
Iteration 70, Training loss = 0.21175504250330152
Iteration 80, Training loss = 0.21069435205211362
Iteration 90, Training loss = 0.20699389304144908
Iteration 100, Training loss = 0.20713985861069353
Iteration 110, Training loss = 0.20666670740624896
Iteration 120, Training loss = 0.204084587100776
Iteration 130, Training loss = 0.2033507839819421
Iteration 140, Training loss = 0.2035319774206412
Iteration 150, Training loss = 0.20193427708225448
Iteration 160, Training loss = 0.20158746383055937
Iteration 170, Training loss = 0.20096645442899722
Iteration 180, Training loss = 0.19979878175850355
Iteration 190, Training loss = 0.19995070935269824
Iteration 200, Training loss = 0.19893086261451967
Iteration 210, Training loss = 0.19925405985290143
Iteration 220, Training loss = 0.19918642673848905
Iteration 230, Training loss = 0.198175715232949
Iteration 240, Training loss = 0.1975481497083247
Iteration 250, Training loss = 0.1978862166278443
Iteration 260, Training loss = 0.19846006351146514
Iteration 270, Training loss = 0.19753832034496072
Iteration 280, Training loss = 0.19749180023664423
Iteration 290, Training loss = 0.19577668218469793
Model training time: 163.3315830230713
Device: cuda
Iteration 0, Training loss = 0.41546983955558797
Iteration 10, Training loss = 0.244796867214017
Iteration 20, Training loss = 0.22910542205228643
Iteration 30, Training loss = 0.2206128069242467
Iteration 40, Training loss = 0.21693380953649344
Iteration 50, Training loss = 0.21571542456132736
Iteration 60, Training loss = 0.2141894881791699
Iteration 70, Training loss = 0.21166150858197316
Iteration 80, Training loss = 0.2116166868001295
Iteration 90, Training loss = 0.20878741756694946
Iteration 100, Training loss = 0.20736868306092432
Iteration 110, Training loss = 0.20635461513196585
Iteration 120, Training loss = 0.20486814548547852
Iteration 130, Training loss = 0.20482848914786633
Iteration 140, Training loss = 0.2021108663237268
Iteration 150, Training loss = 0.201321259961795
Iteration 160, Training loss = 0.20009636538008513
Iteration 170, Training loss = 0.201912843223129
Iteration 180, Training loss = 0.19946470957691387
Iteration 190, Training loss = 0.19918215878253698
Iteration 200, Training loss = 0.200222959367617
Iteration 210, Training loss = 0.1970289554762638
Iteration 220, Training loss = 0.19703415368140176
Iteration 230, Training loss = 0.19736463243714544
Iteration 240, Training loss = 0.19672547888741365
Iteration 250, Training loss = 0.19600720457353835
Iteration 260, Training loss = 0.19548826026065008
Iteration 270, Training loss = 0.19545733846597751
Iteration 280, Training loss = 0.1943190508222176
Iteration 290, Training loss = 0.1952922510357515
Model training time: 158.45014023780823
Device: cuda
Iteration 0, Training loss = 0.43642396835235936
Iteration 10, Training loss = 0.24955586538837263
Iteration 20, Training loss = 0.2353904585085683
Iteration 30, Training loss = 0.2279752370051264
Iteration 40, Training loss = 0.22029347562616733
Iteration 50, Training loss = 0.21399630123600544
Iteration 60, Training loss = 0.20853796447052217
Iteration 70, Training loss = 0.20618513887680473
Iteration 80, Training loss = 0.2035357112709893
Iteration 90, Training loss = 0.2029548974991855
Iteration 100, Training loss = 0.2016440946744083
Iteration 110, Training loss = 0.2005760032079266
Iteration 120, Training loss = 0.20030121691975986
Iteration 130, Training loss = 0.1992169850332252
Iteration 140, Training loss = 0.1977679993208615
Iteration 150, Training loss = 0.19722126730093079
Iteration 160, Training loss = 0.19768949931816673
Iteration 170, Training loss = 0.1966224149067956
Iteration 180, Training loss = 0.1963817786795175
Iteration 190, Training loss = 0.1955254545872783
Iteration 200, Training loss = 0.19546646339769225
Iteration 210, Training loss = 0.19517722575232999
Iteration 220, Training loss = 0.19585017316025335
Iteration 230, Training loss = 0.19484129570623
Iteration 240, Training loss = 0.19560958285416877
Iteration 250, Training loss = 0.19363623106126057
Iteration 260, Training loss = 0.19445052334348745
Iteration 270, Training loss = 0.1945452062237205
Iteration 280, Training loss = 0.1941648555184392
Iteration 290, Training loss = 0.19423747032014857
Model training time: 153.26736736297607
Device: cuda
Iteration 0, Training loss = 0.45147379893656214
Iteration 10, Training loss = 0.24971380544776778
Iteration 20, Training loss = 0.23113712399620698
Iteration 30, Training loss = 0.2234984793616842
Iteration 40, Training loss = 0.21821537779838063
Iteration 50, Training loss = 0.21368109675679023
Iteration 60, Training loss = 0.21007745731035676
Iteration 70, Training loss = 0.2092620429851241
Iteration 80, Training loss = 0.20437392398498538
Iteration 90, Training loss = 0.20405233667635744
Iteration 100, Training loss = 0.2014406399742743
Iteration 110, Training loss = 0.19979769514634593
Iteration 120, Training loss = 0.19827053015803597
Iteration 130, Training loss = 0.19836883749316736
Iteration 140, Training loss = 0.19850411031566578
Iteration 150, Training loss = 0.19797982585776516
Iteration 160, Training loss = 0.19711677668746966
Iteration 170, Training loss = 0.19672295177856022
Iteration 180, Training loss = 0.19692644098047482
Iteration 190, Training loss = 0.19747932397785256
Iteration 200, Training loss = 0.19758328105397432
Iteration 210, Training loss = 0.19624696229077135
Iteration 220, Training loss = 0.1957078859047653
Iteration 230, Training loss = 0.19604203362946937
Iteration 240, Training loss = 0.19455487272965993
Iteration 250, Training loss = 0.19547722152193003
Iteration 260, Training loss = 0.1945748942347185
Iteration 270, Training loss = 0.1947131219159893
Iteration 280, Training loss = 0.194676373532768
Iteration 290, Training loss = 0.19389810716196643
Model training time: 152.29351663589478
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0002835517068174
Iteration 10, Training loss = 0.3507631903302294
Iteration 20, Training loss = 0.3158030571826434
Iteration 30, Training loss = 0.2990307433950122
Iteration 40, Training loss = 0.2860836983629058
Iteration 50, Training loss = 0.2771374035545469
Iteration 60, Training loss = 0.2703059311712625
Iteration 70, Training loss = 0.2656659589805268
Iteration 80, Training loss = 0.2617506764414235
Iteration 90, Training loss = 0.25830542572185433
Iteration 100, Training loss = 0.2554303373933993
Iteration 110, Training loss = 0.25299819062754836
Iteration 120, Training loss = 0.2508238502586436
Iteration 130, Training loss = 0.24874741972827738
Iteration 140, Training loss = 0.24668875939187934
Iteration 150, Training loss = 0.24477533790447523
Iteration 160, Training loss = 0.24318716242079585
Iteration 170, Training loss = 0.24166152710162697
Iteration 180, Training loss = 0.24012250925885564
Iteration 190, Training loss = 0.23862756488362177
Iteration 200, Training loss = 0.23741329214186127
Iteration 210, Training loss = 0.2363585545692692
Iteration 220, Training loss = 0.23504431426669437
Iteration 230, Training loss = 0.234126245192552
Iteration 240, Training loss = 0.23292138955170247
Iteration 250, Training loss = 0.23212095499760302
Iteration 260, Training loss = 0.23105487317640613
Iteration 270, Training loss = 0.22990949939007332
Iteration 280, Training loss = 0.22855987233847164
Iteration 290, Training loss = 0.22812725109280454
Model training time: 128.88444805145264
Device: cuda
Iteration 0, Training loss = 0.9966433137727418
Iteration 10, Training loss = 0.34912852472095746
Iteration 20, Training loss = 0.3086551372818739
Iteration 30, Training loss = 0.2935478052634015
Iteration 40, Training loss = 0.28165087884982043
Iteration 50, Training loss = 0.272672150671915
Iteration 60, Training loss = 0.26674217656795973
Iteration 70, Training loss = 0.2620649237465339
Iteration 80, Training loss = 0.2591293957309919
Iteration 90, Training loss = 0.2565145140049244
Iteration 100, Training loss = 0.254154490374624
Iteration 110, Training loss = 0.25191931332716355
Iteration 120, Training loss = 0.24982449226532374
Iteration 130, Training loss = 0.2477757450346508
Iteration 140, Training loss = 0.24602890628013427
Iteration 150, Training loss = 0.24434957278049022
Iteration 160, Training loss = 0.242698233962492
Iteration 170, Training loss = 0.24126645805399874
Iteration 180, Training loss = 0.23970806732160416
Iteration 190, Training loss = 0.23827658901239135
Iteration 200, Training loss = 0.23646248866224404
Iteration 210, Training loss = 0.23554966504940397
Iteration 220, Training loss = 0.23408773682163644
Iteration 230, Training loss = 0.23290305882331533
Iteration 240, Training loss = 0.23170515341057327
Iteration 250, Training loss = 0.2308509894996232
Iteration 260, Training loss = 0.22980474142163776
Iteration 270, Training loss = 0.22863524330477739
Iteration 280, Training loss = 0.2274064698209197
Iteration 290, Training loss = 0.22609350547988247
Model training time: 132.1166443824768
Device: cuda
Iteration 0, Training loss = 0.9471780867755557
Iteration 10, Training loss = 0.3368941775498321
Iteration 20, Training loss = 0.3078462324910245
Iteration 30, Training loss = 0.29169619022068044
Iteration 40, Training loss = 0.27822064581275274
Iteration 50, Training loss = 0.26958233394071496
Iteration 60, Training loss = 0.2624998029150051
Iteration 70, Training loss = 0.2578044612564994
Iteration 80, Training loss = 0.25432224328162883
Iteration 90, Training loss = 0.2514623650879606
Iteration 100, Training loss = 0.24885984418178586
Iteration 110, Training loss = 0.2468236581609555
Iteration 120, Training loss = 0.24444545551088184
Iteration 130, Training loss = 0.2428915982853991
Iteration 140, Training loss = 0.2411427792639767
Iteration 150, Training loss = 0.23965828537435854
Iteration 160, Training loss = 0.23850182937255207
Iteration 170, Training loss = 0.2370488913256233
Iteration 180, Training loss = 0.23559048155787204
Iteration 190, Training loss = 0.23442965643097066
Iteration 200, Training loss = 0.23276532683115606
Iteration 210, Training loss = 0.23145118746320215
Iteration 220, Training loss = 0.23028617157413653
Iteration 230, Training loss = 0.22885324983670405
Iteration 240, Training loss = 0.22721777116703928
Iteration 250, Training loss = 0.2257414480747957
Iteration 260, Training loss = 0.22397007430581145
Iteration 270, Training loss = 0.22283556090428813
Iteration 280, Training loss = 0.22116593307574206
Iteration 290, Training loss = 0.22020833898723557
Model training time: 128.41688561439514
Device: cuda
Iteration 0, Training loss = 0.9598812875822728
Iteration 10, Training loss = 0.3317115374324397
Iteration 20, Training loss = 0.3075846674836288
Iteration 30, Training loss = 0.2932610239914774
Iteration 40, Training loss = 0.2832186187619736
Iteration 50, Training loss = 0.2754322176948009
Iteration 60, Training loss = 0.26934228967379137
Iteration 70, Training loss = 0.26471356006641367
Iteration 80, Training loss = 0.26152340062954815
Iteration 90, Training loss = 0.2580769000164533
Iteration 100, Training loss = 0.2552718280952144
Iteration 110, Training loss = 0.2523412671454305
Iteration 120, Training loss = 0.2502347459119111
Iteration 130, Training loss = 0.24866292598605444
Iteration 140, Training loss = 0.24697925900864542
Iteration 150, Training loss = 0.24545707032264866
Iteration 160, Training loss = 0.2438504231358556
Iteration 170, Training loss = 0.2424244903587256
Iteration 180, Training loss = 0.24118540347632716
Iteration 190, Training loss = 0.23990072751052444
Iteration 200, Training loss = 0.23888719080074647
Iteration 210, Training loss = 0.23736252015136344
Iteration 220, Training loss = 0.23607080498344962
Iteration 230, Training loss = 0.23446915565694504
Iteration 240, Training loss = 0.23328459690399378
Iteration 250, Training loss = 0.2312760722550584
Iteration 260, Training loss = 0.22984113847300158
Iteration 270, Training loss = 0.2283380474651697
Iteration 280, Training loss = 0.22673136627270005
Iteration 290, Training loss = 0.22520534317155727
Model training time: 128.5042655467987
Device: cuda
Iteration 0, Training loss = 0.9466654592651432
Iteration 10, Training loss = 0.31297498932401435
Iteration 20, Training loss = 0.2894584132994347
Iteration 30, Training loss = 0.27406454030379257
Iteration 40, Training loss = 0.2650877619410254
Iteration 50, Training loss = 0.2594781077802903
Iteration 60, Training loss = 0.25556631732650875
Iteration 70, Training loss = 0.2525044822700087
Iteration 80, Training loss = 0.25019555330529053
Iteration 90, Training loss = 0.24770682985211112
Iteration 100, Training loss = 0.24571825648696313
Iteration 110, Training loss = 0.24318389444821684
Iteration 120, Training loss = 0.24072126084465092
Iteration 130, Training loss = 0.2386269738252746
Iteration 140, Training loss = 0.23694840912452333
Iteration 150, Training loss = 0.23502282642242983
Iteration 160, Training loss = 0.23347296547730956
Iteration 170, Training loss = 0.2318173986442441
Iteration 180, Training loss = 0.23018119581602964
Iteration 190, Training loss = 0.22869534496461508
Iteration 200, Training loss = 0.22721150646450733
Iteration 210, Training loss = 0.2263103290472279
Iteration 220, Training loss = 0.22535895581467677
Iteration 230, Training loss = 0.22420479725261577
Iteration 240, Training loss = 0.22346576843546032
Iteration 250, Training loss = 0.22255614084207406
Iteration 260, Training loss = 0.2222301839844078
Iteration 270, Training loss = 0.22138044084749267
Iteration 280, Training loss = 0.22062418353687477
Iteration 290, Training loss = 0.21998386808443302
Model training time: 130.27577304840088
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.46694207326768383
Iteration 10, Training loss = 0.25226089475590724
Iteration 20, Training loss = 0.24311174180800632
Iteration 30, Training loss = 0.24013076099334849
Iteration 40, Training loss = 0.23710923075459484
Iteration 50, Training loss = 0.2338253380347395
Iteration 60, Training loss = 0.23272036140464408
Iteration 70, Training loss = 0.22889544978990392
Iteration 80, Training loss = 0.22773426166819025
Iteration 90, Training loss = 0.22709921298246408
Iteration 100, Training loss = 0.22671405382941479
Iteration 110, Training loss = 0.22561125718868963
Iteration 120, Training loss = 0.22294370005624345
Iteration 130, Training loss = 0.22311807728708513
Iteration 140, Training loss = 0.22052117931424273
Iteration 150, Training loss = 0.2199285320991754
Iteration 160, Training loss = 0.21957382977405704
Iteration 170, Training loss = 0.21787560811451215
Iteration 180, Training loss = 0.21841443604259744
Iteration 190, Training loss = 0.21707815041914402
Iteration 200, Training loss = 0.21715951270759828
Iteration 210, Training loss = 0.21683397047564423
Iteration 220, Training loss = 0.21868077054006424
Iteration 230, Training loss = 0.2166404268877991
Iteration 240, Training loss = 0.21532204242977912
Iteration 250, Training loss = 0.2155406303595544
Iteration 260, Training loss = 0.21437147572708765
Iteration 270, Training loss = 0.21451121289959543
Iteration 280, Training loss = 0.21499104569632263
Iteration 290, Training loss = 0.21305482610879742
Model training time: 154.1764566898346
Device: cuda
Iteration 0, Training loss = 0.676587052264456
Iteration 10, Training loss = 0.2584022210852286
Iteration 20, Training loss = 0.24772287571473503
Iteration 30, Training loss = 0.24221660004653595
Iteration 40, Training loss = 0.23565767111919694
Iteration 50, Training loss = 0.23208607666501122
Iteration 60, Training loss = 0.22873933959671142
Iteration 70, Training loss = 0.22546889654588467
Iteration 80, Training loss = 0.2226232714301593
Iteration 90, Training loss = 0.2217830897694494
Iteration 100, Training loss = 0.22035590696349272
Iteration 110, Training loss = 0.21943312047216273
Iteration 120, Training loss = 0.21955660040564745
Iteration 130, Training loss = 0.21669840238104432
Iteration 140, Training loss = 0.21699571701501819
Iteration 150, Training loss = 0.2163563365924012
Iteration 160, Training loss = 0.21516991616846862
Iteration 170, Training loss = 0.21580753672967234
Iteration 180, Training loss = 0.21521690458997397
Iteration 190, Training loss = 0.21564688551584688
Iteration 200, Training loss = 0.2144832850927735
Iteration 210, Training loss = 0.21440474032345463
Iteration 220, Training loss = 0.21365165867398495
Iteration 230, Training loss = 0.21358520529496755
Iteration 240, Training loss = 0.21357416339020174
Iteration 250, Training loss = 0.21197419563087366
Iteration 260, Training loss = 0.2118185800263437
Iteration 270, Training loss = 0.21050417675810346
Iteration 280, Training loss = 0.2111141032767065
Iteration 290, Training loss = 0.21045648544738138
Model training time: 151.55045104026794
Device: cuda
Iteration 0, Training loss = 0.5428112510763127
Iteration 10, Training loss = 0.2503544082256264
Iteration 20, Training loss = 0.23783195670017607
Iteration 30, Training loss = 0.2319736155809848
Iteration 40, Training loss = 0.2272697004419695
Iteration 50, Training loss = 0.2230017079766524
Iteration 60, Training loss = 0.2205155171977117
Iteration 70, Training loss = 0.2185425591490459
Iteration 80, Training loss = 0.21408448353878812
Iteration 90, Training loss = 0.21406660137864925
Iteration 100, Training loss = 0.21320087448231534
Iteration 110, Training loss = 0.2143116320774284
Iteration 120, Training loss = 0.2115395514262935
Iteration 130, Training loss = 0.20950668918523604
Iteration 140, Training loss = 0.21086171675320112
Iteration 150, Training loss = 0.2098672971688662
Iteration 160, Training loss = 0.20952139625249128
Iteration 170, Training loss = 0.20766493054805887
Iteration 180, Training loss = 0.20754892529680713
Iteration 190, Training loss = 0.20727679564574727
Iteration 200, Training loss = 0.20674416033176765
Iteration 210, Training loss = 0.20753282346462798
Iteration 220, Training loss = 0.20520753099540243
Iteration 230, Training loss = 0.20616757097602179
Iteration 240, Training loss = 0.20660199252009104
Iteration 250, Training loss = 0.2056793718086894
Iteration 260, Training loss = 0.20573645749875766
Iteration 270, Training loss = 0.20559784358696556
Iteration 280, Training loss = 0.20444077692619247
Iteration 290, Training loss = 0.20484128360778597
Model training time: 152.74820852279663
Device: cuda
Iteration 0, Training loss = 0.46145463613996207
Iteration 10, Training loss = 0.25068730433397085
Iteration 20, Training loss = 0.23807336399038537
Iteration 30, Training loss = 0.2350897471675284
Iteration 40, Training loss = 0.23395674742451592
Iteration 50, Training loss = 0.2311180421031705
Iteration 60, Training loss = 0.23016639676206627
Iteration 70, Training loss = 0.22766872025467005
Iteration 80, Training loss = 0.22556130598878746
Iteration 90, Training loss = 0.2250622855657238
Iteration 100, Training loss = 0.22357106456853287
Iteration 110, Training loss = 0.22292570729196504
Iteration 120, Training loss = 0.22178998304481368
Iteration 130, Training loss = 0.21981212726769378
Iteration 140, Training loss = 0.21846999829639535
Iteration 150, Training loss = 0.21858462022017625
Iteration 160, Training loss = 0.21695027904490294
Iteration 170, Training loss = 0.2164352165747064
Iteration 180, Training loss = 0.21573422320602015
Iteration 190, Training loss = 0.21579740281255136
Iteration 200, Training loss = 0.215037911535031
Iteration 210, Training loss = 0.21407810720382822
Iteration 220, Training loss = 0.21478045501771043
Iteration 230, Training loss = 0.21359514611035801
Iteration 240, Training loss = 0.21418669567269794
Iteration 250, Training loss = 0.2145828629773985
Iteration 260, Training loss = 0.21216696090364862
Iteration 270, Training loss = 0.21276505937299198
Iteration 280, Training loss = 0.2124830538871502
Iteration 290, Training loss = 0.2108549234202497
Model training time: 154.36370587348938
Device: cuda
Iteration 0, Training loss = 0.501072104686542
Iteration 10, Training loss = 0.2556793232590465
Iteration 20, Training loss = 0.2463502598065008
Iteration 30, Training loss = 0.24020343465103652
Iteration 40, Training loss = 0.23415595746754733
Iteration 50, Training loss = 0.2308102352747617
Iteration 60, Training loss = 0.2272371172490045
Iteration 70, Training loss = 0.22601293813951367
Iteration 80, Training loss = 0.2238797165687956
Iteration 90, Training loss = 0.2229699531601647
Iteration 100, Training loss = 0.22028412620286675
Iteration 110, Training loss = 0.2190033607816292
Iteration 120, Training loss = 0.21667740248666836
Iteration 130, Training loss = 0.21330441331567546
Iteration 140, Training loss = 0.21135073829866205
Iteration 150, Training loss = 0.2111821865108198
Iteration 160, Training loss = 0.20920606930774002
Iteration 170, Training loss = 0.2093422833929339
Iteration 180, Training loss = 0.2071229305939149
Iteration 190, Training loss = 0.20744462496179356
Iteration 200, Training loss = 0.20767957984174423
Iteration 210, Training loss = 0.2088449758096411
Iteration 220, Training loss = 0.20748481754276712
Iteration 230, Training loss = 0.20636467552646887
Iteration 240, Training loss = 0.20633987380647198
Iteration 250, Training loss = 0.20683268636248592
Iteration 260, Training loss = 0.20693343765041441
Iteration 270, Training loss = 0.20632864617839564
Iteration 280, Training loss = 0.20511150447602133
Iteration 290, Training loss = 0.20581936024436073
Model training time: 161.88067150115967
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9527705032513736
Iteration 10, Training loss = 0.3116858952270582
Iteration 20, Training loss = 0.29490866614889
Iteration 30, Training loss = 0.28401859737842483
Iteration 40, Training loss = 0.2756398174614075
Iteration 50, Training loss = 0.26931247383933377
Iteration 60, Training loss = 0.26457649983519793
Iteration 70, Training loss = 0.2617760506293964
Iteration 80, Training loss = 0.2595703062075968
Iteration 90, Training loss = 0.25792454132589243
Iteration 100, Training loss = 0.25668207518124986
Iteration 110, Training loss = 0.2553498464259917
Iteration 120, Training loss = 0.2543360387802701
Iteration 130, Training loss = 0.25347219679964655
Iteration 140, Training loss = 0.25265230072468303
Iteration 150, Training loss = 0.25167482450929163
Iteration 160, Training loss = 0.25111973245986724
Iteration 170, Training loss = 0.2504583326581027
Iteration 180, Training loss = 0.24978268359388625
Iteration 190, Training loss = 0.24945258233723283
Iteration 200, Training loss = 0.24820683551974793
Iteration 210, Training loss = 0.2478281463751204
Iteration 220, Training loss = 0.2472748327233601
Iteration 230, Training loss = 0.24638593851726223
Iteration 240, Training loss = 0.24586383687201482
Iteration 250, Training loss = 0.24510554164720216
Iteration 260, Training loss = 0.24444737370164285
Iteration 270, Training loss = 0.24366690167936228
Iteration 280, Training loss = 0.24277012618416446
Iteration 290, Training loss = 0.2420342025362839
Model training time: 130.79236793518066
Device: cuda
Iteration 0, Training loss = 1.0091814830285875
Iteration 10, Training loss = 0.37155112797353806
Iteration 20, Training loss = 0.32891322906889003
Iteration 30, Training loss = 0.31281164174867887
Iteration 40, Training loss = 0.29989952904332345
Iteration 50, Training loss = 0.2906547908199901
Iteration 60, Training loss = 0.28469950252959
Iteration 70, Training loss = 0.27996450806934087
Iteration 80, Training loss = 0.27624348958742245
Iteration 90, Training loss = 0.2724243169582785
Iteration 100, Training loss = 0.26952145807896055
Iteration 110, Training loss = 0.2667303226581497
Iteration 120, Training loss = 0.2642244255896342
Iteration 130, Training loss = 0.2618339393134556
Iteration 140, Training loss = 0.25984192513308285
Iteration 150, Training loss = 0.25821972779081753
Iteration 160, Training loss = 0.2566319756282011
Iteration 170, Training loss = 0.25534737335639773
Iteration 180, Training loss = 0.2540595025617909
Iteration 190, Training loss = 0.25296447592050053
Iteration 200, Training loss = 0.25180531350882235
Iteration 210, Training loss = 0.2508749504535308
Iteration 220, Training loss = 0.25022981811955247
Iteration 230, Training loss = 0.24940606113748748
Iteration 240, Training loss = 0.24884999298082425
Iteration 250, Training loss = 0.2481978437964766
Iteration 260, Training loss = 0.24752203640291245
Iteration 270, Training loss = 0.24701957781652562
Iteration 280, Training loss = 0.2462795645305233
Iteration 290, Training loss = 0.24546841777337955
Model training time: 130.6895215511322
Device: cuda
Iteration 0, Training loss = 0.9246176600311916
Iteration 10, Training loss = 0.333347272894573
Iteration 20, Training loss = 0.3077600658048152
Iteration 30, Training loss = 0.2943722135398636
Iteration 40, Training loss = 0.284977191657766
Iteration 50, Training loss = 0.27859432887078484
Iteration 60, Training loss = 0.2734622977150843
Iteration 70, Training loss = 0.26890687432509935
Iteration 80, Training loss = 0.2645200967842817
Iteration 90, Training loss = 0.2616400295286721
Iteration 100, Training loss = 0.25923559343843716
Iteration 110, Training loss = 0.25721921239580425
Iteration 120, Training loss = 0.2549789423623616
Iteration 130, Training loss = 0.25189513493031623
Iteration 140, Training loss = 0.24948868346704986
Iteration 150, Training loss = 0.2478452402981084
Iteration 160, Training loss = 0.24569175465451892
Iteration 170, Training loss = 0.24459176120905265
Iteration 180, Training loss = 0.24331781845404507
Iteration 190, Training loss = 0.2425295519789108
Iteration 200, Training loss = 0.241375427990791
Iteration 210, Training loss = 0.23995494218340221
Iteration 220, Training loss = 0.23879512195581384
Iteration 230, Training loss = 0.23759298374736568
Iteration 240, Training loss = 0.23627296220015093
Iteration 250, Training loss = 0.2355666184306289
Iteration 260, Training loss = 0.2349641697783447
Iteration 270, Training loss = 0.23417476685514751
Iteration 280, Training loss = 0.2336825794804183
Iteration 290, Training loss = 0.2330909303545086
Model training time: 128.70433115959167
Device: cuda
Iteration 0, Training loss = 1.0437701935485257
Iteration 10, Training loss = 0.48832485505512785
Iteration 20, Training loss = 0.3490025396966184
Iteration 30, Training loss = 0.3108956825538351
Iteration 40, Training loss = 0.28988026271937256
Iteration 50, Training loss = 0.2751246218459081
Iteration 60, Training loss = 0.26729344535104876
Iteration 70, Training loss = 0.263119833828029
Iteration 80, Training loss = 0.2605834044606287
Iteration 90, Training loss = 0.25831456857645485
Iteration 100, Training loss = 0.256259976662966
Iteration 110, Training loss = 0.25466416191679514
Iteration 120, Training loss = 0.25291638141249917
Iteration 130, Training loss = 0.2512702328709944
Iteration 140, Training loss = 0.249703353600121
Iteration 150, Training loss = 0.24837501135995255
Iteration 160, Training loss = 0.24730018235869328
Iteration 170, Training loss = 0.2448658792360643
Iteration 180, Training loss = 0.24358026413630343
Iteration 190, Training loss = 0.2415905137508025
Iteration 200, Training loss = 0.24000693817981508
Iteration 210, Training loss = 0.23880990787655043
Iteration 220, Training loss = 0.2378504189141726
Iteration 230, Training loss = 0.23721525542095268
Iteration 240, Training loss = 0.23657820908663926
Iteration 250, Training loss = 0.23586887462159334
Iteration 260, Training loss = 0.23520851400444062
Iteration 270, Training loss = 0.23453345544834692
Iteration 280, Training loss = 0.2338746663805359
Iteration 290, Training loss = 0.23326436630461464
Model training time: 132.61388850212097
Device: cuda
Iteration 0, Training loss = 0.9913748597261692
Iteration 10, Training loss = 0.3427434743677444
Iteration 20, Training loss = 0.312564315793012
Iteration 30, Training loss = 0.2940411486868131
Iteration 40, Training loss = 0.28099716059665125
Iteration 50, Training loss = 0.27304596156242683
Iteration 60, Training loss = 0.2674424982929634
Iteration 70, Training loss = 0.2629398758236779
Iteration 80, Training loss = 0.25882924981325073
Iteration 90, Training loss = 0.2560240832534021
Iteration 100, Training loss = 0.25347206930992966
Iteration 110, Training loss = 0.25158216243217413
Iteration 120, Training loss = 0.24958554711237946
Iteration 130, Training loss = 0.2483148112820944
Iteration 140, Training loss = 0.24655055963675565
Iteration 150, Training loss = 0.24534665110360912
Iteration 160, Training loss = 0.24420623706775485
Iteration 170, Training loss = 0.24290274121498656
Iteration 180, Training loss = 0.2419659806365828
Iteration 190, Training loss = 0.2404659960337759
Iteration 200, Training loss = 0.2393992313569452
Iteration 210, Training loss = 0.2383066193712537
Iteration 220, Training loss = 0.23741931506132674
Iteration 230, Training loss = 0.2364877750849031
Iteration 240, Training loss = 0.23482191284834328
Iteration 250, Training loss = 0.23445553940520159
Iteration 260, Training loss = 0.23344521231714807
Iteration 270, Training loss = 0.23266140038720343
Iteration 280, Training loss = 0.23198035082466378
Iteration 290, Training loss = 0.23120196571361643
Model training time: 131.92183923721313
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.4513653599754084
Iteration 10, Training loss = 0.252509767690236
Iteration 20, Training loss = 0.24222846367096496
Iteration 30, Training loss = 0.2326377264740392
Iteration 40, Training loss = 0.22692722304933874
Iteration 50, Training loss = 0.2228287233188568
Iteration 60, Training loss = 0.220716777837161
Iteration 70, Training loss = 0.21803621360558575
Iteration 80, Training loss = 0.2150731901784786
Iteration 90, Training loss = 0.21497004408813272
Iteration 100, Training loss = 0.21368552894121798
Iteration 110, Training loss = 0.21143311211618326
Iteration 120, Training loss = 0.20977965378840668
Iteration 130, Training loss = 0.21071492723633534
Iteration 140, Training loss = 0.2095023448292626
Iteration 150, Training loss = 0.20823284135748052
Iteration 160, Training loss = 0.2091080733094608
Iteration 170, Training loss = 0.2078935481499673
Iteration 180, Training loss = 0.20746509976161884
Iteration 190, Training loss = 0.20579932305339463
Iteration 200, Training loss = 0.20709501586619936
Iteration 210, Training loss = 0.20556247217206344
Iteration 220, Training loss = 0.20520167481199303
Iteration 230, Training loss = 0.20562996529493724
Iteration 240, Training loss = 0.20572227716770647
Iteration 250, Training loss = 0.20496430780206407
Iteration 260, Training loss = 0.2052541683376124
Iteration 270, Training loss = 0.20479160126019044
Iteration 280, Training loss = 0.20405285313869792
Iteration 290, Training loss = 0.20364643102985316
Model training time: 154.22682738304138
Device: cuda
Iteration 0, Training loss = 0.4386907970898088
Iteration 10, Training loss = 0.24325250215449576
Iteration 20, Training loss = 0.23103049592302152
Iteration 30, Training loss = 0.22356511082400998
Iteration 40, Training loss = 0.21874569360951535
Iteration 50, Training loss = 0.21736320878489543
Iteration 60, Training loss = 0.21230005628043744
Iteration 70, Training loss = 0.21074038338971485
Iteration 80, Training loss = 0.20980067865936577
Iteration 90, Training loss = 0.2079275045047805
Iteration 100, Training loss = 0.2071122937355434
Iteration 110, Training loss = 0.20455277566831856
Iteration 120, Training loss = 0.20474142759821895
Iteration 130, Training loss = 0.20346448349498086
Iteration 140, Training loss = 0.2027183964794398
Iteration 150, Training loss = 0.20307237074933676
Iteration 160, Training loss = 0.20123347295470734
Iteration 170, Training loss = 0.20055963036764332
Iteration 180, Training loss = 0.1996258926052447
Iteration 190, Training loss = 0.1985913446448184
Iteration 200, Training loss = 0.19751106564307327
Iteration 210, Training loss = 0.1973623564471344
Iteration 220, Training loss = 0.1971795363644855
Iteration 230, Training loss = 0.19653575890866665
Iteration 240, Training loss = 0.19540064481535777
Iteration 250, Training loss = 0.19481831645597558
Iteration 260, Training loss = 0.19452223381970177
Iteration 270, Training loss = 0.19415242814844516
Iteration 280, Training loss = 0.19194768436848395
Iteration 290, Training loss = 0.19028958651185324
Model training time: 157.06832242012024
Device: cuda
Iteration 0, Training loss = 0.43355574664495183
Iteration 10, Training loss = 0.249646534750883
Iteration 20, Training loss = 0.2372508839169657
Iteration 30, Training loss = 0.23077382688516565
Iteration 40, Training loss = 0.22640479009679673
Iteration 50, Training loss = 0.22311272892478592
Iteration 60, Training loss = 0.21899182978142548
Iteration 70, Training loss = 0.21715602184900648
Iteration 80, Training loss = 0.21380575390906945
Iteration 90, Training loss = 0.2116383582840676
Iteration 100, Training loss = 0.21067086378107924
Iteration 110, Training loss = 0.2093955395296757
Iteration 120, Training loss = 0.20961873288340777
Iteration 130, Training loss = 0.20960708809497858
Iteration 140, Training loss = 0.2077979626760933
Iteration 150, Training loss = 0.20761076708213758
Iteration 160, Training loss = 0.2070483844862579
Iteration 170, Training loss = 0.20538888503650776
Iteration 180, Training loss = 0.20748787992983406
Iteration 190, Training loss = 0.20526396978384934
Iteration 200, Training loss = 0.20541797627945213
Iteration 210, Training loss = 0.20442533134043073
Iteration 220, Training loss = 0.20416475922663044
Iteration 230, Training loss = 0.20253586836754553
Iteration 240, Training loss = 0.20210014039213087
Iteration 250, Training loss = 0.20109455512958346
Iteration 260, Training loss = 0.20167851502179523
Iteration 270, Training loss = 0.20048123994079975
Iteration 280, Training loss = 0.201970926028187
Iteration 290, Training loss = 0.20049063713440884
Model training time: 156.72450733184814
Device: cuda
Iteration 0, Training loss = 0.44812743244895636
Iteration 10, Training loss = 0.25643123494186065
Iteration 20, Training loss = 0.2427153835754129
Iteration 30, Training loss = 0.23329472953124428
Iteration 40, Training loss = 0.23053689765547725
Iteration 50, Training loss = 0.22629192066502918
Iteration 60, Training loss = 0.22458899297162926
Iteration 70, Training loss = 0.22311571715782977
Iteration 80, Training loss = 0.22333019158236916
Iteration 90, Training loss = 0.22130791992670687
Iteration 100, Training loss = 0.21776061279577436
Iteration 110, Training loss = 0.21811573099137507
Iteration 120, Training loss = 0.21625249003021826
Iteration 130, Training loss = 0.21505607072868302
Iteration 140, Training loss = 0.2126019189616814
Iteration 150, Training loss = 0.21085077624307996
Iteration 160, Training loss = 0.21022043231035836
Iteration 170, Training loss = 0.2080127542193519
Iteration 180, Training loss = 0.2076420871265792
Iteration 190, Training loss = 0.20577676323655153
Iteration 200, Training loss = 0.2060304891958508
Iteration 210, Training loss = 0.20626252177047383
Iteration 220, Training loss = 0.204161559563067
Iteration 230, Training loss = 0.20419044550012155
Iteration 240, Training loss = 0.20238466970351937
Iteration 250, Training loss = 0.2024157882169719
Iteration 260, Training loss = 0.20333013541942357
Iteration 270, Training loss = 0.20160294282667285
Iteration 280, Training loss = 0.20069113104056216
Iteration 290, Training loss = 0.20100006463849515
Model training time: 155.54637122154236
Device: cuda
Iteration 0, Training loss = 0.43104788549154205
Iteration 10, Training loss = 0.2474830633899779
Iteration 20, Training loss = 0.23864477062845924
Iteration 30, Training loss = 0.23341690232981782
Iteration 40, Training loss = 0.22856573176910744
Iteration 50, Training loss = 0.22457419331332096
Iteration 60, Training loss = 0.21956444060831323
Iteration 70, Training loss = 0.2182051523635953
Iteration 80, Training loss = 0.2157633976501212
Iteration 90, Training loss = 0.21232213512892872
Iteration 100, Training loss = 0.21040742478128208
Iteration 110, Training loss = 0.20876164194061161
Iteration 120, Training loss = 0.20753883423405467
Iteration 130, Training loss = 0.20658221548608083
Iteration 140, Training loss = 0.2059480641172527
Iteration 150, Training loss = 0.20447126730879628
Iteration 160, Training loss = 0.20430451194072463
Iteration 170, Training loss = 0.20291134667273872
Iteration 180, Training loss = 0.20271257633122347
Iteration 190, Training loss = 0.20205696554940203
Iteration 200, Training loss = 0.20141498977205655
Iteration 210, Training loss = 0.20132003495465178
Iteration 220, Training loss = 0.20124538521086188
Iteration 230, Training loss = 0.20001255060584436
Iteration 240, Training loss = 0.19974478591260553
Iteration 250, Training loss = 0.19956397221899494
Iteration 260, Training loss = 0.1999026599721239
Iteration 270, Training loss = 0.1994731660490319
Iteration 280, Training loss = 0.199284174459249
Iteration 290, Training loss = 0.19886181499926286
Model training time: 155.40806436538696
{'activation_functions': ['relu', 'relu'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.038632632672931
Iteration 10, Training loss = 0.3563128631138051
Iteration 20, Training loss = 0.31125769293337124
Iteration 30, Training loss = 0.2933961929658712
Iteration 40, Training loss = 0.2833800894043636
Iteration 50, Training loss = 0.2760883227495824
Iteration 60, Training loss = 0.27049292272213876
Iteration 70, Training loss = 0.2660885999479825
Iteration 80, Training loss = 0.26277049127560265
Iteration 90, Training loss = 0.2599896158454782
Iteration 100, Training loss = 0.2573445244685501
Iteration 110, Training loss = 0.2553655438431816
Iteration 120, Training loss = 0.25367378438067495
Iteration 130, Training loss = 0.2518737734933598
Iteration 140, Training loss = 0.2503146473183037
Iteration 150, Training loss = 0.24875091806451
Iteration 160, Training loss = 0.24715893061664723
Iteration 170, Training loss = 0.2455237220801395
Iteration 180, Training loss = 0.2447169822512181
Iteration 190, Training loss = 0.24341086871903975
Iteration 200, Training loss = 0.24254465570389214
Iteration 210, Training loss = 0.24170958492498998
Iteration 220, Training loss = 0.2408502060405856
Iteration 230, Training loss = 0.24020263411952567
Iteration 240, Training loss = 0.23906035008716237
Iteration 250, Training loss = 0.23791576361793293
Iteration 260, Training loss = 0.23761673152591067
Iteration 270, Training loss = 0.23690153606975628
Iteration 280, Training loss = 0.23576891538630676
Iteration 290, Training loss = 0.23501932043717502
Model training time: 129.60973978042603
Device: cuda
Iteration 0, Training loss = 0.9722920549406555
Iteration 10, Training loss = 0.3304066568180089
Iteration 20, Training loss = 0.30416598490306307
Iteration 30, Training loss = 0.2931181043206058
Iteration 40, Training loss = 0.2837982745277391
Iteration 50, Training loss = 0.27546542884723324
Iteration 60, Training loss = 0.26955898438036874
Iteration 70, Training loss = 0.2653967101695174
Iteration 80, Training loss = 0.2616209890558125
Iteration 90, Training loss = 0.258075149617097
Iteration 100, Training loss = 0.2549029423451597
Iteration 110, Training loss = 0.25181498591388973
Iteration 120, Training loss = 0.2495375718677881
Iteration 130, Training loss = 0.24780373706439504
Iteration 140, Training loss = 0.2460814525371025
Iteration 150, Training loss = 0.24451710677031455
Iteration 160, Training loss = 0.24277691194025425
Iteration 170, Training loss = 0.2417132862579736
Iteration 180, Training loss = 0.24080031098162002
Iteration 190, Training loss = 0.23975884550889237
Iteration 200, Training loss = 0.23861807943761493
Iteration 210, Training loss = 0.2381613760483034
Iteration 220, Training loss = 0.2372447002237126
Iteration 230, Training loss = 0.23635663306814128
Iteration 240, Training loss = 0.23563736401155555
Iteration 250, Training loss = 0.23454881542459238
Iteration 260, Training loss = 0.23345401071318703
Iteration 270, Training loss = 0.23246893016102815
Iteration 280, Training loss = 0.23193154525525922
Iteration 290, Training loss = 0.231244629163286
Model training time: 132.85222959518433
Device: cuda
Iteration 0, Training loss = 0.9842990033563055
Iteration 10, Training loss = 0.33854735649887646
Iteration 20, Training loss = 0.30911004090496763
Iteration 30, Training loss = 0.29402252387697414
Iteration 40, Training loss = 0.2839310176255628
Iteration 50, Training loss = 0.2753279163134589
Iteration 60, Training loss = 0.26852985733069173
Iteration 70, Training loss = 0.2629602769926443
Iteration 80, Training loss = 0.25892603847471046
Iteration 90, Training loss = 0.2558532721987648
Iteration 100, Training loss = 0.2536696474232483
Iteration 110, Training loss = 0.25155183377046564
Iteration 120, Training loss = 0.24954565084983593
Iteration 130, Training loss = 0.2481972416073589
Iteration 140, Training loss = 0.24675747011641613
Iteration 150, Training loss = 0.24510949216873248
Iteration 160, Training loss = 0.24442608198083343
Iteration 170, Training loss = 0.2429932748795133
Iteration 180, Training loss = 0.24245305447760274
Iteration 190, Training loss = 0.24141553037103095
Iteration 200, Training loss = 0.24015027818610535
Iteration 210, Training loss = 0.23889795883804488
Iteration 220, Training loss = 0.23791884051829792
Iteration 230, Training loss = 0.23730240084826223
Iteration 240, Training loss = 0.23626993856331915
Iteration 250, Training loss = 0.23552756792624407
Iteration 260, Training loss = 0.23455052236523524
Iteration 270, Training loss = 0.23352206724788316
Iteration 280, Training loss = 0.2325913930181008
Iteration 290, Training loss = 0.23210781098278035
Model training time: 140.49001216888428
Device: cuda
Iteration 0, Training loss = 0.9847395351521905
Iteration 10, Training loss = 0.32574406293272684
Iteration 20, Training loss = 0.29659831099951817
Iteration 30, Training loss = 0.2830063246064267
Iteration 40, Training loss = 0.27491121816721725
Iteration 50, Training loss = 0.2692066575727509
Iteration 60, Training loss = 0.26408793999121205
Iteration 70, Training loss = 0.2597818932074323
Iteration 80, Training loss = 0.25704677301948353
Iteration 90, Training loss = 0.2545367521808886
Iteration 100, Training loss = 0.2525844232113829
Iteration 110, Training loss = 0.25079345033835554
Iteration 120, Training loss = 0.2492175859559246
Iteration 130, Training loss = 0.2479081688218775
Iteration 140, Training loss = 0.24667076276700953
Iteration 150, Training loss = 0.24569263157703108
Iteration 160, Training loss = 0.244776350642232
Iteration 170, Training loss = 0.24321300431761267
Iteration 180, Training loss = 0.2423158286378401
Iteration 190, Training loss = 0.241472044919074
Iteration 200, Training loss = 0.240269772095051
Iteration 210, Training loss = 0.2397811586628093
Iteration 220, Training loss = 0.23845080975398025
Iteration 230, Training loss = 0.23835384945856458
Iteration 240, Training loss = 0.23753886678894265
Iteration 250, Training loss = 0.23701810716825017
Iteration 260, Training loss = 0.2360664848012728
Iteration 270, Training loss = 0.23548752063159215
Iteration 280, Training loss = 0.23515018474391816
Iteration 290, Training loss = 0.23454609367157586
Model training time: 128.2914640903473
Device: cuda
Iteration 0, Training loss = 0.9810783372375637
Iteration 10, Training loss = 0.33646408390912247
Iteration 20, Training loss = 0.3040986469344712
Iteration 30, Training loss = 0.28939289390391354
Iteration 40, Training loss = 0.27856394951626406
Iteration 50, Training loss = 0.2701916464448841
Iteration 60, Training loss = 0.26416000195262507
Iteration 70, Training loss = 0.2590694503619654
Iteration 80, Training loss = 0.25565126463410065
Iteration 90, Training loss = 0.2530820882421429
Iteration 100, Training loss = 0.25129882625098954
Iteration 110, Training loss = 0.24945237952088328
Iteration 120, Training loss = 0.24815734665272599
Iteration 130, Training loss = 0.24681873356170284
Iteration 140, Training loss = 0.24551605587468886
Iteration 150, Training loss = 0.24457649551061394
Iteration 160, Training loss = 0.24370145386414147
Iteration 170, Training loss = 0.24279110997770947
Iteration 180, Training loss = 0.24188093036485353
Iteration 190, Training loss = 0.24089225740611697
Iteration 200, Training loss = 0.24023680015496424
Iteration 210, Training loss = 0.23920959066464306
Iteration 220, Training loss = 0.23848120682392512
Iteration 230, Training loss = 0.23773682019143358
Iteration 240, Training loss = 0.23698016454535592
Iteration 250, Training loss = 0.23648246269465648
Iteration 260, Training loss = 0.23581847847951237
Iteration 270, Training loss = 0.23556494124720806
Iteration 280, Training loss = 0.23492395072345584
Iteration 290, Training loss = 0.2347229087756852
Model training time: 129.92441868782043
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6346828096010835
Iteration 10, Training loss = 0.2663955890038163
Iteration 20, Training loss = 0.2511519766660128
Iteration 30, Training loss = 0.24457944087360217
Iteration 40, Training loss = 0.2410221750969472
Iteration 50, Training loss = 0.2365763622565546
Iteration 60, Training loss = 0.23258341283757905
Iteration 70, Training loss = 0.22969550405435515
Iteration 80, Training loss = 0.2266863245822957
Iteration 90, Training loss = 0.22070379918756117
Model training time: 29.083610773086548
Device: cuda
Iteration 0, Training loss = 0.6733722225335486
Iteration 10, Training loss = 0.2591701443284606
Iteration 20, Training loss = 0.24644126303961889
Iteration 30, Training loss = 0.24241415076497672
Iteration 40, Training loss = 0.23713982889905644
Iteration 50, Training loss = 0.23607262382328797
Iteration 60, Training loss = 0.23186294532916396
Iteration 70, Training loss = 0.22895541733157806
Iteration 80, Training loss = 0.2270166319637483
Iteration 90, Training loss = 0.2255166418094566
Model training time: 29.3985493183136
Device: cuda
Iteration 0, Training loss = 0.882280429204305
Iteration 10, Training loss = 0.2601025919407462
Iteration 20, Training loss = 0.2473599191233156
Iteration 30, Training loss = 0.2413640524742108
Iteration 40, Training loss = 0.23474427683341906
Iteration 50, Training loss = 0.229371745409309
Iteration 60, Training loss = 0.22670487699589292
Iteration 70, Training loss = 0.22408016198787137
Iteration 80, Training loss = 0.22036630318360628
Iteration 90, Training loss = 0.21913514869368594
Model training time: 29.33198642730713
Device: cuda
Iteration 0, Training loss = 0.6773420696097295
Iteration 10, Training loss = 0.2536381217448608
Iteration 20, Training loss = 0.24175307842124488
Iteration 30, Training loss = 0.23133935642559172
Iteration 40, Training loss = 0.224489724110578
Iteration 50, Training loss = 0.21851107636511613
Iteration 60, Training loss = 0.21346772714081594
Iteration 70, Training loss = 0.21114330011721394
Iteration 80, Training loss = 0.20876457265033815
Iteration 90, Training loss = 0.20627617296101391
Model training time: 30.33696222305298
Device: cuda
Iteration 0, Training loss = 0.6767849668068586
Iteration 10, Training loss = 0.25864754412053287
Iteration 20, Training loss = 0.24233941798624786
Iteration 30, Training loss = 0.23460261257374343
Iteration 40, Training loss = 0.23078205756807096
Iteration 50, Training loss = 0.2271609796202125
Iteration 60, Training loss = 0.223848946860447
Iteration 70, Training loss = 0.22139697601109887
Iteration 80, Training loss = 0.2199055221705621
Iteration 90, Training loss = 0.2169907385672348
Model training time: 30.085612535476685
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9788695582445117
Iteration 10, Training loss = 0.6926184669496932
Iteration 20, Training loss = 0.39379073377104773
Iteration 30, Training loss = 0.3558407960018674
Iteration 40, Training loss = 0.3358272167939495
Iteration 50, Training loss = 0.32348656078467625
Iteration 60, Training loss = 0.31472197196621826
Iteration 70, Training loss = 0.3085206842796814
Iteration 80, Training loss = 0.30339505649419224
Iteration 90, Training loss = 0.298600810593453
Model training time: 25.46924138069153
Device: cuda
Iteration 0, Training loss = 1.0148376957805836
Iteration 10, Training loss = 0.6091166754851595
Iteration 20, Training loss = 0.3663695261674227
Iteration 30, Training loss = 0.3350500205288763
Iteration 40, Training loss = 0.3229204919819095
Iteration 50, Training loss = 0.3135257754924793
Iteration 60, Training loss = 0.3073082234044582
Iteration 70, Training loss = 0.30188244390458874
Iteration 80, Training loss = 0.2977047644663548
Iteration 90, Training loss = 0.29408517184752775
Model training time: 25.46889877319336
Device: cuda
Iteration 0, Training loss = 1.0291477666384932
Iteration 10, Training loss = 0.5651630026418806
Iteration 20, Training loss = 0.38853106370582674
Iteration 30, Training loss = 0.3439166473161771
Iteration 40, Training loss = 0.32749476848881026
Iteration 50, Training loss = 0.31895622337497953
Iteration 60, Training loss = 0.3126718004544576
Iteration 70, Training loss = 0.30762995246815794
Iteration 80, Training loss = 0.30219390880370484
Iteration 90, Training loss = 0.2974211655665135
Model training time: 25.89153003692627
Device: cuda
Iteration 0, Training loss = 1.0417055536583426
Iteration 10, Training loss = 0.5752280704353167
Iteration 20, Training loss = 0.3564896438864694
Iteration 30, Training loss = 0.33435255375461304
Iteration 40, Training loss = 0.32189451863512325
Iteration 50, Training loss = 0.313146045818421
Iteration 60, Training loss = 0.30642929363653854
Iteration 70, Training loss = 0.30152465395881356
Iteration 80, Training loss = 0.29670227592550036
Iteration 90, Training loss = 0.29274778481986785
Model training time: 25.382952213287354
Device: cuda
Iteration 0, Training loss = 0.9835534615505145
Iteration 10, Training loss = 0.5019724022795037
Iteration 20, Training loss = 0.3777283381868676
Iteration 30, Training loss = 0.3378036884581985
Iteration 40, Training loss = 0.3170862506337212
Iteration 50, Training loss = 0.3057859169976147
Iteration 60, Training loss = 0.2985117792147369
Iteration 70, Training loss = 0.2932738530751012
Iteration 80, Training loss = 0.2898118656110648
Iteration 90, Training loss = 0.28467202355752247
Model training time: 25.996763706207275
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8339744302385671
Iteration 10, Training loss = 0.26870491060513807
Iteration 20, Training loss = 0.25578350156257695
Iteration 30, Training loss = 0.24805003480202908
Iteration 40, Training loss = 0.24218858109004254
Iteration 50, Training loss = 0.23617860451700606
Iteration 60, Training loss = 0.23256777460880326
Iteration 70, Training loss = 0.22827257100798656
Iteration 80, Training loss = 0.22541310911737203
Iteration 90, Training loss = 0.22317283341418143
Model training time: 29.91529154777527
Device: cuda
Iteration 0, Training loss = 0.7693279774292655
Iteration 10, Training loss = 0.2744846223777043
Iteration 20, Training loss = 0.25669186526306587
Iteration 30, Training loss = 0.25035118801150347
Iteration 40, Training loss = 0.24642821696501424
Iteration 50, Training loss = 0.24461247275273004
Iteration 60, Training loss = 0.24299988809271136
Iteration 70, Training loss = 0.2417088634006067
Iteration 80, Training loss = 0.2395297931732187
Iteration 90, Training loss = 0.23812589686417926
Model training time: 29.904741287231445
Device: cuda
Iteration 0, Training loss = 0.9160032612114137
Iteration 10, Training loss = 0.333295072046455
Iteration 20, Training loss = 0.26490983417356645
Iteration 30, Training loss = 0.2507582618922427
Iteration 40, Training loss = 0.24485455604567044
Iteration 50, Training loss = 0.24156257499387299
Iteration 60, Training loss = 0.2386848203441947
Iteration 70, Training loss = 0.2358311627438103
Iteration 80, Training loss = 0.2367346176634664
Iteration 90, Training loss = 0.23336285827816397
Model training time: 29.433301210403442
Device: cuda
Iteration 0, Training loss = 0.6409684981124989
Iteration 10, Training loss = 0.26486654889180467
Iteration 20, Training loss = 0.2532405936415645
Iteration 30, Training loss = 0.24742479967898218
Iteration 40, Training loss = 0.24376622720617028
Iteration 50, Training loss = 0.24104464863953384
Iteration 60, Training loss = 0.2391426439806459
Iteration 70, Training loss = 0.23599156100248944
Iteration 80, Training loss = 0.23436872599925396
Iteration 90, Training loss = 0.23051222327394763
Model training time: 29.79677700996399
Device: cuda
Iteration 0, Training loss = 0.8623846564891834
Iteration 10, Training loss = 0.25977625960601125
Iteration 20, Training loss = 0.2513409791433293
Iteration 30, Training loss = 0.24625601029194497
Iteration 40, Training loss = 0.24184784480339086
Iteration 50, Training loss = 0.23948315779368082
Iteration 60, Training loss = 0.237661753155759
Iteration 70, Training loss = 0.2357230904885536
Iteration 80, Training loss = 0.23381599614729628
Iteration 90, Training loss = 0.23321608058064455
Model training time: 29.919318675994873
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.026449253593666
Iteration 10, Training loss = 0.8296277456813388
Iteration 20, Training loss = 0.4236378697813421
Iteration 30, Training loss = 0.36315525218772426
Iteration 40, Training loss = 0.33521511380511204
Iteration 50, Training loss = 0.3205802623488477
Iteration 60, Training loss = 0.3091744149940601
Iteration 70, Training loss = 0.30249592992994523
Iteration 80, Training loss = 0.2983390284333252
Iteration 90, Training loss = 0.2941791090556389
Model training time: 25.753427267074585
Device: cuda
Iteration 0, Training loss = 1.0577349766441013
Iteration 10, Training loss = 0.809218243268377
Iteration 20, Training loss = 0.4799549051960885
Iteration 30, Training loss = 0.40848854644862925
Iteration 40, Training loss = 0.3683500978105886
Iteration 50, Training loss = 0.34747954907912565
Iteration 60, Training loss = 0.3364544821102262
Iteration 70, Training loss = 0.32988913702792017
Iteration 80, Training loss = 0.32361966738666315
Iteration 90, Training loss = 0.31773000458876294
Model training time: 25.68272042274475
Device: cuda
Iteration 0, Training loss = 0.9663969168916416
Iteration 10, Training loss = 0.7144074932388638
Iteration 20, Training loss = 0.4651590210660068
Iteration 30, Training loss = 0.366477369758242
Iteration 40, Training loss = 0.32826466563243223
Iteration 50, Training loss = 0.3148612482248297
Iteration 60, Training loss = 0.3075935092669178
Iteration 70, Training loss = 0.30409864084300214
Iteration 80, Training loss = 0.29978150489249666
Iteration 90, Training loss = 0.29570500905387065
Model training time: 25.615395069122314
Device: cuda
Iteration 0, Training loss = 1.0016337109072773
Iteration 10, Training loss = 0.6990367649258047
Iteration 20, Training loss = 0.45983318689364744
Iteration 30, Training loss = 0.39188903246236884
Iteration 40, Training loss = 0.3557406825863797
Iteration 50, Training loss = 0.3347484832944501
Iteration 60, Training loss = 0.3206610453589527
Iteration 70, Training loss = 0.30947104845070034
Iteration 80, Training loss = 0.30104806141001017
Iteration 90, Training loss = 0.2947615142461758
Model training time: 25.935067892074585
Device: cuda
Iteration 0, Training loss = 1.145142939067693
Iteration 10, Training loss = 0.8552825714367024
Iteration 20, Training loss = 0.45257590581541474
Iteration 30, Training loss = 0.385072053414612
Iteration 40, Training loss = 0.35175754345845484
Iteration 50, Training loss = 0.33637369262135547
Iteration 60, Training loss = 0.32507567820341693
Iteration 70, Training loss = 0.31790522252016024
Iteration 80, Training loss = 0.31411316915266757
Iteration 90, Training loss = 0.3091942624769349
Model training time: 25.356962203979492
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6893984875529284
Iteration 10, Training loss = 0.2677981536745449
Iteration 20, Training loss = 0.25418389268255465
Iteration 30, Training loss = 0.2472383561341659
Iteration 40, Training loss = 0.24060025926373432
Iteration 50, Training loss = 0.23620911781194706
Iteration 60, Training loss = 0.23132402968147528
Iteration 70, Training loss = 0.22899575013181436
Iteration 80, Training loss = 0.2269956386031736
Iteration 90, Training loss = 0.22623578397836086
Model training time: 29.978673458099365
Device: cuda
Iteration 0, Training loss = 0.7204778335520611
Iteration 10, Training loss = 0.26726179638346614
Iteration 20, Training loss = 0.2549352656312035
Iteration 30, Training loss = 0.2473239228129387
Iteration 40, Training loss = 0.24235614169622965
Iteration 50, Training loss = 0.2372535722879108
Iteration 60, Training loss = 0.2325653348495995
Iteration 70, Training loss = 0.22794910405137112
Iteration 80, Training loss = 0.2240433257415099
Iteration 90, Training loss = 0.2225737618147463
Model training time: 29.325310707092285
Device: cuda
Iteration 0, Training loss = 0.7718742330005204
Iteration 10, Training loss = 0.2611107953288705
Iteration 20, Training loss = 0.2458557707891948
Iteration 30, Training loss = 0.23923109867722517
Iteration 40, Training loss = 0.2356400685969758
Iteration 50, Training loss = 0.2323524620797899
Iteration 60, Training loss = 0.22898007141075272
Iteration 70, Training loss = 0.22599379469950995
Iteration 80, Training loss = 0.22519730114274555
Iteration 90, Training loss = 0.22297565592227928
Model training time: 30.67891263961792
Device: cuda
Iteration 0, Training loss = 0.736748244739385
Iteration 10, Training loss = 0.2619288868590254
Iteration 20, Training loss = 0.2507928182609415
Iteration 30, Training loss = 0.24599170519245994
Iteration 40, Training loss = 0.24224470033881745
Iteration 50, Training loss = 0.23865247219081087
Iteration 60, Training loss = 0.23365924302218616
Iteration 70, Training loss = 0.2308326760927836
Iteration 80, Training loss = 0.2298168181869143
Iteration 90, Training loss = 0.2257318168806569
Model training time: 29.38479709625244
Device: cuda
Iteration 0, Training loss = 0.7480841064510714
Iteration 10, Training loss = 0.26206138931610734
Iteration 20, Training loss = 0.24852331648558235
Iteration 30, Training loss = 0.2407511433109569
Iteration 40, Training loss = 0.2363954307736406
Iteration 50, Training loss = 0.23437601155129031
Iteration 60, Training loss = 0.23009297838389586
Iteration 70, Training loss = 0.22877231909744983
Iteration 80, Training loss = 0.22667109984274647
Iteration 90, Training loss = 0.22502550018438394
Model training time: 29.403533220291138
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9590559512520758
Iteration 10, Training loss = 0.44421972521549263
Iteration 20, Training loss = 0.3748337398980551
Iteration 30, Training loss = 0.3468971879441957
Iteration 40, Training loss = 0.3314237087101176
Iteration 50, Training loss = 0.3236179672145613
Iteration 60, Training loss = 0.3178070361055614
Iteration 70, Training loss = 0.312089482943217
Iteration 80, Training loss = 0.3069765901076045
Iteration 90, Training loss = 0.3033899112744032
Model training time: 25.32869029045105
Device: cuda
Iteration 0, Training loss = 0.9516887270310075
Iteration 10, Training loss = 0.47521389243395434
Iteration 20, Training loss = 0.376418960267219
Iteration 30, Training loss = 0.3425016018071612
Iteration 40, Training loss = 0.3284937604757899
Iteration 50, Training loss = 0.31933736678770774
Iteration 60, Training loss = 0.31319505321806757
Iteration 70, Training loss = 0.3091393830839563
Iteration 80, Training loss = 0.3034015321645184
Iteration 90, Training loss = 0.29905508721364293
Model training time: 25.637069702148438
Device: cuda
Iteration 0, Training loss = 1.0504234179782406
Iteration 10, Training loss = 0.9150162342089961
Iteration 20, Training loss = 0.6163179094088826
Iteration 30, Training loss = 0.44652572958077785
Iteration 40, Training loss = 0.37436215442735793
Iteration 50, Training loss = 0.3318253277580519
Iteration 60, Training loss = 0.31165511445434774
Iteration 70, Training loss = 0.3035366702194951
Iteration 80, Training loss = 0.29866487907182765
Iteration 90, Training loss = 0.295278361670061
Model training time: 26.019428491592407
Device: cuda
Iteration 0, Training loss = 0.9680893071319746
Iteration 10, Training loss = 0.6297932965456
Iteration 20, Training loss = 0.3922737066584509
Iteration 30, Training loss = 0.3495967159236687
Iteration 40, Training loss = 0.3311079306446988
Iteration 50, Training loss = 0.3192517170007678
Iteration 60, Training loss = 0.3097655701176556
Iteration 70, Training loss = 0.30268012368736635
Iteration 80, Training loss = 0.296745582551196
Iteration 90, Training loss = 0.291614705381762
Model training time: 25.430317163467407
Device: cuda
Iteration 0, Training loss = 1.0693331657400453
Iteration 10, Training loss = 0.5977800429443231
Iteration 20, Training loss = 0.37834177508158384
Iteration 30, Training loss = 0.3415721212176309
Iteration 40, Training loss = 0.32478456062395217
Iteration 50, Training loss = 0.3164629940537439
Iteration 60, Training loss = 0.31046988552319255
Iteration 70, Training loss = 0.3049448777512076
Iteration 80, Training loss = 0.3007895743501359
Iteration 90, Training loss = 0.29742887346208957
Model training time: 25.81826424598694
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6140675641200393
Iteration 10, Training loss = 0.25427831536617834
Iteration 20, Training loss = 0.24139104308425516
Iteration 30, Training loss = 0.23494920498075117
Iteration 40, Training loss = 0.230499891123334
Iteration 50, Training loss = 0.22733964083563302
Iteration 60, Training loss = 0.22478048024690095
Iteration 70, Training loss = 0.22116051754657773
Iteration 80, Training loss = 0.2201548409850701
Iteration 90, Training loss = 0.21796529754492397
Iteration 100, Training loss = 0.21709474078986957
Iteration 110, Training loss = 0.21499657386167037
Iteration 120, Training loss = 0.21487499082434006
Iteration 130, Training loss = 0.21264818029991095
Iteration 140, Training loss = 0.2114596969767469
Iteration 150, Training loss = 0.21014125645160675
Iteration 160, Training loss = 0.20865532946615403
Iteration 170, Training loss = 0.20870586984975326
Iteration 180, Training loss = 0.20812130010358376
Iteration 190, Training loss = 0.20705484044580644
Model training time: 59.74485230445862
Device: cuda
Iteration 0, Training loss = 0.6544976154531258
Iteration 10, Training loss = 0.2615523529801392
Iteration 20, Training loss = 0.2461862774286869
Iteration 30, Training loss = 0.2373222510019938
Iteration 40, Training loss = 0.23138676680948422
Iteration 50, Training loss = 0.2267201267146834
Iteration 60, Training loss = 0.22373464059282616
Iteration 70, Training loss = 0.22075858291076578
Iteration 80, Training loss = 0.21725425534490225
Iteration 90, Training loss = 0.21693110116869932
Iteration 100, Training loss = 0.2149474893287184
Iteration 110, Training loss = 0.214526509821127
Iteration 120, Training loss = 0.21311853444518675
Iteration 130, Training loss = 0.21205981612061534
Iteration 140, Training loss = 0.2117654536873246
Iteration 150, Training loss = 0.21091775812100674
Iteration 160, Training loss = 0.2108580969310037
Iteration 170, Training loss = 0.21065677824803597
Iteration 180, Training loss = 0.20967971601923882
Iteration 190, Training loss = 0.21043863662199122
Model training time: 58.966161012649536
Device: cuda
Iteration 0, Training loss = 0.6701389658422285
Iteration 10, Training loss = 0.26117152774679486
Iteration 20, Training loss = 0.24832371039235074
Iteration 30, Training loss = 0.2408252448419442
Iteration 40, Training loss = 0.2369937931281933
Iteration 50, Training loss = 0.23379239090830808
Iteration 60, Training loss = 0.23209756652370167
Iteration 70, Training loss = 0.2303433989485105
Iteration 80, Training loss = 0.23062081537816836
Iteration 90, Training loss = 0.2257607948520909
Iteration 100, Training loss = 0.22413285620114654
Iteration 110, Training loss = 0.22046515903467143
Iteration 120, Training loss = 0.21849111035682153
Iteration 130, Training loss = 0.21548411480470556
Iteration 140, Training loss = 0.21239397840367424
Iteration 150, Training loss = 0.21006801144944298
Iteration 160, Training loss = 0.20714471813129343
Iteration 170, Training loss = 0.2068175291237624
Iteration 180, Training loss = 0.2055897964299589
Iteration 190, Training loss = 0.20537949072709982
Model training time: 59.85005474090576
Device: cuda
Iteration 0, Training loss = 0.7607714284733298
Iteration 10, Training loss = 0.2606085764181211
Iteration 20, Training loss = 0.24270886823462978
Iteration 30, Training loss = 0.23472257573967395
Iteration 40, Training loss = 0.22743401514447253
Iteration 50, Training loss = 0.2234516570677504
Iteration 60, Training loss = 0.22087310322960793
Iteration 70, Training loss = 0.21735397312376234
Iteration 80, Training loss = 0.21618589589705214
Iteration 90, Training loss = 0.21420451440816915
Iteration 100, Training loss = 0.21286230555910995
Iteration 110, Training loss = 0.21090331500855045
Iteration 120, Training loss = 0.20927684516578482
Iteration 130, Training loss = 0.2086062321556363
Iteration 140, Training loss = 0.20778551888494676
Iteration 150, Training loss = 0.20924611882743052
Iteration 160, Training loss = 0.2069513183357059
Iteration 170, Training loss = 0.20539972972538736
Iteration 180, Training loss = 0.20502988954529094
Iteration 190, Training loss = 0.2043577860950848
Model training time: 59.72964024543762
Device: cuda
Iteration 0, Training loss = 0.7136579286936976
Iteration 10, Training loss = 0.2624690493091869
Iteration 20, Training loss = 0.24863877226621056
Iteration 30, Training loss = 0.23961239149748992
Iteration 40, Training loss = 0.23362571760508175
Iteration 50, Training loss = 0.2286226661596897
Iteration 60, Training loss = 0.22479480681356023
Iteration 70, Training loss = 0.22283410180162116
Iteration 80, Training loss = 0.21901757522049733
Iteration 90, Training loss = 0.2155283058539105
Iteration 100, Training loss = 0.21441112077178587
Iteration 110, Training loss = 0.2132669011319893
Iteration 120, Training loss = 0.21156885194173758
Iteration 130, Training loss = 0.2115388633908281
Iteration 140, Training loss = 0.20938941012114143
Iteration 150, Training loss = 0.20936764218812978
Iteration 160, Training loss = 0.2101205680249394
Iteration 170, Training loss = 0.20903568862428987
Iteration 180, Training loss = 0.2084515534233356
Iteration 190, Training loss = 0.20741026398640325
Model training time: 58.489025592803955
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0807835592163935
Iteration 10, Training loss = 0.6378528154702579
Iteration 20, Training loss = 0.3726593981618467
Iteration 30, Training loss = 0.3403224993299171
Iteration 40, Training loss = 0.3283421570696117
Iteration 50, Training loss = 0.32159574452229744
Iteration 60, Training loss = 0.31495353090014433
Iteration 70, Training loss = 0.31062169427025144
Iteration 80, Training loss = 0.3069848431913173
Iteration 90, Training loss = 0.30273496399179173
Iteration 100, Training loss = 0.29920427522797516
Iteration 110, Training loss = 0.2966315197771874
Iteration 120, Training loss = 0.29243499742469925
Iteration 130, Training loss = 0.2893182579158009
Iteration 140, Training loss = 0.2865100395708268
Iteration 150, Training loss = 0.283702082824016
Iteration 160, Training loss = 0.28134199602592397
Iteration 170, Training loss = 0.279273325694356
Iteration 180, Training loss = 0.27682411044404126
Iteration 190, Training loss = 0.27486218735215745
Model training time: 50.76017498970032
Device: cuda
Iteration 0, Training loss = 1.0509146869470531
Iteration 10, Training loss = 0.6595360458185131
Iteration 20, Training loss = 0.374223989708988
Iteration 30, Training loss = 0.3456716703044044
Iteration 40, Training loss = 0.3349003945139871
Iteration 50, Training loss = 0.3276692987352178
Iteration 60, Training loss = 0.32267820136846553
Iteration 70, Training loss = 0.318055200368022
Iteration 80, Training loss = 0.31561307889827783
Iteration 90, Training loss = 0.3126525179919413
Iteration 100, Training loss = 0.30935389688913373
Iteration 110, Training loss = 0.30804309184136597
Iteration 120, Training loss = 0.30271420102332525
Iteration 130, Training loss = 0.29968250553677045
Iteration 140, Training loss = 0.2968174011091103
Iteration 150, Training loss = 0.29298659172898905
Iteration 160, Training loss = 0.28907099765711936
Iteration 170, Training loss = 0.28680538238534603
Iteration 180, Training loss = 0.2829108150396946
Iteration 190, Training loss = 0.27947613409751854
Model training time: 50.68936228752136
Device: cuda
Iteration 0, Training loss = 0.9745946022623403
Iteration 10, Training loss = 0.5540016162510656
Iteration 20, Training loss = 0.3564154091808531
Iteration 30, Training loss = 0.33458948768855296
Iteration 40, Training loss = 0.3246127839969552
Iteration 50, Training loss = 0.31676318046551394
Iteration 60, Training loss = 0.31091613412479274
Iteration 70, Training loss = 0.3056752890492407
Iteration 80, Training loss = 0.30127435533896735
Iteration 90, Training loss = 0.29670890856192306
Iteration 100, Training loss = 0.29292552961387497
Iteration 110, Training loss = 0.2892208038176891
Iteration 120, Training loss = 0.2858713632044585
Iteration 130, Training loss = 0.2832434928215644
Iteration 140, Training loss = 0.2797269382770511
Iteration 150, Training loss = 0.2771631934216633
Iteration 160, Training loss = 0.27534844283608423
Iteration 170, Training loss = 0.27216443952155
Iteration 180, Training loss = 0.27050421149402426
Iteration 190, Training loss = 0.2684679842657513
Model training time: 51.68398594856262
Device: cuda
Iteration 0, Training loss = 1.0050071238895546
Iteration 10, Training loss = 0.7913522839834148
Iteration 20, Training loss = 0.3969892662936363
Iteration 30, Training loss = 0.3577488148270022
Iteration 40, Training loss = 0.3375351974641643
Iteration 50, Training loss = 0.32537679993300045
Iteration 60, Training loss = 0.31684763787161324
Iteration 70, Training loss = 0.3095393824404564
Iteration 80, Training loss = 0.3029849113185625
Iteration 90, Training loss = 0.2984772849342097
Iteration 100, Training loss = 0.2940336707709492
Iteration 110, Training loss = 0.2911252002209281
Iteration 120, Training loss = 0.2879128642560203
Iteration 130, Training loss = 0.28488558095290467
Iteration 140, Training loss = 0.28224653963017576
Iteration 150, Training loss = 0.28013440998999967
Iteration 160, Training loss = 0.27763003917132023
Iteration 170, Training loss = 0.2756391837113146
Iteration 180, Training loss = 0.27376550324441157
Iteration 190, Training loss = 0.2729161229830433
Model training time: 50.658530950546265
Device: cuda
Iteration 0, Training loss = 1.0110569173011228
Iteration 10, Training loss = 0.7493741890946448
Iteration 20, Training loss = 0.402328003719809
Iteration 30, Training loss = 0.3549531543456414
Iteration 40, Training loss = 0.3391127579166118
Iteration 50, Training loss = 0.3295644449726971
Iteration 60, Training loss = 0.32244478878767596
Iteration 70, Training loss = 0.31613423729288403
Iteration 80, Training loss = 0.31098620003260274
Iteration 90, Training loss = 0.30571189357174766
Iteration 100, Training loss = 0.30183442354490214
Iteration 110, Training loss = 0.2983315171682892
Iteration 120, Training loss = 0.2957271400137224
Iteration 130, Training loss = 0.29464840673018194
Iteration 140, Training loss = 0.290360893820219
Iteration 150, Training loss = 0.2882961618223628
Iteration 160, Training loss = 0.2862431564624759
Iteration 170, Training loss = 0.2842124334855932
Iteration 180, Training loss = 0.28219128749220845
Iteration 190, Training loss = 0.280571387477831
Model training time: 51.054250717163086
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7913807067606184
Iteration 10, Training loss = 0.268590337700314
Iteration 20, Training loss = 0.25606830684459153
Iteration 30, Training loss = 0.24907628702368714
Iteration 40, Training loss = 0.24401445449262427
Iteration 50, Training loss = 0.23958883513718987
Iteration 60, Training loss = 0.235350354174197
Iteration 70, Training loss = 0.2330632954116029
Iteration 80, Training loss = 0.23040702600697965
Iteration 90, Training loss = 0.22796913999434254
Iteration 100, Training loss = 0.22679982384765782
Iteration 110, Training loss = 0.22516616628221844
Iteration 120, Training loss = 0.2237367985257204
Iteration 130, Training loss = 0.22360380577004474
Iteration 140, Training loss = 0.2239360171527678
Iteration 150, Training loss = 0.22209222906309625
Iteration 160, Training loss = 0.2215312057071262
Iteration 170, Training loss = 0.22099123761993675
Iteration 180, Training loss = 0.2203550885480959
Iteration 190, Training loss = 0.21935229723292274
Model training time: 58.60412096977234
Device: cuda
Iteration 0, Training loss = 0.8399792948494786
Iteration 10, Training loss = 0.2688777813876885
Iteration 20, Training loss = 0.2552810582705742
Iteration 30, Training loss = 0.25042047330002853
Iteration 40, Training loss = 0.24788206538572405
Iteration 50, Training loss = 0.24350294744334935
Iteration 60, Training loss = 0.2396508690311713
Iteration 70, Training loss = 0.2365504695093574
Iteration 80, Training loss = 0.2346683501261444
Iteration 90, Training loss = 0.23171674805706827
Iteration 100, Training loss = 0.23032096366663485
Iteration 110, Training loss = 0.23018475409579162
Iteration 120, Training loss = 0.22751954651397208
Iteration 130, Training loss = 0.22654652905061048
Iteration 140, Training loss = 0.22606654839527204
Iteration 150, Training loss = 0.2265747968773335
Iteration 160, Training loss = 0.22471503416697183
Iteration 170, Training loss = 0.22518345033345016
Iteration 180, Training loss = 0.22438790523199645
Iteration 190, Training loss = 0.22361218997677743
Model training time: 64.47553324699402
Device: cuda
Iteration 0, Training loss = 0.6710233691233943
Iteration 10, Training loss = 0.27203046642063894
Iteration 20, Training loss = 0.2545994572377435
Iteration 30, Training loss = 0.24745453516210336
Iteration 40, Training loss = 0.24068685099122605
Iteration 50, Training loss = 0.2381226550337773
Iteration 60, Training loss = 0.23496817365504694
Iteration 70, Training loss = 0.2336036330403913
Iteration 80, Training loss = 0.23176633724987794
Iteration 90, Training loss = 0.23171581219935763
Iteration 100, Training loss = 0.2271446426205589
Iteration 110, Training loss = 0.2252578302713984
Iteration 120, Training loss = 0.22424740520652364
Iteration 130, Training loss = 0.2238076951912636
Iteration 140, Training loss = 0.22308805206979532
Iteration 150, Training loss = 0.22241723652623127
Iteration 160, Training loss = 0.22079797377045027
Iteration 170, Training loss = 0.2198835284885577
Iteration 180, Training loss = 0.22003103695485904
Iteration 190, Training loss = 0.21937866224614894
Model training time: 64.37117171287537
Device: cuda
Iteration 0, Training loss = 0.8577026409515436
Iteration 10, Training loss = 0.2696232332483582
Iteration 20, Training loss = 0.2576233003522463
Iteration 30, Training loss = 0.2521555110236297
Iteration 40, Training loss = 0.24819619467724924
Iteration 50, Training loss = 0.24191315688084866
Iteration 60, Training loss = 0.2384023305370612
Iteration 70, Training loss = 0.23434562221673375
Iteration 80, Training loss = 0.23156011100984425
Iteration 90, Training loss = 0.22960052286513186
Iteration 100, Training loss = 0.22781617817095512
Iteration 110, Training loss = 0.226302201513219
Iteration 120, Training loss = 0.22444722308340856
Iteration 130, Training loss = 0.2217317662305302
Iteration 140, Training loss = 0.22055838440639386
Iteration 150, Training loss = 0.21855399372497042
Iteration 160, Training loss = 0.2170968561068825
Iteration 170, Training loss = 0.21615876282614785
Iteration 180, Training loss = 0.21546490566022153
Iteration 190, Training loss = 0.2145529180261248
Model training time: 60.42936992645264
Device: cuda
Iteration 0, Training loss = 0.9006146206947916
Iteration 10, Training loss = 0.26812176359592427
Iteration 20, Training loss = 0.25370300208456853
Iteration 30, Training loss = 0.24767137610394022
Iteration 40, Training loss = 0.24171833485220942
Iteration 50, Training loss = 0.2373083159327507
Iteration 60, Training loss = 0.2362637844134644
Iteration 70, Training loss = 0.23246204313160718
Iteration 80, Training loss = 0.22996436779337806
Iteration 90, Training loss = 0.2292087885780611
Iteration 100, Training loss = 0.22770544443873392
Iteration 110, Training loss = 0.22574683116829913
Iteration 120, Training loss = 0.22463578342095666
Iteration 130, Training loss = 0.22389251537225097
Iteration 140, Training loss = 0.2223567366240105
Iteration 150, Training loss = 0.22167006106192363
Iteration 160, Training loss = 0.22092565854534435
Iteration 170, Training loss = 0.22030546771731355
Iteration 180, Training loss = 0.2199596929665349
Iteration 190, Training loss = 0.21787032765754755
Model training time: 60.17827391624451
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1072126304469823
Iteration 10, Training loss = 0.5856493842486598
Iteration 20, Training loss = 0.3896298011863865
Iteration 30, Training loss = 0.35081910928666304
Iteration 40, Training loss = 0.3329434882903445
Iteration 50, Training loss = 0.3231167535563022
Iteration 60, Training loss = 0.31660368189143673
Iteration 70, Training loss = 0.3126071161693997
Iteration 80, Training loss = 0.3079024346338378
Iteration 90, Training loss = 0.30462167955539077
Iteration 100, Training loss = 0.302429479389375
Iteration 110, Training loss = 0.2988098948330119
Iteration 120, Training loss = 0.29796229554835146
Iteration 130, Training loss = 0.29382407060567883
Iteration 140, Training loss = 0.29193607289434054
Iteration 150, Training loss = 0.29017528294077244
Iteration 160, Training loss = 0.28788622580288686
Iteration 170, Training loss = 0.2861898357885471
Iteration 180, Training loss = 0.2842547250686636
Iteration 190, Training loss = 0.2829041704895416
Model training time: 51.431758403778076
Device: cuda
Iteration 0, Training loss = 1.0517518782385304
Iteration 10, Training loss = 0.595978079498678
Iteration 20, Training loss = 0.4277228053641204
Iteration 30, Training loss = 0.37091449313405633
Iteration 40, Training loss = 0.34326259496707273
Iteration 50, Training loss = 0.33101848300528414
Iteration 60, Training loss = 0.3235111029251762
Iteration 70, Training loss = 0.31754620711584597
Iteration 80, Training loss = 0.3128734459551636
Iteration 90, Training loss = 0.30844607520938494
Iteration 100, Training loss = 0.3041891719120136
Iteration 110, Training loss = 0.300087277871975
Iteration 120, Training loss = 0.29694708721073354
Iteration 130, Training loss = 0.29392356767458616
Iteration 140, Training loss = 0.2912654630515886
Iteration 150, Training loss = 0.2889437982857515
Iteration 160, Training loss = 0.28665244377753585
Iteration 170, Training loss = 0.2846717809997319
Iteration 180, Training loss = 0.2832944516974371
Iteration 190, Training loss = 0.28139284314308766
Model training time: 51.46193075180054
Device: cuda
Iteration 0, Training loss = 1.0255951486923844
Iteration 10, Training loss = 0.7491077901660532
Iteration 20, Training loss = 0.3973414744587912
Iteration 30, Training loss = 0.3494064585454222
Iteration 40, Training loss = 0.33411598162374634
Iteration 50, Training loss = 0.3255612551446122
Iteration 60, Training loss = 0.31853381173622203
Iteration 70, Training loss = 0.31447790095195677
Iteration 80, Training loss = 0.3095855295370166
Iteration 90, Training loss = 0.30560646346513776
Iteration 100, Training loss = 0.3023568554488933
Iteration 110, Training loss = 0.2988733834978463
Iteration 120, Training loss = 0.2962933120808164
Iteration 130, Training loss = 0.29353271532749786
Iteration 140, Training loss = 0.2922827352360251
Iteration 150, Training loss = 0.2901139783398541
Iteration 160, Training loss = 0.28799478817677154
Iteration 170, Training loss = 0.2868440799091173
Iteration 180, Training loss = 0.28490945653638977
Iteration 190, Training loss = 0.28334376158345725
Model training time: 52.17417883872986
Device: cuda
Iteration 0, Training loss = 1.0227145843459788
Iteration 10, Training loss = 0.8190255382498681
Iteration 20, Training loss = 0.4134970305910433
Iteration 30, Training loss = 0.34936327584411786
Iteration 40, Training loss = 0.3294092073388722
Iteration 50, Training loss = 0.3207876405854156
Iteration 60, Training loss = 0.315003441799666
Iteration 70, Training loss = 0.3106421056576973
Iteration 80, Training loss = 0.30609551172901467
Iteration 90, Training loss = 0.3021367375833401
Iteration 100, Training loss = 0.2987011173641048
Iteration 110, Training loss = 0.2945891038159241
Iteration 120, Training loss = 0.2910046543620059
Iteration 130, Training loss = 0.28838849549996104
Iteration 140, Training loss = 0.28558750911323344
Iteration 150, Training loss = 0.2830411319931348
Iteration 160, Training loss = 0.28321339366372655
Iteration 170, Training loss = 0.2795038095490944
Iteration 180, Training loss = 0.27801219327150334
Iteration 190, Training loss = 0.2762602566233004
Model training time: 50.64943814277649
Device: cuda
Iteration 0, Training loss = 1.003140928664645
Iteration 10, Training loss = 0.8301926520135667
Iteration 20, Training loss = 0.3897473442093762
Iteration 30, Training loss = 0.3336370319559954
Iteration 40, Training loss = 0.31732196429213466
Iteration 50, Training loss = 0.3092150763037124
Iteration 60, Training loss = 0.3072481312325611
Iteration 70, Training loss = 0.30036243396824686
Iteration 80, Training loss = 0.2959435104531942
Iteration 90, Training loss = 0.29277754585812055
Iteration 100, Training loss = 0.28958902713181317
Iteration 110, Training loss = 0.286346467004882
Iteration 120, Training loss = 0.2832262272708082
Iteration 130, Training loss = 0.2809625309014666
Iteration 140, Training loss = 0.2786088200007084
Iteration 150, Training loss = 0.2767370048064541
Iteration 160, Training loss = 0.27485429520768245
Iteration 170, Training loss = 0.2736415485828971
Iteration 180, Training loss = 0.2710949136439153
Iteration 190, Training loss = 0.27065796243107837
Model training time: 51.53640842437744
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7360919670062365
Iteration 10, Training loss = 0.2599590426481864
Iteration 20, Training loss = 0.24646513991885716
Iteration 30, Training loss = 0.24077820040033635
Iteration 40, Training loss = 0.23803399600412534
Iteration 50, Training loss = 0.23594408723467214
Iteration 60, Training loss = 0.23303208633321496
Iteration 70, Training loss = 0.23203979464976685
Iteration 80, Training loss = 0.22991800437802853
Iteration 90, Training loss = 0.22838491240993214
Iteration 100, Training loss = 0.2266533068124799
Iteration 110, Training loss = 0.22528993183575965
Iteration 120, Training loss = 0.22451750574192564
Iteration 130, Training loss = 0.22257739168722274
Iteration 140, Training loss = 0.2208116279851987
Iteration 150, Training loss = 0.2185242286554857
Iteration 160, Training loss = 0.21738888629248754
Iteration 170, Training loss = 0.21560965845550317
Iteration 180, Training loss = 0.21535317646564492
Iteration 190, Training loss = 0.21339972669951582
Model training time: 58.955915212631226
Device: cuda
Iteration 0, Training loss = 0.6927503314283159
Iteration 10, Training loss = 0.26239494371097444
Iteration 20, Training loss = 0.2505905008762355
Iteration 30, Training loss = 0.24363803089672817
Iteration 40, Training loss = 0.2393167690522429
Iteration 50, Training loss = 0.23488104930102538
Iteration 60, Training loss = 0.2294332629096681
Iteration 70, Training loss = 0.2272519706815913
Iteration 80, Training loss = 0.2259005161659153
Iteration 90, Training loss = 0.22397114937576118
Iteration 100, Training loss = 0.221182851039845
Iteration 110, Training loss = 0.21920719901144792
Iteration 120, Training loss = 0.21767686915714385
Iteration 130, Training loss = 0.21588149141286306
Iteration 140, Training loss = 0.21481945394461857
Iteration 150, Training loss = 0.21308407730958312
Iteration 160, Training loss = 0.2110983394626258
Iteration 170, Training loss = 0.2095236749464763
Iteration 180, Training loss = 0.20859238203021063
Iteration 190, Training loss = 0.2058096808799799
Model training time: 58.3255717754364
Device: cuda
Iteration 0, Training loss = 0.6225971412543513
Iteration 10, Training loss = 0.2580428609741483
Iteration 20, Training loss = 0.24535697173093252
Iteration 30, Training loss = 0.2393573327484914
Iteration 40, Training loss = 0.2352446897522263
Iteration 50, Training loss = 0.2332460743865529
Iteration 60, Training loss = 0.23087019808050516
Iteration 70, Training loss = 0.22918470344249753
Iteration 80, Training loss = 0.22685401436787297
Iteration 90, Training loss = 0.225763193399146
Iteration 100, Training loss = 0.22392445248394197
Iteration 110, Training loss = 0.22293969078196418
Iteration 120, Training loss = 0.22191781896180002
Iteration 130, Training loss = 0.22029797999179307
Iteration 140, Training loss = 0.21894820811955826
Iteration 150, Training loss = 0.2176301681621063
Iteration 160, Training loss = 0.2172678910591752
Iteration 170, Training loss = 0.2169883654313387
Iteration 180, Training loss = 0.21631442651080623
Iteration 190, Training loss = 0.21538033815109786
Model training time: 60.29790449142456
Device: cuda
Iteration 0, Training loss = 0.624035188973238
Iteration 10, Training loss = 0.26318054527476215
Iteration 20, Training loss = 0.24421912719662062
Iteration 30, Training loss = 0.2349908499830011
Iteration 40, Training loss = 0.2300327316214497
Iteration 50, Training loss = 0.2263050287746001
Iteration 60, Training loss = 0.22214230311953503
Iteration 70, Training loss = 0.2204782533760808
Iteration 80, Training loss = 0.21840061265345356
Iteration 90, Training loss = 0.21585406197441948
Iteration 100, Training loss = 0.21357889617410836
Iteration 110, Training loss = 0.2127181557929458
Iteration 120, Training loss = 0.21152697010460683
Iteration 130, Training loss = 0.21018886753326452
Iteration 140, Training loss = 0.20918221213823354
Iteration 150, Training loss = 0.2080557130385136
Iteration 160, Training loss = 0.20617968203940829
Iteration 170, Training loss = 0.20643124200295712
Iteration 180, Training loss = 0.2060403522230001
Iteration 190, Training loss = 0.20643300854641458
Model training time: 58.5335807800293
Device: cuda
Iteration 0, Training loss = 0.9023696986085551
Iteration 10, Training loss = 0.2680346478442639
Iteration 20, Training loss = 0.2507012910097118
Iteration 30, Training loss = 0.24193230285736675
Iteration 40, Training loss = 0.23578180725447798
Iteration 50, Training loss = 0.23200255618003254
Iteration 60, Training loss = 0.22971258172090503
Iteration 70, Training loss = 0.22789246686126874
Iteration 80, Training loss = 0.22583897980514933
Iteration 90, Training loss = 0.22454590933478397
Iteration 100, Training loss = 0.22334800761391
Iteration 110, Training loss = 0.2226428996393646
Iteration 120, Training loss = 0.22081262405943755
Iteration 130, Training loss = 0.22244158070444484
Iteration 140, Training loss = 0.21968056754645518
Iteration 150, Training loss = 0.2182179111717404
Iteration 160, Training loss = 0.2178258367134753
Iteration 170, Training loss = 0.21760753452201972
Iteration 180, Training loss = 0.2162658275253531
Iteration 190, Training loss = 0.2162696610470325
Model training time: 58.939911127090454
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0621620690189122
Iteration 10, Training loss = 0.9043934777163077
Iteration 20, Training loss = 0.469915875994065
Iteration 30, Training loss = 0.37655016808694114
Iteration 40, Training loss = 0.3408101405498486
Iteration 50, Training loss = 0.3280180876237759
Iteration 60, Training loss = 0.3217059203248093
Iteration 70, Training loss = 0.31809838778442806
Iteration 80, Training loss = 0.31269435390182165
Iteration 90, Training loss = 0.3097755097680622
Iteration 100, Training loss = 0.3067900742885571
Iteration 110, Training loss = 0.3036320136797025
Iteration 120, Training loss = 0.3012687575413985
Iteration 130, Training loss = 0.29832735569508756
Iteration 140, Training loss = 0.29633078360615145
Iteration 150, Training loss = 0.2941890462585118
Iteration 160, Training loss = 0.29230808153532556
Iteration 170, Training loss = 0.29022118434813865
Iteration 180, Training loss = 0.28778419481671375
Iteration 190, Training loss = 0.2855149716282812
Model training time: 50.753602743148804
Device: cuda
Iteration 0, Training loss = 1.011992725773134
Iteration 10, Training loss = 0.7846352409049508
Iteration 20, Training loss = 0.3815645158722781
Iteration 30, Training loss = 0.3450770298927878
Iteration 40, Training loss = 0.33082288316482505
Iteration 50, Training loss = 0.32265657474453324
Iteration 60, Training loss = 0.317096599418184
Iteration 70, Training loss = 0.3130763864171678
Iteration 80, Training loss = 0.3076403872402394
Iteration 90, Training loss = 0.3040912170772967
Iteration 100, Training loss = 0.3000035553738691
Iteration 110, Training loss = 0.2973515317203918
Iteration 120, Training loss = 0.2943261592572438
Iteration 130, Training loss = 0.29237683290156763
Iteration 140, Training loss = 0.2889147318071789
Iteration 150, Training loss = 0.28674441000113743
Iteration 160, Training loss = 0.2842220478299735
Iteration 170, Training loss = 0.2820306977500086
Iteration 180, Training loss = 0.2802505787947904
Iteration 190, Training loss = 0.27808158643147796
Model training time: 50.598557233810425
Device: cuda
Iteration 0, Training loss = 1.0322697488006187
Iteration 10, Training loss = 0.714979636784337
Iteration 20, Training loss = 0.3897246207304047
Iteration 30, Training loss = 0.3503383825654569
Iteration 40, Training loss = 0.33132322802059894
Iteration 50, Training loss = 0.3211163493314227
Iteration 60, Training loss = 0.3129293451418623
Iteration 70, Training loss = 0.30644403966728617
Iteration 80, Training loss = 0.30221947214165745
Iteration 90, Training loss = 0.29808371496085384
Iteration 100, Training loss = 0.2941181358795811
Iteration 110, Training loss = 0.29078140106178135
Iteration 120, Training loss = 0.28744533737212563
Iteration 130, Training loss = 0.2845451686405329
Iteration 140, Training loss = 0.2816319180931446
Iteration 150, Training loss = 0.2795488041452164
Iteration 160, Training loss = 0.27755219680099674
Iteration 170, Training loss = 0.27477944019624
Iteration 180, Training loss = 0.27305126438538235
Iteration 190, Training loss = 0.27168355421455587
Model training time: 50.961639642715454
Device: cuda
Iteration 0, Training loss = 1.0938320954640706
Iteration 10, Training loss = 0.8222734441503811
Iteration 20, Training loss = 0.3941032224231296
Iteration 30, Training loss = 0.34716884230358014
Iteration 40, Training loss = 0.32863734763313607
Iteration 50, Training loss = 0.31611577684176717
Iteration 60, Training loss = 0.3072703960509116
Iteration 70, Training loss = 0.30192740945424434
Iteration 80, Training loss = 0.2955338142200369
Iteration 90, Training loss = 0.2909323640059734
Iteration 100, Training loss = 0.2874241900688784
Iteration 110, Training loss = 0.2844212749154095
Iteration 120, Training loss = 0.28179701666037243
Iteration 130, Training loss = 0.2795530557272515
Iteration 140, Training loss = 0.27829587581940896
Iteration 150, Training loss = 0.2771157804462645
Iteration 160, Training loss = 0.2736751140963628
Iteration 170, Training loss = 0.2719497263863467
Iteration 180, Training loss = 0.2711776506497664
Iteration 190, Training loss = 0.2701398088520276
Model training time: 51.242239236831665
Device: cuda
Iteration 0, Training loss = 0.9977017413590841
Iteration 10, Training loss = 0.5352637607982194
Iteration 20, Training loss = 0.3528915177940746
Iteration 30, Training loss = 0.33320715400763756
Iteration 40, Training loss = 0.31880351440342153
Iteration 50, Training loss = 0.3084537258758637
Iteration 60, Training loss = 0.30221073552606187
Iteration 70, Training loss = 0.29838962284263204
Iteration 80, Training loss = 0.29319112035243405
Iteration 90, Training loss = 0.28903514197195207
Iteration 100, Training loss = 0.28576814282487556
Iteration 110, Training loss = 0.28323431658572046
Iteration 120, Training loss = 0.2801680474177651
Iteration 130, Training loss = 0.27686954368859673
Iteration 140, Training loss = 0.274588614522259
Iteration 150, Training loss = 0.2724441342451722
Iteration 160, Training loss = 0.270142357526482
Iteration 170, Training loss = 0.26907269505486975
Iteration 180, Training loss = 0.26723116251149615
Iteration 190, Training loss = 0.2653693335787686
Model training time: 51.00550031661987
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6775870282868832
Iteration 10, Training loss = 0.2620247463889168
Iteration 20, Training loss = 0.24735497526716496
Iteration 30, Training loss = 0.24183566004469775
Iteration 40, Training loss = 0.23853336058665012
Iteration 50, Training loss = 0.2355748925016122
Iteration 60, Training loss = 0.23272086715928597
Iteration 70, Training loss = 0.23058903130932132
Iteration 80, Training loss = 0.22804704794417258
Iteration 90, Training loss = 0.22604053957018874
Iteration 100, Training loss = 0.22356765807250847
Iteration 110, Training loss = 0.22176320152582177
Iteration 120, Training loss = 0.22002122659182202
Iteration 130, Training loss = 0.21580202971103687
Iteration 140, Training loss = 0.2132602477275231
Iteration 150, Training loss = 0.2131284925960688
Iteration 160, Training loss = 0.21084102891061618
Iteration 170, Training loss = 0.2082462650206354
Iteration 180, Training loss = 0.2074471282714231
Iteration 190, Training loss = 0.2059738250890216
Iteration 200, Training loss = 0.20512592400185728
Iteration 210, Training loss = 0.2052491807923225
Iteration 220, Training loss = 0.20455439367156097
Iteration 230, Training loss = 0.20456926236693987
Iteration 240, Training loss = 0.2035399196660461
Iteration 250, Training loss = 0.20258695894969259
Iteration 260, Training loss = 0.20304039162066248
Iteration 270, Training loss = 0.20254220244821142
Iteration 280, Training loss = 0.20140753290503496
Iteration 290, Training loss = 0.201769280584826
Model training time: 89.18423986434937
Device: cuda
Iteration 0, Training loss = 0.5731812486614006
Iteration 10, Training loss = 0.25079636261371024
Iteration 20, Training loss = 0.2412346781451921
Iteration 30, Training loss = 0.2339670321215754
Iteration 40, Training loss = 0.22910437056263863
Iteration 50, Training loss = 0.22572147738242496
Iteration 60, Training loss = 0.22455366870055452
Iteration 70, Training loss = 0.221926622632621
Iteration 80, Training loss = 0.21998061614048078
Iteration 90, Training loss = 0.2184208484303548
Iteration 100, Training loss = 0.21764741528437334
Iteration 110, Training loss = 0.21548420075633098
Iteration 120, Training loss = 0.21531716406633314
Iteration 130, Training loss = 0.21304474922193997
Iteration 140, Training loss = 0.2123364465320168
Iteration 150, Training loss = 0.21266157572396135
Iteration 160, Training loss = 0.2106313786788839
Iteration 170, Training loss = 0.210589445443545
Iteration 180, Training loss = 0.20883229000556872
Iteration 190, Training loss = 0.2070484511230303
Iteration 200, Training loss = 0.20758291330314488
Iteration 210, Training loss = 0.20651038980858338
Iteration 220, Training loss = 0.20571136474609375
Iteration 230, Training loss = 0.2049887705468325
Iteration 240, Training loss = 0.2035263752663769
Iteration 250, Training loss = 0.20401166379451752
Iteration 260, Training loss = 0.2039155646942664
Iteration 270, Training loss = 0.20329155916896996
Iteration 280, Training loss = 0.2020615674375336
Iteration 290, Training loss = 0.20062994330689526
Model training time: 92.56868362426758
Device: cuda
Iteration 0, Training loss = 0.6895975592919594
Iteration 10, Training loss = 0.2555250206935233
Iteration 20, Training loss = 0.24014169841572858
Iteration 30, Training loss = 0.23356544712315436
Iteration 40, Training loss = 0.22753146618316714
Iteration 50, Training loss = 0.22388175565407473
Iteration 60, Training loss = 0.2201033428599293
Iteration 70, Training loss = 0.21829837776612546
Iteration 80, Training loss = 0.21579840556147017
Iteration 90, Training loss = 0.2138162713119949
Iteration 100, Training loss = 0.21213779317727988
Iteration 110, Training loss = 0.2099444346079504
Iteration 120, Training loss = 0.21066053020925338
Iteration 130, Training loss = 0.20801162309404733
Iteration 140, Training loss = 0.2063603111439281
Iteration 150, Training loss = 0.20505310411470523
Iteration 160, Training loss = 0.20425244859883174
Iteration 170, Training loss = 0.20212858173870235
Iteration 180, Training loss = 0.20213060869254929
Iteration 190, Training loss = 0.20186097919940948
Iteration 200, Training loss = 0.20073174947989736
Iteration 210, Training loss = 0.20044204099598714
Iteration 220, Training loss = 0.19969440885068138
Iteration 230, Training loss = 0.2000186602058618
Iteration 240, Training loss = 0.19873777161473813
Iteration 250, Training loss = 0.19783657961997433
Iteration 260, Training loss = 0.1975524741526387
Iteration 270, Training loss = 0.197460251117962
Iteration 280, Training loss = 0.19564273940840204
Iteration 290, Training loss = 0.1955717335216665
Model training time: 89.32776689529419
Device: cuda
Iteration 0, Training loss = 0.6609924056967675
Iteration 10, Training loss = 0.2513858423284862
Iteration 20, Training loss = 0.23975703181419972
Iteration 30, Training loss = 0.2309371895980144
Iteration 40, Training loss = 0.22358481301633631
Iteration 50, Training loss = 0.2195161366880228
Iteration 60, Training loss = 0.21421540625717328
Iteration 70, Training loss = 0.21259616390950437
Iteration 80, Training loss = 0.20880982495736386
Iteration 90, Training loss = 0.2073986271513257
Iteration 100, Training loss = 0.20474965175712742
Iteration 110, Training loss = 0.20503659028074014
Iteration 120, Training loss = 0.20415988870432988
Iteration 130, Training loss = 0.20289118731943306
Iteration 140, Training loss = 0.20145854411493752
Iteration 150, Training loss = 0.20104175248151815
Iteration 160, Training loss = 0.20003663694513016
Iteration 170, Training loss = 0.19894678667547622
Iteration 180, Training loss = 0.19846137271123232
Iteration 190, Training loss = 0.19694239462631336
Iteration 200, Training loss = 0.19761644879688964
Iteration 210, Training loss = 0.19518501725461748
Iteration 220, Training loss = 0.19479782609403998
Iteration 230, Training loss = 0.19559660899466363
Iteration 240, Training loss = 0.1940206874899818
Iteration 250, Training loss = 0.19403530920041356
Iteration 260, Training loss = 0.19363010062831612
Iteration 270, Training loss = 0.1931956718509324
Iteration 280, Training loss = 0.19347772398144727
Iteration 290, Training loss = 0.19246225408165926
Model training time: 89.80176448822021
Device: cuda
Iteration 0, Training loss = 0.6545914340278377
Iteration 10, Training loss = 0.25651016586644637
Iteration 20, Training loss = 0.24259960046712903
Iteration 30, Training loss = 0.23508591701587042
Iteration 40, Training loss = 0.22950910182966702
Iteration 50, Training loss = 0.2271277319261993
Iteration 60, Training loss = 0.22380021405248826
Iteration 70, Training loss = 0.2213746481327619
Iteration 80, Training loss = 0.21985563236302225
Iteration 90, Training loss = 0.21697307288070808
Iteration 100, Training loss = 0.2160389016287914
Iteration 110, Training loss = 0.21498028221337692
Iteration 120, Training loss = 0.2126132840839561
Iteration 130, Training loss = 0.21080505480800849
Iteration 140, Training loss = 0.21003374387172685
Iteration 150, Training loss = 0.21044109185824647
Iteration 160, Training loss = 0.20908401402586324
Iteration 170, Training loss = 0.20930860580309577
Iteration 180, Training loss = 0.2082686579745749
Iteration 190, Training loss = 0.20703794900777836
Iteration 200, Training loss = 0.2063295988788927
Iteration 210, Training loss = 0.2056899024848489
Iteration 220, Training loss = 0.20487369301814387
Iteration 230, Training loss = 0.20522814805524936
Iteration 240, Training loss = 0.2047546885655698
Iteration 250, Training loss = 0.20315607021684232
Iteration 260, Training loss = 0.20243885458091607
Iteration 270, Training loss = 0.20152400783150667
Iteration 280, Training loss = 0.20072353606062812
Iteration 290, Training loss = 0.20089883803601427
Model training time: 88.3829038143158
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0190042945497855
Iteration 10, Training loss = 0.5410332034751413
Iteration 20, Training loss = 0.4063832953500287
Iteration 30, Training loss = 0.35953866953147207
Iteration 40, Training loss = 0.33618115025442
Iteration 50, Training loss = 0.3245468741453788
Iteration 60, Training loss = 0.3177357582366409
Iteration 70, Training loss = 0.3112023045907274
Iteration 80, Training loss = 0.3076156253975946
Iteration 90, Training loss = 0.30236682598141656
Iteration 100, Training loss = 0.2981591195156033
Iteration 110, Training loss = 0.295024119425511
Iteration 120, Training loss = 0.29157761473586596
Iteration 130, Training loss = 0.2889787970101776
Iteration 140, Training loss = 0.2859324929866814
Iteration 150, Training loss = 0.28290847501749
Iteration 160, Training loss = 0.2799108386183706
Iteration 170, Training loss = 0.27744283833077565
Iteration 180, Training loss = 0.2756091158315179
Iteration 190, Training loss = 0.2721978806787067
Iteration 200, Training loss = 0.2698973664195065
Iteration 210, Training loss = 0.26855338566401155
Iteration 220, Training loss = 0.26574485885780214
Iteration 230, Training loss = 0.263881857952346
Iteration 240, Training loss = 0.26223084352154663
Iteration 250, Training loss = 0.261394869050254
Iteration 260, Training loss = 0.25980012136812947
Iteration 270, Training loss = 0.25914392566335376
Iteration 280, Training loss = 0.2579151438774118
Iteration 290, Training loss = 0.25678205922030023
Model training time: 77.54498314857483
Device: cuda
Iteration 0, Training loss = 0.9959692393524059
Iteration 10, Training loss = 0.6059149428842149
Iteration 20, Training loss = 0.4492527111860865
Iteration 30, Training loss = 0.3953970626068576
Iteration 40, Training loss = 0.3610433743915696
Iteration 50, Training loss = 0.3401859660367459
Iteration 60, Training loss = 0.32750913990292574
Iteration 70, Training loss = 0.31734011541818075
Iteration 80, Training loss = 0.3087516068548396
Iteration 90, Training loss = 0.3012801976379565
Iteration 100, Training loss = 0.295453198398081
Iteration 110, Training loss = 0.29062080224930953
Iteration 120, Training loss = 0.28644778957401495
Iteration 130, Training loss = 0.2825710166623627
Iteration 140, Training loss = 0.27968940149614774
Iteration 150, Training loss = 0.2764352456383083
Iteration 160, Training loss = 0.2738867094335349
Iteration 170, Training loss = 0.2715449318073798
Iteration 180, Training loss = 0.2693714969184088
Iteration 190, Training loss = 0.2670748593366664
Iteration 200, Training loss = 0.2654940390932387
Iteration 210, Training loss = 0.2634089535129243
Iteration 220, Training loss = 0.2622415216072746
Iteration 230, Training loss = 0.26089024950484724
Iteration 240, Training loss = 0.2594858247084894
Iteration 250, Training loss = 0.2591705117824573
Iteration 260, Training loss = 0.2581165602817628
Iteration 270, Training loss = 0.25683728238378745
Iteration 280, Training loss = 0.25548710594862556
Iteration 290, Training loss = 0.2547338481110651
Model training time: 77.38009071350098
Device: cuda
Iteration 0, Training loss = 1.0421097975422218
Iteration 10, Training loss = 0.6189498871132948
Iteration 20, Training loss = 0.381713904692355
Iteration 30, Training loss = 0.35291768401717216
Iteration 40, Training loss = 0.33946930145583865
Iteration 50, Training loss = 0.32914360364278156
Iteration 60, Training loss = 0.32204161260439
Iteration 70, Training loss = 0.3162464272716771
Iteration 80, Training loss = 0.3114349335648011
Iteration 90, Training loss = 0.305403882826584
Iteration 100, Training loss = 0.3020510802378401
Iteration 110, Training loss = 0.2992071097599711
Iteration 120, Training loss = 0.29547847875794353
Iteration 130, Training loss = 0.2928531756579588
Iteration 140, Training loss = 0.2901607259100186
Iteration 150, Training loss = 0.28693423740529783
Iteration 160, Training loss = 0.28471681122906545
Iteration 170, Training loss = 0.28241662008030977
Iteration 180, Training loss = 0.2811171134600893
Iteration 190, Training loss = 0.27843895691316484
Iteration 200, Training loss = 0.27604712988587393
Iteration 210, Training loss = 0.2748338026556992
Iteration 220, Training loss = 0.27233913083727235
Iteration 230, Training loss = 0.2715240011828533
Iteration 240, Training loss = 0.2694149005456247
Iteration 250, Training loss = 0.2687545797387183
Iteration 260, Training loss = 0.26709589441329384
Iteration 270, Training loss = 0.26530649066691236
Iteration 280, Training loss = 0.26368843353744864
Iteration 290, Training loss = 0.26293088963642214
Model training time: 76.94247841835022
Device: cuda
Iteration 0, Training loss = 1.0581726089191898
Iteration 10, Training loss = 0.6815201074316881
Iteration 20, Training loss = 0.34844703950743744
Iteration 30, Training loss = 0.3248777023836035
Iteration 40, Training loss = 0.315885701869133
Iteration 50, Training loss = 0.3092524753388575
Iteration 60, Training loss = 0.30465404005442265
Iteration 70, Training loss = 0.2992350519567296
Iteration 80, Training loss = 0.29466075720130536
Iteration 90, Training loss = 0.2915080488447982
Iteration 100, Training loss = 0.28677339203979657
Iteration 110, Training loss = 0.28409644936161915
Iteration 120, Training loss = 0.28125976853900486
Iteration 130, Training loss = 0.2779299314111327
Iteration 140, Training loss = 0.27589254164033467
Iteration 150, Training loss = 0.2737056382972261
Iteration 160, Training loss = 0.2718133761183075
Iteration 170, Training loss = 0.27044698095695985
Iteration 180, Training loss = 0.268678056013181
Iteration 190, Training loss = 0.26723492030360274
Iteration 200, Training loss = 0.2661818581574781
Iteration 210, Training loss = 0.264831966644036
Iteration 220, Training loss = 0.2641310606529747
Iteration 230, Training loss = 0.26290948451429175
Iteration 240, Training loss = 0.26217456248359405
Iteration 250, Training loss = 0.26120712323753154
Iteration 260, Training loss = 0.25961479400667015
Iteration 270, Training loss = 0.25896000473395636
Iteration 280, Training loss = 0.259051565170864
Iteration 290, Training loss = 0.2578202490717317
Model training time: 76.77936506271362
Device: cuda
Iteration 0, Training loss = 1.0123741073884827
Iteration 10, Training loss = 0.6323859586519895
Iteration 20, Training loss = 0.3953312261812929
Iteration 30, Training loss = 0.3551249803552305
Iteration 40, Training loss = 0.33890994915351774
Iteration 50, Training loss = 0.32722929850724586
Iteration 60, Training loss = 0.32129117126626094
Iteration 70, Training loss = 0.3140564389562837
Iteration 80, Training loss = 0.3085013522906004
Iteration 90, Training loss = 0.3038198612450402
Iteration 100, Training loss = 0.2999358449653151
Iteration 110, Training loss = 0.2957188979439113
Iteration 120, Training loss = 0.29275936420988924
Iteration 130, Training loss = 0.28965402837248816
Iteration 140, Training loss = 0.2859534093939164
Iteration 150, Training loss = 0.28354192795096966
Iteration 160, Training loss = 0.280390327607376
Iteration 170, Training loss = 0.2782247892900366
Iteration 180, Training loss = 0.2760959844946285
Iteration 190, Training loss = 0.2732009261054693
Iteration 200, Training loss = 0.27113559017866706
Iteration 210, Training loss = 0.2689024195578939
Iteration 220, Training loss = 0.2668241976899801
Iteration 230, Training loss = 0.26581174505937505
Iteration 240, Training loss = 0.26463343793355326
Iteration 250, Training loss = 0.26207490391345417
Iteration 260, Training loss = 0.26063848945541657
Iteration 270, Training loss = 0.2591682457621547
Iteration 280, Training loss = 0.25857558770888095
Iteration 290, Training loss = 0.25795442985307765
Model training time: 75.94053745269775
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.706468151603344
Iteration 10, Training loss = 0.2694998623236366
Iteration 20, Training loss = 0.24910392802982514
Iteration 30, Training loss = 0.2413295049958183
Iteration 40, Training loss = 0.23615193277021537
Iteration 50, Training loss = 0.2326487088980882
Iteration 60, Training loss = 0.23135149662477383
Iteration 70, Training loss = 0.23055712407193898
Iteration 80, Training loss = 0.22717294906792435
Iteration 90, Training loss = 0.2261980393010637
Iteration 100, Training loss = 0.22410749316503462
Iteration 110, Training loss = 0.22300569479160262
Iteration 120, Training loss = 0.22058429637392937
Iteration 130, Training loss = 0.21963960569405902
Iteration 140, Training loss = 0.22055849432945251
Iteration 150, Training loss = 0.2193847650995001
Iteration 160, Training loss = 0.21979407336256931
Iteration 170, Training loss = 0.21733724185522052
Iteration 180, Training loss = 0.2186946737881444
Iteration 190, Training loss = 0.21685046900585653
Iteration 200, Training loss = 0.21713000751923825
Iteration 210, Training loss = 0.21683096576140123
Iteration 220, Training loss = 0.21779608582528892
Iteration 230, Training loss = 0.2170093094813075
Iteration 240, Training loss = 0.21770680220662683
Iteration 250, Training loss = 0.21477576852708624
Iteration 260, Training loss = 0.21383013237933607
Iteration 270, Training loss = 0.21148726779625612
Iteration 280, Training loss = 0.211847133970491
Iteration 290, Training loss = 0.21132884361749685
Model training time: 96.72672033309937
Device: cuda
Iteration 0, Training loss = 0.7887829160632719
Iteration 10, Training loss = 0.27434871543720724
Iteration 20, Training loss = 0.25627849913305706
Iteration 30, Training loss = 0.24942103773355484
Iteration 40, Training loss = 0.2444827154206769
Iteration 50, Training loss = 0.24097558200935235
Iteration 60, Training loss = 0.2390239355356797
Iteration 70, Training loss = 0.23623149204945218
Iteration 80, Training loss = 0.23425985322050427
Iteration 90, Training loss = 0.2319988228632632
Iteration 100, Training loss = 0.23064219404533864
Iteration 110, Training loss = 0.22950930079976142
Iteration 120, Training loss = 0.22848397547352142
Iteration 130, Training loss = 0.22791292068463018
Iteration 140, Training loss = 0.22710174649665896
Iteration 150, Training loss = 0.22681268293788467
Iteration 160, Training loss = 0.2274622751246904
Iteration 170, Training loss = 0.22543438847946085
Iteration 180, Training loss = 0.22445750873589862
Iteration 190, Training loss = 0.22469581555629123
Iteration 200, Training loss = 0.2242570857063008
Iteration 210, Training loss = 0.22441740744355795
Iteration 220, Training loss = 0.2229311782596768
Iteration 230, Training loss = 0.22184679444861297
Iteration 240, Training loss = 0.22258561169323715
Iteration 250, Training loss = 0.22179534847753635
Iteration 260, Training loss = 0.22131919407326242
Iteration 270, Training loss = 0.22063421234848418
Iteration 280, Training loss = 0.22067215088484943
Iteration 290, Training loss = 0.21871056105347647
Model training time: 90.87982130050659
Device: cuda
Iteration 0, Training loss = 0.8785027069026146
Iteration 10, Training loss = 0.267876499912877
Iteration 20, Training loss = 0.2508402921727314
Iteration 30, Training loss = 0.24384430421579287
Iteration 40, Training loss = 0.23844084589953585
Iteration 50, Training loss = 0.2351054053231714
Iteration 60, Training loss = 0.23294154630191083
Iteration 70, Training loss = 0.23046053744456618
Iteration 80, Training loss = 0.22900527139792695
Iteration 90, Training loss = 0.2287067038497487
Iteration 100, Training loss = 0.22661900901852022
Iteration 110, Training loss = 0.22600643361968117
Iteration 120, Training loss = 0.22531835713248322
Iteration 130, Training loss = 0.22508589577847632
Iteration 140, Training loss = 0.22354908136353976
Iteration 150, Training loss = 0.22306519861958454
Iteration 160, Training loss = 0.22189162491600295
Iteration 170, Training loss = 0.2210449689756269
Iteration 180, Training loss = 0.22125577318351625
Iteration 190, Training loss = 0.2201974939752892
Iteration 200, Training loss = 0.22013624278819505
Iteration 210, Training loss = 0.21871385514592204
Iteration 220, Training loss = 0.21873575315815239
Iteration 230, Training loss = 0.21812615129682753
Iteration 240, Training loss = 0.21734916875903734
Iteration 250, Training loss = 0.21732945358695616
Iteration 260, Training loss = 0.21654427130729104
Iteration 270, Training loss = 0.21867125819702654
Iteration 280, Training loss = 0.216179256097994
Iteration 290, Training loss = 0.2163195970337748
Model training time: 88.46125435829163
Device: cuda
Iteration 0, Training loss = 0.8987458451934482
Iteration 10, Training loss = 0.32012755564157513
Iteration 20, Training loss = 0.26362567700913564
Iteration 30, Training loss = 0.24942619342734848
Iteration 40, Training loss = 0.2432778812404992
Iteration 50, Training loss = 0.2375240071096282
Iteration 60, Training loss = 0.23389780521392822
Iteration 70, Training loss = 0.23091893711527764
Iteration 80, Training loss = 0.23011146838538313
Iteration 90, Training loss = 0.22695402199951348
Iteration 100, Training loss = 0.22605050481171998
Iteration 110, Training loss = 0.22509015751057776
Iteration 120, Training loss = 0.22415336142271614
Iteration 130, Training loss = 0.22249439279526328
Iteration 140, Training loss = 0.22263273567969097
Iteration 150, Training loss = 0.21996178690362092
Iteration 160, Training loss = 0.2186430177250922
Iteration 170, Training loss = 0.21842207953981732
Iteration 180, Training loss = 0.21903782546664205
Iteration 190, Training loss = 0.21823536867392812
Iteration 200, Training loss = 0.21755877367540258
Iteration 210, Training loss = 0.21730846368172319
Iteration 220, Training loss = 0.21739297273798266
Iteration 230, Training loss = 0.21703597240977818
Iteration 240, Training loss = 0.21712402614274462
Iteration 250, Training loss = 0.21718664132598517
Iteration 260, Training loss = 0.21654160464731392
Iteration 270, Training loss = 0.21591877091478034
Iteration 280, Training loss = 0.21683023078142158
Iteration 290, Training loss = 0.21538684304785613
Model training time: 91.41152048110962
Device: cuda
Iteration 0, Training loss = 1.0311721219244787
Iteration 10, Training loss = 0.3663436153516677
Iteration 20, Training loss = 0.2728304387146724
Iteration 30, Training loss = 0.2519655882736335
Iteration 40, Training loss = 0.24555050822847707
Iteration 50, Training loss = 0.23895170356052509
Iteration 60, Training loss = 0.2363083701994684
Iteration 70, Training loss = 0.23299282198942803
Iteration 80, Training loss = 0.23079941259778064
Iteration 90, Training loss = 0.22765635058355793
Iteration 100, Training loss = 0.22799085663712543
Iteration 110, Training loss = 0.22317151642507976
Iteration 120, Training loss = 0.22364017727294405
Iteration 130, Training loss = 0.22271108382565963
Iteration 140, Training loss = 0.2214368679313268
Iteration 150, Training loss = 0.2205355747598381
Iteration 160, Training loss = 0.21942994635606158
Iteration 170, Training loss = 0.21881220065021284
Iteration 180, Training loss = 0.2192737791273329
Iteration 190, Training loss = 0.21972366231651122
Iteration 200, Training loss = 0.21845311999033037
Iteration 210, Training loss = 0.21759915762189505
Iteration 220, Training loss = 0.217784831777287
Iteration 230, Training loss = 0.2160766825151904
Iteration 240, Training loss = 0.21571766354755503
Iteration 250, Training loss = 0.21531530440861477
Iteration 260, Training loss = 0.2155424314348594
Iteration 270, Training loss = 0.2158837836721669
Iteration 280, Training loss = 0.2153521425266197
Iteration 290, Training loss = 0.21434756566360952
Model training time: 88.7825243473053
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0350455955606728
Iteration 10, Training loss = 0.5071771673966146
Iteration 20, Training loss = 0.3801089244476263
Iteration 30, Training loss = 0.3463464281409259
Iteration 40, Training loss = 0.32786591984511576
Iteration 50, Training loss = 0.31615852762535573
Iteration 60, Training loss = 0.30946844218721714
Iteration 70, Training loss = 0.30528978578710325
Iteration 80, Training loss = 0.3015085463794533
Iteration 90, Training loss = 0.29831238990820547
Iteration 100, Training loss = 0.29546838178150897
Iteration 110, Training loss = 0.2931298468855844
Iteration 120, Training loss = 0.2901912398528362
Iteration 130, Training loss = 0.2879935618184039
Iteration 140, Training loss = 0.28513419725324796
Iteration 150, Training loss = 0.282877659884052
Iteration 160, Training loss = 0.28043512096583556
Iteration 170, Training loss = 0.27801868816216785
Iteration 180, Training loss = 0.2757013066883248
Iteration 190, Training loss = 0.2740530971671648
Iteration 200, Training loss = 0.2726567837495159
Iteration 210, Training loss = 0.27113252410278227
Iteration 220, Training loss = 0.2693029230703478
Iteration 230, Training loss = 0.26797749749992206
Iteration 240, Training loss = 0.2678577674975718
Iteration 250, Training loss = 0.26564870504797367
Iteration 260, Training loss = 0.26519643742105237
Iteration 270, Training loss = 0.26381118717976815
Iteration 280, Training loss = 0.26397636018081566
Iteration 290, Training loss = 0.2633674735752281
Model training time: 75.7117657661438
Device: cuda
Iteration 0, Training loss = 1.10521364931899
Iteration 10, Training loss = 0.6065738160829037
Iteration 20, Training loss = 0.36835194001163263
Iteration 30, Training loss = 0.33039191070098234
Iteration 40, Training loss = 0.31663748924283014
Iteration 50, Training loss = 0.3109817031213051
Iteration 60, Training loss = 0.3040918216181262
Iteration 70, Training loss = 0.2987398092729458
Iteration 80, Training loss = 0.29528864411916134
Iteration 90, Training loss = 0.29176245989718874
Iteration 100, Training loss = 0.2887933096591977
Iteration 110, Training loss = 0.28663243396558624
Iteration 120, Training loss = 0.28301649375094307
Iteration 130, Training loss = 0.28155875313973083
Iteration 140, Training loss = 0.27902472083983215
Iteration 150, Training loss = 0.27719773431331063
Iteration 160, Training loss = 0.27631199475071855
Iteration 170, Training loss = 0.27508405038124123
Iteration 180, Training loss = 0.2736550650158942
Iteration 190, Training loss = 0.2736900719467569
Iteration 200, Training loss = 0.2723497998527283
Iteration 210, Training loss = 0.27136828710779476
Iteration 220, Training loss = 0.27052348114298164
Iteration 230, Training loss = 0.27029437809318735
Iteration 240, Training loss = 0.2691888695825701
Iteration 250, Training loss = 0.2687056615300801
Iteration 260, Training loss = 0.26812993861051
Iteration 270, Training loss = 0.2675701091110994
Iteration 280, Training loss = 0.26675980192595633
Iteration 290, Training loss = 0.2662796717551019
Model training time: 76.20183968544006
Device: cuda
Iteration 0, Training loss = 1.0295500153504709
Iteration 10, Training loss = 0.4599201708024251
Iteration 20, Training loss = 0.3776109412960384
Iteration 30, Training loss = 0.3465878609297932
Iteration 40, Training loss = 0.331963247075173
Iteration 50, Training loss = 0.32212032795240336
Iteration 60, Training loss = 0.31649774298575767
Iteration 70, Training loss = 0.31186134803698257
Iteration 80, Training loss = 0.3073334109739981
Iteration 90, Training loss = 0.30462029374739974
Iteration 100, Training loss = 0.3016263242577009
Iteration 110, Training loss = 0.29884418877570523
Iteration 120, Training loss = 0.29652534464851094
Iteration 130, Training loss = 0.29376205362847463
Iteration 140, Training loss = 0.29130812316845006
Iteration 150, Training loss = 0.28855412935721125
Iteration 160, Training loss = 0.28752022780082076
Iteration 170, Training loss = 0.2843565024111582
Iteration 180, Training loss = 0.2832217470603289
Iteration 190, Training loss = 0.2804076463559975
Iteration 200, Training loss = 0.2786378969532856
Iteration 210, Training loss = 0.27690164622477287
Iteration 220, Training loss = 0.27530077974433487
Iteration 230, Training loss = 0.2744032788823768
Iteration 240, Training loss = 0.271847244909996
Iteration 250, Training loss = 0.2704245700784352
Iteration 260, Training loss = 0.2688768241140578
Iteration 270, Training loss = 0.26764696289375783
Iteration 280, Training loss = 0.2672016729335278
Iteration 290, Training loss = 0.2653956199109842
Model training time: 76.43644785881042
Device: cuda
Iteration 0, Training loss = 1.0941488270598334
Iteration 10, Training loss = 0.5250042121911395
Iteration 20, Training loss = 0.352752234192862
Iteration 30, Training loss = 0.3304910456788713
Iteration 40, Training loss = 0.3207598549588291
Iteration 50, Training loss = 0.3141523699685571
Iteration 60, Training loss = 0.3090637078607716
Iteration 70, Training loss = 0.3049026761509946
Iteration 80, Training loss = 0.3002491482861952
Iteration 90, Training loss = 0.29653957414166365
Iteration 100, Training loss = 0.2931947595831277
Iteration 110, Training loss = 0.2901159259648139
Iteration 120, Training loss = 0.2874280872984209
Iteration 130, Training loss = 0.28445950639996553
Iteration 140, Training loss = 0.28213733794608553
Iteration 150, Training loss = 0.27909826123772036
Iteration 160, Training loss = 0.27730171915125734
Iteration 170, Training loss = 0.2747399334746283
Iteration 180, Training loss = 0.27286918698877527
Iteration 190, Training loss = 0.27145279188087024
Iteration 200, Training loss = 0.2698747777348555
Iteration 210, Training loss = 0.2678963750384856
Iteration 220, Training loss = 0.2665164084106252
Iteration 230, Training loss = 0.2650368434651462
Iteration 240, Training loss = 0.26405078892546574
Iteration 250, Training loss = 0.26303472747837287
Iteration 260, Training loss = 0.26142959325497855
Iteration 270, Training loss = 0.26106585594622983
Iteration 280, Training loss = 0.2599067366209583
Iteration 290, Training loss = 0.25910334433046517
Model training time: 74.99887108802795
Device: cuda
Iteration 0, Training loss = 1.009186165056367
Iteration 10, Training loss = 0.9293872997668631
Iteration 20, Training loss = 0.5187367248362389
Iteration 30, Training loss = 0.355039751328132
Iteration 40, Training loss = 0.33979064838033946
Iteration 50, Training loss = 0.33015834212591105
Iteration 60, Training loss = 0.32351409237165957
Iteration 70, Training loss = 0.31578436853805025
Iteration 80, Training loss = 0.31004642144493433
Iteration 90, Training loss = 0.3049536554349793
Iteration 100, Training loss = 0.3004632022789711
Iteration 110, Training loss = 0.29651049924069556
Iteration 120, Training loss = 0.2923593762992085
Iteration 130, Training loss = 0.28943697769860716
Iteration 140, Training loss = 0.28672543211259705
Iteration 150, Training loss = 0.2840387582922903
Iteration 160, Training loss = 0.28245341453863226
Iteration 170, Training loss = 0.2792273144359174
Iteration 180, Training loss = 0.27740991468302867
Iteration 190, Training loss = 0.2760209562121958
Iteration 200, Training loss = 0.27479574429816095
Iteration 210, Training loss = 0.27288034529501687
Iteration 220, Training loss = 0.27167231114446255
Iteration 230, Training loss = 0.27062371826690174
Iteration 240, Training loss = 0.26986756629270053
Iteration 250, Training loss = 0.26884908128331825
Iteration 260, Training loss = 0.2688683775312083
Iteration 270, Training loss = 0.2670831426186262
Iteration 280, Training loss = 0.2666768160995078
Iteration 290, Training loss = 0.26665844185196835
Model training time: 76.35779452323914
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7586416061517697
Iteration 10, Training loss = 0.26817154067294036
Iteration 20, Training loss = 0.25218499138735345
Iteration 30, Training loss = 0.24536341846277174
Iteration 40, Training loss = 0.24103794516860574
Iteration 50, Training loss = 0.2380302344687319
Iteration 60, Training loss = 0.2352405061252451
Iteration 70, Training loss = 0.233675976306344
Iteration 80, Training loss = 0.2314983717557313
Iteration 90, Training loss = 0.22944761873875263
Iteration 100, Training loss = 0.2267447507755768
Iteration 110, Training loss = 0.22462815265868596
Iteration 120, Training loss = 0.22298598699811575
Iteration 130, Training loss = 0.22129252516993003
Iteration 140, Training loss = 0.21991966330054877
Iteration 150, Training loss = 0.2191585463602186
Iteration 160, Training loss = 0.21757391393904524
Iteration 170, Training loss = 0.21719417094752408
Iteration 180, Training loss = 0.21558091271182764
Iteration 190, Training loss = 0.21442644812778575
Iteration 200, Training loss = 0.21272032214823552
Iteration 210, Training loss = 0.21222241129276256
Iteration 220, Training loss = 0.21206391079054362
Iteration 230, Training loss = 0.21046811784955038
Iteration 240, Training loss = 0.2095458622355968
Iteration 250, Training loss = 0.2088163116775849
Iteration 260, Training loss = 0.20827353213000413
Iteration 270, Training loss = 0.20874755242884446
Iteration 280, Training loss = 0.20795166121732786
Iteration 290, Training loss = 0.2076769505362004
Model training time: 88.05681157112122
Device: cuda
Iteration 0, Training loss = 0.8005998616921153
Iteration 10, Training loss = 0.2755966885870206
Iteration 20, Training loss = 0.258901736802525
Iteration 30, Training loss = 0.25156236990638403
Iteration 40, Training loss = 0.24683605847151383
Iteration 50, Training loss = 0.24280295531818832
Iteration 60, Training loss = 0.2387130808643097
Iteration 70, Training loss = 0.23607399527001496
Iteration 80, Training loss = 0.2346093766571243
Iteration 90, Training loss = 0.23213129220665366
Iteration 100, Training loss = 0.2304320431129944
Iteration 110, Training loss = 0.23085480914023762
Iteration 120, Training loss = 0.22760104960289554
Iteration 130, Training loss = 0.226510191741197
Iteration 140, Training loss = 0.22536706200976303
Iteration 150, Training loss = 0.2262193440383183
Iteration 160, Training loss = 0.22563684580550677
Iteration 170, Training loss = 0.22388713716884742
Iteration 180, Training loss = 0.2228807452076299
Iteration 190, Training loss = 0.22252562476529014
Iteration 200, Training loss = 0.22074621015988685
Iteration 210, Training loss = 0.22079739928389516
Iteration 220, Training loss = 0.2202470000959249
Iteration 230, Training loss = 0.2193932529664846
Iteration 240, Training loss = 0.21927819221060058
Iteration 250, Training loss = 0.21690961804942807
Iteration 260, Training loss = 0.21626781539064674
Iteration 270, Training loss = 0.21616209863464614
Iteration 280, Training loss = 0.2169197628967428
Iteration 290, Training loss = 0.21567733650622162
Model training time: 88.9786581993103
Device: cuda
Iteration 0, Training loss = 0.7254094738459241
Iteration 10, Training loss = 0.2660261897217248
Iteration 20, Training loss = 0.25162347150597597
Iteration 30, Training loss = 0.24452219644318457
Iteration 40, Training loss = 0.23833439412756244
Iteration 50, Training loss = 0.2346843450469671
Iteration 60, Training loss = 0.2315000691563611
Iteration 70, Training loss = 0.22928127228925768
Iteration 80, Training loss = 0.22669979883132926
Iteration 90, Training loss = 0.22529330050599747
Iteration 100, Training loss = 0.22248801128299917
Iteration 110, Training loss = 0.221483388348766
Iteration 120, Training loss = 0.22133401547365142
Iteration 130, Training loss = 0.21917752014554065
Iteration 140, Training loss = 0.22018378328729943
Iteration 150, Training loss = 0.21702918467890236
Iteration 160, Training loss = 0.21592797839267241
Iteration 170, Training loss = 0.21571235070769915
Iteration 180, Training loss = 0.21510174183021996
Iteration 190, Training loss = 0.21335455951627325
Iteration 200, Training loss = 0.21287419315841463
Iteration 210, Training loss = 0.2137279187279623
Iteration 220, Training loss = 0.21220543379512963
Iteration 230, Training loss = 0.21148275997903612
Iteration 240, Training loss = 0.21155732780119071
Iteration 250, Training loss = 0.21064782056255618
Iteration 260, Training loss = 0.21079410112278474
Iteration 270, Training loss = 0.20884666131170476
Iteration 280, Training loss = 0.20906194061904715
Iteration 290, Training loss = 0.2076597399469735
Model training time: 88.84724402427673
Device: cuda
Iteration 0, Training loss = 0.6498932831241313
Iteration 10, Training loss = 0.25756497252822497
Iteration 20, Training loss = 0.24459929321555124
Iteration 30, Training loss = 0.23942424169773066
Iteration 40, Training loss = 0.2357043382410266
Iteration 50, Training loss = 0.23308247169434737
Iteration 60, Training loss = 0.2310833352919362
Iteration 70, Training loss = 0.2294502996880075
Iteration 80, Training loss = 0.22753559034947612
Iteration 90, Training loss = 0.22659982470930487
Iteration 100, Training loss = 0.22503591318061386
Iteration 110, Training loss = 0.2239019269672569
Iteration 120, Training loss = 0.2242472111314967
Iteration 130, Training loss = 0.2222833969598807
Iteration 140, Training loss = 0.2217172133029947
Iteration 150, Training loss = 0.220536385073466
Iteration 160, Training loss = 0.219474069810144
Iteration 170, Training loss = 0.219070078994053
Iteration 180, Training loss = 0.21792827528599956
Iteration 190, Training loss = 0.2172350148503907
Iteration 200, Training loss = 0.21566221852233444
Iteration 210, Training loss = 0.21534245392838539
Iteration 220, Training loss = 0.2160682250984049
Iteration 230, Training loss = 0.2141358758804303
Iteration 240, Training loss = 0.2137406702346848
Iteration 250, Training loss = 0.21187659594171865
Iteration 260, Training loss = 0.2119533224526235
Iteration 270, Training loss = 0.2099850390844299
Iteration 280, Training loss = 0.20906643155548307
Iteration 290, Training loss = 0.20758694867437014
Model training time: 89.50725483894348
Device: cuda
Iteration 0, Training loss = 0.6082610261181126
Iteration 10, Training loss = 0.2669182859973055
Iteration 20, Training loss = 0.25474002237481197
Iteration 30, Training loss = 0.24605351553303031
Iteration 40, Training loss = 0.24242631536750978
Iteration 50, Training loss = 0.23863072419800044
Iteration 60, Training loss = 0.23619279942074836
Iteration 70, Training loss = 0.23489955305189325
Iteration 80, Training loss = 0.23288263690068525
Iteration 90, Training loss = 0.23123358704761607
Iteration 100, Training loss = 0.22911879251112685
Iteration 110, Training loss = 0.22755533852726942
Iteration 120, Training loss = 0.22558454730084554
Iteration 130, Training loss = 0.22324930984472882
Iteration 140, Training loss = 0.22228386134341144
Iteration 150, Training loss = 0.22002441385661922
Iteration 160, Training loss = 0.22066293980764307
Iteration 170, Training loss = 0.2188949085376113
Iteration 180, Training loss = 0.2185308548131427
Iteration 190, Training loss = 0.21650155039801114
Iteration 200, Training loss = 0.2174216949781358
Iteration 210, Training loss = 0.21654561136799735
Iteration 220, Training loss = 0.21535098376337458
Iteration 230, Training loss = 0.21448933336757808
Iteration 240, Training loss = 0.21466204887570967
Iteration 250, Training loss = 0.2133579671670849
Iteration 260, Training loss = 0.2126783981271412
Iteration 270, Training loss = 0.21253766457815676
Iteration 280, Training loss = 0.21247223530702544
Iteration 290, Training loss = 0.2116643970715251
Model training time: 88.94830703735352
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0764934699892421
Iteration 10, Training loss = 0.8725027894052331
Iteration 20, Training loss = 0.4323444392370141
Iteration 30, Training loss = 0.36251674952426394
Iteration 40, Training loss = 0.3401645491088646
Iteration 50, Training loss = 0.3281904818211201
Iteration 60, Training loss = 0.31896460585404135
Iteration 70, Training loss = 0.31312564864826664
Iteration 80, Training loss = 0.30728474961242813
Iteration 90, Training loss = 0.3022118753929069
Iteration 100, Training loss = 0.2972554944563603
Iteration 110, Training loss = 0.29454813818424797
Iteration 120, Training loss = 0.2890955445487142
Iteration 130, Training loss = 0.2858487332356725
Iteration 140, Training loss = 0.2827047473854489
Iteration 150, Training loss = 0.27930322329059315
Iteration 160, Training loss = 0.27671835438352854
Iteration 170, Training loss = 0.27427755051476943
Iteration 180, Training loss = 0.2727935748975634
Iteration 190, Training loss = 0.2705489082828812
Iteration 200, Training loss = 0.2696618005849313
Iteration 210, Training loss = 0.2675190111937154
Iteration 220, Training loss = 0.2659286361385659
Iteration 230, Training loss = 0.26555053680991203
Iteration 240, Training loss = 0.26377003686727535
Iteration 250, Training loss = 0.2627261732367502
Iteration 260, Training loss = 0.261881161103214
Iteration 270, Training loss = 0.26089481847441715
Iteration 280, Training loss = 0.2612794773734134
Iteration 290, Training loss = 0.26003499141925773
Model training time: 77.29217886924744
Device: cuda
Iteration 0, Training loss = 1.0230001971341562
Iteration 10, Training loss = 0.7222564920135166
Iteration 20, Training loss = 0.38175528226555255
Iteration 30, Training loss = 0.3542703528335129
Iteration 40, Training loss = 0.343134348159251
Iteration 50, Training loss = 0.3359979399736377
Iteration 60, Training loss = 0.3281255609891265
Iteration 70, Training loss = 0.3243749430358122
Iteration 80, Training loss = 0.31965760181203556
Iteration 90, Training loss = 0.3158560070582634
Iteration 100, Training loss = 0.31292925278345746
Iteration 110, Training loss = 0.31097510932148364
Iteration 120, Training loss = 0.30812399492459597
Iteration 130, Training loss = 0.30608475229878357
Iteration 140, Training loss = 0.3033455372720525
Iteration 150, Training loss = 0.3014295339440378
Iteration 160, Training loss = 0.2994260911060416
Iteration 170, Training loss = 0.2977939735432178
Iteration 180, Training loss = 0.2947946380517909
Iteration 190, Training loss = 0.2937413811827627
Iteration 200, Training loss = 0.2917137966999685
Iteration 210, Training loss = 0.29045133900959136
Iteration 220, Training loss = 0.2886915835349456
Iteration 230, Training loss = 0.2872273066409544
Iteration 240, Training loss = 0.28566026572444014
Iteration 250, Training loss = 0.2846635878805953
Iteration 260, Training loss = 0.2824038380154089
Iteration 270, Training loss = 0.2807966315228006
Iteration 280, Training loss = 0.27882616128322585
Iteration 290, Training loss = 0.2768255823116372
Model training time: 79.187255859375
Device: cuda
Iteration 0, Training loss = 1.0221446650327692
Iteration 10, Training loss = 0.5487704976745273
Iteration 20, Training loss = 0.33837112395659735
Iteration 30, Training loss = 0.3230753504999594
Iteration 40, Training loss = 0.3142733852500501
Iteration 50, Training loss = 0.3082825804246221
Iteration 60, Training loss = 0.3048274353650457
Iteration 70, Training loss = 0.29862551549494554
Iteration 80, Training loss = 0.29462454881933
Iteration 90, Training loss = 0.2932022835537431
Iteration 100, Training loss = 0.2878340214202945
Iteration 110, Training loss = 0.2849786027474104
Iteration 120, Training loss = 0.2819601862830816
Iteration 130, Training loss = 0.27889989661997644
Iteration 140, Training loss = 0.27650012320654405
Iteration 150, Training loss = 0.2742119777029839
Iteration 160, Training loss = 0.27200607840277724
Iteration 170, Training loss = 0.270798929143643
Iteration 180, Training loss = 0.2684417727632799
Iteration 190, Training loss = 0.2669316782899525
Iteration 200, Training loss = 0.2655524754365861
Iteration 210, Training loss = 0.2636784102389778
Iteration 220, Training loss = 0.2631799533315327
Iteration 230, Training loss = 0.2615751594233052
Iteration 240, Training loss = 0.26056238692163847
Iteration 250, Training loss = 0.26047819194154465
Iteration 260, Training loss = 0.2594375474657413
Iteration 270, Training loss = 0.25854362914527673
Iteration 280, Training loss = 0.25749606851506346
Iteration 290, Training loss = 0.25713747174699525
Model training time: 76.85669946670532
Device: cuda
Iteration 0, Training loss = 1.0874442357371972
Iteration 10, Training loss = 0.43590318343201695
Iteration 20, Training loss = 0.35777093758041734
Iteration 30, Training loss = 0.3337829720570845
Iteration 40, Training loss = 0.32020489317207523
Iteration 50, Training loss = 0.3099508054446483
Iteration 60, Training loss = 0.3043028705793878
Iteration 70, Training loss = 0.2994603606093909
Iteration 80, Training loss = 0.2958913218715917
Iteration 90, Training loss = 0.2920389607332755
Iteration 100, Training loss = 0.28895666612231213
Iteration 110, Training loss = 0.28594055786224953
Iteration 120, Training loss = 0.28293518765249115
Iteration 130, Training loss = 0.28127067190581473
Iteration 140, Training loss = 0.27891919945461163
Iteration 150, Training loss = 0.27716492710338125
Iteration 160, Training loss = 0.2751626716431788
Iteration 170, Training loss = 0.2738642102998236
Iteration 180, Training loss = 0.272536501288414
Iteration 190, Training loss = 0.27158046549788994
Iteration 200, Training loss = 0.2703167989778058
Iteration 210, Training loss = 0.26924318312734796
Iteration 220, Training loss = 0.26817526844676565
Iteration 230, Training loss = 0.2671139177276892
Iteration 240, Training loss = 0.2664354836955163
Iteration 250, Training loss = 0.2662773089132447
Iteration 260, Training loss = 0.26519731831723364
Iteration 270, Training loss = 0.2647388144823664
Iteration 280, Training loss = 0.26368893380614294
Iteration 290, Training loss = 0.2630814996678472
Model training time: 76.08821082115173
Device: cuda
Iteration 0, Training loss = 1.0463993511914056
Iteration 10, Training loss = 0.5157950000054594
Iteration 20, Training loss = 0.3665888205098645
Iteration 30, Training loss = 0.337654325409212
Iteration 40, Training loss = 0.32353437580348215
Iteration 50, Training loss = 0.31417586989160895
Iteration 60, Training loss = 0.3083471858127106
Iteration 70, Training loss = 0.30172264295211737
Iteration 80, Training loss = 0.2977300828349763
Iteration 90, Training loss = 0.29456610228992314
Iteration 100, Training loss = 0.29041604892067285
Iteration 110, Training loss = 0.2870080668281242
Iteration 120, Training loss = 0.2844879046298455
Iteration 130, Training loss = 0.2818362356527992
Iteration 140, Training loss = 0.2792389896756785
Iteration 150, Training loss = 0.27689345303365
Iteration 160, Training loss = 0.27512596972322695
Iteration 170, Training loss = 0.2746957649211377
Iteration 180, Training loss = 0.2708743083232267
Iteration 190, Training loss = 0.2689963576298405
Iteration 200, Training loss = 0.2677617991604091
Iteration 210, Training loss = 0.26565194345902704
Iteration 220, Training loss = 0.26481969953303175
Iteration 230, Training loss = 0.26358159001610704
Iteration 240, Training loss = 0.26232428172072353
Iteration 250, Training loss = 0.261491715764078
Iteration 260, Training loss = 0.26077339593051135
Iteration 270, Training loss = 0.25959522194332546
Iteration 280, Training loss = 0.2592840527998652
Iteration 290, Training loss = 0.2581301666976173
Model training time: 77.49240756034851
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5172656673740074
Iteration 10, Training loss = 0.2518623085773509
Iteration 20, Training loss = 0.23448587813671085
Iteration 30, Training loss = 0.22798212549248756
Iteration 40, Training loss = 0.22189430586525782
Iteration 50, Training loss = 0.21913682079113625
Iteration 60, Training loss = 0.2135965704989894
Iteration 70, Training loss = 0.2110867075226157
Iteration 80, Training loss = 0.20866122195758105
Iteration 90, Training loss = 0.2076548450594939
Model training time: 29.049490928649902
Device: cuda
Iteration 0, Training loss = 0.4828202511377381
Iteration 10, Training loss = 0.24660433270937002
Iteration 20, Training loss = 0.23085003216629443
Iteration 30, Training loss = 0.2254530735997762
Iteration 40, Training loss = 0.2204452307831838
Iteration 50, Training loss = 0.21710221969707
Iteration 60, Training loss = 0.21374028651178748
Iteration 70, Training loss = 0.21142398666788415
Iteration 80, Training loss = 0.20938922990347453
Iteration 90, Training loss = 0.2048191886017288
Model training time: 29.604186534881592
Device: cuda
Iteration 0, Training loss = 0.5107372440577705
Iteration 10, Training loss = 0.2475235072023051
Iteration 20, Training loss = 0.23554677634999371
Iteration 30, Training loss = 0.22773066227850708
Iteration 40, Training loss = 0.22371936917017046
Iteration 50, Training loss = 0.21801611539966242
Iteration 60, Training loss = 0.21305165097909273
Iteration 70, Training loss = 0.21148074655861093
Iteration 80, Training loss = 0.2097977219860335
Iteration 90, Training loss = 0.20818762646781075
Model training time: 29.55753779411316
Device: cuda
Iteration 0, Training loss = 0.5895635670293933
Iteration 10, Training loss = 0.2523145830789626
Iteration 20, Training loss = 0.23978957299880935
Iteration 30, Training loss = 0.23225776664012873
Iteration 40, Training loss = 0.2255938040173572
Iteration 50, Training loss = 0.2195539777189637
Iteration 60, Training loss = 0.21444253421923964
Iteration 70, Training loss = 0.2110389362714717
Iteration 80, Training loss = 0.20971028477961315
Iteration 90, Training loss = 0.2075808693245413
Model training time: 29.468655824661255
Device: cuda
Iteration 0, Training loss = 0.5007851299456352
Iteration 10, Training loss = 0.2477254199520977
Iteration 20, Training loss = 0.234342772387652
Iteration 30, Training loss = 0.22525630650169032
Iteration 40, Training loss = 0.21786987220031628
Iteration 50, Training loss = 0.21493583813237682
Iteration 60, Training loss = 0.21310216688735473
Iteration 70, Training loss = 0.21023258073750326
Iteration 80, Training loss = 0.20823331267217507
Iteration 90, Training loss = 0.206391424552542
Model training time: 29.844254970550537
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0905267535776333
Iteration 10, Training loss = 0.4784324236826044
Iteration 20, Training loss = 0.3339353390937842
Iteration 30, Training loss = 0.3099948561925819
Iteration 40, Training loss = 0.29926359430315413
Iteration 50, Training loss = 0.2926756344411684
Iteration 60, Training loss = 0.2879721931213342
Iteration 70, Training loss = 0.28370584684293626
Iteration 80, Training loss = 0.2809193065560958
Iteration 90, Training loss = 0.2784552921491545
Model training time: 25.362117767333984
Device: cuda
Iteration 0, Training loss = 0.9888533466104148
Iteration 10, Training loss = 0.38673547385395435
Iteration 20, Training loss = 0.3334249304832468
Iteration 30, Training loss = 0.31659134610551565
Iteration 40, Training loss = 0.3044019823057064
Iteration 50, Training loss = 0.2959732984143179
Iteration 60, Training loss = 0.2880616461597203
Iteration 70, Training loss = 0.283121343612095
Iteration 80, Training loss = 0.27715941147815776
Iteration 90, Training loss = 0.2728749544580202
Model training time: 26.24738883972168
Device: cuda
Iteration 0, Training loss = 0.9422295362765086
Iteration 10, Training loss = 0.34076909922459275
Iteration 20, Training loss = 0.31454605828736715
Iteration 30, Training loss = 0.3021063726161413
Iteration 40, Training loss = 0.2929199309164775
Iteration 50, Training loss = 0.2848287880564657
Iteration 60, Training loss = 0.2779119053612585
Iteration 70, Training loss = 0.27382463569514415
Iteration 80, Training loss = 0.26915365680692277
Iteration 90, Training loss = 0.26566929974849673
Model training time: 27.348185539245605
Device: cuda
Iteration 0, Training loss = 1.0097953230862455
Iteration 10, Training loss = 0.4439465703739636
Iteration 20, Training loss = 0.335387308312499
Iteration 30, Training loss = 0.3168288701830279
Iteration 40, Training loss = 0.3061272691989291
Iteration 50, Training loss = 0.29888243609724413
Iteration 60, Training loss = 0.290478704776165
Iteration 70, Training loss = 0.2829662615046409
Iteration 80, Training loss = 0.27735577761263086
Iteration 90, Training loss = 0.2736328388901724
Model training time: 28.62916874885559
Device: cuda
Iteration 0, Training loss = 1.009307072358431
Iteration 10, Training loss = 0.43040974056663145
Iteration 20, Training loss = 0.3377489590846398
Iteration 30, Training loss = 0.312437264635655
Iteration 40, Training loss = 0.3006295604262375
Iteration 50, Training loss = 0.29246176347352454
Iteration 60, Training loss = 0.286594411097287
Iteration 70, Training loss = 0.28111734270041694
Iteration 80, Training loss = 0.27722526053731567
Iteration 90, Training loss = 0.27364117914927755
Model training time: 28.561444520950317
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5758111882180983
Iteration 10, Training loss = 0.260519141016375
Iteration 20, Training loss = 0.25044294907419
Iteration 30, Training loss = 0.24492617714520237
Iteration 40, Training loss = 0.24102839733955364
Iteration 50, Training loss = 0.23833800312401593
Iteration 60, Training loss = 0.23662570979140232
Iteration 70, Training loss = 0.23440578381940363
Iteration 80, Training loss = 0.2305819390044696
Iteration 90, Training loss = 0.23053702431312506
Model training time: 32.06838321685791
Device: cuda
Iteration 0, Training loss = 0.6248654501449659
Iteration 10, Training loss = 0.25972659119229385
Iteration 20, Training loss = 0.2486020360088003
Iteration 30, Training loss = 0.2441556186635713
Iteration 40, Training loss = 0.24068296196812017
Iteration 50, Training loss = 0.2402177800734838
Iteration 60, Training loss = 0.2384714047258027
Iteration 70, Training loss = 0.23623044529686804
Iteration 80, Training loss = 0.23510034249168663
Iteration 90, Training loss = 0.23440770585323878
Model training time: 30.829171180725098
Device: cuda
Iteration 0, Training loss = 0.5807905318800378
Iteration 10, Training loss = 0.2541614565008504
Iteration 20, Training loss = 0.2416317893759064
Iteration 30, Training loss = 0.23797168982201727
Iteration 40, Training loss = 0.23248881987039594
Iteration 50, Training loss = 0.22908286678330333
Iteration 60, Training loss = 0.22640632024565757
Iteration 70, Training loss = 0.22535167404131037
Iteration 80, Training loss = 0.22344854013355459
Iteration 90, Training loss = 0.22086541451837705
Model training time: 29.220703601837158
Device: cuda
Iteration 0, Training loss = 0.5601594180300615
Iteration 10, Training loss = 0.2546811077905738
Iteration 20, Training loss = 0.2408193282171148
Iteration 30, Training loss = 0.23372790624122114
Iteration 40, Training loss = 0.23011692940900866
Iteration 50, Training loss = 0.2271419717133909
Iteration 60, Training loss = 0.22251124856408666
Iteration 70, Training loss = 0.21855919602988422
Iteration 80, Training loss = 0.21860820040610676
Iteration 90, Training loss = 0.21716185418016093
Model training time: 29.441875457763672
Device: cuda
Iteration 0, Training loss = 0.6400950217016653
Iteration 10, Training loss = 0.26452851457440335
Iteration 20, Training loss = 0.25352315385128565
Iteration 30, Training loss = 0.24770426415446875
Iteration 40, Training loss = 0.24401797314628887
Iteration 50, Training loss = 0.23962258014845964
Iteration 60, Training loss = 0.2377390246388417
Iteration 70, Training loss = 0.2351257989300046
Iteration 80, Training loss = 0.23282827066194609
Iteration 90, Training loss = 0.22999568794660522
Model training time: 29.732015371322632
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0577279288987607
Iteration 10, Training loss = 0.9982633384817464
Iteration 20, Training loss = 0.9648581374670573
Iteration 30, Training loss = 0.41053372319193854
Iteration 40, Training loss = 0.3464343280751924
Iteration 50, Training loss = 0.32667268323149656
Iteration 60, Training loss = 0.31469935178756714
Iteration 70, Training loss = 0.30578838216797743
Iteration 80, Training loss = 0.29939087912224344
Iteration 90, Training loss = 0.2928201538929041
Model training time: 25.6152081489563
Device: cuda
Iteration 0, Training loss = 1.0252976742919517
Iteration 10, Training loss = 0.4203912858369846
Iteration 20, Training loss = 0.34261858110554555
Iteration 30, Training loss = 0.32246235329747774
Iteration 40, Training loss = 0.31306210829727893
Iteration 50, Training loss = 0.30578753121809105
Iteration 60, Training loss = 0.30103762980532534
Iteration 70, Training loss = 0.29554117024664717
Iteration 80, Training loss = 0.29074280928586416
Iteration 90, Training loss = 0.2858769304798421
Model training time: 25.472635746002197
Device: cuda
Iteration 0, Training loss = 1.079286806249388
Iteration 10, Training loss = 0.36188822718346175
Iteration 20, Training loss = 0.3327599040408065
Iteration 30, Training loss = 0.3200531493926394
Iteration 40, Training loss = 0.3119177397178567
Iteration 50, Training loss = 0.30406287485274713
Iteration 60, Training loss = 0.2978742687742491
Iteration 70, Training loss = 0.29204017643767277
Iteration 80, Training loss = 0.2875392640342459
Iteration 90, Training loss = 0.28354174694145357
Model training time: 26.19012999534607
Device: cuda
Iteration 0, Training loss = 1.0951968042746834
Iteration 10, Training loss = 0.3570581514046388
Iteration 20, Training loss = 0.324611609360734
Iteration 30, Training loss = 0.30849953945996106
Iteration 40, Training loss = 0.2967156042223391
Iteration 50, Training loss = 0.2895241216328985
Iteration 60, Training loss = 0.2840576744525905
Iteration 70, Training loss = 0.28051546961069107
Iteration 80, Training loss = 0.27751084939005294
Iteration 90, Training loss = 0.27461781399549495
Model training time: 26.101779222488403
Device: cuda
Iteration 0, Training loss = 0.9367451477741849
Iteration 10, Training loss = 0.33600954358704427
Iteration 20, Training loss = 0.3122051677842071
Iteration 30, Training loss = 0.30219072960137167
Iteration 40, Training loss = 0.29648951120710604
Iteration 50, Training loss = 0.2916793602964152
Iteration 60, Training loss = 0.287395597152088
Iteration 70, Training loss = 0.2846891101863649
Iteration 80, Training loss = 0.2807933515396671
Iteration 90, Training loss = 0.2770433016301353
Model training time: 27.321919918060303
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5326829621037423
Iteration 10, Training loss = 0.25860058008760645
Iteration 20, Training loss = 0.24291558520517487
Iteration 30, Training loss = 0.23776904115642328
Iteration 40, Training loss = 0.23156439297009204
Iteration 50, Training loss = 0.22761288414830746
Iteration 60, Training loss = 0.22508715582642577
Iteration 70, Training loss = 0.22087523499548722
Iteration 80, Training loss = 0.22060308508251025
Iteration 90, Training loss = 0.22106183050335318
Model training time: 29.66927146911621
Device: cuda
Iteration 0, Training loss = 0.5767620099339508
Iteration 10, Training loss = 0.2599422071435025
Iteration 20, Training loss = 0.24550217681604883
Iteration 30, Training loss = 0.24066179180922714
Iteration 40, Training loss = 0.2370923215136436
Iteration 50, Training loss = 0.2316419265192488
Iteration 60, Training loss = 0.2272607658940237
Iteration 70, Training loss = 0.22353791650654614
Iteration 80, Training loss = 0.2208325496474326
Iteration 90, Training loss = 0.21776049098242883
Model training time: 30.191397428512573
Device: cuda
Iteration 0, Training loss = 0.6067282653229248
Iteration 10, Training loss = 0.26136740675007086
Iteration 20, Training loss = 0.2421613353246076
Iteration 30, Training loss = 0.23496646243305022
Iteration 40, Training loss = 0.22779193106624815
Iteration 50, Training loss = 0.22598587487630797
Iteration 60, Training loss = 0.22083272151469033
Iteration 70, Training loss = 0.2188400206502509
Iteration 80, Training loss = 0.21643234850559834
Iteration 90, Training loss = 0.21832438094028528
Model training time: 29.610398530960083
Device: cuda
Iteration 0, Training loss = 0.5009822432257703
Iteration 10, Training loss = 0.24932068575551544
Iteration 20, Training loss = 0.23338792998577662
Iteration 30, Training loss = 0.22698051820774584
Iteration 40, Training loss = 0.22096236507241854
Iteration 50, Training loss = 0.21842434466029134
Iteration 60, Training loss = 0.21679460710805395
Iteration 70, Training loss = 0.21351961334402433
Iteration 80, Training loss = 0.21098814105642014
Iteration 90, Training loss = 0.20972766914373434
Model training time: 30.88464069366455
Device: cuda
Iteration 0, Training loss = 0.5069848668316136
Iteration 10, Training loss = 0.24701946016383056
Iteration 20, Training loss = 0.2359084689962691
Iteration 30, Training loss = 0.23119885780385152
Iteration 40, Training loss = 0.22689669071764185
Iteration 50, Training loss = 0.22257943721353143
Iteration 60, Training loss = 0.21961082932021883
Iteration 70, Training loss = 0.21505474990260773
Iteration 80, Training loss = 0.21245036079831744
Iteration 90, Training loss = 0.2088287243808525
Model training time: 29.293392181396484
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9845179405765258
Iteration 10, Training loss = 0.3856646101111951
Iteration 20, Training loss = 0.33677528417052854
Iteration 30, Training loss = 0.3201744652168762
Iteration 40, Training loss = 0.31100782140153616
Iteration 50, Training loss = 0.3039503009710911
Iteration 60, Training loss = 0.299322742914808
Iteration 70, Training loss = 0.29429470434568933
Iteration 80, Training loss = 0.29004457938498346
Iteration 90, Training loss = 0.2856838415570305
Model training time: 26.626880168914795
Device: cuda
Iteration 0, Training loss = 1.0667250288857355
Iteration 10, Training loss = 0.37082841531666005
Iteration 20, Training loss = 0.3333477482991518
Iteration 30, Training loss = 0.31626144303503817
Iteration 40, Training loss = 0.30497876957850756
Iteration 50, Training loss = 0.2945718542461234
Iteration 60, Training loss = 0.2853509688003052
Iteration 70, Training loss = 0.279155766474452
Iteration 80, Training loss = 0.2740886108814806
Iteration 90, Training loss = 0.269913451143221
Model training time: 26.664008140563965
Device: cuda
Iteration 0, Training loss = 1.0352687268441425
Iteration 10, Training loss = 0.718379567210801
Iteration 20, Training loss = 0.379807784649485
Iteration 30, Training loss = 0.3407047407494651
Iteration 40, Training loss = 0.3268412373924025
Iteration 50, Training loss = 0.3161560588556787
Iteration 60, Training loss = 0.3079427535407209
Iteration 70, Training loss = 0.30105907775929586
Iteration 80, Training loss = 0.2961238989795464
Iteration 90, Training loss = 0.28868618629116943
Model training time: 26.321229934692383
Device: cuda
Iteration 0, Training loss = 0.9943286094112672
Iteration 10, Training loss = 0.44050269663909786
Iteration 20, Training loss = 0.35388480249234444
Iteration 30, Training loss = 0.32973960466718905
Iteration 40, Training loss = 0.3122871395758385
Iteration 50, Training loss = 0.29848011269949487
Iteration 60, Training loss = 0.2884133178830723
Iteration 70, Training loss = 0.28071341271273753
Iteration 80, Training loss = 0.2757044182955355
Iteration 90, Training loss = 0.271049076664275
Model training time: 25.502075672149658
Device: cuda
Iteration 0, Training loss = 1.034512051925567
Iteration 10, Training loss = 0.3998696639198036
Iteration 20, Training loss = 0.3359772994178505
Iteration 30, Training loss = 0.3155141854343783
Iteration 40, Training loss = 0.3045586047757075
Iteration 50, Training loss = 0.29556719246117963
Iteration 60, Training loss = 0.289621587500768
Iteration 70, Training loss = 0.28439796395635836
Iteration 80, Training loss = 0.27874182341035436
Iteration 90, Training loss = 0.27578083900869754
Model training time: 25.14145565032959
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5238704269634928
Iteration 10, Training loss = 0.2544803308044079
Iteration 20, Training loss = 0.24407339653963053
Iteration 30, Training loss = 0.23547082235991668
Iteration 40, Training loss = 0.2276446563034242
Iteration 50, Training loss = 0.22277480921739543
Iteration 60, Training loss = 0.21684618339230474
Iteration 70, Training loss = 0.21699109533126804
Iteration 80, Training loss = 0.21295889852127592
Iteration 90, Training loss = 0.21042365814752625
Iteration 100, Training loss = 0.20803817375990505
Iteration 110, Training loss = 0.20839560985709157
Iteration 120, Training loss = 0.2064523690060717
Iteration 130, Training loss = 0.20468682277461756
Iteration 140, Training loss = 0.20615174470172412
Iteration 150, Training loss = 0.2040082236059046
Iteration 160, Training loss = 0.20311956168804768
Iteration 170, Training loss = 0.20263345183237738
Iteration 180, Training loss = 0.20212884711614554
Iteration 190, Training loss = 0.20016606609170565
Model training time: 61.060330867767334
Device: cuda
Iteration 0, Training loss = 0.5824687221056022
Iteration 10, Training loss = 0.248975050010255
Iteration 20, Training loss = 0.2359874380239542
Iteration 30, Training loss = 0.22556623964062059
Iteration 40, Training loss = 0.21583692639490257
Iteration 50, Training loss = 0.21355750249779742
Iteration 60, Training loss = 0.21359482465158913
Iteration 70, Training loss = 0.20972338326023396
Iteration 80, Training loss = 0.2095198372495923
Iteration 90, Training loss = 0.20739052988192885
Iteration 100, Training loss = 0.20574984674292487
Iteration 110, Training loss = 0.20675299665778155
Iteration 120, Training loss = 0.20622155421238014
Iteration 130, Training loss = 0.20243616243779372
Iteration 140, Training loss = 0.2030254812632206
Iteration 150, Training loss = 0.201358445348659
Iteration 160, Training loss = 0.20137141385804053
Iteration 170, Training loss = 0.20069518242625223
Iteration 180, Training loss = 0.19881195931330972
Iteration 190, Training loss = 0.1981972855862212
Model training time: 58.38770866394043
Device: cuda
Iteration 0, Training loss = 0.5238818442763914
Iteration 10, Training loss = 0.24655971348573621
Iteration 20, Training loss = 0.23154633884556627
Iteration 30, Training loss = 0.22358507785388237
Iteration 40, Training loss = 0.21973550550027746
Iteration 50, Training loss = 0.21539794041770668
Iteration 60, Training loss = 0.208841634977267
Iteration 70, Training loss = 0.20610488436504262
Iteration 80, Training loss = 0.20506312007057495
Iteration 90, Training loss = 0.20283007654158966
Iteration 100, Training loss = 0.20210169118959548
Iteration 110, Training loss = 0.19968509540897636
Iteration 120, Training loss = 0.20049190402462863
Iteration 130, Training loss = 0.198128841565427
Iteration 140, Training loss = 0.19832212589501183
Iteration 150, Training loss = 0.19739608984926474
Iteration 160, Training loss = 0.1979568870243243
Iteration 170, Training loss = 0.19592120401237323
Iteration 180, Training loss = 0.1947951553668377
Iteration 190, Training loss = 0.19507546442142432
Model training time: 59.369075775146484
Device: cuda
Iteration 0, Training loss = 0.5720027130871004
Iteration 10, Training loss = 0.25731101758094227
Iteration 20, Training loss = 0.2489449010739004
Iteration 30, Training loss = 0.24485406538714533
Iteration 40, Training loss = 0.23839062589090226
Iteration 50, Training loss = 0.23425889407523012
Iteration 60, Training loss = 0.23028595885936765
Iteration 70, Training loss = 0.225273500426092
Iteration 80, Training loss = 0.22197117537691974
Iteration 90, Training loss = 0.21905959336366054
Iteration 100, Training loss = 0.21583007150082198
Iteration 110, Training loss = 0.21490123968769387
Iteration 120, Training loss = 0.2148361275881385
Iteration 130, Training loss = 0.2132524876418897
Iteration 140, Training loss = 0.2137126586071535
Iteration 150, Training loss = 0.21320113136572538
Iteration 160, Training loss = 0.21120021963752986
Iteration 170, Training loss = 0.21185418279562596
Iteration 180, Training loss = 0.21002889852880854
Iteration 190, Training loss = 0.2103412845644398
Model training time: 58.66111445426941
Device: cuda
Iteration 0, Training loss = 0.5392644748739575
Iteration 10, Training loss = 0.24851833197517673
Iteration 20, Training loss = 0.2360071028776215
Iteration 30, Training loss = 0.2293293218033901
Iteration 40, Training loss = 0.22211869046595936
Iteration 50, Training loss = 0.21677012086490502
Iteration 60, Training loss = 0.2145272883888028
Iteration 70, Training loss = 0.2121216148354005
Iteration 80, Training loss = 0.21037681641929967
Iteration 90, Training loss = 0.20781963476956178
Iteration 100, Training loss = 0.2069941039606569
Iteration 110, Training loss = 0.20434025977400766
Iteration 120, Training loss = 0.20452583750376954
Iteration 130, Training loss = 0.20402522557887479
Iteration 140, Training loss = 0.20263455865751717
Iteration 150, Training loss = 0.20083134880532388
Iteration 160, Training loss = 0.20141794874472319
Iteration 170, Training loss = 0.20106949063314908
Iteration 180, Training loss = 0.2006270974010661
Iteration 190, Training loss = 0.19999116813934945
Model training time: 59.192708253860474
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0252718093314608
Iteration 10, Training loss = 0.36685822270630636
Iteration 20, Training loss = 0.32612753288757396
Iteration 30, Training loss = 0.3117437218698326
Iteration 40, Training loss = 0.30212741449547276
Iteration 50, Training loss = 0.29504168631086025
Iteration 60, Training loss = 0.28841404588038216
Iteration 70, Training loss = 0.28268632314343384
Iteration 80, Training loss = 0.27770007448495876
Iteration 90, Training loss = 0.2738437718455342
Iteration 100, Training loss = 0.27011043063684365
Iteration 110, Training loss = 0.266801950328304
Iteration 120, Training loss = 0.2651828145203383
Iteration 130, Training loss = 0.2628255185945598
Iteration 140, Training loss = 0.26041776208198014
Iteration 150, Training loss = 0.2580907472953704
Iteration 160, Training loss = 0.2569491775067532
Iteration 170, Training loss = 0.25557172021284197
Iteration 180, Training loss = 0.25440519609025136
Iteration 190, Training loss = 0.25369802146142234
Model training time: 52.266709089279175
Device: cuda
Iteration 0, Training loss = 0.9446772953162447
Iteration 10, Training loss = 0.35971244399386326
Iteration 20, Training loss = 0.327508633479404
Iteration 30, Training loss = 0.3128176226420103
Iteration 40, Training loss = 0.30140773124165005
Iteration 50, Training loss = 0.29371943994276767
Iteration 60, Training loss = 0.2881129561127096
Iteration 70, Training loss = 0.28314374200099907
Iteration 80, Training loss = 0.2785556596402385
Iteration 90, Training loss = 0.27468186458527755
Iteration 100, Training loss = 0.2714240362750735
Iteration 110, Training loss = 0.2695183384677638
Iteration 120, Training loss = 0.26694011144735963
Iteration 130, Training loss = 0.26491502863197514
Iteration 140, Training loss = 0.26417449624210165
Iteration 150, Training loss = 0.261741837117695
Iteration 160, Training loss = 0.26006684855418505
Iteration 170, Training loss = 0.25906566073353166
Iteration 180, Training loss = 0.25830371623885806
Iteration 190, Training loss = 0.2572142620737426
Model training time: 51.14088535308838
Device: cuda
Iteration 0, Training loss = 0.9875255563984746
Iteration 10, Training loss = 0.39021159564527336
Iteration 20, Training loss = 0.3294575445461964
Iteration 30, Training loss = 0.3150753899328951
Iteration 40, Training loss = 0.3046758556423556
Iteration 50, Training loss = 0.29531086139056995
Iteration 60, Training loss = 0.28648945991543756
Iteration 70, Training loss = 0.2798952652492385
Iteration 80, Training loss = 0.27543288818448064
Iteration 90, Training loss = 0.2711820452685517
Iteration 100, Training loss = 0.26826078304346057
Iteration 110, Training loss = 0.2660109544361847
Iteration 120, Training loss = 0.2631496288566198
Iteration 130, Training loss = 0.26146963328698986
Iteration 140, Training loss = 0.2600305035782321
Iteration 150, Training loss = 0.2579380603876091
Iteration 160, Training loss = 0.25676898078786003
Iteration 170, Training loss = 0.2555732492015558
Iteration 180, Training loss = 0.25403354309751214
Iteration 190, Training loss = 0.2528943223653784
Model training time: 50.40967106819153
Device: cuda
Iteration 0, Training loss = 0.9746292604916338
Iteration 10, Training loss = 0.3769948278072376
Iteration 20, Training loss = 0.3261669321696539
Iteration 30, Training loss = 0.31031860403968514
Iteration 40, Training loss = 0.2989627159232103
Iteration 50, Training loss = 0.29092213439480696
Iteration 60, Training loss = 0.28410963609742657
Iteration 70, Training loss = 0.27895090840576925
Iteration 80, Training loss = 0.273764338830243
Iteration 90, Training loss = 0.26962180993551216
Iteration 100, Training loss = 0.2667987210235158
Iteration 110, Training loss = 0.26553017185362066
Iteration 120, Training loss = 0.2623247928233538
Iteration 130, Training loss = 0.26076464315399456
Iteration 140, Training loss = 0.25861059838303047
Iteration 150, Training loss = 0.25806056243785913
Iteration 160, Training loss = 0.2573702054971082
Iteration 170, Training loss = 0.25534267914323994
Iteration 180, Training loss = 0.2543680959061724
Iteration 190, Training loss = 0.25345005966039097
Model training time: 51.41258358955383
Device: cuda
Iteration 0, Training loss = 1.0486023987548938
Iteration 10, Training loss = 0.5544314560106987
Iteration 20, Training loss = 0.33582482339391384
Iteration 30, Training loss = 0.3132714617655473
Iteration 40, Training loss = 0.2995928040017252
Iteration 50, Training loss = 0.2877356869299055
Iteration 60, Training loss = 0.2801180561455551
Iteration 70, Training loss = 0.2747568007972505
Iteration 80, Training loss = 0.27070311798853575
Iteration 90, Training loss = 0.26821310328688597
Iteration 100, Training loss = 0.26437320052713587
Iteration 110, Training loss = 0.26256019755262106
Iteration 120, Training loss = 0.2606180339043843
Iteration 130, Training loss = 0.25865703936360307
Iteration 140, Training loss = 0.25661312130050384
Iteration 150, Training loss = 0.2556917042907885
Iteration 160, Training loss = 0.254145951770642
Iteration 170, Training loss = 0.25309238939181616
Iteration 180, Training loss = 0.2524670490608123
Iteration 190, Training loss = 0.2513251924428387
Model training time: 52.23778200149536
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7966307940114523
Iteration 10, Training loss = 0.2701269206073549
Iteration 20, Training loss = 0.24931855632919045
Iteration 30, Training loss = 0.24441115032647542
Iteration 40, Training loss = 0.24094954503763125
Iteration 50, Training loss = 0.23860078434581342
Iteration 60, Training loss = 0.2372648432706865
Iteration 70, Training loss = 0.23574488016142361
Iteration 80, Training loss = 0.23583584563167775
Iteration 90, Training loss = 0.23343877832670717
Iteration 100, Training loss = 0.2348202885924906
Iteration 110, Training loss = 0.23281237666589627
Iteration 120, Training loss = 0.23354251653963817
Iteration 130, Training loss = 0.23252508155821602
Iteration 140, Training loss = 0.2316262165201459
Iteration 150, Training loss = 0.23078445913855006
Iteration 160, Training loss = 0.231313889729228
Iteration 170, Training loss = 0.23188786399393266
Iteration 180, Training loss = 0.23079738258451654
Iteration 190, Training loss = 0.23145270315201386
Model training time: 58.382750034332275
Device: cuda
Iteration 0, Training loss = 0.5905745673726722
Iteration 10, Training loss = 0.2569469182891546
Iteration 20, Training loss = 0.2432855307911905
Iteration 30, Training loss = 0.23636711140473685
Iteration 40, Training loss = 0.23323627651313653
Iteration 50, Training loss = 0.22712594600043434
Iteration 60, Training loss = 0.22320656183261225
Iteration 70, Training loss = 0.2190970712237888
Iteration 80, Training loss = 0.21799721374459888
Iteration 90, Training loss = 0.21393773283647455
Iteration 100, Training loss = 0.21269170353234101
Iteration 110, Training loss = 0.2122921540181418
Iteration 120, Training loss = 0.2120450912538358
Iteration 130, Training loss = 0.21079202849363934
Iteration 140, Training loss = 0.20944956171771753
Iteration 150, Training loss = 0.20953668776341683
Iteration 160, Training loss = 0.20995789230013814
Iteration 170, Training loss = 0.2075967079271441
Iteration 180, Training loss = 0.2084847931628642
Iteration 190, Training loss = 0.20729143989546864
Model training time: 58.68978667259216
Device: cuda
Iteration 0, Training loss = 0.5858097361481708
Iteration 10, Training loss = 0.252922968695993
Iteration 20, Training loss = 0.2413647048499273
Iteration 30, Training loss = 0.2382047995565018
Iteration 40, Training loss = 0.23534481266990376
Iteration 50, Training loss = 0.22949857739866644
Iteration 60, Training loss = 0.22498265272321333
Iteration 70, Training loss = 0.22182789506111744
Iteration 80, Training loss = 0.22080695845078732
Iteration 90, Training loss = 0.21928242054106534
Iteration 100, Training loss = 0.2167733087055925
Iteration 110, Training loss = 0.21628373191840405
Iteration 120, Training loss = 0.21465361399063165
Iteration 130, Training loss = 0.21416252325986318
Iteration 140, Training loss = 0.2139224050845501
Iteration 150, Training loss = 0.21354746102278935
Iteration 160, Training loss = 0.21344373058437724
Iteration 170, Training loss = 0.21225471769410056
Iteration 180, Training loss = 0.21158933891478368
Iteration 190, Training loss = 0.21149470790284844
Model training time: 59.97468948364258
Device: cuda
Iteration 0, Training loss = 0.5319563072660695
Iteration 10, Training loss = 0.25164877007836883
Iteration 20, Training loss = 0.24009755185404838
Iteration 30, Training loss = 0.2348140677896099
Iteration 40, Training loss = 0.2291214581992891
Iteration 50, Training loss = 0.22394424961240972
Iteration 60, Training loss = 0.22254380036667348
Iteration 70, Training loss = 0.22014728652826254
Iteration 80, Training loss = 0.217674704025621
Iteration 90, Training loss = 0.21689086187864848
Iteration 100, Training loss = 0.21600210342718207
Iteration 110, Training loss = 0.21564228724742282
Iteration 120, Training loss = 0.2138969412676378
Iteration 130, Training loss = 0.2160257398362321
Iteration 140, Training loss = 0.21417874403765813
Iteration 150, Training loss = 0.2124652493979044
Iteration 160, Training loss = 0.21240238809355216
Iteration 170, Training loss = 0.21338858869340685
Iteration 180, Training loss = 0.21205695423383067
Iteration 190, Training loss = 0.21346270807699305
Model training time: 58.811248540878296
Device: cuda
Iteration 0, Training loss = 0.6367931383243506
Iteration 10, Training loss = 0.25856357942456787
Iteration 20, Training loss = 0.2472685705060544
Iteration 30, Training loss = 0.2393755926458156
Iteration 40, Training loss = 0.2348058720069807
Iteration 50, Training loss = 0.23199150606917873
Iteration 60, Training loss = 0.22848513470467738
Iteration 70, Training loss = 0.22505124715935204
Iteration 80, Training loss = 0.22288025937218597
Iteration 90, Training loss = 0.22076412344324417
Iteration 100, Training loss = 0.21807109125426427
Iteration 110, Training loss = 0.21691537126538835
Iteration 120, Training loss = 0.21700764076721266
Iteration 130, Training loss = 0.21480504124636812
Iteration 140, Training loss = 0.21537070264274946
Iteration 150, Training loss = 0.21411413346655703
Iteration 160, Training loss = 0.21286120974787193
Iteration 170, Training loss = 0.21297467370396075
Iteration 180, Training loss = 0.21127499553604404
Iteration 190, Training loss = 0.21129843499061565
Model training time: 59.35784316062927
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1550036320363841
Iteration 10, Training loss = 0.5781740305792307
Iteration 20, Training loss = 0.41208787190453444
Iteration 30, Training loss = 0.3613497343472237
Iteration 40, Training loss = 0.3317549784762272
Iteration 50, Training loss = 0.31502813575924304
Iteration 60, Training loss = 0.3017073022786546
Iteration 70, Training loss = 0.29210642246520463
Iteration 80, Training loss = 0.2847297675367715
Iteration 90, Training loss = 0.2791823371165041
Iteration 100, Training loss = 0.2743109645690895
Iteration 110, Training loss = 0.27131408524973954
Iteration 120, Training loss = 0.26897003764403615
Iteration 130, Training loss = 0.267543264489243
Iteration 140, Training loss = 0.2642239425421337
Iteration 150, Training loss = 0.2632181963338944
Iteration 160, Training loss = 0.2609628892823118
Iteration 170, Training loss = 0.25996835544201485
Iteration 180, Training loss = 0.2582510705443396
Iteration 190, Training loss = 0.2582781683779569
Model training time: 51.01583695411682
Device: cuda
Iteration 0, Training loss = 0.9176143744717473
Iteration 10, Training loss = 0.38232420773609826
Iteration 20, Training loss = 0.3352238722325523
Iteration 30, Training loss = 0.3136454824232249
Iteration 40, Training loss = 0.3011295210192169
Iteration 50, Training loss = 0.2912786129664108
Iteration 60, Training loss = 0.2848806149429745
Iteration 70, Training loss = 0.28048892124839453
Iteration 80, Training loss = 0.27666034164348086
Iteration 90, Training loss = 0.27464896210149864
Iteration 100, Training loss = 0.27144503510660595
Iteration 110, Training loss = 0.2683481156178143
Iteration 120, Training loss = 0.26678295923028017
Iteration 130, Training loss = 0.26438000866612377
Iteration 140, Training loss = 0.2626011306105029
Iteration 150, Training loss = 0.2620027491795844
Iteration 160, Training loss = 0.25933236153661343
Iteration 170, Training loss = 0.25795120414760375
Iteration 180, Training loss = 0.25711125804894214
Iteration 190, Training loss = 0.2549854752018256
Model training time: 50.45109438896179
Device: cuda
Iteration 0, Training loss = 0.9954423659665573
Iteration 10, Training loss = 0.4572767302897817
Iteration 20, Training loss = 0.3677837138518619
Iteration 30, Training loss = 0.3286871608472677
Iteration 40, Training loss = 0.3065857061945298
Iteration 50, Training loss = 0.292832113046577
Iteration 60, Training loss = 0.2835832683792437
Iteration 70, Training loss = 0.2775249172020074
Iteration 80, Training loss = 0.27274452218254985
Iteration 90, Training loss = 0.26826787203694313
Iteration 100, Training loss = 0.2652170531415709
Iteration 110, Training loss = 0.26257600458923747
Iteration 120, Training loss = 0.26052535026545687
Iteration 130, Training loss = 0.2594236350721783
Iteration 140, Training loss = 0.2572585472090233
Iteration 150, Training loss = 0.25715626473876013
Iteration 160, Training loss = 0.2549181933996182
Iteration 170, Training loss = 0.25404324472526424
Iteration 180, Training loss = 0.25276907180674413
Iteration 190, Training loss = 0.2519292223856645
Model training time: 50.522143840789795
Device: cuda
Iteration 0, Training loss = 0.9559388045527509
Iteration 10, Training loss = 0.45680621079200706
Iteration 20, Training loss = 0.3757691846953498
Iteration 30, Training loss = 0.34201899257259094
Iteration 40, Training loss = 0.3225400650126923
Iteration 50, Training loss = 0.31019399246731816
Iteration 60, Training loss = 0.3018278740166466
Iteration 70, Training loss = 0.2946549039604007
Iteration 80, Training loss = 0.28757847615198234
Iteration 90, Training loss = 0.28060192737170464
Iteration 100, Training loss = 0.2758707778105413
Iteration 110, Training loss = 0.27219414113512364
Iteration 120, Training loss = 0.2698235657192083
Iteration 130, Training loss = 0.2675892028255739
Iteration 140, Training loss = 0.265714080185418
Iteration 150, Training loss = 0.2642756933031451
Iteration 160, Training loss = 0.26250112297886236
Iteration 170, Training loss = 0.26160294465396716
Iteration 180, Training loss = 0.25994593300969127
Iteration 190, Training loss = 0.2591188429778325
Model training time: 50.546573638916016
Device: cuda
Iteration 0, Training loss = 1.0148334874623064
Iteration 10, Training loss = 0.45976109629955847
Iteration 20, Training loss = 0.3541512242405887
Iteration 30, Training loss = 0.3267520585497796
Iteration 40, Training loss = 0.31604474718156067
Iteration 50, Training loss = 0.3097896780368786
Iteration 60, Training loss = 0.3061522475403288
Iteration 70, Training loss = 0.2999804929834633
Iteration 80, Training loss = 0.29579086940069704
Iteration 90, Training loss = 0.29108456162294905
Iteration 100, Training loss = 0.2870468421690706
Iteration 110, Training loss = 0.2830946232745613
Iteration 120, Training loss = 0.27943604641058595
Iteration 130, Training loss = 0.27558439487708364
Iteration 140, Training loss = 0.2734156606565927
Iteration 150, Training loss = 0.270664576926957
Iteration 160, Training loss = 0.26831218485095076
Iteration 170, Training loss = 0.2656753205374819
Iteration 180, Training loss = 0.26393451653240957
Iteration 190, Training loss = 0.2619456023985637
Model training time: 50.92184615135193
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.508171847405065
Iteration 10, Training loss = 0.2582100369216163
Iteration 20, Training loss = 0.2449505191638274
Iteration 30, Training loss = 0.2394158042571395
Iteration 40, Training loss = 0.23464065225084046
Iteration 50, Training loss = 0.2313165351172576
Iteration 60, Training loss = 0.22936115259133674
Iteration 70, Training loss = 0.22749693007860783
Iteration 80, Training loss = 0.22692813414738375
Iteration 90, Training loss = 0.22476762144461923
Iteration 100, Training loss = 0.22210158371695
Iteration 110, Training loss = 0.22119459581835835
Iteration 120, Training loss = 0.21966088440827125
Iteration 130, Training loss = 0.21889536295104142
Iteration 140, Training loss = 0.21586200324953467
Iteration 150, Training loss = 0.214303330162873
Iteration 160, Training loss = 0.21329930511073789
Iteration 170, Training loss = 0.21183925201207543
Iteration 180, Training loss = 0.21232561648323917
Iteration 190, Training loss = 0.21019988006727708
Model training time: 58.467230796813965
Device: cuda
Iteration 0, Training loss = 0.5364574162280502
Iteration 10, Training loss = 0.254730002567676
Iteration 20, Training loss = 0.2401270904186843
Iteration 30, Training loss = 0.23429436189828864
Iteration 40, Training loss = 0.2264169489847865
Iteration 50, Training loss = 0.22090851227586397
Iteration 60, Training loss = 0.21436069077915615
Iteration 70, Training loss = 0.20965483597511256
Iteration 80, Training loss = 0.2082761329945159
Iteration 90, Training loss = 0.20897702753975772
Iteration 100, Training loss = 0.206393636043233
Iteration 110, Training loss = 0.20476666949509423
Iteration 120, Training loss = 0.20478608023717207
Iteration 130, Training loss = 0.20407708680284195
Iteration 140, Training loss = 0.20382142243321966
Iteration 150, Training loss = 0.20338348245275192
Iteration 160, Training loss = 0.20378608605711931
Iteration 170, Training loss = 0.2001099888469286
Iteration 180, Training loss = 0.20111358421292283
Iteration 190, Training loss = 0.20309110626506344
Model training time: 59.23322892189026
Device: cuda
Iteration 0, Training loss = 0.5650459031263987
Iteration 10, Training loss = 0.2580960875979944
Iteration 20, Training loss = 0.24519126271999978
Iteration 30, Training loss = 0.23620440598559264
Iteration 40, Training loss = 0.22937544343480165
Iteration 50, Training loss = 0.2265283707259358
Iteration 60, Training loss = 0.22364243094759864
Iteration 70, Training loss = 0.2217157565165257
Iteration 80, Training loss = 0.21946074363690068
Iteration 90, Training loss = 0.21641231457824292
Iteration 100, Training loss = 0.21463123986110594
Iteration 110, Training loss = 0.21404374210874816
Iteration 120, Training loss = 0.21126244706664107
Iteration 130, Training loss = 0.21220871194261284
Iteration 140, Training loss = 0.20985477594073842
Iteration 150, Training loss = 0.20783560233559586
Iteration 160, Training loss = 0.20730751727658195
Iteration 170, Training loss = 0.20537338161094176
Iteration 180, Training loss = 0.20621163495208905
Iteration 190, Training loss = 0.20570266038035426
Model training time: 63.769309520721436
Device: cuda
Iteration 0, Training loss = 0.522536716023505
Iteration 10, Training loss = 0.25515946645091697
Iteration 20, Training loss = 0.23779945874559708
Iteration 30, Training loss = 0.22720361864509214
Iteration 40, Training loss = 0.21912191585066237
Iteration 50, Training loss = 0.211569769370959
Iteration 60, Training loss = 0.20708352312949543
Iteration 70, Training loss = 0.20484820447394236
Iteration 80, Training loss = 0.20423356029722425
Iteration 90, Training loss = 0.20424289295926762
Iteration 100, Training loss = 0.20185756078664807
Iteration 110, Training loss = 0.20108637343282285
Iteration 120, Training loss = 0.20102707350599594
Iteration 130, Training loss = 0.20082782216118153
Iteration 140, Training loss = 0.19974987521983575
Iteration 150, Training loss = 0.20074525457505443
Iteration 160, Training loss = 0.1984552462103862
Iteration 170, Training loss = 0.19758470698399244
Iteration 180, Training loss = 0.1974792666193368
Iteration 190, Training loss = 0.19848951211442117
Model training time: 64.8908953666687
Device: cuda
Iteration 0, Training loss = 0.44778833743454755
Iteration 10, Training loss = 0.2525303141362425
Iteration 20, Training loss = 0.24491726448714446
Iteration 30, Training loss = 0.24017013843364762
Iteration 40, Training loss = 0.23523642734628944
Iteration 50, Training loss = 0.23463731968172508
Iteration 60, Training loss = 0.23039381825117675
Iteration 70, Training loss = 0.22965432177995138
Iteration 80, Training loss = 0.22567455693719468
Iteration 90, Training loss = 0.2227919446889329
Iteration 100, Training loss = 0.2190360087127501
Iteration 110, Training loss = 0.21764950734981592
Iteration 120, Training loss = 0.21428910972199577
Iteration 130, Training loss = 0.214457955841281
Iteration 140, Training loss = 0.21259237523528113
Iteration 150, Training loss = 0.21089800389636543
Iteration 160, Training loss = 0.20988976138369472
Iteration 170, Training loss = 0.2082650883546198
Iteration 180, Training loss = 0.2070066881712508
Iteration 190, Training loss = 0.2042275514219694
Model training time: 59.95123791694641
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9668527496033821
Iteration 10, Training loss = 0.3585779272415788
Iteration 20, Training loss = 0.328856504553758
Iteration 30, Training loss = 0.31624161434058407
Iteration 40, Training loss = 0.30748654585241697
Iteration 50, Training loss = 0.30170816406247697
Iteration 60, Training loss = 0.2962480035525013
Iteration 70, Training loss = 0.29103102927334645
Iteration 80, Training loss = 0.28715944952434963
Iteration 90, Training loss = 0.28361089020103647
Iteration 100, Training loss = 0.2801235742902986
Iteration 110, Training loss = 0.27806413540805597
Iteration 120, Training loss = 0.27547088517371005
Iteration 130, Training loss = 0.2757525551290328
Iteration 140, Training loss = 0.27303189924661664
Iteration 150, Training loss = 0.27048632632131164
Iteration 160, Training loss = 0.26933316605678503
Iteration 170, Training loss = 0.26841463321360987
Iteration 180, Training loss = 0.2664813845629853
Iteration 190, Training loss = 0.2646556997284797
Model training time: 51.373414278030396
Device: cuda
Iteration 0, Training loss = 0.9253718901947501
Iteration 10, Training loss = 0.3870158741151653
Iteration 20, Training loss = 0.33554563509381335
Iteration 30, Training loss = 0.319325497041002
Iteration 40, Training loss = 0.31051036765897905
Iteration 50, Training loss = 0.3029571521829292
Iteration 60, Training loss = 0.29532077159858555
Iteration 70, Training loss = 0.28902948842100473
Iteration 80, Training loss = 0.2832031070754148
Iteration 90, Training loss = 0.27982121454056913
Iteration 100, Training loss = 0.2766278488336554
Iteration 110, Training loss = 0.27478832956673443
Iteration 120, Training loss = 0.2727383555493493
Iteration 130, Training loss = 0.2712849228134478
Iteration 140, Training loss = 0.2692968098509715
Iteration 150, Training loss = 0.26829474379762935
Iteration 160, Training loss = 0.2662271858701383
Iteration 170, Training loss = 0.2655634363204385
Iteration 180, Training loss = 0.2643682715541499
Iteration 190, Training loss = 0.2636732756371659
Model training time: 51.984132289886475
Device: cuda
Iteration 0, Training loss = 0.9786714062022702
Iteration 10, Training loss = 0.373946993895199
Iteration 20, Training loss = 0.32865999912582156
Iteration 30, Training loss = 0.31092404253816835
Iteration 40, Training loss = 0.29991469665425985
Iteration 50, Training loss = 0.29268716592431643
Iteration 60, Training loss = 0.28627196825356876
Iteration 70, Training loss = 0.28077571579943533
Iteration 80, Training loss = 0.2782752881735419
Iteration 90, Training loss = 0.272388210007246
Iteration 100, Training loss = 0.269391532825387
Iteration 110, Training loss = 0.26581485843024966
Iteration 120, Training loss = 0.2634984378653448
Iteration 130, Training loss = 0.2620613628971404
Iteration 140, Training loss = 0.25963284118451935
Iteration 150, Training loss = 0.2592360716007182
Iteration 160, Training loss = 0.2586436654634522
Iteration 170, Training loss = 0.25628156715257155
Iteration 180, Training loss = 0.25499075989504366
Iteration 190, Training loss = 0.254680340056834
Model training time: 51.330010652542114
Device: cuda
Iteration 0, Training loss = 1.0402272838901208
Iteration 10, Training loss = 0.37522529555115725
Iteration 20, Training loss = 0.33434985491676605
Iteration 30, Training loss = 0.3179800500328414
Iteration 40, Training loss = 0.3070285398027171
Iteration 50, Training loss = 0.2988719091467235
Iteration 60, Training loss = 0.2928673290976004
Iteration 70, Training loss = 0.28736614165962604
Iteration 80, Training loss = 0.2830107267856022
Iteration 90, Training loss = 0.2782086959639609
Iteration 100, Training loss = 0.2742265430841469
Iteration 110, Training loss = 0.2716466495812227
Iteration 120, Training loss = 0.26814254758438627
Iteration 130, Training loss = 0.2659804040323133
Iteration 140, Training loss = 0.26205843478297264
Iteration 150, Training loss = 0.2598225500992531
Iteration 160, Training loss = 0.2583018808765112
Iteration 170, Training loss = 0.25641921144608715
Iteration 180, Training loss = 0.2543237860436025
Iteration 190, Training loss = 0.25313675878704456
Model training time: 51.9103467464447
Device: cuda
Iteration 0, Training loss = 0.9915948219345387
Iteration 10, Training loss = 0.3659438773773719
Iteration 20, Training loss = 0.3117652685889875
Iteration 30, Training loss = 0.2966064870141555
Iteration 40, Training loss = 0.28654903806925974
Iteration 50, Training loss = 0.27934395367108683
Iteration 60, Training loss = 0.27328159238981165
Iteration 70, Training loss = 0.26835749471101206
Iteration 80, Training loss = 0.26501856773083915
Iteration 90, Training loss = 0.26239356513760514
Iteration 100, Training loss = 0.26019744488640106
Iteration 110, Training loss = 0.2587073516226621
Iteration 120, Training loss = 0.25726119454068264
Iteration 130, Training loss = 0.25612254415589253
Iteration 140, Training loss = 0.25531284219544864
Iteration 150, Training loss = 0.25412683095333083
Iteration 160, Training loss = 0.2537364818983608
Iteration 170, Training loss = 0.2523478844747451
Iteration 180, Training loss = 0.25177664900027613
Iteration 190, Training loss = 0.25121867923057023
Model training time: 50.56019067764282
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.48270420096634664
Iteration 10, Training loss = 0.25323394156884454
Iteration 20, Training loss = 0.24022826696363625
Iteration 30, Training loss = 0.22941718297304162
Iteration 40, Training loss = 0.22520133730581993
Iteration 50, Training loss = 0.2211505363960773
Iteration 60, Training loss = 0.21744508670579984
Iteration 70, Training loss = 0.21805548635513886
Iteration 80, Training loss = 0.21396443165010876
Iteration 90, Training loss = 0.21237402727854424
Iteration 100, Training loss = 0.21198167867850568
Iteration 110, Training loss = 0.20890942089943496
Iteration 120, Training loss = 0.2091249213486478
Iteration 130, Training loss = 0.20823761644426753
Iteration 140, Training loss = 0.20595205953155737
Iteration 150, Training loss = 0.20595815738186168
Iteration 160, Training loss = 0.20396876025603014
Iteration 170, Training loss = 0.20404490187836155
Iteration 180, Training loss = 0.203197896660526
Iteration 190, Training loss = 0.20365493719416541
Iteration 200, Training loss = 0.20315402864977933
Iteration 210, Training loss = 0.20109293008772072
Iteration 220, Training loss = 0.20069436387019457
Iteration 230, Training loss = 0.20034363707482528
Iteration 240, Training loss = 0.2009097911813409
Iteration 250, Training loss = 0.19906870495294027
Iteration 260, Training loss = 0.1990435594593845
Iteration 270, Training loss = 0.19947262910973046
Iteration 280, Training loss = 0.1988958388135053
Iteration 290, Training loss = 0.19788828895287813
Model training time: 90.98783946037292
Device: cuda
Iteration 0, Training loss = 0.5771894077747917
Iteration 10, Training loss = 0.24603286450323852
Iteration 20, Training loss = 0.23256370846344077
Iteration 30, Training loss = 0.22530258508120182
Iteration 40, Training loss = 0.21759485662127462
Iteration 50, Training loss = 0.21360789890882473
Iteration 60, Training loss = 0.21061996312964942
Iteration 70, Training loss = 0.20768723924810759
Iteration 80, Training loss = 0.20585860027639186
Iteration 90, Training loss = 0.20606383465338443
Iteration 100, Training loss = 0.2057766371231148
Iteration 110, Training loss = 0.20472481393295786
Iteration 120, Training loss = 0.2055568382287947
Iteration 130, Training loss = 0.2031572270364577
Iteration 140, Training loss = 0.20286011559087874
Iteration 150, Training loss = 0.20258815712542924
Iteration 160, Training loss = 0.20297703283708451
Iteration 170, Training loss = 0.20166637032216297
Iteration 180, Training loss = 0.2006252566397478
Iteration 190, Training loss = 0.2014807437137129
Iteration 200, Training loss = 0.199401268924492
Iteration 210, Training loss = 0.19977407340986142
Iteration 220, Training loss = 0.19973305159288904
Iteration 230, Training loss = 0.1990896563815034
Iteration 240, Training loss = 0.19954687886048053
Iteration 250, Training loss = 0.19953078209705974
Iteration 260, Training loss = 0.19876621253248575
Iteration 270, Training loss = 0.1975435256454104
Iteration 280, Training loss = 0.196160691767787
Iteration 290, Training loss = 0.1974263471249797
Model training time: 89.03484344482422
Device: cuda
Iteration 0, Training loss = 0.45742320049788066
Iteration 10, Training loss = 0.25029361773516245
Iteration 20, Training loss = 0.2337426482407367
Iteration 30, Training loss = 0.22458094293656555
Iteration 40, Training loss = 0.2216948063477226
Iteration 50, Training loss = 0.21725762232346235
Iteration 60, Training loss = 0.21462566671884004
Iteration 70, Training loss = 0.21244300185626255
Iteration 80, Training loss = 0.21047594984948348
Iteration 90, Training loss = 0.21052405482904923
Iteration 100, Training loss = 0.20767536834962125
Iteration 110, Training loss = 0.2057527514975428
Iteration 120, Training loss = 0.20364793141682944
Iteration 130, Training loss = 0.20306901374588843
Iteration 140, Training loss = 0.20160554723751142
Iteration 150, Training loss = 0.203618836035763
Iteration 160, Training loss = 0.20031721695609714
Iteration 170, Training loss = 0.2003416074214926
Iteration 180, Training loss = 0.19818463162091618
Iteration 190, Training loss = 0.19781818564819253
Iteration 200, Training loss = 0.19585639216761658
Iteration 210, Training loss = 0.19387686418162453
Iteration 220, Training loss = 0.19531595480182898
Iteration 230, Training loss = 0.1944227152040615
Iteration 240, Training loss = 0.19391577393896337
Iteration 250, Training loss = 0.1935702830841
Iteration 260, Training loss = 0.19462782457686853
Iteration 270, Training loss = 0.1914960557711873
Iteration 280, Training loss = 0.19144656660332196
Iteration 290, Training loss = 0.19146112849315008
Model training time: 88.96024250984192
Device: cuda
Iteration 0, Training loss = 0.5061239151707018
Iteration 10, Training loss = 0.24547478031133108
Iteration 20, Training loss = 0.23283758471553453
Iteration 30, Training loss = 0.22209807301776996
Iteration 40, Training loss = 0.21779750661861494
Iteration 50, Training loss = 0.21251251649309472
Iteration 60, Training loss = 0.20959025589020355
Iteration 70, Training loss = 0.20965643644620832
Iteration 80, Training loss = 0.20714529398558795
Iteration 90, Training loss = 0.20455630451152865
Iteration 100, Training loss = 0.2033388432889169
Iteration 110, Training loss = 0.20552450396876404
Iteration 120, Training loss = 0.20247370753311306
Iteration 130, Training loss = 0.20044425335051358
Iteration 140, Training loss = 0.20157607144491685
Iteration 150, Training loss = 0.1996566790745454
Iteration 160, Training loss = 0.19982386812351752
Iteration 170, Training loss = 0.19863846282596173
Iteration 180, Training loss = 0.19910144802308888
Iteration 190, Training loss = 0.1971816379882863
Iteration 200, Training loss = 0.19694294722903755
Iteration 210, Training loss = 0.19798871540073035
Iteration 220, Training loss = 0.19651650734569714
Iteration 230, Training loss = 0.1950267471265102
Iteration 240, Training loss = 0.1945552281351481
Iteration 250, Training loss = 0.19427318223144696
Iteration 260, Training loss = 0.19501713713730015
Iteration 270, Training loss = 0.1955536746100527
Iteration 280, Training loss = 0.1945405595616442
Iteration 290, Training loss = 0.19308082359856454
Model training time: 87.75382375717163
Device: cuda
Iteration 0, Training loss = 0.518826712300812
Iteration 10, Training loss = 0.2460831325987111
Iteration 20, Training loss = 0.23563521560551465
Iteration 30, Training loss = 0.22744323982708697
Iteration 40, Training loss = 0.22302290080970036
Iteration 50, Training loss = 0.2188761158697847
Iteration 60, Training loss = 0.21439323048804693
Iteration 70, Training loss = 0.2128726188543338
Iteration 80, Training loss = 0.2100060047302845
Iteration 90, Training loss = 0.21111261349729293
Iteration 100, Training loss = 0.20897513563218323
Iteration 110, Training loss = 0.20652929405515322
Iteration 120, Training loss = 0.20411218904354722
Iteration 130, Training loss = 0.20368379430062528
Iteration 140, Training loss = 0.20196067513043178
Iteration 150, Training loss = 0.2002449880403597
Iteration 160, Training loss = 0.19971247294530775
Iteration 170, Training loss = 0.19847887155154478
Iteration 180, Training loss = 0.19749976194279206
Iteration 190, Training loss = 0.19581540645176662
Iteration 200, Training loss = 0.19672449447826487
Iteration 210, Training loss = 0.19707063833872476
Iteration 220, Training loss = 0.19681787631217984
Iteration 230, Training loss = 0.19565730236002787
Iteration 240, Training loss = 0.19430322293642063
Iteration 250, Training loss = 0.19363877346406236
Iteration 260, Training loss = 0.19323318592016248
Iteration 270, Training loss = 0.19207625058682068
Iteration 280, Training loss = 0.19320385777144042
Iteration 290, Training loss = 0.19276365958118208
Model training time: 90.91588425636292
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0040909365755348
Iteration 10, Training loss = 0.39342228657957434
Iteration 20, Training loss = 0.333990634135578
Iteration 30, Training loss = 0.31824804403355733
Iteration 40, Training loss = 0.3080943150508807
Iteration 50, Training loss = 0.30093113898079177
Iteration 60, Training loss = 0.2949355650639188
Iteration 70, Training loss = 0.28946952161869566
Iteration 80, Training loss = 0.2847461727074379
Iteration 90, Training loss = 0.2801621680818318
Iteration 100, Training loss = 0.27679449075086104
Iteration 110, Training loss = 0.27345254596160806
Iteration 120, Training loss = 0.27022154391675757
Iteration 130, Training loss = 0.2673817228363908
Iteration 140, Training loss = 0.2650034155894593
Iteration 150, Training loss = 0.26336310634722454
Iteration 160, Training loss = 0.2607541648662033
Iteration 170, Training loss = 0.2586208915940805
Iteration 180, Training loss = 0.2569766077010528
Iteration 190, Training loss = 0.2553956065920816
Iteration 200, Training loss = 0.2539107852080018
Iteration 210, Training loss = 0.2530253022765192
Iteration 220, Training loss = 0.2524002834362684
Iteration 230, Training loss = 0.25054213746590315
Iteration 240, Training loss = 0.2494871024636255
Iteration 250, Training loss = 0.24801659771209753
Iteration 260, Training loss = 0.24729084792200495
Iteration 270, Training loss = 0.24679025179809994
Iteration 280, Training loss = 0.24572883827098901
Iteration 290, Training loss = 0.24577980722494172
Model training time: 77.02893328666687
Device: cuda
Iteration 0, Training loss = 0.9748212090436963
Iteration 10, Training loss = 0.3997956240810634
Iteration 20, Training loss = 0.3346967953414733
Iteration 30, Training loss = 0.318963714627828
Iteration 40, Training loss = 0.3081300942506191
Iteration 50, Training loss = 0.3003779414339342
Iteration 60, Training loss = 0.29171239552290545
Iteration 70, Training loss = 0.2849330881223586
Iteration 80, Training loss = 0.2788626893562971
Iteration 90, Training loss = 0.27290713272376915
Iteration 100, Training loss = 0.26903098828838645
Iteration 110, Training loss = 0.26525696791744463
Iteration 120, Training loss = 0.2627304152086161
Iteration 130, Training loss = 0.2594791894805604
Iteration 140, Training loss = 0.25785558620368804
Iteration 150, Training loss = 0.25596500569639574
Iteration 160, Training loss = 0.25450211738186757
Iteration 170, Training loss = 0.25304092405642864
Iteration 180, Training loss = 0.2509131798349717
Iteration 190, Training loss = 0.24986575824626978
Iteration 200, Training loss = 0.24871630725509303
Iteration 210, Training loss = 0.2471707707657906
Iteration 220, Training loss = 0.24659196802095515
Iteration 230, Training loss = 0.24508866926898126
Iteration 240, Training loss = 0.2445240694903521
Iteration 250, Training loss = 0.2431145803148044
Iteration 260, Training loss = 0.24242064986683895
Iteration 270, Training loss = 0.24187377573931274
Iteration 280, Training loss = 0.2408035172212527
Iteration 290, Training loss = 0.24054274394892264
Model training time: 76.68055748939514
Device: cuda
Iteration 0, Training loss = 0.897867538790772
Iteration 10, Training loss = 0.4216156375149022
Iteration 20, Training loss = 0.34804105729872475
Iteration 30, Training loss = 0.32541366878914946
Iteration 40, Training loss = 0.31307679508331315
Iteration 50, Training loss = 0.3020559005115343
Iteration 60, Training loss = 0.29407112797101337
Iteration 70, Training loss = 0.28708787244443157
Iteration 80, Training loss = 0.27973730351038023
Iteration 90, Training loss = 0.27495094064785086
Iteration 100, Training loss = 0.2705267600103277
Iteration 110, Training loss = 0.26704365875266023
Iteration 120, Training loss = 0.26466806923997577
Iteration 130, Training loss = 0.26308023533671376
Iteration 140, Training loss = 0.2611590365424824
Iteration 150, Training loss = 0.2596141128724324
Iteration 160, Training loss = 0.2578726926287591
Iteration 170, Training loss = 0.2564162406080587
Iteration 180, Training loss = 0.25553294210042354
Iteration 190, Training loss = 0.2537772364302534
Iteration 200, Training loss = 0.2523918146096566
Iteration 210, Training loss = 0.2511443185849466
Iteration 220, Training loss = 0.25020787958505647
Iteration 230, Training loss = 0.24891926056664923
Iteration 240, Training loss = 0.24882373724870635
Iteration 250, Training loss = 0.2480621420674854
Iteration 260, Training loss = 0.24591188888618912
Iteration 270, Training loss = 0.24520702177775655
Iteration 280, Training loss = 0.2442535691431179
Iteration 290, Training loss = 0.24379211775778573
Model training time: 75.60666465759277
Device: cuda
Iteration 0, Training loss = 1.0442834070339295
Iteration 10, Training loss = 0.36353769005784664
Iteration 20, Training loss = 0.3229199461458962
Iteration 30, Training loss = 0.30737496127828884
Iteration 40, Training loss = 0.29544407410034235
Iteration 50, Training loss = 0.28620517869358475
Iteration 60, Training loss = 0.27973485702045875
Iteration 70, Training loss = 0.27488800624142523
Iteration 80, Training loss = 0.2714472122958317
Iteration 90, Training loss = 0.2680933658339551
Iteration 100, Training loss = 0.2654950109800855
Iteration 110, Training loss = 0.2653283312700797
Iteration 120, Training loss = 0.2617029884947095
Iteration 130, Training loss = 0.2610541345992526
Iteration 140, Training loss = 0.25865095821411715
Iteration 150, Training loss = 0.2578920089608229
Iteration 160, Training loss = 0.25664046125999396
Iteration 170, Training loss = 0.2556033609210005
Iteration 180, Training loss = 0.2539756646838741
Iteration 190, Training loss = 0.2536412557757995
Iteration 200, Training loss = 0.25236930680159786
Iteration 210, Training loss = 0.25133348007996875
Iteration 220, Training loss = 0.2503681524076324
Iteration 230, Training loss = 0.24882736740912792
Iteration 240, Training loss = 0.24761147685528953
Iteration 250, Training loss = 0.24668003445950107
Iteration 260, Training loss = 0.24616353537725366
Iteration 270, Training loss = 0.24509813434979766
Iteration 280, Training loss = 0.2435273700506215
Iteration 290, Training loss = 0.24373960667762204
Model training time: 76.77609205245972
Device: cuda
Iteration 0, Training loss = 0.9615413500490971
Iteration 10, Training loss = 0.36854697317604856
Iteration 20, Training loss = 0.32679471380756675
Iteration 30, Training loss = 0.3112079027482277
Iteration 40, Training loss = 0.30096804454994663
Iteration 50, Training loss = 0.29327464550013704
Iteration 60, Training loss = 0.2865140620925
Iteration 70, Training loss = 0.2802213717270013
Iteration 80, Training loss = 0.2762539940683738
Iteration 90, Training loss = 0.272345626771738
Iteration 100, Training loss = 0.2702374490706817
Iteration 110, Training loss = 0.2677836389357341
Iteration 120, Training loss = 0.26498583776219453
Iteration 130, Training loss = 0.26318437663685296
Iteration 140, Training loss = 0.261280323356246
Iteration 150, Training loss = 0.2592613448339384
Iteration 160, Training loss = 0.257790911204861
Iteration 170, Training loss = 0.2560016104348616
Iteration 180, Training loss = 0.2544381144829994
Iteration 190, Training loss = 0.2538246573601368
Iteration 200, Training loss = 0.2520479622166514
Iteration 210, Training loss = 0.2509414453221404
Iteration 220, Training loss = 0.25003673640138285
Iteration 230, Training loss = 0.24888597594367134
Iteration 240, Training loss = 0.24852066659840985
Iteration 250, Training loss = 0.24749631589449547
Iteration 260, Training loss = 0.24677329211707277
Iteration 270, Training loss = 0.24662357989860617
Iteration 280, Training loss = 0.2459016146435254
Iteration 290, Training loss = 0.24529221128438405
Model training time: 76.88917374610901
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7727983077079201
Iteration 10, Training loss = 0.2598067845987237
Iteration 20, Training loss = 0.24855471791132636
Iteration 30, Training loss = 0.24271055328961155
Iteration 40, Training loss = 0.23822727849805989
Iteration 50, Training loss = 0.23591800214011888
Iteration 60, Training loss = 0.23449075092440066
Iteration 70, Training loss = 0.2315483490266086
Iteration 80, Training loss = 0.22924809337814073
Iteration 90, Training loss = 0.22814128310352133
Iteration 100, Training loss = 0.2272034059976034
Iteration 110, Training loss = 0.2267597136290177
Iteration 120, Training loss = 0.22413815471573154
Iteration 130, Training loss = 0.22355862163403184
Iteration 140, Training loss = 0.22372077855367015
Iteration 150, Training loss = 0.22227752179915203
Iteration 160, Training loss = 0.22133286952396522
Iteration 170, Training loss = 0.22097343199207012
Iteration 180, Training loss = 0.2204010688164384
Iteration 190, Training loss = 0.2200593164793535
Iteration 200, Training loss = 0.21933701285273557
Iteration 210, Training loss = 0.2184907292254305
Iteration 220, Training loss = 0.21818260866518757
Iteration 230, Training loss = 0.2176826159303315
Iteration 240, Training loss = 0.21745441771215862
Iteration 250, Training loss = 0.21993569108311106
Iteration 260, Training loss = 0.21828833290776192
Iteration 270, Training loss = 0.21717384446790253
Iteration 280, Training loss = 0.21618326361052656
Iteration 290, Training loss = 0.21536311248074408
Model training time: 88.38955211639404
Device: cuda
Iteration 0, Training loss = 0.549349329393843
Iteration 10, Training loss = 0.26054742442812895
Iteration 20, Training loss = 0.2506089876023468
Iteration 30, Training loss = 0.24441546169312103
Iteration 40, Training loss = 0.24080586757348932
Iteration 50, Training loss = 0.2359268788770201
Iteration 60, Training loss = 0.23008213102241645
Iteration 70, Training loss = 0.2284768907632229
Iteration 80, Training loss = 0.2256525770045709
Iteration 90, Training loss = 0.22360436013643292
Iteration 100, Training loss = 0.22251006539749063
Iteration 110, Training loss = 0.22318544192014686
Iteration 120, Training loss = 0.22133470373453148
Iteration 130, Training loss = 0.22125906983147497
Iteration 140, Training loss = 0.2201309106560145
Iteration 150, Training loss = 0.2198612438019923
Iteration 160, Training loss = 0.21993520971081684
Iteration 170, Training loss = 0.22018929495327716
Iteration 180, Training loss = 0.21852132775213406
Iteration 190, Training loss = 0.21717187423924894
Iteration 200, Training loss = 0.21772698129432788
Iteration 210, Training loss = 0.21645153806981257
Iteration 220, Training loss = 0.21553644375956577
Iteration 230, Training loss = 0.21621787951188387
Iteration 240, Training loss = 0.21487290282612262
Iteration 250, Training loss = 0.2160898227478571
Iteration 260, Training loss = 0.21419062630997765
Iteration 270, Training loss = 0.21395856314811154
Iteration 280, Training loss = 0.21391648608417327
Iteration 290, Training loss = 0.21442770806775577
Model training time: 88.0910861492157
Device: cuda
Iteration 0, Training loss = 0.5762512843677963
Iteration 10, Training loss = 0.2559976727490264
Iteration 20, Training loss = 0.244480788167836
Iteration 30, Training loss = 0.23775106715263375
Iteration 40, Training loss = 0.23296746111290467
Iteration 50, Training loss = 0.229326277917278
Iteration 60, Training loss = 0.22888283856249086
Iteration 70, Training loss = 0.2249886471292247
Iteration 80, Training loss = 0.22266689842737816
Iteration 90, Training loss = 0.2210340885364491
Iteration 100, Training loss = 0.22028689100834484
Iteration 110, Training loss = 0.2198418008820446
Iteration 120, Training loss = 0.21727497904484974
Iteration 130, Training loss = 0.21648194410518748
Iteration 140, Training loss = 0.2172980033905034
Iteration 150, Training loss = 0.21486234135817792
Iteration 160, Training loss = 0.21506083832270856
Iteration 170, Training loss = 0.21475588555497246
Iteration 180, Training loss = 0.21456585338582163
Iteration 190, Training loss = 0.2139899647538213
Iteration 200, Training loss = 0.2143448346838859
Iteration 210, Training loss = 0.21429695750492206
Iteration 220, Training loss = 0.2132228325314568
Iteration 230, Training loss = 0.21337197001141625
Iteration 240, Training loss = 0.21215139397820412
Iteration 250, Training loss = 0.2120562211614876
Iteration 260, Training loss = 0.21135780913530341
Iteration 270, Training loss = 0.21162522422230762
Iteration 280, Training loss = 0.2117172948980101
Iteration 290, Training loss = 0.21147702123231935
Model training time: 89.42047309875488
Device: cuda
Iteration 0, Training loss = 0.56883295290712
Iteration 10, Training loss = 0.2719674149285192
Iteration 20, Training loss = 0.26282285715358844
Iteration 30, Training loss = 0.25289886009290025
Iteration 40, Training loss = 0.24323173601558243
Iteration 50, Training loss = 0.23945678277436086
Iteration 60, Training loss = 0.23836859628774118
Iteration 70, Training loss = 0.23743627990645486
Iteration 80, Training loss = 0.2372343094236609
Iteration 90, Training loss = 0.233856791243461
Iteration 100, Training loss = 0.23171811993571295
Iteration 110, Training loss = 0.23033878657121012
Iteration 120, Training loss = 0.2304082863068811
Iteration 130, Training loss = 0.22892065411028656
Iteration 140, Training loss = 0.228060413925832
Iteration 150, Training loss = 0.22644826456688452
Iteration 160, Training loss = 0.22634752555457865
Iteration 170, Training loss = 0.22597105221615899
Iteration 180, Training loss = 0.2251585147878974
Iteration 190, Training loss = 0.2239720016573938
Iteration 200, Training loss = 0.2227489557027241
Iteration 210, Training loss = 0.22193169554218578
Iteration 220, Training loss = 0.22208955117325851
Iteration 230, Training loss = 0.2219552276071143
Iteration 240, Training loss = 0.21906638793323352
Iteration 250, Training loss = 0.2198287121915587
Iteration 260, Training loss = 0.21821975063730553
Iteration 270, Training loss = 0.218450253640396
Iteration 280, Training loss = 0.2173815018672874
Iteration 290, Training loss = 0.21853605474250903
Model training time: 90.92696809768677
Device: cuda
Iteration 0, Training loss = 0.6989190487757974
Iteration 10, Training loss = 0.2669330472531526
Iteration 20, Training loss = 0.25109193312085193
Iteration 30, Training loss = 0.24549629906381387
Iteration 40, Training loss = 0.24204532708522777
Iteration 50, Training loss = 0.23687271572253554
Iteration 60, Training loss = 0.23335216993439026
Iteration 70, Training loss = 0.23127257546796892
Iteration 80, Training loss = 0.22665331656662163
Iteration 90, Training loss = 0.22361757728644616
Iteration 100, Training loss = 0.2208911862206344
Iteration 110, Training loss = 0.21942204236984253
Iteration 120, Training loss = 0.2176427304888693
Iteration 130, Training loss = 0.21616227593686846
Iteration 140, Training loss = 0.21592470622005094
Iteration 150, Training loss = 0.21443252747761454
Iteration 160, Training loss = 0.21323268507845736
Iteration 170, Training loss = 0.2128448447815462
Iteration 180, Training loss = 0.21145074281859513
Iteration 190, Training loss = 0.2113112001027462
Iteration 200, Training loss = 0.2126541830563315
Iteration 210, Training loss = 0.2121707540059435
Iteration 220, Training loss = 0.21139483060237865
Iteration 230, Training loss = 0.20946240504294777
Iteration 240, Training loss = 0.21104590727943154
Iteration 250, Training loss = 0.20928551720968191
Iteration 260, Training loss = 0.20942765989021403
Iteration 270, Training loss = 0.21060340344042017
Iteration 280, Training loss = 0.20991132818702338
Iteration 290, Training loss = 0.2106685332486019
Model training time: 89.05267572402954
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.019299734617777
Iteration 10, Training loss = 0.4142826000129543
Iteration 20, Training loss = 0.34101141600505164
Iteration 30, Training loss = 0.3202375861757619
Iteration 40, Training loss = 0.3115889646724802
Iteration 50, Training loss = 0.3054909679840728
Iteration 60, Training loss = 0.3000990934849937
Iteration 70, Training loss = 0.2960145112109069
Iteration 80, Training loss = 0.2910228731551608
Iteration 90, Training loss = 0.2877846571126422
Iteration 100, Training loss = 0.2839408166670569
Iteration 110, Training loss = 0.28124329498136674
Iteration 120, Training loss = 0.27783418583553193
Iteration 130, Training loss = 0.27508048436060045
Iteration 140, Training loss = 0.2729504987381507
Iteration 150, Training loss = 0.27044402573995546
Iteration 160, Training loss = 0.26801920934143847
Iteration 170, Training loss = 0.2664392244628662
Iteration 180, Training loss = 0.2651645165062757
Iteration 190, Training loss = 0.2644727862111612
Iteration 200, Training loss = 0.2638247041526624
Iteration 210, Training loss = 0.2621346541504929
Iteration 220, Training loss = 0.26136443785567215
Iteration 230, Training loss = 0.26047939969145734
Iteration 240, Training loss = 0.2597671118767365
Iteration 250, Training loss = 0.25918381610786284
Iteration 260, Training loss = 0.25891027398011535
Iteration 270, Training loss = 0.2582977743828354
Iteration 280, Training loss = 0.25781630937028044
Iteration 290, Training loss = 0.25771905226264025
Model training time: 79.45630383491516
Device: cuda
Iteration 0, Training loss = 1.0240294662650657
Iteration 10, Training loss = 0.4190848448858169
Iteration 20, Training loss = 0.34740793287466115
Iteration 30, Training loss = 0.3282990036667257
Iteration 40, Training loss = 0.31791295096782096
Iteration 50, Training loss = 0.30845114192812917
Iteration 60, Training loss = 0.3006650743565122
Iteration 70, Training loss = 0.2928130440953849
Iteration 80, Training loss = 0.28742639742035797
Iteration 90, Training loss = 0.2819976893744031
Iteration 100, Training loss = 0.27756588247375213
Iteration 110, Training loss = 0.27435622195114834
Iteration 120, Training loss = 0.2709619332338877
Iteration 130, Training loss = 0.26956962401740214
Iteration 140, Training loss = 0.26779707582388523
Iteration 150, Training loss = 0.26552629830756624
Iteration 160, Training loss = 0.26496635211838615
Iteration 170, Training loss = 0.26315430756928265
Iteration 180, Training loss = 0.2626481486691369
Iteration 190, Training loss = 0.2616155751157498
Iteration 200, Training loss = 0.261582613243285
Iteration 210, Training loss = 0.25999062652317223
Iteration 220, Training loss = 0.25927276452238435
Iteration 230, Training loss = 0.25828862258633556
Iteration 240, Training loss = 0.25772868726708464
Iteration 250, Training loss = 0.2570171047020074
Iteration 260, Training loss = 0.25669081044801767
Iteration 270, Training loss = 0.25624950615248243
Iteration 280, Training loss = 0.25554410414995204
Iteration 290, Training loss = 0.25544500214178206
Model training time: 76.56613302230835
Device: cuda
Iteration 0, Training loss = 1.0579374146058365
Iteration 10, Training loss = 0.584935677656229
Iteration 20, Training loss = 0.35232300739645384
Iteration 30, Training loss = 0.32482114297468306
Iteration 40, Training loss = 0.315591002288072
Iteration 50, Training loss = 0.30914922792842425
Iteration 60, Training loss = 0.30298748137294385
Iteration 70, Training loss = 0.29684522776787986
Iteration 80, Training loss = 0.2916509632903021
Iteration 90, Training loss = 0.28550407754770224
Iteration 100, Training loss = 0.28145777578083214
Iteration 110, Training loss = 0.27620383211667987
Iteration 120, Training loss = 0.2729564946198809
Iteration 130, Training loss = 0.26985446860392887
Iteration 140, Training loss = 0.26801040765024037
Iteration 150, Training loss = 0.2647544064239603
Iteration 160, Training loss = 0.26338218112930584
Iteration 170, Training loss = 0.26075973829641436
Iteration 180, Training loss = 0.26102342420586067
Iteration 190, Training loss = 0.25787314697020297
Iteration 200, Training loss = 0.25685366815414984
Iteration 210, Training loss = 0.2556207054407124
Iteration 220, Training loss = 0.25441602642697414
Iteration 230, Training loss = 0.2534774437474744
Iteration 240, Training loss = 0.2525515757177187
Iteration 250, Training loss = 0.25153928296865474
Iteration 260, Training loss = 0.25085347685692966
Iteration 270, Training loss = 0.2504769045733599
Iteration 280, Training loss = 0.24928945810466574
Iteration 290, Training loss = 0.2489036681211513
Model training time: 84.46094751358032
Device: cuda
Iteration 0, Training loss = 1.0129643920539082
Iteration 10, Training loss = 0.38572296600986794
Iteration 20, Training loss = 0.335136327550607
Iteration 30, Training loss = 0.3188288421302602
Iteration 40, Training loss = 0.3094068701932396
Iteration 50, Training loss = 0.303156067567747
Iteration 60, Training loss = 0.2969296093004337
Iteration 70, Training loss = 0.2932450962815308
Iteration 80, Training loss = 0.2885829090784137
Iteration 90, Training loss = 0.2847286618274191
Iteration 100, Training loss = 0.2817187900992407
Iteration 110, Training loss = 0.2790300756836859
Iteration 120, Training loss = 0.27526022778185094
Iteration 130, Training loss = 0.2720949958991889
Iteration 140, Training loss = 0.2693951269566725
Iteration 150, Training loss = 0.2671320749005834
Iteration 160, Training loss = 0.2643390274134235
Iteration 170, Training loss = 0.26210178870365813
Iteration 180, Training loss = 0.2604016415450884
Iteration 190, Training loss = 0.25895633213761926
Iteration 200, Training loss = 0.2582501126228323
Iteration 210, Training loss = 0.257904079405294
Iteration 220, Training loss = 0.25654318277242677
Iteration 230, Training loss = 0.2556664819878656
Iteration 240, Training loss = 0.2563008802452525
Iteration 250, Training loss = 0.255411468266289
Iteration 260, Training loss = 0.25424784155571517
Iteration 270, Training loss = 0.2534138672593711
Iteration 280, Training loss = 0.25359099174755206
Iteration 290, Training loss = 0.25231977624161805
Model training time: 81.38677883148193
Device: cuda
Iteration 0, Training loss = 1.0532069633836332
Iteration 10, Training loss = 0.39002260875298783
Iteration 20, Training loss = 0.3401335807526169
Iteration 30, Training loss = 0.32435397954954615
Iteration 40, Training loss = 0.3135093952290678
Iteration 50, Training loss = 0.30579361197165245
Iteration 60, Training loss = 0.300304454161925
Iteration 70, Training loss = 0.29435482664384705
Iteration 80, Training loss = 0.28939798229558455
Iteration 90, Training loss = 0.28545248925973826
Iteration 100, Training loss = 0.28136421279342855
Iteration 110, Training loss = 0.27745315338966353
Iteration 120, Training loss = 0.2740022015312444
Iteration 130, Training loss = 0.2712173390647639
Iteration 140, Training loss = 0.2695766225241233
Iteration 150, Training loss = 0.26664519633935846
Iteration 160, Training loss = 0.2651201819884028
Iteration 170, Training loss = 0.2641185324837044
Iteration 180, Training loss = 0.26166310190146674
Iteration 190, Training loss = 0.26125071179751613
Iteration 200, Training loss = 0.2590854898599035
Iteration 210, Training loss = 0.2596820268941962
Iteration 220, Training loss = 0.2565711780950643
Iteration 230, Training loss = 0.25651411776957306
Iteration 240, Training loss = 0.2553474443114322
Iteration 250, Training loss = 0.2542315837409761
Iteration 260, Training loss = 0.2532472811675302
Iteration 270, Training loss = 0.2538416012041811
Iteration 280, Training loss = 0.25243951254276836
Iteration 290, Training loss = 0.2519772460927134
Model training time: 76.72090101242065
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5622216386783526
Iteration 10, Training loss = 0.2533563316444268
Iteration 20, Training loss = 0.24198673805896787
Iteration 30, Training loss = 0.23482783001978041
Iteration 40, Training loss = 0.23204350014383665
Iteration 50, Training loss = 0.2271172777178207
Iteration 60, Training loss = 0.22198475576973192
Iteration 70, Training loss = 0.21827861508309554
Iteration 80, Training loss = 0.21625505492163166
Iteration 90, Training loss = 0.21372844894295154
Iteration 100, Training loss = 0.21163574091478246
Iteration 110, Training loss = 0.211941701490522
Iteration 120, Training loss = 0.21144245219403418
Iteration 130, Training loss = 0.21005217784988708
Iteration 140, Training loss = 0.2109814056235811
Iteration 150, Training loss = 0.20979945944703143
Iteration 160, Training loss = 0.20716077283672665
Iteration 170, Training loss = 0.20762652957784958
Iteration 180, Training loss = 0.20929639523731913
Iteration 190, Training loss = 0.20633737454955706
Iteration 200, Training loss = 0.2056044890468823
Iteration 210, Training loss = 0.2038517101591336
Iteration 220, Training loss = 0.20601551074552651
Iteration 230, Training loss = 0.20465879563404166
Iteration 240, Training loss = 0.20443184583803306
Iteration 250, Training loss = 0.20505882871611683
Iteration 260, Training loss = 0.20461486425736677
Iteration 270, Training loss = 0.20335889769637067
Iteration 280, Training loss = 0.20495134106148843
Iteration 290, Training loss = 0.20239641306840855
Model training time: 87.92907905578613
Device: cuda
Iteration 0, Training loss = 0.5120936739415938
Iteration 10, Training loss = 0.2568666182710353
Iteration 20, Training loss = 0.2418435678101968
Iteration 30, Training loss = 0.23523595914748555
Iteration 40, Training loss = 0.2305806635874481
Iteration 50, Training loss = 0.22645075345672847
Iteration 60, Training loss = 0.22329143035239069
Iteration 70, Training loss = 0.2197965079387605
Iteration 80, Training loss = 0.2187045066828889
Iteration 90, Training loss = 0.21684583722825213
Iteration 100, Training loss = 0.21336211109362938
Iteration 110, Training loss = 0.2136434204696457
Iteration 120, Training loss = 0.21079226021749387
Iteration 130, Training loss = 0.209916266626206
Iteration 140, Training loss = 0.20918188316090672
Iteration 150, Training loss = 0.20666954108482397
Iteration 160, Training loss = 0.20694134632746378
Iteration 170, Training loss = 0.2059840305847822
Iteration 180, Training loss = 0.20423164808951713
Iteration 190, Training loss = 0.20512144022805678
Iteration 200, Training loss = 0.20431705834209055
Iteration 210, Training loss = 0.2040524729999943
Iteration 220, Training loss = 0.20456158085433757
Iteration 230, Training loss = 0.20317149075908938
Iteration 240, Training loss = 0.2023575007195634
Iteration 250, Training loss = 0.20303846791314617
Iteration 260, Training loss = 0.20190671266277055
Iteration 270, Training loss = 0.20190583176658924
Iteration 280, Training loss = 0.20258613675832748
Iteration 290, Training loss = 0.20099642951995278
Model training time: 89.38233995437622
Device: cuda
Iteration 0, Training loss = 0.5434298001337743
Iteration 10, Training loss = 0.24547373575432865
Iteration 20, Training loss = 0.23857535785379042
Iteration 30, Training loss = 0.22801150475147267
Iteration 40, Training loss = 0.2250733490727374
Iteration 50, Training loss = 0.22172500959772995
Iteration 60, Training loss = 0.2214524886674351
Iteration 70, Training loss = 0.21802084767012206
Iteration 80, Training loss = 0.21714615389920663
Iteration 90, Training loss = 0.2145380535779368
Iteration 100, Training loss = 0.21261805612683873
Iteration 110, Training loss = 0.21110966996006345
Iteration 120, Training loss = 0.21152991153623746
Iteration 130, Training loss = 0.21015898685380457
Iteration 140, Training loss = 0.2078878925618342
Iteration 150, Training loss = 0.20722244863492856
Iteration 160, Training loss = 0.20341831560872028
Iteration 170, Training loss = 0.20243677885636038
Iteration 180, Training loss = 0.20244315766913878
Iteration 190, Training loss = 0.20149699602150112
Iteration 200, Training loss = 0.20061185930806083
Iteration 210, Training loss = 0.20093001335283409
Iteration 220, Training loss = 0.20022253674153545
Iteration 230, Training loss = 0.2000764071005554
Iteration 240, Training loss = 0.19956324832594913
Iteration 250, Training loss = 0.19977490207999224
Iteration 260, Training loss = 0.199004086787286
Iteration 270, Training loss = 0.19739083153902046
Iteration 280, Training loss = 0.19840709469168658
Iteration 290, Training loss = 0.1977231550403839
Model training time: 89.05045557022095
Device: cuda
Iteration 0, Training loss = 0.5539747069855243
Iteration 10, Training loss = 0.2545584986607234
Iteration 20, Training loss = 0.23696862485097803
Iteration 30, Training loss = 0.22838696524284888
Iteration 40, Training loss = 0.22347829796841756
Iteration 50, Training loss = 0.21652426797410715
Iteration 60, Training loss = 0.21222422693086707
Iteration 70, Training loss = 0.21132314309549793
Iteration 80, Training loss = 0.20957435645918915
Iteration 90, Training loss = 0.2095995642568754
Iteration 100, Training loss = 0.20763546440768357
Iteration 110, Training loss = 0.2076076029435448
Iteration 120, Training loss = 0.20766358480649294
Iteration 130, Training loss = 0.20704942268593876
Iteration 140, Training loss = 0.20534634197823667
Iteration 150, Training loss = 0.20468107060245846
Iteration 160, Training loss = 0.20509333660205206
Iteration 170, Training loss = 0.20515151818593344
Iteration 180, Training loss = 0.2038003217950823
Iteration 190, Training loss = 0.20436027091770356
Iteration 200, Training loss = 0.20450804436552353
Iteration 210, Training loss = 0.20378243757618797
Iteration 220, Training loss = 0.20475240181321683
Iteration 230, Training loss = 0.2031845463718769
Iteration 240, Training loss = 0.20220866489813524
Iteration 250, Training loss = 0.20455292795879254
Iteration 260, Training loss = 0.20518590905816084
Iteration 270, Training loss = 0.20256343890647382
Iteration 280, Training loss = 0.20238833788080493
Iteration 290, Training loss = 0.20273781150291506
Model training time: 90.44669580459595
Device: cuda
Iteration 0, Training loss = 0.5891173865628128
Iteration 10, Training loss = 0.25789744165784495
Iteration 20, Training loss = 0.24567377077784516
Iteration 30, Training loss = 0.23763239898399455
Iteration 40, Training loss = 0.23285990118404518
Iteration 50, Training loss = 0.23036369314228278
Iteration 60, Training loss = 0.227105763943299
Iteration 70, Training loss = 0.22532888037571008
Iteration 80, Training loss = 0.22415737976918473
Iteration 90, Training loss = 0.2218182552335919
Iteration 100, Training loss = 0.22120568044663627
Iteration 110, Training loss = 0.21937662413442768
Iteration 120, Training loss = 0.2174491186576765
Iteration 130, Training loss = 0.2151956245087195
Iteration 140, Training loss = 0.214417378935549
Iteration 150, Training loss = 0.2131868699106617
Iteration 160, Training loss = 0.2125906835935542
Iteration 170, Training loss = 0.21047962064616346
Iteration 180, Training loss = 0.21029919227540206
Iteration 190, Training loss = 0.20858106247468847
Iteration 200, Training loss = 0.20918230618831615
Iteration 210, Training loss = 0.2074973528151927
Iteration 220, Training loss = 0.20900409031605374
Iteration 230, Training loss = 0.2088892223034504
Iteration 240, Training loss = 0.20799342157760103
Iteration 250, Training loss = 0.2071136107335344
Iteration 260, Training loss = 0.2067599838266626
Iteration 270, Training loss = 0.20617938855132042
Iteration 280, Training loss = 0.20581073612694578
Iteration 290, Training loss = 0.20506493581665886
Model training time: 88.94941973686218
{'activation_functions': ['relu', 'relu'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0097841390089137
Iteration 10, Training loss = 0.3718042239042872
Iteration 20, Training loss = 0.3327661793732989
Iteration 30, Training loss = 0.317546169107087
Iteration 40, Training loss = 0.30659530776134436
Iteration 50, Training loss = 0.29744418843644826
Iteration 60, Training loss = 0.29053747034879124
Iteration 70, Training loss = 0.28337052896402887
Iteration 80, Training loss = 0.27949949573491506
Iteration 90, Training loss = 0.276219484526754
Iteration 100, Training loss = 0.27354858887656297
Iteration 110, Training loss = 0.2717579060058663
Iteration 120, Training loss = 0.2702178945000045
Iteration 130, Training loss = 0.26924754823175606
Iteration 140, Training loss = 0.2676631169042725
Iteration 150, Training loss = 0.2669839481080788
Iteration 160, Training loss = 0.2660325331532437
Iteration 170, Training loss = 0.2644653112632065
Iteration 180, Training loss = 0.26398268298826355
Iteration 190, Training loss = 0.2636644528899792
Iteration 200, Training loss = 0.26188548611126083
Iteration 210, Training loss = 0.2613506285464706
Iteration 220, Training loss = 0.26049748372628495
Iteration 230, Training loss = 0.2596015022788647
Iteration 240, Training loss = 0.2593493942477277
Iteration 250, Training loss = 0.2592300170501649
Iteration 260, Training loss = 0.2586237709735327
Iteration 270, Training loss = 0.25704782114224733
Iteration 280, Training loss = 0.25683869212721855
Iteration 290, Training loss = 0.2566392397247075
Model training time: 76.9126386642456
Device: cuda
Iteration 0, Training loss = 1.0052009210494406
Iteration 10, Training loss = 0.5252209431019382
Iteration 20, Training loss = 0.3816364388822933
Iteration 30, Training loss = 0.33936494214523244
Iteration 40, Training loss = 0.32302550759148485
Iteration 50, Training loss = 0.3102012157872103
Iteration 60, Training loss = 0.29867027527180273
Iteration 70, Training loss = 0.29105967923926845
Iteration 80, Training loss = 0.28619873948431246
Iteration 90, Training loss = 0.2808493660124028
Iteration 100, Training loss = 0.27725546151975505
Iteration 110, Training loss = 0.2755821457735582
Iteration 120, Training loss = 0.2718742160837431
Iteration 130, Training loss = 0.27100886458504025
Iteration 140, Training loss = 0.26924840141321726
Iteration 150, Training loss = 0.26801725455384323
Iteration 160, Training loss = 0.2678292825602103
Iteration 170, Training loss = 0.2661128662850546
Iteration 180, Training loss = 0.2646704545415542
Iteration 190, Training loss = 0.264958027948216
Iteration 200, Training loss = 0.2630042237216148
Iteration 210, Training loss = 0.262492795497323
Iteration 220, Training loss = 0.26125482854923765
Iteration 230, Training loss = 0.25952958851908714
Iteration 240, Training loss = 0.25860000041803877
Iteration 250, Training loss = 0.25797198612044975
Iteration 260, Training loss = 0.257129054426571
Iteration 270, Training loss = 0.2562058292293318
Iteration 280, Training loss = 0.2547795275127254
Iteration 290, Training loss = 0.2540673722607502
Model training time: 76.59066271781921
Device: cuda
Iteration 0, Training loss = 1.0125732084979182
Iteration 10, Training loss = 0.41988009682312105
Iteration 20, Training loss = 0.3290439438128817
Iteration 30, Training loss = 0.31116584915181866
Iteration 40, Training loss = 0.3019077734670777
Iteration 50, Training loss = 0.29531904839087225
Iteration 60, Training loss = 0.29080115762597697
Iteration 70, Training loss = 0.2860210377381044
Iteration 80, Training loss = 0.28176336033620697
Iteration 90, Training loss = 0.2793717557105465
Iteration 100, Training loss = 0.27542391210650474
Iteration 110, Training loss = 0.2719758515052749
Iteration 120, Training loss = 0.26935717722643976
Iteration 130, Training loss = 0.2672345173603671
Iteration 140, Training loss = 0.26513850138239237
Iteration 150, Training loss = 0.262715771758326
Iteration 160, Training loss = 0.26148615719903495
Iteration 170, Training loss = 0.25934124439234896
Iteration 180, Training loss = 0.2583651050997241
Iteration 190, Training loss = 0.25643939258539733
Iteration 200, Training loss = 0.2553523451017873
Iteration 210, Training loss = 0.25464449552954105
Iteration 220, Training loss = 0.253388202348769
Iteration 230, Training loss = 0.25283204908100304
Iteration 240, Training loss = 0.25159440132010963
Iteration 250, Training loss = 0.25075154362813284
Iteration 260, Training loss = 0.25138879812138093
Iteration 270, Training loss = 0.2500265586923286
Iteration 280, Training loss = 0.24937722425241976
Iteration 290, Training loss = 0.24872817397837477
Model training time: 75.81168794631958
Device: cuda
Iteration 0, Training loss = 0.9417310311022588
Iteration 10, Training loss = 0.34438988811151994
Iteration 20, Training loss = 0.3167778095185469
Iteration 30, Training loss = 0.3060475616639363
Iteration 40, Training loss = 0.299607347175119
Iteration 50, Training loss = 0.2950454871147727
Iteration 60, Training loss = 0.29129469844598127
Iteration 70, Training loss = 0.28861343083174335
Iteration 80, Training loss = 0.28597614298696106
Iteration 90, Training loss = 0.28205503660123704
Iteration 100, Training loss = 0.2789256384839182
Iteration 110, Training loss = 0.2767465205656158
Iteration 120, Training loss = 0.27308549857945835
Iteration 130, Training loss = 0.2706555658636462
Iteration 140, Training loss = 0.2671113724725834
Iteration 150, Training loss = 0.2642994319687143
Iteration 160, Training loss = 0.26134558371155736
Iteration 170, Training loss = 0.2585650727080838
Iteration 180, Training loss = 0.2564483851842258
Iteration 190, Training loss = 0.2550264704198653
Iteration 200, Training loss = 0.2539147169549684
Iteration 210, Training loss = 0.2523455817054435
Iteration 220, Training loss = 0.2511073540950167
Iteration 230, Training loss = 0.25007975652166037
Iteration 240, Training loss = 0.24983562957405467
Iteration 250, Training loss = 0.24812906922925498
Iteration 260, Training loss = 0.24721592754701485
Iteration 270, Training loss = 0.24639732840556453
Iteration 280, Training loss = 0.24577558209786668
Iteration 290, Training loss = 0.24544008238160092
Model training time: 76.8154149055481
Device: cuda
Iteration 0, Training loss = 1.0106182236602341
Iteration 10, Training loss = 0.38056515481161035
Iteration 20, Training loss = 0.3266846511674964
Iteration 30, Training loss = 0.3101470284703849
Iteration 40, Training loss = 0.29917765412353664
Iteration 50, Training loss = 0.2925022475529408
Iteration 60, Training loss = 0.28649336124819835
Iteration 70, Training loss = 0.28132740368589687
Iteration 80, Training loss = 0.27717752286777403
Iteration 90, Training loss = 0.27402197159286856
Iteration 100, Training loss = 0.2700857125258676
Iteration 110, Training loss = 0.26722172622519413
Iteration 120, Training loss = 0.2643939267825965
Iteration 130, Training loss = 0.26281604638710115
Iteration 140, Training loss = 0.26098365811766056
Iteration 150, Training loss = 0.25894873294133497
Iteration 160, Training loss = 0.25853575071418916
Iteration 170, Training loss = 0.2567251638513832
Iteration 180, Training loss = 0.2562133185745437
Iteration 190, Training loss = 0.254204379727587
Iteration 200, Training loss = 0.2533342833968176
Iteration 210, Training loss = 0.25261221037826675
Iteration 220, Training loss = 0.25135312189802456
Iteration 230, Training loss = 0.2506406742017626
Iteration 240, Training loss = 0.24994344264268875
Iteration 250, Training loss = 0.24889978582876315
Iteration 260, Training loss = 0.24850122040308617
Iteration 270, Training loss = 0.24747285488003118
Iteration 280, Training loss = 0.24794702958945491
Iteration 290, Training loss = 0.2468109289949067
Model training time: 77.17636466026306
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8027912894120584
Iteration 10, Training loss = 0.2697789075856025
Iteration 20, Training loss = 0.25248031690716743
Iteration 30, Training loss = 0.24736217309076053
Iteration 40, Training loss = 0.24090281694840926
Iteration 50, Training loss = 0.23581128782377794
Iteration 60, Training loss = 0.23146237475940815
Iteration 70, Training loss = 0.22811720627718246
Iteration 80, Training loss = 0.22426826205964273
Iteration 90, Training loss = 0.2232614351579776
Model training time: 18.62119770050049
Device: cuda
Iteration 0, Training loss = 0.836036239965604
Iteration 10, Training loss = 0.27422858172884357
Iteration 20, Training loss = 0.2552756443619728
Iteration 30, Training loss = 0.24707325992102808
Iteration 40, Training loss = 0.2425840525672986
Iteration 50, Training loss = 0.2403431054777824
Iteration 60, Training loss = 0.23639013517934543
Iteration 70, Training loss = 0.23312843519334608
Iteration 80, Training loss = 0.2307045188947366
Iteration 90, Training loss = 0.2270380393960155
Model training time: 18.89542293548584
Device: cuda
Iteration 0, Training loss = 0.8841212672682909
Iteration 10, Training loss = 0.26728152419225526
Iteration 20, Training loss = 0.24934840166511443
Iteration 30, Training loss = 0.2406329415165461
Iteration 40, Training loss = 0.23778452566609934
Iteration 50, Training loss = 0.23159177214480364
Iteration 60, Training loss = 0.2274828660660065
Iteration 70, Training loss = 0.22464389927112138
Iteration 80, Training loss = 0.22105520447859398
Iteration 90, Training loss = 0.2178273592144251
Model training time: 18.400141954421997
Device: cuda
Iteration 0, Training loss = 0.9536653731304866
Iteration 10, Training loss = 0.27777137621664083
Iteration 20, Training loss = 0.25697175313073856
Iteration 30, Training loss = 0.24852557408695036
Iteration 40, Training loss = 0.23933905012045914
Iteration 50, Training loss = 0.23558220754449183
Iteration 60, Training loss = 0.2330961784777733
Iteration 70, Training loss = 0.22989774910876384
Iteration 80, Training loss = 0.2246403215596309
Iteration 90, Training loss = 0.22262585549973524
Model training time: 18.980197191238403
Device: cuda
Iteration 0, Training loss = 0.907315160219486
Iteration 10, Training loss = 0.27214109009275067
Iteration 20, Training loss = 0.2517386040148827
Iteration 30, Training loss = 0.24191825099002856
Iteration 40, Training loss = 0.23659200020707571
Iteration 50, Training loss = 0.2319129094338188
Iteration 60, Training loss = 0.22697092435107782
Iteration 70, Training loss = 0.2254259968176484
Iteration 80, Training loss = 0.22048509042136943
Iteration 90, Training loss = 0.21611921424762562
Model training time: 18.73987078666687
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0004175981649985
Iteration 10, Training loss = 0.8074133132512753
Iteration 20, Training loss = 0.5135017203596922
Iteration 30, Training loss = 0.42811417379058325
Iteration 40, Training loss = 0.4007759210295402
Iteration 50, Training loss = 0.38500392551605517
Iteration 60, Training loss = 0.36782425011579806
Iteration 70, Training loss = 0.3595116869188272
Iteration 80, Training loss = 0.3495937057126027
Iteration 90, Training loss = 0.34285261811545265
Model training time: 16.703819036483765
Device: cuda
Iteration 0, Training loss = 1.0059561660656562
Iteration 10, Training loss = 0.8077570526645734
Iteration 20, Training loss = 0.4415264055132866
Iteration 30, Training loss = 0.35553457416020906
Iteration 40, Training loss = 0.3407779394720609
Iteration 50, Training loss = 0.3358564621840532
Iteration 60, Training loss = 0.3287391674060088
Iteration 70, Training loss = 0.32673951539282614
Iteration 80, Training loss = 0.31882345862686634
Iteration 90, Training loss = 0.31638927729083943
Model training time: 16.764781713485718
Device: cuda
Iteration 0, Training loss = 1.2177960402690446
Iteration 10, Training loss = 0.9273657248570368
Iteration 20, Training loss = 0.7266848368140367
Iteration 30, Training loss = 0.5116484546317503
Iteration 40, Training loss = 0.41133762738452506
Iteration 50, Training loss = 0.37605004327801556
Iteration 60, Training loss = 0.35670239564317924
Iteration 70, Training loss = 0.34114490965237987
Iteration 80, Training loss = 0.331781253648492
Iteration 90, Training loss = 0.3253785429092554
Model training time: 16.878279447555542
Device: cuda
Iteration 0, Training loss = 1.0038112894846842
Iteration 10, Training loss = 0.8945567441674379
Iteration 20, Training loss = 0.6495299408069024
Iteration 30, Training loss = 0.43069237585251147
Iteration 40, Training loss = 0.3904305686457799
Iteration 50, Training loss = 0.3687077166082767
Iteration 60, Training loss = 0.35285293525801253
Iteration 70, Training loss = 0.3422098241459865
Iteration 80, Training loss = 0.3295506058404079
Iteration 90, Training loss = 0.3233904145084895
Model training time: 16.94067907333374
Device: cuda
Iteration 0, Training loss = 0.9945807978510857
Iteration 10, Training loss = 0.913536348595069
Iteration 20, Training loss = 0.6830832935296572
Iteration 30, Training loss = 0.45897235492101085
Iteration 40, Training loss = 0.40538829736984694
Iteration 50, Training loss = 0.38265299080656123
Iteration 60, Training loss = 0.3675638157874346
Iteration 70, Training loss = 0.35632765837586844
Iteration 80, Training loss = 0.3441400949198466
Iteration 90, Training loss = 0.3378724589084203
Model training time: 16.451518774032593
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0173893679793065
Iteration 10, Training loss = 0.28563936706632376
Iteration 20, Training loss = 0.2663924489170313
Iteration 30, Training loss = 0.25653420574963093
Iteration 40, Training loss = 0.2525745682561627
Iteration 50, Training loss = 0.2504822386858555
Iteration 60, Training loss = 0.2480425053777603
Iteration 70, Training loss = 0.247991548349651
Iteration 80, Training loss = 0.24453870966457403
Iteration 90, Training loss = 0.2418935694373571
Model training time: 19.215112447738647
Device: cuda
Iteration 0, Training loss = 0.98786513077525
Iteration 10, Training loss = 0.2822724666733008
Iteration 20, Training loss = 0.267444080601518
Iteration 30, Training loss = 0.2626113034784794
Iteration 40, Training loss = 0.2554324852445951
Iteration 50, Training loss = 0.25223397449232066
Iteration 60, Training loss = 0.24928520791805708
Iteration 70, Training loss = 0.24554515085541284
Iteration 80, Training loss = 0.24662429868028715
Iteration 90, Training loss = 0.24111071357933375
Model training time: 18.484349012374878
Device: cuda
Iteration 0, Training loss = 0.9255045864444512
Iteration 10, Training loss = 0.28800665114361507
Iteration 20, Training loss = 0.26743810093746734
Iteration 30, Training loss = 0.2576583823045859
Iteration 40, Training loss = 0.25351089984178543
Iteration 50, Training loss = 0.24809434537131053
Iteration 60, Training loss = 0.24577953609136435
Iteration 70, Training loss = 0.2426308082559934
Iteration 80, Training loss = 0.2390911802649498
Iteration 90, Training loss = 0.2365668024867773
Model training time: 18.58508324623108
Device: cuda
Iteration 0, Training loss = 0.9634089332360488
Iteration 10, Training loss = 0.28153186205487984
Iteration 20, Training loss = 0.2593698813938178
Iteration 30, Training loss = 0.25337489723012996
Iteration 40, Training loss = 0.24771623900876596
Iteration 50, Training loss = 0.24350723194388244
Iteration 60, Training loss = 0.23977313233682743
Iteration 70, Training loss = 0.24092086218297482
Iteration 80, Training loss = 0.2403631957534414
Iteration 90, Training loss = 0.23571152283022037
Model training time: 18.722060918807983
Device: cuda
Iteration 0, Training loss = 0.9078353823950658
Iteration 10, Training loss = 0.28578142492243874
Iteration 20, Training loss = 0.2655172419662659
Iteration 30, Training loss = 0.25576156148543727
Iteration 40, Training loss = 0.24673363287001848
Iteration 50, Training loss = 0.24248744490054938
Iteration 60, Training loss = 0.23912585684313223
Iteration 70, Training loss = 0.24029604259591836
Iteration 80, Training loss = 0.23530653634896645
Iteration 90, Training loss = 0.23359077545599297
Model training time: 19.21948742866516
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0419151393266826
Iteration 10, Training loss = 0.9773633508728101
Iteration 20, Training loss = 0.9238260611891747
Iteration 30, Training loss = 0.7385947131193601
Iteration 40, Training loss = 0.5126734636723995
Iteration 50, Training loss = 0.4458333153564196
Iteration 60, Training loss = 0.41437551522484195
Iteration 70, Training loss = 0.39359607891394544
Iteration 80, Training loss = 0.37888555973768234
Iteration 90, Training loss = 0.36519077907388026
Model training time: 16.60374689102173
Device: cuda
Iteration 0, Training loss = 1.047497269625847
Iteration 10, Training loss = 0.7558667525075949
Iteration 20, Training loss = 0.4295644894815408
Iteration 30, Training loss = 0.37197284758664095
Iteration 40, Training loss = 0.3579834632288951
Iteration 50, Training loss = 0.3430303934101875
Iteration 60, Training loss = 0.33377123208573234
Iteration 70, Training loss = 0.32498887138297927
Iteration 80, Training loss = 0.31957399802139175
Iteration 90, Training loss = 0.31441968579131824
Model training time: 16.73274540901184
Device: cuda
Iteration 0, Training loss = 1.0997936926209009
Iteration 10, Training loss = 0.8985408194936239
Iteration 20, Training loss = 0.628228031958525
Iteration 30, Training loss = 0.4514575219498231
Iteration 40, Training loss = 0.4167908776838046
Iteration 50, Training loss = 0.39082316352197755
Iteration 60, Training loss = 0.37104386564057606
Iteration 70, Training loss = 0.3558754137215706
Iteration 80, Training loss = 0.3459631476837855
Iteration 90, Training loss = 0.33790211436840206
Model training time: 16.865188360214233
Device: cuda
Iteration 0, Training loss = 0.9770380831681765
Iteration 10, Training loss = 0.6748595197613423
Iteration 20, Training loss = 0.47339642621003664
Iteration 30, Training loss = 0.41886264143081814
Iteration 40, Training loss = 0.3890097144131477
Iteration 50, Training loss = 0.3688778792722867
Iteration 60, Training loss = 0.35242999688937116
Iteration 70, Training loss = 0.34324938990175724
Iteration 80, Training loss = 0.3333424779658134
Iteration 90, Training loss = 0.3261370878093518
Model training time: 16.70808482170105
Device: cuda
Iteration 0, Training loss = 1.1476234541489527
Iteration 10, Training loss = 0.8622214324199237
Iteration 20, Training loss = 0.5963817975268915
Iteration 30, Training loss = 0.46305257368546265
Iteration 40, Training loss = 0.4195939167760886
Iteration 50, Training loss = 0.3914689239687644
Iteration 60, Training loss = 0.36886036739899564
Iteration 70, Training loss = 0.35218063254769033
Iteration 80, Training loss = 0.3419346259190486
Iteration 90, Training loss = 0.3332174615218089
Model training time: 16.688445329666138
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.950378319964959
Iteration 10, Training loss = 0.2794969007372856
Iteration 20, Training loss = 0.2614347371630944
Iteration 30, Training loss = 0.2543895985358037
Iteration 40, Training loss = 0.2499749636134276
Iteration 50, Training loss = 0.24557348233289444
Iteration 60, Training loss = 0.2404043574889119
Iteration 70, Training loss = 0.2396958781263003
Iteration 80, Training loss = 0.2389062631588716
Iteration 90, Training loss = 0.23402601559288227
Model training time: 18.74880313873291
Device: cuda
Iteration 0, Training loss = 0.915133403470883
Iteration 10, Training loss = 0.27516359434678006
Iteration 20, Training loss = 0.25627484115270466
Iteration 30, Training loss = 0.24749305500433996
Iteration 40, Training loss = 0.24133088153142196
Iteration 50, Training loss = 0.23767696106089994
Iteration 60, Training loss = 0.23483817967084739
Iteration 70, Training loss = 0.2310901039208357
Iteration 80, Training loss = 0.2302705803169654
Iteration 90, Training loss = 0.2254512724108421
Model training time: 18.566757440567017
Device: cuda
Iteration 0, Training loss = 0.8828341250236218
Iteration 10, Training loss = 0.27361505951445836
Iteration 20, Training loss = 0.25295820001226205
Iteration 30, Training loss = 0.24399017119923463
Iteration 40, Training loss = 0.23716462733080754
Iteration 50, Training loss = 0.23339015890199405
Iteration 60, Training loss = 0.23258018278731749
Iteration 70, Training loss = 0.23018004809721157
Iteration 80, Training loss = 0.22747076811412206
Iteration 90, Training loss = 0.22607540267591292
Model training time: 18.607895135879517
Device: cuda
Iteration 0, Training loss = 0.8440804481506348
Iteration 10, Training loss = 0.2671495774904123
Iteration 20, Training loss = 0.244357505096839
Iteration 30, Training loss = 0.23527131509035826
Iteration 40, Training loss = 0.2280153200890009
Iteration 50, Training loss = 0.22433833928348926
Iteration 60, Training loss = 0.22023079845194632
Iteration 70, Training loss = 0.22062898169343287
Iteration 80, Training loss = 0.21755103630801806
Iteration 90, Training loss = 0.21520549861284402
Model training time: 18.45023536682129
Device: cuda
Iteration 0, Training loss = 0.8151417901882758
Iteration 10, Training loss = 0.27794699958310676
Iteration 20, Training loss = 0.26379442630478966
Iteration 30, Training loss = 0.251422489205232
Iteration 40, Training loss = 0.24523724966610855
Iteration 50, Training loss = 0.2445301811855573
Iteration 60, Training loss = 0.23971611848817423
Iteration 70, Training loss = 0.23663804594140786
Iteration 80, Training loss = 0.2331284796102689
Iteration 90, Training loss = 0.23047387614273107
Model training time: 18.47531247138977
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.008270729619723
Iteration 10, Training loss = 0.9164556614481486
Iteration 20, Training loss = 0.7400464656261297
Iteration 30, Training loss = 0.49492653287374055
Iteration 40, Training loss = 0.40266227750824046
Iteration 50, Training loss = 0.3662416861894039
Iteration 60, Training loss = 0.3500854905981284
Iteration 70, Training loss = 0.3400042698933528
Iteration 80, Training loss = 0.3354573997740562
Iteration 90, Training loss = 0.32846179461249936
Model training time: 16.575428247451782
Device: cuda
Iteration 0, Training loss = 1.0583336559625773
Iteration 10, Training loss = 0.9754354437956443
Iteration 20, Training loss = 0.9283738921468074
Iteration 30, Training loss = 0.8102322381276351
Iteration 40, Training loss = 0.6119153264623421
Iteration 50, Training loss = 0.47203148815494317
Iteration 60, Training loss = 0.40815468304432356
Iteration 70, Training loss = 0.3731993114432463
Iteration 80, Training loss = 0.35293901496781754
Iteration 90, Training loss = 0.33771919702681213
Model training time: 16.39305019378662
Device: cuda
Iteration 0, Training loss = 1.011787824332714
Iteration 10, Training loss = 0.8608203100470396
Iteration 20, Training loss = 0.49749546813277096
Iteration 30, Training loss = 0.3616721337804428
Iteration 40, Training loss = 0.3459245254500554
Iteration 50, Training loss = 0.3383312170895246
Iteration 60, Training loss = 0.32907665177033496
Iteration 70, Training loss = 0.32424713456286836
Iteration 80, Training loss = 0.32307649145905787
Iteration 90, Training loss = 0.31804639674150026
Model training time: 16.471134901046753
Device: cuda
Iteration 0, Training loss = 1.106137133561648
Iteration 10, Training loss = 0.794654708642226
Iteration 20, Training loss = 0.5176284155593469
Iteration 30, Training loss = 0.4259243088846023
Iteration 40, Training loss = 0.3877605858903665
Iteration 50, Training loss = 0.36145257792220664
Iteration 60, Training loss = 0.3432398276546827
Iteration 70, Training loss = 0.3285667781646435
Iteration 80, Training loss = 0.3201030482752965
Iteration 90, Training loss = 0.3116686771122309
Model training time: 16.57901692390442
Device: cuda
Iteration 0, Training loss = 1.077412597835064
Iteration 10, Training loss = 0.942313653918413
Iteration 20, Training loss = 0.7651057203228657
Iteration 30, Training loss = 0.4431905820965767
Iteration 40, Training loss = 0.37180587391440684
Iteration 50, Training loss = 0.3529809763511786
Iteration 60, Training loss = 0.34191594272851944
Iteration 70, Training loss = 0.33372837505661523
Iteration 80, Training loss = 0.3301174375586785
Iteration 90, Training loss = 0.32451885437163025
Model training time: 16.54410433769226
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.760555891176829
Iteration 10, Training loss = 0.27038825002427286
Iteration 20, Training loss = 0.2545361080421851
Iteration 30, Training loss = 0.2457732715858863
Iteration 40, Training loss = 0.23704547922198588
Iteration 50, Training loss = 0.23224966786801815
Iteration 60, Training loss = 0.2271301792218135
Iteration 70, Training loss = 0.22289278064496243
Iteration 80, Training loss = 0.22020955899587044
Iteration 90, Training loss = 0.21876712401325887
Iteration 100, Training loss = 0.21687780291988298
Iteration 110, Training loss = 0.2142491930236037
Iteration 120, Training loss = 0.2119509199490914
Iteration 130, Training loss = 0.21121832757042006
Iteration 140, Training loss = 0.20924553160484022
Iteration 150, Training loss = 0.20890582646601474
Iteration 160, Training loss = 0.20741577410640624
Iteration 170, Training loss = 0.20683450079881227
Iteration 180, Training loss = 0.20668745442078665
Iteration 190, Training loss = 0.20690022688359022
Model training time: 37.960312843322754
Device: cuda
Iteration 0, Training loss = 0.8632857564550179
Iteration 10, Training loss = 0.26144685968756676
Iteration 20, Training loss = 0.24321081432012412
Iteration 30, Training loss = 0.2339503182671391
Iteration 40, Training loss = 0.22836383919303233
Iteration 50, Training loss = 0.2238971973554446
Iteration 60, Training loss = 0.2183901037161167
Iteration 70, Training loss = 0.2173318165855912
Iteration 80, Training loss = 0.21368439925404695
Iteration 90, Training loss = 0.21134545940619248
Iteration 100, Training loss = 0.21151429923394552
Iteration 110, Training loss = 0.20940116563668618
Iteration 120, Training loss = 0.20863166057433075
Iteration 130, Training loss = 0.20822897051962522
Iteration 140, Training loss = 0.20515350431490403
Iteration 150, Training loss = 0.20490493804502946
Iteration 160, Training loss = 0.2050546295940876
Iteration 170, Training loss = 0.20413910160557583
Iteration 180, Training loss = 0.20614325871261266
Iteration 190, Training loss = 0.2029283419251442
Model training time: 37.49552845954895
Device: cuda
Iteration 0, Training loss = 0.9666076941558948
Iteration 10, Training loss = 0.273292454246145
Iteration 20, Training loss = 0.2535004154420816
Iteration 30, Training loss = 0.24315887302733385
Iteration 40, Training loss = 0.237048565195157
Iteration 50, Training loss = 0.23116217267054778
Iteration 60, Training loss = 0.22779154820510975
Iteration 70, Training loss = 0.22451183665543795
Iteration 80, Training loss = 0.22477690717921808
Iteration 90, Training loss = 0.22090484775029695
Iteration 100, Training loss = 0.21913661814939517
Iteration 110, Training loss = 0.21843906678259373
Iteration 120, Training loss = 0.2165644087900336
Iteration 130, Training loss = 0.21443236963107035
Iteration 140, Training loss = 0.21463955344202426
Iteration 150, Training loss = 0.21412321695914635
Iteration 160, Training loss = 0.21063906226593715
Iteration 170, Training loss = 0.2102635555828993
Iteration 180, Training loss = 0.21027004747436598
Iteration 190, Training loss = 0.2090281547548679
Model training time: 38.13103890419006
Device: cuda
Iteration 0, Training loss = 0.8635995565698698
Iteration 10, Training loss = 0.38709099934651303
Iteration 20, Training loss = 0.2952632699161768
Iteration 30, Training loss = 0.25737267832916516
Iteration 40, Training loss = 0.23817375682007808
Iteration 50, Training loss = 0.23451430565462664
Iteration 60, Training loss = 0.23128786525474146
Iteration 70, Training loss = 0.22920280852569982
Iteration 80, Training loss = 0.22520233733722797
Iteration 90, Training loss = 0.22221848784157863
Iteration 100, Training loss = 0.21929970698860976
Iteration 110, Training loss = 0.22015197116595048
Iteration 120, Training loss = 0.21494580439936656
Iteration 130, Training loss = 0.21639247539524847
Iteration 140, Training loss = 0.21241275020516837
Iteration 150, Training loss = 0.21249587504336467
Iteration 160, Training loss = 0.21213942701713398
Iteration 170, Training loss = 0.21131319753252542
Iteration 180, Training loss = 0.21130767013304508
Iteration 190, Training loss = 0.2101796083868696
Model training time: 36.78372764587402
Device: cuda
Iteration 0, Training loss = 0.8312957318356404
Iteration 10, Training loss = 0.2674426639882418
Iteration 20, Training loss = 0.24860532108980876
Iteration 30, Training loss = 0.23921906833465284
Iteration 40, Training loss = 0.2327022677144179
Iteration 50, Training loss = 0.22869808212495768
Iteration 60, Training loss = 0.2239868019062739
Iteration 70, Training loss = 0.22059319376085812
Iteration 80, Training loss = 0.2181272622770988
Iteration 90, Training loss = 0.2154753622957147
Iteration 100, Training loss = 0.2130090229643079
Iteration 110, Training loss = 0.21221369390304273
Iteration 120, Training loss = 0.21102984048999274
Iteration 130, Training loss = 0.2083951595167701
Iteration 140, Training loss = 0.20815675390454438
Iteration 150, Training loss = 0.20730522690484157
Iteration 160, Training loss = 0.2052131722896145
Iteration 170, Training loss = 0.206054731630362
Iteration 180, Training loss = 0.2038627192378044
Iteration 190, Training loss = 0.20479187713219568
Model training time: 37.537590980529785
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.076810437326248
Iteration 10, Training loss = 0.9223172830847594
Iteration 20, Training loss = 0.6993797776790766
Iteration 30, Training loss = 0.44523109452655685
Iteration 40, Training loss = 0.38464300936231244
Iteration 50, Training loss = 0.3575211049845585
Iteration 60, Training loss = 0.33703300012991977
Iteration 70, Training loss = 0.32716978126420426
Iteration 80, Training loss = 0.3212667311040255
Iteration 90, Training loss = 0.3195248938237245
Iteration 100, Training loss = 0.31459319261977303
Iteration 110, Training loss = 0.31256959601663625
Iteration 120, Training loss = 0.30884332954883575
Iteration 130, Training loss = 0.30683152119700724
Iteration 140, Training loss = 0.30451818512609374
Iteration 150, Training loss = 0.3016127273440361
Iteration 160, Training loss = 0.29955541672041786
Iteration 170, Training loss = 0.2970229633725606
Iteration 180, Training loss = 0.2961871335999324
Iteration 190, Training loss = 0.293854214824163
Model training time: 33.07530331611633
Device: cuda
Iteration 0, Training loss = 1.0117819154491792
Iteration 10, Training loss = 0.9198736731822674
Iteration 20, Training loss = 0.7629590865511161
Iteration 30, Training loss = 0.5419601150430166
Iteration 40, Training loss = 0.42250925904283154
Iteration 50, Training loss = 0.3708496088018784
Iteration 60, Training loss = 0.34554560826374936
Iteration 70, Training loss = 0.3331699026032136
Iteration 80, Training loss = 0.32507855187241846
Iteration 90, Training loss = 0.3173272740812256
Iteration 100, Training loss = 0.3144442795847471
Iteration 110, Training loss = 0.3110337329025452
Iteration 120, Training loss = 0.30861476565209717
Iteration 130, Training loss = 0.30387116538790554
Iteration 140, Training loss = 0.30010349504076517
Iteration 150, Training loss = 0.29744237202864426
Iteration 160, Training loss = 0.2982674070562308
Iteration 170, Training loss = 0.29188863173700297
Iteration 180, Training loss = 0.2902307445899798
Iteration 190, Training loss = 0.2896776805416896
Model training time: 32.8233687877655
Device: cuda
Iteration 0, Training loss = 1.0181996936981494
Iteration 10, Training loss = 0.9934555807938943
Iteration 20, Training loss = 0.9691119125256171
Iteration 30, Training loss = 0.8864970488043932
Iteration 40, Training loss = 0.6368047764094976
Iteration 50, Training loss = 0.43370062284744704
Iteration 60, Training loss = 0.38796557615009636
Iteration 70, Training loss = 0.3632599811714429
Iteration 80, Training loss = 0.3471869400774057
Iteration 90, Training loss = 0.3327634893357754
Iteration 100, Training loss = 0.32613530626090675
Iteration 110, Training loss = 0.3188216257840395
Iteration 120, Training loss = 0.3152934852987528
Iteration 130, Training loss = 0.3113137293033875
Iteration 140, Training loss = 0.30912676052405286
Iteration 150, Training loss = 0.30638059744468105
Iteration 160, Training loss = 0.3023069936495561
Iteration 170, Training loss = 0.3001375272870064
Iteration 180, Training loss = 0.2977486073684234
Iteration 190, Training loss = 0.29598988965153694
Model training time: 34.557196855545044
Device: cuda
Iteration 0, Training loss = 1.1015631791490774
Iteration 10, Training loss = 0.8511000029169596
Iteration 20, Training loss = 0.563695493225868
Iteration 30, Training loss = 0.3926440811214539
Iteration 40, Training loss = 0.36481250249422514
Iteration 50, Training loss = 0.347848659954392
Iteration 60, Training loss = 0.3362740587729674
Iteration 70, Training loss = 0.32834976739608324
Iteration 80, Training loss = 0.32246677720775974
Iteration 90, Training loss = 0.3165301475673914
Iteration 100, Training loss = 0.3119374059427243
Iteration 110, Training loss = 0.3084767563984944
Iteration 120, Training loss = 0.30470320195532763
Iteration 130, Training loss = 0.3008102992406258
Iteration 140, Training loss = 0.30086825071619105
Iteration 150, Training loss = 0.2972255109880979
Iteration 160, Training loss = 0.29152559102154696
Iteration 170, Training loss = 0.29182065507540333
Iteration 180, Training loss = 0.2886050654431948
Iteration 190, Training loss = 0.28415384831336826
Model training time: 38.12333345413208
Device: cuda
Iteration 0, Training loss = 1.0643732301317728
Iteration 10, Training loss = 0.8936910623541245
Iteration 20, Training loss = 0.706949675312409
Iteration 30, Training loss = 0.5096600133065994
Iteration 40, Training loss = 0.3999921398667189
Iteration 50, Training loss = 0.3593998813571838
Iteration 60, Training loss = 0.3369481609417842
Iteration 70, Training loss = 0.326083403510543
Iteration 80, Training loss = 0.31795026204333854
Iteration 90, Training loss = 0.31406059187765306
Iteration 100, Training loss = 0.3101814357707134
Iteration 110, Training loss = 0.30850583888017213
Iteration 120, Training loss = 0.30524853645608974
Iteration 130, Training loss = 0.3038576404349162
Iteration 140, Training loss = 0.3004678674042225
Iteration 150, Training loss = 0.2997633698754586
Iteration 160, Training loss = 0.29770564975646824
Iteration 170, Training loss = 0.2955982288202414
Iteration 180, Training loss = 0.29408277112704057
Iteration 190, Training loss = 0.29137830737118536
Model training time: 37.10084104537964
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1056573058550174
Iteration 10, Training loss = 0.28752141116330254
Iteration 20, Training loss = 0.2648317458537909
Iteration 30, Training loss = 0.25691075966908383
Iteration 40, Training loss = 0.25318359218251246
Iteration 50, Training loss = 0.2475915142836479
Iteration 60, Training loss = 0.24408536640783915
Iteration 70, Training loss = 0.24180096144286486
Iteration 80, Training loss = 0.23681365512311459
Iteration 90, Training loss = 0.2339622382647716
Iteration 100, Training loss = 0.22900049230800226
Iteration 110, Training loss = 0.22633018165540236
Iteration 120, Training loss = 0.22449169150338724
Iteration 130, Training loss = 0.22736011287913874
Iteration 140, Training loss = 0.22148514338410819
Iteration 150, Training loss = 0.22375947874612534
Iteration 160, Training loss = 0.21925907202351552
Iteration 170, Training loss = 0.21774752380756232
Iteration 180, Training loss = 0.21630626795097038
Iteration 190, Training loss = 0.21667109759381184
Model training time: 40.33741855621338
Device: cuda
Iteration 0, Training loss = 0.8625676018687395
Iteration 10, Training loss = 0.271446394519164
Iteration 20, Training loss = 0.25492092150335127
Iteration 30, Training loss = 0.24855154767059362
Iteration 40, Training loss = 0.2465247494670061
Iteration 50, Training loss = 0.24373767259888923
Iteration 60, Training loss = 0.24603362940251827
Iteration 70, Training loss = 0.23967313293654186
Iteration 80, Training loss = 0.2401394366931457
Iteration 90, Training loss = 0.2369224472114673
Iteration 100, Training loss = 0.23751161233163798
Iteration 110, Training loss = 0.23550556155924612
Iteration 120, Training loss = 0.23305933786412844
Iteration 130, Training loss = 0.23125470315034574
Iteration 140, Training loss = 0.23126231191249993
Iteration 150, Training loss = 0.22874498209701136
Iteration 160, Training loss = 0.22917087426265845
Iteration 170, Training loss = 0.2278838957158419
Iteration 180, Training loss = 0.22663979676480478
Iteration 190, Training loss = 0.22711423168388697
Model training time: 38.360623598098755
Device: cuda
Iteration 0, Training loss = 0.8009464749349997
Iteration 10, Training loss = 0.28664201139830625
Iteration 20, Training loss = 0.26521826076966065
Iteration 30, Training loss = 0.2550458899484231
Iteration 40, Training loss = 0.2501456750413546
Iteration 50, Training loss = 0.24648739698414618
Iteration 60, Training loss = 0.24178020355220026
Iteration 70, Training loss = 0.2417819399673205
Iteration 80, Training loss = 0.23645409827048963
Iteration 90, Training loss = 0.23514361593585748
Iteration 100, Training loss = 0.23489660253891578
Iteration 110, Training loss = 0.23367936866214642
Iteration 120, Training loss = 0.22925718408077955
Iteration 130, Training loss = 0.22898439790766972
Iteration 140, Training loss = 0.2272168527572201
Iteration 150, Training loss = 0.22911865908939105
Iteration 160, Training loss = 0.22527337074279785
Iteration 170, Training loss = 0.22374273342295334
Iteration 180, Training loss = 0.22120815787750941
Iteration 190, Training loss = 0.22027650174613184
Model training time: 37.349247455596924
Device: cuda
Iteration 0, Training loss = 1.038284580868024
Iteration 10, Training loss = 0.26977694435761523
Iteration 20, Training loss = 0.2580465220201474
Iteration 30, Training loss = 0.2538449762818905
Iteration 40, Training loss = 0.24474915482390386
Iteration 50, Training loss = 0.24079295477041832
Iteration 60, Training loss = 0.23630639423544592
Iteration 70, Training loss = 0.2307736033287186
Iteration 80, Training loss = 0.22740966568772608
Iteration 90, Training loss = 0.22364931044956812
Iteration 100, Training loss = 0.2210791357434713
Iteration 110, Training loss = 0.21887577998523527
Iteration 120, Training loss = 0.21810895089919752
Iteration 130, Training loss = 0.21793308173521206
Iteration 140, Training loss = 0.2174082907060018
Iteration 150, Training loss = 0.21720999522277942
Iteration 160, Training loss = 0.2155263931848682
Iteration 170, Training loss = 0.21466750243248847
Iteration 180, Training loss = 0.21905190454652676
Iteration 190, Training loss = 0.21569577977061272
Model training time: 38.01501703262329
Device: cuda
Iteration 0, Training loss = 0.8449719055340841
Iteration 10, Training loss = 0.27569260061360323
Iteration 20, Training loss = 0.25645269138308674
Iteration 30, Training loss = 0.24834158601096043
Iteration 40, Training loss = 0.24122308739102805
Iteration 50, Training loss = 0.24104142690507266
Iteration 60, Training loss = 0.23256870960960022
Iteration 70, Training loss = 0.22857496271339747
Iteration 80, Training loss = 0.22798874300832933
Iteration 90, Training loss = 0.22484472293693286
Iteration 100, Training loss = 0.2249443197193054
Iteration 110, Training loss = 0.22311315348801705
Iteration 120, Training loss = 0.2231104438408063
Iteration 130, Training loss = 0.22088635999422807
Iteration 140, Training loss = 0.21902914646153265
Iteration 150, Training loss = 0.21705063346486825
Iteration 160, Training loss = 0.22033478097560313
Iteration 170, Training loss = 0.21528662289850986
Iteration 180, Training loss = 0.21666902138923222
Iteration 190, Training loss = 0.2146509217384916
Model training time: 37.50937032699585
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0824928690607731
Iteration 10, Training loss = 0.7688439952639433
Iteration 20, Training loss = 0.4929783639426415
Iteration 30, Training loss = 0.4276599706365512
Iteration 40, Training loss = 0.3990059251395556
Iteration 50, Training loss = 0.3770571120369893
Iteration 60, Training loss = 0.3631755312761435
Iteration 70, Training loss = 0.35128126250436675
Iteration 80, Training loss = 0.34371464957411474
Iteration 90, Training loss = 0.3371528722345829
Iteration 100, Training loss = 0.33432560371091735
Iteration 110, Training loss = 0.32852370435228717
Iteration 120, Training loss = 0.32655353657901287
Iteration 130, Training loss = 0.3241529414573541
Iteration 140, Training loss = 0.32315866147669464
Iteration 150, Training loss = 0.3180794817610429
Iteration 160, Training loss = 0.3183222827143394
Iteration 170, Training loss = 0.3132305485554613
Iteration 180, Training loss = 0.31477486815017003
Iteration 190, Training loss = 0.3103195642336057
Model training time: 34.28021049499512
Device: cuda
Iteration 0, Training loss = 1.0914798075189958
Iteration 10, Training loss = 0.8953287951075114
Iteration 20, Training loss = 0.609967520317206
Iteration 30, Training loss = 0.44501694492422617
Iteration 40, Training loss = 0.3988347308566937
Iteration 50, Training loss = 0.37411570878556144
Iteration 60, Training loss = 0.35448816848488957
Iteration 70, Training loss = 0.34605341189755845
Iteration 80, Training loss = 0.33429234790114254
Iteration 90, Training loss = 0.3285602623453507
Iteration 100, Training loss = 0.32481888867914677
Iteration 110, Training loss = 0.32135278846208865
Iteration 120, Training loss = 0.3163993893047938
Iteration 130, Training loss = 0.3149001326125402
Iteration 140, Training loss = 0.3123275524435135
Iteration 150, Training loss = 0.30921704350755763
Iteration 160, Training loss = 0.30704743630037856
Iteration 170, Training loss = 0.30401609651744366
Iteration 180, Training loss = 0.303108701768976
Iteration 190, Training loss = 0.30016670676951224
Model training time: 33.174538135528564
Device: cuda
Iteration 0, Training loss = 1.0335547264951925
Iteration 10, Training loss = 0.7826193393423007
Iteration 20, Training loss = 0.436074208181638
Iteration 30, Training loss = 0.374640495588
Iteration 40, Training loss = 0.35561865740097487
Iteration 50, Training loss = 0.34026501471033466
Iteration 60, Training loss = 0.33364645577967167
Iteration 70, Training loss = 0.3255048626317428
Iteration 80, Training loss = 0.32110456405923915
Iteration 90, Training loss = 0.3174274338839146
Iteration 100, Training loss = 0.3150883208100612
Iteration 110, Training loss = 0.31077272430635416
Iteration 120, Training loss = 0.3109917566180229
Iteration 130, Training loss = 0.30723927513911176
Iteration 140, Training loss = 0.30639239529577583
Iteration 150, Training loss = 0.30471478015757525
Iteration 160, Training loss = 0.30139552243053913
Iteration 170, Training loss = 0.30104784997036826
Iteration 180, Training loss = 0.29939861294741815
Iteration 190, Training loss = 0.29660824385400003
Model training time: 33.25535488128662
Device: cuda
Iteration 0, Training loss = 1.0511748326512484
Iteration 10, Training loss = 0.8325833328641378
Iteration 20, Training loss = 0.6559296356370816
Iteration 30, Training loss = 0.5176676612060803
Iteration 40, Training loss = 0.4448966286503352
Iteration 50, Training loss = 0.39642679834595096
Iteration 60, Training loss = 0.3613252436312345
Iteration 70, Training loss = 0.3414822005881713
Iteration 80, Training loss = 0.33141559978517204
Iteration 90, Training loss = 0.31604982769260037
Iteration 100, Training loss = 0.3124265528928775
Iteration 110, Training loss = 0.3049847692824327
Iteration 120, Training loss = 0.30084686626035434
Iteration 130, Training loss = 0.2977974840368216
Iteration 140, Training loss = 0.2963499231980397
Iteration 150, Training loss = 0.2918382099328133
Iteration 160, Training loss = 0.28858795389533043
Iteration 170, Training loss = 0.2863377845631196
Iteration 180, Training loss = 0.2845440734750949
Iteration 190, Training loss = 0.28438257053494453
Model training time: 33.61931085586548
Device: cuda
Iteration 0, Training loss = 1.207818555717285
Iteration 10, Training loss = 0.8875963948667049
Iteration 20, Training loss = 0.6330283605135404
Iteration 30, Training loss = 0.5088320855910962
Iteration 40, Training loss = 0.45562035647722393
Iteration 50, Training loss = 0.4199300166219473
Iteration 60, Training loss = 0.3954919381783559
Iteration 70, Training loss = 0.3775235954672098
Iteration 80, Training loss = 0.3591432895224828
Iteration 90, Training loss = 0.34912002602448833
Iteration 100, Training loss = 0.34056506922038704
Iteration 110, Training loss = 0.3359620270247643
Iteration 120, Training loss = 0.3321787262192139
Iteration 130, Training loss = 0.32815224233155066
Iteration 140, Training loss = 0.32672549669559187
Iteration 150, Training loss = 0.3241800098465039
Iteration 160, Training loss = 0.3207739647477865
Iteration 170, Training loss = 0.3224038637887973
Iteration 180, Training loss = 0.31737838886105096
Iteration 190, Training loss = 0.3166042804144896
Model training time: 33.540278673172
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7819039489214237
Iteration 10, Training loss = 0.27376356119146716
Iteration 20, Training loss = 0.2593921246007085
Iteration 30, Training loss = 0.2510541446793538
Iteration 40, Training loss = 0.2478683406057266
Iteration 50, Training loss = 0.24495433106158787
Iteration 60, Training loss = 0.24043707649868268
Iteration 70, Training loss = 0.23903370906527227
Iteration 80, Training loss = 0.23632874325490916
Iteration 90, Training loss = 0.2339779303337519
Iteration 100, Training loss = 0.2318471483886242
Iteration 110, Training loss = 0.23131180124787185
Iteration 120, Training loss = 0.23107914655254438
Iteration 130, Training loss = 0.22919917959146774
Iteration 140, Training loss = 0.22710675022636467
Iteration 150, Training loss = 0.22535742862293354
Iteration 160, Training loss = 0.22563346589987093
Iteration 170, Training loss = 0.2247038705704304
Iteration 180, Training loss = 0.22343918706218785
Iteration 190, Training loss = 0.22544577853897443
Model training time: 37.723724365234375
Device: cuda
Iteration 0, Training loss = 0.8906923460845764
Iteration 10, Training loss = 0.276708942049971
Iteration 20, Training loss = 0.2570647464062159
Iteration 30, Training loss = 0.2489911588625266
Iteration 40, Training loss = 0.2426475856739741
Iteration 50, Training loss = 0.24070422551952875
Iteration 60, Training loss = 0.23681097907515672
Iteration 70, Training loss = 0.23203497988959917
Iteration 80, Training loss = 0.22941865098591036
Iteration 90, Training loss = 0.22757751150773123
Iteration 100, Training loss = 0.2253504401932542
Iteration 110, Training loss = 0.2235927411283438
Iteration 120, Training loss = 0.2227877018113549
Iteration 130, Training loss = 0.2207935185959706
Iteration 140, Training loss = 0.2204789358835954
Iteration 150, Training loss = 0.21935451259979835
Iteration 160, Training loss = 0.21726322589585415
Iteration 170, Training loss = 0.21666350552382377
Iteration 180, Training loss = 0.21506544123761928
Iteration 190, Training loss = 0.21972155012190342
Model training time: 38.21153712272644
Device: cuda
Iteration 0, Training loss = 0.9509923068376688
Iteration 10, Training loss = 0.2757389175777252
Iteration 20, Training loss = 0.259139197663619
Iteration 30, Training loss = 0.24645193436971077
Iteration 40, Training loss = 0.2398452376230405
Iteration 50, Training loss = 0.23916585070009416
Iteration 60, Training loss = 0.2335666849349554
Iteration 70, Training loss = 0.228579537011683
Iteration 80, Training loss = 0.22624017799702975
Iteration 90, Training loss = 0.22489771662423244
Iteration 100, Training loss = 0.22249713009939745
Iteration 110, Training loss = 0.22310177924541327
Iteration 120, Training loss = 0.22108312896811044
Iteration 130, Training loss = 0.21882360528867978
Iteration 140, Training loss = 0.21755311451852322
Iteration 150, Training loss = 0.2163897196117502
Iteration 160, Training loss = 0.216430040792777
Iteration 170, Training loss = 0.21739599939722282
Iteration 180, Training loss = 0.21789454489659804
Iteration 190, Training loss = 0.21441533946647093
Model training time: 37.142170429229736
Device: cuda
Iteration 0, Training loss = 0.9096138867048117
Iteration 10, Training loss = 0.2717452605183308
Iteration 20, Training loss = 0.2539112731241263
Iteration 30, Training loss = 0.24675927655054972
Iteration 40, Training loss = 0.24006922313800225
Iteration 50, Training loss = 0.2350919392819588
Iteration 60, Training loss = 0.23252845426591542
Iteration 70, Training loss = 0.2277118403177995
Iteration 80, Training loss = 0.22495487726365143
Iteration 90, Training loss = 0.22255218157974574
Iteration 100, Training loss = 0.21969764951903087
Iteration 110, Training loss = 0.21886311041621062
Iteration 120, Training loss = 0.21657361312267873
Iteration 130, Training loss = 0.21638857659239036
Iteration 140, Training loss = 0.21488033707898396
Iteration 150, Training loss = 0.2156170548584599
Iteration 160, Training loss = 0.21182335984821504
Iteration 170, Training loss = 0.2115325487147157
Iteration 180, Training loss = 0.21262404745301375
Iteration 190, Training loss = 0.2093850510338178
Model training time: 37.16525411605835
Device: cuda
Iteration 0, Training loss = 0.9612144375076661
Iteration 10, Training loss = 0.27710228943480897
Iteration 20, Training loss = 0.25734564375418884
Iteration 30, Training loss = 0.25368235947994083
Iteration 40, Training loss = 0.24658460958072773
Iteration 50, Training loss = 0.24249460046681073
Iteration 60, Training loss = 0.24077985951533684
Iteration 70, Training loss = 0.24005471872022519
Iteration 80, Training loss = 0.2362542347266124
Iteration 90, Training loss = 0.23335106269671366
Iteration 100, Training loss = 0.23043392118639672
Iteration 110, Training loss = 0.22828179633674714
Iteration 120, Training loss = 0.22644234685083994
Iteration 130, Training loss = 0.22472390374885157
Iteration 140, Training loss = 0.22363473088122332
Iteration 150, Training loss = 0.22294211874787623
Iteration 160, Training loss = 0.22178219588330159
Iteration 170, Training loss = 0.22186073531898168
Iteration 180, Training loss = 0.22128957853867456
Iteration 190, Training loss = 0.219687695400073
Model training time: 37.2111713886261
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0085902099425976
Iteration 10, Training loss = 0.9312540590763092
Iteration 20, Training loss = 0.8045866443560674
Iteration 30, Training loss = 0.6092570054416473
Iteration 40, Training loss = 0.4814182365169892
Iteration 50, Training loss = 0.4179780875834135
Iteration 60, Training loss = 0.3731413364696961
Iteration 70, Training loss = 0.34687541296275765
Iteration 80, Training loss = 0.3382455180757321
Iteration 90, Training loss = 0.32505250063080054
Iteration 100, Training loss = 0.3185725610416669
Iteration 110, Training loss = 0.3157057029983172
Iteration 120, Training loss = 0.3104533107521442
Iteration 130, Training loss = 0.30644621191403043
Iteration 140, Training loss = 0.3051854877804334
Iteration 150, Training loss = 0.30178532735086405
Iteration 160, Training loss = 0.30028245655389935
Iteration 170, Training loss = 0.29766288285072035
Iteration 180, Training loss = 0.2976552184957724
Iteration 190, Training loss = 0.2951080286158965
Model training time: 34.22571921348572
Device: cuda
Iteration 0, Training loss = 1.148022788648422
Iteration 10, Training loss = 0.8585599786960162
Iteration 20, Training loss = 0.5275108854358013
Iteration 30, Training loss = 0.3704030265888342
Iteration 40, Training loss = 0.34677322667378646
Iteration 50, Training loss = 0.33867504361730355
Iteration 60, Training loss = 0.3364287189279611
Iteration 70, Training loss = 0.32777187185218704
Iteration 80, Training loss = 0.3216789880624184
Iteration 90, Training loss = 0.3196380647042623
Iteration 100, Training loss = 0.316781332429785
Iteration 110, Training loss = 0.3144792318344116
Iteration 120, Training loss = 0.313414574529116
Iteration 130, Training loss = 0.3123676673724101
Iteration 140, Training loss = 0.3090622235949223
Iteration 150, Training loss = 0.30519482421760374
Iteration 160, Training loss = 0.30619362756036794
Iteration 170, Training loss = 0.3038133385662849
Iteration 180, Training loss = 0.29851606173010975
Iteration 190, Training loss = 0.2971616378770425
Model training time: 34.00188660621643
Device: cuda
Iteration 0, Training loss = 1.0881094909631288
Iteration 10, Training loss = 0.9271776269261653
Iteration 20, Training loss = 0.706241528575237
Iteration 30, Training loss = 0.47485654514569503
Iteration 40, Training loss = 0.3947594412244283
Iteration 50, Training loss = 0.363300717364137
Iteration 60, Training loss = 0.34615214126041305
Iteration 70, Training loss = 0.3349243230544604
Iteration 80, Training loss = 0.324520859007652
Iteration 90, Training loss = 0.3202298117371706
Iteration 100, Training loss = 0.3133356331919248
Iteration 110, Training loss = 0.312942141523728
Iteration 120, Training loss = 0.30494413739786697
Iteration 130, Training loss = 0.3027245273383764
Iteration 140, Training loss = 0.3007947107633719
Iteration 150, Training loss = 0.2961126067317449
Iteration 160, Training loss = 0.29317285693608797
Iteration 170, Training loss = 0.2920527280523227
Iteration 180, Training loss = 0.2885436392747439
Iteration 190, Training loss = 0.2874214401325354
Model training time: 33.92987942695618
Device: cuda
Iteration 0, Training loss = 0.9538574757484289
Iteration 10, Training loss = 0.7124619632959366
Iteration 20, Training loss = 0.4085194030060218
Iteration 30, Training loss = 0.3574445548538978
Iteration 40, Training loss = 0.3437378035428432
Iteration 50, Training loss = 0.33174581467532194
Iteration 60, Training loss = 0.3273740913718939
Iteration 70, Training loss = 0.32282844830590945
Iteration 80, Training loss = 0.3176962114297427
Iteration 90, Training loss = 0.31424109250880206
Iteration 100, Training loss = 0.31454614005409753
Iteration 110, Training loss = 0.3088484862102912
Iteration 120, Training loss = 0.3081205983001452
Iteration 130, Training loss = 0.3050025873459302
Iteration 140, Training loss = 0.30357296335009426
Iteration 150, Training loss = 0.30297629520870173
Iteration 160, Training loss = 0.30024340089697105
Iteration 170, Training loss = 0.2994345181549971
Iteration 180, Training loss = 0.2978935561214502
Iteration 190, Training loss = 0.2960810149804904
Model training time: 33.18952488899231
Device: cuda
Iteration 0, Training loss = 1.0806437948575387
Iteration 10, Training loss = 0.9158572119015914
Iteration 20, Training loss = 0.6376254515579114
Iteration 30, Training loss = 0.3834637301759078
Iteration 40, Training loss = 0.35606909070450526
Iteration 50, Training loss = 0.34269961370871616
Iteration 60, Training loss = 0.33259749627457214
Iteration 70, Training loss = 0.3270406164228916
Iteration 80, Training loss = 0.32023488357663155
Iteration 90, Training loss = 0.31566593366173595
Iteration 100, Training loss = 0.31460110370356303
Iteration 110, Training loss = 0.3079024672221679
Iteration 120, Training loss = 0.30688042379915714
Iteration 130, Training loss = 0.30197186352541816
Iteration 140, Training loss = 0.29958663866497004
Iteration 150, Training loss = 0.2952788146929099
Iteration 160, Training loss = 0.29171939624043614
Iteration 170, Training loss = 0.28909092926635194
Iteration 180, Training loss = 0.2888827607608758
Iteration 190, Training loss = 0.285714128269599
Model training time: 33.48871374130249
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8378460874351171
Iteration 10, Training loss = 0.2708862197513764
Iteration 20, Training loss = 0.25453987130178857
Iteration 30, Training loss = 0.24539518886460707
Iteration 40, Training loss = 0.23994723342072505
Iteration 50, Training loss = 0.2377336976619867
Iteration 60, Training loss = 0.23538384557916567
Iteration 70, Training loss = 0.23091883283968156
Iteration 80, Training loss = 0.2299107460018534
Iteration 90, Training loss = 0.2293226933823182
Iteration 100, Training loss = 0.22719606757164001
Iteration 110, Training loss = 0.22580461390316486
Iteration 120, Training loss = 0.22327646570136914
Iteration 130, Training loss = 0.22290815169421527
Iteration 140, Training loss = 0.22022639831098226
Iteration 150, Training loss = 0.21889777280963385
Iteration 160, Training loss = 0.21932365716649935
Iteration 170, Training loss = 0.2161998239417489
Iteration 180, Training loss = 0.21562139732906452
Iteration 190, Training loss = 0.2166100128625448
Iteration 200, Training loss = 0.21365055742745215
Iteration 210, Training loss = 0.2125675674671164
Iteration 220, Training loss = 0.2119043580471323
Iteration 230, Training loss = 0.21331089863983485
Iteration 240, Training loss = 0.21109645484158626
Iteration 250, Training loss = 0.21068295277655125
Iteration 260, Training loss = 0.20928821791536534
Iteration 270, Training loss = 0.20826931584339875
Iteration 280, Training loss = 0.20911339491319197
Iteration 290, Training loss = 0.2095200471006907
Model training time: 55.57524085044861
Device: cuda
Iteration 0, Training loss = 0.9180047225493652
Iteration 10, Training loss = 0.27088784469434846
Iteration 20, Training loss = 0.2530409349845006
Iteration 30, Training loss = 0.24535195271556193
Iteration 40, Training loss = 0.24087068137641138
Iteration 50, Training loss = 0.23836977822849384
Iteration 60, Training loss = 0.23841645196080208
Iteration 70, Training loss = 0.23518676740618852
Iteration 80, Training loss = 0.23256350924762395
Iteration 90, Training loss = 0.22925584906568894
Iteration 100, Training loss = 0.22823679418517992
Iteration 110, Training loss = 0.22766221629885527
Iteration 120, Training loss = 0.22255436789530975
Iteration 130, Training loss = 0.220662591405786
Iteration 140, Training loss = 0.22011595907119605
Iteration 150, Training loss = 0.2186221145093441
Iteration 160, Training loss = 0.2181547315647969
Iteration 170, Training loss = 0.21749582299246237
Iteration 180, Training loss = 0.21550635807216167
Iteration 190, Training loss = 0.21452512506109017
Iteration 200, Training loss = 0.21405772743030235
Iteration 210, Training loss = 0.21417286146718723
Iteration 220, Training loss = 0.21418927215899414
Iteration 230, Training loss = 0.21102376683400229
Iteration 240, Training loss = 0.2095630421088292
Iteration 250, Training loss = 0.21107232241103283
Iteration 260, Training loss = 0.20944059905237877
Iteration 270, Training loss = 0.20853107673330948
Iteration 280, Training loss = 0.20895614919181055
Iteration 290, Training loss = 0.20752078803399435
Model training time: 55.731900691986084
Device: cuda
Iteration 0, Training loss = 0.835566276541123
Iteration 10, Training loss = 0.2682206036093143
Iteration 20, Training loss = 0.2497145481980764
Iteration 30, Training loss = 0.2413350292123281
Iteration 40, Training loss = 0.23779523143401513
Iteration 50, Training loss = 0.2327506999270274
Iteration 60, Training loss = 0.22968824136142546
Iteration 70, Training loss = 0.22700096065035233
Iteration 80, Training loss = 0.22458540841650504
Iteration 90, Training loss = 0.22135847061872482
Iteration 100, Training loss = 0.2185213851670806
Iteration 110, Training loss = 0.21615658507037622
Iteration 120, Training loss = 0.21554369429269662
Iteration 130, Training loss = 0.2147206189827277
Iteration 140, Training loss = 0.21219773733845124
Iteration 150, Training loss = 0.21118073179744756
Iteration 160, Training loss = 0.21084437949153093
Iteration 170, Training loss = 0.20925573811221582
Iteration 180, Training loss = 0.20970447941754872
Iteration 190, Training loss = 0.2067971730318207
Iteration 200, Training loss = 0.20609717773130307
Iteration 210, Training loss = 0.2062077302342424
Iteration 220, Training loss = 0.20615810693170017
Iteration 230, Training loss = 0.2041021236576713
Iteration 240, Training loss = 0.20289407161852488
Iteration 250, Training loss = 0.20371642981011134
Iteration 260, Training loss = 0.20199191269393152
Iteration 270, Training loss = 0.20341978336756045
Iteration 280, Training loss = 0.2036116231376162
Iteration 290, Training loss = 0.20176621046490395
Model training time: 55.9023220539093
Device: cuda
Iteration 0, Training loss = 0.8206005787047056
Iteration 10, Training loss = 0.2700585692834396
Iteration 20, Training loss = 0.2526100524342977
Iteration 30, Training loss = 0.24523144516234213
Iteration 40, Training loss = 0.24082132765593436
Iteration 50, Training loss = 0.23671326972544193
Iteration 60, Training loss = 0.23341685261290807
Iteration 70, Training loss = 0.2311269176694063
Iteration 80, Training loss = 0.2283072269593294
Iteration 90, Training loss = 0.226392345646253
Iteration 100, Training loss = 0.22431083496373433
Iteration 110, Training loss = 0.22476523613127378
Iteration 120, Training loss = 0.22442394272925761
Iteration 130, Training loss = 0.22232546915228552
Iteration 140, Training loss = 0.221662084500377
Iteration 150, Training loss = 0.22265361844060513
Iteration 160, Training loss = 0.22076999281461424
Iteration 170, Training loss = 0.22101276611479428
Iteration 180, Training loss = 0.2181716957487739
Iteration 190, Training loss = 0.21610079247217912
Iteration 200, Training loss = 0.21862389247577924
Iteration 210, Training loss = 0.2154140311938066
Iteration 220, Training loss = 0.21445293975277588
Iteration 230, Training loss = 0.2124138159247545
Iteration 240, Training loss = 0.21247328330691045
Iteration 250, Training loss = 0.21058170210856658
Iteration 260, Training loss = 0.20929902906601244
Iteration 270, Training loss = 0.20881560136778995
Iteration 280, Training loss = 0.20808866519767505
Iteration 290, Training loss = 0.20868919080553147
Model training time: 55.6842041015625
Device: cuda
Iteration 0, Training loss = 0.7986161180127126
Iteration 10, Training loss = 0.26705913861783653
Iteration 20, Training loss = 0.2497392216554055
Iteration 30, Training loss = 0.2371765416688644
Iteration 40, Training loss = 0.22992829161767775
Iteration 50, Training loss = 0.22395130347173947
Iteration 60, Training loss = 0.22137600319603315
Iteration 70, Training loss = 0.2175426371395588
Iteration 80, Training loss = 0.21612158718590552
Iteration 90, Training loss = 0.2148971978861552
Iteration 100, Training loss = 0.21183482493058994
Iteration 110, Training loss = 0.2095198268070817
Iteration 120, Training loss = 0.20917301524717075
Iteration 130, Training loss = 0.20668259614075607
Iteration 140, Training loss = 0.20835450807443032
Iteration 150, Training loss = 0.20628066303638312
Iteration 160, Training loss = 0.20659084926144436
Iteration 170, Training loss = 0.20441697423274702
Iteration 180, Training loss = 0.20400643649582678
Iteration 190, Training loss = 0.20344400799904877
Iteration 200, Training loss = 0.2027478523265857
Iteration 210, Training loss = 0.2035357906268193
Iteration 220, Training loss = 0.20215376316068265
Iteration 230, Training loss = 0.20245117866075957
Iteration 240, Training loss = 0.20241862943825814
Iteration 250, Training loss = 0.20391365291120914
Iteration 260, Training loss = 0.20324326757914746
Iteration 270, Training loss = 0.2016645812501128
Iteration 280, Training loss = 0.20270032041634506
Iteration 290, Training loss = 0.20187707890111667
Model training time: 56.008511543273926
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.054627160040232
Iteration 10, Training loss = 0.8841181374513186
Iteration 20, Training loss = 0.5903686929780704
Iteration 30, Training loss = 0.4215059793339326
Iteration 40, Training loss = 0.3766768491612031
Iteration 50, Training loss = 0.3544721422860256
Iteration 60, Training loss = 0.3384693547223623
Iteration 70, Training loss = 0.3312706769658969
Iteration 80, Training loss = 0.3255285652211079
Iteration 90, Training loss = 0.32023021894005627
Iteration 100, Training loss = 0.315396314773422
Iteration 110, Training loss = 0.31283018852655703
Iteration 120, Training loss = 0.30990665463300854
Iteration 130, Training loss = 0.306794584227296
Iteration 140, Training loss = 0.30511151984907114
Iteration 150, Training loss = 0.3017321155908016
Iteration 160, Training loss = 0.30043732933700085
Iteration 170, Training loss = 0.30019700283614487
Iteration 180, Training loss = 0.29664324983381307
Iteration 190, Training loss = 0.29456061425690466
Iteration 200, Training loss = 0.29383966541634154
Iteration 210, Training loss = 0.29260828546606577
Iteration 220, Training loss = 0.29160436815940416
Iteration 230, Training loss = 0.2883342226537374
Iteration 240, Training loss = 0.2869935820882137
Iteration 250, Training loss = 0.28669605088921696
Iteration 260, Training loss = 0.28578843964407075
Iteration 270, Training loss = 0.28460962783831817
Iteration 280, Training loss = 0.2834712258325173
Iteration 290, Training loss = 0.28176512755453587
Model training time: 49.63507080078125
Device: cuda
Iteration 0, Training loss = 1.0578798118692179
Iteration 10, Training loss = 0.7507389514492109
Iteration 20, Training loss = 0.463750958442688
Iteration 30, Training loss = 0.3879246019686644
Iteration 40, Training loss = 0.3562711716557925
Iteration 50, Training loss = 0.33910215072906935
Iteration 60, Training loss = 0.3297177492999114
Iteration 70, Training loss = 0.32294416141051513
Iteration 80, Training loss = 0.3211298424464006
Iteration 90, Training loss = 0.31196393753187013
Iteration 100, Training loss = 0.3140572937062153
Iteration 110, Training loss = 0.3083027212952192
Iteration 120, Training loss = 0.3055200535230912
Iteration 130, Training loss = 0.2985629508128533
Iteration 140, Training loss = 0.296478483825922
Iteration 150, Training loss = 0.29603443896541226
Iteration 160, Training loss = 0.2903330829352714
Iteration 170, Training loss = 0.29112505669204086
Iteration 180, Training loss = 0.28715541314047116
Iteration 190, Training loss = 0.28613062532475364
Iteration 200, Training loss = 0.2836610320955515
Iteration 210, Training loss = 0.28163292746131235
Iteration 220, Training loss = 0.2837786485369389
Iteration 230, Training loss = 0.2794212167366193
Iteration 240, Training loss = 0.27863759447175723
Iteration 250, Training loss = 0.2768593469204811
Iteration 260, Training loss = 0.2777201726746101
Iteration 270, Training loss = 0.27458877947468024
Iteration 280, Training loss = 0.27280200014893824
Iteration 290, Training loss = 0.27085809939755845
Model training time: 49.73695516586304
Device: cuda
Iteration 0, Training loss = 1.0107171971064348
Iteration 10, Training loss = 0.6565728987065645
Iteration 20, Training loss = 0.39739882501845175
Iteration 30, Training loss = 0.3609397461494574
Iteration 40, Training loss = 0.34497094713151455
Iteration 50, Training loss = 0.33143527963413644
Iteration 60, Training loss = 0.3236334245078839
Iteration 70, Training loss = 0.3179690635834749
Iteration 80, Training loss = 0.3141136613602822
Iteration 90, Training loss = 0.3090424549121123
Iteration 100, Training loss = 0.30484685917886406
Iteration 110, Training loss = 0.30141199107926625
Iteration 120, Training loss = 0.2986195021523879
Iteration 130, Training loss = 0.2953204137201493
Iteration 140, Training loss = 0.2960655280890373
Iteration 150, Training loss = 0.29214378646933115
Iteration 160, Training loss = 0.29039338345711047
Iteration 170, Training loss = 0.2902046088129282
Iteration 180, Training loss = 0.2867790199816227
Iteration 190, Training loss = 0.2821844952324262
Iteration 200, Training loss = 0.28165442439226
Iteration 210, Training loss = 0.27967798480620754
Iteration 220, Training loss = 0.2783009950071573
Iteration 230, Training loss = 0.27656798910063046
Iteration 240, Training loss = 0.277422703229464
Iteration 250, Training loss = 0.27455269573972774
Iteration 260, Training loss = 0.2738365652756049
Iteration 270, Training loss = 0.27329146675765514
Iteration 280, Training loss = 0.2726385516042893
Iteration 290, Training loss = 0.2710588641751271
Model training time: 50.066593647003174
Device: cuda
Iteration 0, Training loss = 1.0419957282451482
Iteration 10, Training loss = 0.9448771522595332
Iteration 20, Training loss = 0.7135952797073585
Iteration 30, Training loss = 0.4719617160467001
Iteration 40, Training loss = 0.41016969070411646
Iteration 50, Training loss = 0.3770724907517433
Iteration 60, Training loss = 0.35504330322146416
Iteration 70, Training loss = 0.3408575325917739
Iteration 80, Training loss = 0.3306131266200772
Iteration 90, Training loss = 0.32462883497086853
Iteration 100, Training loss = 0.3189610675550424
Iteration 110, Training loss = 0.3153954783024696
Iteration 120, Training loss = 0.31376094457048637
Iteration 130, Training loss = 0.31008641178218216
Iteration 140, Training loss = 0.306912275031209
Iteration 150, Training loss = 0.30413489946379113
Iteration 160, Training loss = 0.30115772941364694
Iteration 170, Training loss = 0.2986722243233369
Iteration 180, Training loss = 0.2969593023164914
Iteration 190, Training loss = 0.2941696447535203
Iteration 200, Training loss = 0.2922197192047651
Iteration 210, Training loss = 0.2908386235626844
Iteration 220, Training loss = 0.28896681563212323
Iteration 230, Training loss = 0.2877180933092649
Iteration 240, Training loss = 0.2854590665262479
Iteration 250, Training loss = 0.285250570338506
Iteration 260, Training loss = 0.2835762570970334
Iteration 270, Training loss = 0.2830037818505214
Iteration 280, Training loss = 0.28012842384095377
Iteration 290, Training loss = 0.27984810835466933
Model training time: 49.93151903152466
Device: cuda
Iteration 0, Training loss = 1.006908735403648
Iteration 10, Training loss = 0.9507019032652562
Iteration 20, Training loss = 0.8204817998294647
Iteration 30, Training loss = 0.5891266998190147
Iteration 40, Training loss = 0.39939798113818353
Iteration 50, Training loss = 0.3499089883497128
Iteration 60, Training loss = 0.33222863278709924
Iteration 70, Training loss = 0.3207626642229465
Iteration 80, Training loss = 0.3138260374275538
Iteration 90, Training loss = 0.3090817170360914
Iteration 100, Training loss = 0.3045934342707579
Iteration 110, Training loss = 0.30088583136407226
Iteration 120, Training loss = 0.30066613981930107
Iteration 130, Training loss = 0.29754137763610256
Iteration 140, Training loss = 0.29127632482693744
Iteration 150, Training loss = 0.2891679691294065
Iteration 160, Training loss = 0.2876193073506539
Iteration 170, Training loss = 0.28536986373364925
Iteration 180, Training loss = 0.2850363476631733
Iteration 190, Training loss = 0.28122452832758427
Iteration 200, Training loss = 0.2789943480434326
Iteration 210, Training loss = 0.27919560269667554
Iteration 220, Training loss = 0.27739445564265436
Iteration 230, Training loss = 0.27548759101101983
Iteration 240, Training loss = 0.2746559175161215
Iteration 250, Training loss = 0.2730110634404879
Iteration 260, Training loss = 0.27228867520506567
Iteration 270, Training loss = 0.27200764030791247
Iteration 280, Training loss = 0.27007255746194947
Iteration 290, Training loss = 0.2703789554249782
Model training time: 51.08735752105713
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9510732155579787
Iteration 10, Training loss = 0.28411264588626534
Iteration 20, Training loss = 0.267463040466492
Iteration 30, Training loss = 0.25827319697978407
Iteration 40, Training loss = 0.25300100374107176
Iteration 50, Training loss = 0.24931838277440804
Iteration 60, Training loss = 0.24603414707458937
Iteration 70, Training loss = 0.2448243180003304
Iteration 80, Training loss = 0.24343461285416895
Iteration 90, Training loss = 0.24087761922811085
Iteration 100, Training loss = 0.24268828303768084
Iteration 110, Training loss = 0.24239559199374455
Iteration 120, Training loss = 0.24078754092065188
Iteration 130, Training loss = 0.23667727446613404
Iteration 140, Training loss = 0.23528566132657802
Iteration 150, Training loss = 0.2340252804928101
Iteration 160, Training loss = 0.23373358524762666
Iteration 170, Training loss = 0.2308948848109979
Iteration 180, Training loss = 0.23075590913112348
Iteration 190, Training loss = 0.2297513195528434
Iteration 200, Training loss = 0.22947932600688475
Iteration 210, Training loss = 0.22914463009398717
Iteration 220, Training loss = 0.22725330364818758
Iteration 230, Training loss = 0.2274541691518747
Iteration 240, Training loss = 0.2264412107089391
Iteration 250, Training loss = 0.22625225130468607
Iteration 260, Training loss = 0.22541992036769024
Iteration 270, Training loss = 0.22631663041046032
Iteration 280, Training loss = 0.22582143659775072
Iteration 290, Training loss = 0.22574731091467234
Model training time: 56.15040898323059
Device: cuda
Iteration 0, Training loss = 0.9722224588577564
Iteration 10, Training loss = 0.30155265976030093
Iteration 20, Training loss = 0.2668495594452207
Iteration 30, Training loss = 0.2586931839871865
Iteration 40, Training loss = 0.25209160005817044
Iteration 50, Training loss = 0.2471444378965176
Iteration 60, Training loss = 0.24242313120227593
Iteration 70, Training loss = 0.2375354258200297
Iteration 80, Training loss = 0.23391790680873853
Iteration 90, Training loss = 0.23117125822374454
Iteration 100, Training loss = 0.23000038214601004
Iteration 110, Training loss = 0.2272170392366556
Iteration 120, Training loss = 0.22733809154194135
Iteration 130, Training loss = 0.22666348373660675
Iteration 140, Training loss = 0.22592051298572466
Iteration 150, Training loss = 0.2237902358174324
Iteration 160, Training loss = 0.22347855811508802
Iteration 170, Training loss = 0.2233572150938786
Iteration 180, Training loss = 0.22303692559496716
Iteration 190, Training loss = 0.22123902011662722
Iteration 200, Training loss = 0.22197819486833537
Iteration 210, Training loss = 0.22160124592483044
Iteration 220, Training loss = 0.22026524965006572
Iteration 230, Training loss = 0.2215366940228985
Iteration 240, Training loss = 0.21977870990164006
Iteration 250, Training loss = 0.2189087698665949
Iteration 260, Training loss = 0.2188428410400565
Iteration 270, Training loss = 0.21841793829718462
Iteration 280, Training loss = 0.2171823400287674
Iteration 290, Training loss = 0.2175245742815045
Model training time: 56.02054047584534
Device: cuda
Iteration 0, Training loss = 1.0861330955074384
Iteration 10, Training loss = 0.2893098797649145
Iteration 20, Training loss = 0.2660788049778113
Iteration 30, Training loss = 0.25694773360513723
Iteration 40, Training loss = 0.2535302283672186
Iteration 50, Training loss = 0.24763514373738033
Iteration 60, Training loss = 0.2406783803867606
Iteration 70, Training loss = 0.23897098047802082
Iteration 80, Training loss = 0.2347633815728701
Iteration 90, Training loss = 0.2360023439217072
Iteration 100, Training loss = 0.2330454007650797
Iteration 110, Training loss = 0.23135956992896703
Iteration 120, Training loss = 0.22951866872608662
Iteration 130, Training loss = 0.2274542453054052
Iteration 140, Training loss = 0.2281891438536919
Iteration 150, Training loss = 0.22706650355114386
Iteration 160, Training loss = 0.225584683724894
Iteration 170, Training loss = 0.2271226326433512
Iteration 180, Training loss = 0.2274593228044418
Iteration 190, Training loss = 0.2252677779358167
Iteration 200, Training loss = 0.2225222188549546
Iteration 210, Training loss = 0.22340599003319556
Iteration 220, Training loss = 0.22274435297227824
Iteration 230, Training loss = 0.22115238870565707
Iteration 240, Training loss = 0.22220079650959143
Iteration 250, Training loss = 0.21989547919768554
Iteration 260, Training loss = 0.22098366543650627
Iteration 270, Training loss = 0.2192056827390423
Iteration 280, Training loss = 0.21934972474208245
Iteration 290, Training loss = 0.21948325648330724
Model training time: 55.739219665527344
Device: cuda
Iteration 0, Training loss = 0.8622276184077446
Iteration 10, Training loss = 0.2739100754261017
Iteration 20, Training loss = 0.25835086663181966
Iteration 30, Training loss = 0.24790321190196735
Iteration 40, Training loss = 0.24453097538879284
Iteration 50, Training loss = 0.23998539407665914
Iteration 60, Training loss = 0.23799879447771952
Iteration 70, Training loss = 0.23566659009800509
Iteration 80, Training loss = 0.23471021036115977
Iteration 90, Training loss = 0.23288944678810927
Iteration 100, Training loss = 0.23325552261219576
Iteration 110, Training loss = 0.2311595853131551
Iteration 120, Training loss = 0.22594915265933826
Iteration 130, Training loss = 0.22628142813650462
Iteration 140, Training loss = 0.2243448905646801
Iteration 150, Training loss = 0.22346541084922278
Iteration 160, Training loss = 0.22239370156939214
Iteration 170, Training loss = 0.2223185946305211
Iteration 180, Training loss = 0.22205929529781526
Iteration 190, Training loss = 0.22169304065979445
Iteration 200, Training loss = 0.22011851247113484
Iteration 210, Training loss = 0.22007118423397726
Iteration 220, Training loss = 0.21903462421435577
Iteration 230, Training loss = 0.21728797142322248
Iteration 240, Training loss = 0.2179906709262958
Iteration 250, Training loss = 0.21741476826942885
Iteration 260, Training loss = 0.21654276793392804
Iteration 270, Training loss = 0.21507289403906235
Iteration 280, Training loss = 0.2145232308942538
Iteration 290, Training loss = 0.21384067138513693
Model training time: 55.41780138015747
Device: cuda
Iteration 0, Training loss = 0.912454872750319
Iteration 10, Training loss = 0.2838280839988819
Iteration 20, Training loss = 0.2647274404477615
Iteration 30, Training loss = 0.25699000017574203
Iteration 40, Training loss = 0.25075487104746014
Iteration 50, Training loss = 0.24573403768814528
Iteration 60, Training loss = 0.24184197273391944
Iteration 70, Training loss = 0.23911359906196594
Iteration 80, Training loss = 0.23480370468818224
Iteration 90, Training loss = 0.23299624684911507
Iteration 100, Training loss = 0.2306214510821379
Iteration 110, Training loss = 0.22882250151955164
Iteration 120, Training loss = 0.2295239938136477
Iteration 130, Training loss = 0.2276250270123665
Iteration 140, Training loss = 0.22746180055233148
Iteration 150, Training loss = 0.22464301952948937
Iteration 160, Training loss = 0.22685933134590203
Iteration 170, Training loss = 0.2239862451186547
Iteration 180, Training loss = 0.22293683041173679
Iteration 190, Training loss = 0.22344589419662952
Iteration 200, Training loss = 0.22135969179754073
Iteration 210, Training loss = 0.22107383508521777
Iteration 220, Training loss = 0.2206396206926841
Iteration 230, Training loss = 0.22148367676597375
Iteration 240, Training loss = 0.22301678302196357
Iteration 250, Training loss = 0.21847970189096835
Iteration 260, Training loss = 0.2185874552680896
Iteration 270, Training loss = 0.21927836480048987
Iteration 280, Training loss = 0.21875461019002473
Iteration 290, Training loss = 0.21769392877244031
Model training time: 55.15627193450928
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.047782303049014
Iteration 10, Training loss = 0.8897912370470854
Iteration 20, Training loss = 0.7217700707797821
Iteration 30, Training loss = 0.5479741546397026
Iteration 40, Training loss = 0.4679215530363413
Iteration 50, Training loss = 0.4247818519003116
Iteration 60, Training loss = 0.39267988159106326
Iteration 70, Training loss = 0.3636956541584088
Iteration 80, Training loss = 0.34247192912376845
Iteration 90, Training loss = 0.3259015871355167
Iteration 100, Training loss = 0.3160609519825532
Iteration 110, Training loss = 0.3092641132668807
Iteration 120, Training loss = 0.305440981227618
Iteration 130, Training loss = 0.3023737422548808
Iteration 140, Training loss = 0.2987318326934026
Iteration 150, Training loss = 0.2959927882139499
Iteration 160, Training loss = 0.2933208801998542
Iteration 170, Training loss = 0.2880409527570009
Iteration 180, Training loss = 0.2869575953540894
Iteration 190, Training loss = 0.2846226872733006
Iteration 200, Training loss = 0.28128721994849354
Iteration 210, Training loss = 0.2806187038524793
Iteration 220, Training loss = 0.27695315402860826
Iteration 230, Training loss = 0.275094350370077
Iteration 240, Training loss = 0.2775086684582325
Iteration 250, Training loss = 0.27441687065248305
Iteration 260, Training loss = 0.2723767116952401
Iteration 270, Training loss = 0.2706760557798239
Iteration 280, Training loss = 0.27040515266931975
Iteration 290, Training loss = 0.2680345723548761
Model training time: 49.770310401916504
Device: cuda
Iteration 0, Training loss = 1.1236396798720727
Iteration 10, Training loss = 1.007260268697372
Iteration 20, Training loss = 0.9843858354366742
Iteration 30, Training loss = 0.9255990523558396
Iteration 40, Training loss = 0.7043306277348444
Iteration 50, Training loss = 0.4453022666275501
Iteration 60, Training loss = 0.3796154740624703
Iteration 70, Training loss = 0.35920769663957447
Iteration 80, Training loss = 0.3473793756789886
Iteration 90, Training loss = 0.33812974550976205
Iteration 100, Training loss = 0.3315496426075697
Iteration 110, Training loss = 0.32273726107982487
Iteration 120, Training loss = 0.31734552764548707
Iteration 130, Training loss = 0.3129559957350676
Iteration 140, Training loss = 0.30927497793275577
Iteration 150, Training loss = 0.3046306625295144
Iteration 160, Training loss = 0.30044935600688827
Iteration 170, Training loss = 0.2986234725954441
Iteration 180, Training loss = 0.2958168304310395
Iteration 190, Training loss = 0.29357783482051814
Iteration 200, Training loss = 0.2906435997440265
Iteration 210, Training loss = 0.2884258692367719
Iteration 220, Training loss = 0.28637408937972325
Iteration 230, Training loss = 0.28533082641661167
Iteration 240, Training loss = 0.283799322322011
Iteration 250, Training loss = 0.2825042033711305
Iteration 260, Training loss = 0.2811698221529906
Iteration 270, Training loss = 0.2799336014745327
Iteration 280, Training loss = 0.27844886410121733
Iteration 290, Training loss = 0.27828827185126453
Model training time: 49.33700394630432
Device: cuda
Iteration 0, Training loss = 1.1147765843914106
Iteration 10, Training loss = 0.9232985744109521
Iteration 20, Training loss = 0.6944865641685632
Iteration 30, Training loss = 0.4142259939645345
Iteration 40, Training loss = 0.3749438783583733
Iteration 50, Training loss = 0.3541845524540314
Iteration 60, Training loss = 0.34481827771434415
Iteration 70, Training loss = 0.33471077179106384
Iteration 80, Training loss = 0.32825986252954376
Iteration 90, Training loss = 0.32413777393790394
Iteration 100, Training loss = 0.3196127741382672
Iteration 110, Training loss = 0.3162945673729365
Iteration 120, Training loss = 0.31535906946429837
Iteration 130, Training loss = 0.3122712882378927
Iteration 140, Training loss = 0.30940977197427016
Iteration 150, Training loss = 0.30662931817082256
Iteration 160, Training loss = 0.3042090709965963
Iteration 170, Training loss = 0.3030206459359481
Iteration 180, Training loss = 0.2992180519952224
Iteration 190, Training loss = 0.2958508004935888
Iteration 200, Training loss = 0.29514155866434943
Iteration 210, Training loss = 0.29256670219966996
Iteration 220, Training loss = 0.29049954061897904
Iteration 230, Training loss = 0.2894143648445606
Iteration 240, Training loss = 0.2886080531260142
Iteration 250, Training loss = 0.28556402844305223
Iteration 260, Training loss = 0.28332823830155224
Iteration 270, Training loss = 0.2840420902491762
Iteration 280, Training loss = 0.28238271234127194
Iteration 290, Training loss = 0.2799038467212365
Model training time: 49.48122525215149
Device: cuda
Iteration 0, Training loss = 1.019844913138793
Iteration 10, Training loss = 0.7960856488117805
Iteration 20, Training loss = 0.4803641349650346
Iteration 30, Training loss = 0.3974602925949372
Iteration 40, Training loss = 0.36196370895665425
Iteration 50, Training loss = 0.3425193964861907
Iteration 60, Training loss = 0.3363147691751902
Iteration 70, Training loss = 0.32369087607814717
Iteration 80, Training loss = 0.3188182700138826
Iteration 90, Training loss = 0.31419984867366463
Iteration 100, Training loss = 0.3115219339155234
Iteration 110, Training loss = 0.3098294517168632
Iteration 120, Training loss = 0.3070427897171332
Iteration 130, Training loss = 0.3050802583591296
Iteration 140, Training loss = 0.3062120753412063
Iteration 150, Training loss = 0.30123100630365884
Iteration 160, Training loss = 0.30214436165988445
Iteration 170, Training loss = 0.298875167965889
Iteration 180, Training loss = 0.2973106331550158
Iteration 190, Training loss = 0.29567783483519006
Iteration 200, Training loss = 0.29399753347612345
Iteration 210, Training loss = 0.2931165869992513
Iteration 220, Training loss = 0.2917613953065414
Iteration 230, Training loss = 0.29323090326327544
Iteration 240, Training loss = 0.29056728172760743
Iteration 250, Training loss = 0.2895126901566982
Iteration 260, Training loss = 0.2905060805093784
Iteration 270, Training loss = 0.285567389228023
Iteration 280, Training loss = 0.28452289591615015
Iteration 290, Training loss = 0.2856117064276567
Model training time: 49.701308488845825
Device: cuda
Iteration 0, Training loss = 1.03898982073252
Iteration 10, Training loss = 0.8895547779706808
Iteration 20, Training loss = 0.5636423854873731
Iteration 30, Training loss = 0.4057802801521925
Iteration 40, Training loss = 0.3724272296978877
Iteration 50, Training loss = 0.3552503422475778
Iteration 60, Training loss = 0.3463844940639459
Iteration 70, Training loss = 0.3405626295850827
Iteration 80, Training loss = 0.33475394332065034
Iteration 90, Training loss = 0.33273903008263844
Iteration 100, Training loss = 0.32690642602168596
Iteration 110, Training loss = 0.3245067842877828
Iteration 120, Training loss = 0.3225842470733019
Iteration 130, Training loss = 0.32077753701462197
Iteration 140, Training loss = 0.3158838985344538
Iteration 150, Training loss = 0.31358363436391723
Iteration 160, Training loss = 0.31202596612274647
Iteration 170, Training loss = 0.3099553188165793
Iteration 180, Training loss = 0.3078518664607635
Iteration 190, Training loss = 0.3074146075030932
Iteration 200, Training loss = 0.3038503061980009
Iteration 210, Training loss = 0.3021840686694934
Iteration 220, Training loss = 0.3009041683891645
Iteration 230, Training loss = 0.29903261764691424
Iteration 240, Training loss = 0.2945359587096251
Iteration 250, Training loss = 0.29496053878504497
Iteration 260, Training loss = 0.29229269778499234
Iteration 270, Training loss = 0.29051154307447946
Iteration 280, Training loss = 0.2893778240451446
Iteration 290, Training loss = 0.28977007562151325
Model training time: 49.393075466156006
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9702496826648712
Iteration 10, Training loss = 0.27556930983868927
Iteration 20, Training loss = 0.26037484471901107
Iteration 30, Training loss = 0.25472331935396564
Iteration 40, Training loss = 0.25187454429956585
Iteration 50, Training loss = 0.2479218105570628
Iteration 60, Training loss = 0.2481047843511288
Iteration 70, Training loss = 0.24341132047657782
Iteration 80, Training loss = 0.24188189050899103
Iteration 90, Training loss = 0.2416105097016463
Iteration 100, Training loss = 0.23750766868201587
Iteration 110, Training loss = 0.23591537379588073
Iteration 120, Training loss = 0.23329170119877046
Iteration 130, Training loss = 0.2327326159351147
Iteration 140, Training loss = 0.23174489833987677
Iteration 150, Training loss = 0.2329763238533185
Iteration 160, Training loss = 0.2282198231953841
Iteration 170, Training loss = 0.22602766279417735
Iteration 180, Training loss = 0.2241813293299996
Iteration 190, Training loss = 0.2247751519943659
Iteration 200, Training loss = 0.22382807387755468
Iteration 210, Training loss = 0.2227957072452857
Iteration 220, Training loss = 0.22251554621526828
Iteration 230, Training loss = 0.2197482012785398
Iteration 240, Training loss = 0.2193562061740802
Iteration 250, Training loss = 0.2193932538278974
Iteration 260, Training loss = 0.21792403350655848
Iteration 270, Training loss = 0.2179865283318437
Iteration 280, Training loss = 0.21923269933232895
Iteration 290, Training loss = 0.2165581160583175
Model training time: 59.59486651420593
Device: cuda
Iteration 0, Training loss = 0.8477494378502552
Iteration 10, Training loss = 0.294089867375218
Iteration 20, Training loss = 0.2711668186462842
Iteration 30, Training loss = 0.25976840084275377
Iteration 40, Training loss = 0.254866007858744
Iteration 50, Training loss = 0.24912699913749328
Iteration 60, Training loss = 0.2448693080447041
Iteration 70, Training loss = 0.24263086513831064
Iteration 80, Training loss = 0.24076590271523365
Iteration 90, Training loss = 0.23926523786324722
Iteration 100, Training loss = 0.23708021311232677
Iteration 110, Training loss = 0.23322985443071678
Iteration 120, Training loss = 0.23340673300509268
Iteration 130, Training loss = 0.23291295795486525
Iteration 140, Training loss = 0.22891357555412328
Iteration 150, Training loss = 0.22664670184111366
Iteration 160, Training loss = 0.2264016276368728
Iteration 170, Training loss = 0.22559677007106635
Iteration 180, Training loss = 0.2261670297011733
Iteration 190, Training loss = 0.22636411568293205
Iteration 200, Training loss = 0.22246094764425203
Iteration 210, Training loss = 0.22368555802565354
Iteration 220, Training loss = 0.22152033261954784
Iteration 230, Training loss = 0.22477606683969498
Iteration 240, Training loss = 0.22051251049225146
Iteration 250, Training loss = 0.22061874695981926
Iteration 260, Training loss = 0.21973935700953007
Iteration 270, Training loss = 0.21969715183457503
Iteration 280, Training loss = 0.22021239279554441
Iteration 290, Training loss = 0.21961756852956918
Model training time: 62.553040981292725
Device: cuda
Iteration 0, Training loss = 0.9100104587582442
Iteration 10, Training loss = 0.2798855228779408
Iteration 20, Training loss = 0.2589575196974553
Iteration 30, Training loss = 0.2512452106636304
Iteration 40, Training loss = 0.2474331917384496
Iteration 50, Training loss = 0.24564187930753598
Iteration 60, Training loss = 0.2411167982679147
Iteration 70, Training loss = 0.2389924365740556
Iteration 80, Training loss = 0.23638420652311581
Iteration 90, Training loss = 0.23282900946931198
Iteration 100, Training loss = 0.23313159423951918
Iteration 110, Training loss = 0.23029907721166426
Iteration 120, Training loss = 0.22994842265660947
Iteration 130, Training loss = 0.22716533378339732
Iteration 140, Training loss = 0.22577605453821328
Iteration 150, Training loss = 0.22524115514869875
Iteration 160, Training loss = 0.22143894942620626
Iteration 170, Training loss = 0.22222620205810437
Iteration 180, Training loss = 0.22183592689151949
Iteration 190, Training loss = 0.21641638581282818
Iteration 200, Training loss = 0.21700550529819268
Iteration 210, Training loss = 0.2136742384531177
Iteration 220, Training loss = 0.2136521672543425
Iteration 230, Training loss = 0.21055955067276955
Iteration 240, Training loss = 0.21036102493795064
Iteration 250, Training loss = 0.2108130775965177
Iteration 260, Training loss = 0.20895589128709757
Iteration 270, Training loss = 0.20686384534033445
Iteration 280, Training loss = 0.2084801308810711
Iteration 290, Training loss = 0.2063126156393152
Model training time: 59.08414936065674
Device: cuda
Iteration 0, Training loss = 0.80638914841872
Iteration 10, Training loss = 0.2707662758632348
Iteration 20, Training loss = 0.2554079885952748
Iteration 30, Training loss = 0.25103055513822115
Iteration 40, Training loss = 0.24530327792924184
Iteration 50, Training loss = 0.24268388074751085
Iteration 60, Training loss = 0.23940758722332808
Iteration 70, Training loss = 0.2373147506553393
Iteration 80, Training loss = 0.2331956189412337
Iteration 90, Training loss = 0.23066417271128067
Iteration 100, Training loss = 0.22905213514772746
Iteration 110, Training loss = 0.22661948906114468
Iteration 120, Training loss = 0.2273945124963155
Iteration 130, Training loss = 0.2241436429321766
Iteration 140, Training loss = 0.22172966635284516
Iteration 150, Training loss = 0.22226778604090214
Iteration 160, Training loss = 0.22052269586576864
Iteration 170, Training loss = 0.21987242624163628
Iteration 180, Training loss = 0.21843252560267082
Iteration 190, Training loss = 0.2181208457511205
Iteration 200, Training loss = 0.21754394385677117
Iteration 210, Training loss = 0.2161964175219719
Iteration 220, Training loss = 0.21645518800673577
Iteration 230, Training loss = 0.21294775447593287
Iteration 240, Training loss = 0.21091569738032726
Iteration 250, Training loss = 0.21022909134626389
Iteration 260, Training loss = 0.20877175491589767
Iteration 270, Training loss = 0.20992984171383655
Iteration 280, Training loss = 0.20911955217329356
Iteration 290, Training loss = 0.2107231721568566
Model training time: 56.61784052848816
Device: cuda
Iteration 0, Training loss = 0.9351181903710732
Iteration 10, Training loss = 0.2736934651262485
Iteration 20, Training loss = 0.2554747132727733
Iteration 30, Training loss = 0.24883281353574532
Iteration 40, Training loss = 0.24105504190979096
Iteration 50, Training loss = 0.23397267968035662
Iteration 60, Training loss = 0.22814387140365747
Iteration 70, Training loss = 0.22523562194636235
Iteration 80, Training loss = 0.2217204003379895
Iteration 90, Training loss = 0.2202779619882886
Iteration 100, Training loss = 0.2175371290112917
Iteration 110, Training loss = 0.21754491565605769
Iteration 120, Training loss = 0.21526453285836256
Iteration 130, Training loss = 0.212480776083584
Iteration 140, Training loss = 0.21301516783065522
Iteration 150, Training loss = 0.212431041452174
Iteration 160, Training loss = 0.2101583116186353
Iteration 170, Training loss = 0.20921815258379167
Iteration 180, Training loss = 0.20842001845057195
Iteration 190, Training loss = 0.20831165040055147
Iteration 200, Training loss = 0.20695649159069246
Iteration 210, Training loss = 0.20643628252526888
Iteration 220, Training loss = 0.2058885803159613
Iteration 230, Training loss = 0.205092183744105
Iteration 240, Training loss = 0.2055502816175039
Iteration 250, Training loss = 0.20693754132550496
Iteration 260, Training loss = 0.20492959029685992
Iteration 270, Training loss = 0.20457587978587702
Iteration 280, Training loss = 0.2061791566128914
Iteration 290, Training loss = 0.20383015626038498
Model training time: 56.72766709327698
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0425525089869132
Iteration 10, Training loss = 0.9026366134102528
Iteration 20, Training loss = 0.7289412709382864
Iteration 30, Training loss = 0.5471406321112926
Iteration 40, Training loss = 0.43835481714743835
Iteration 50, Training loss = 0.37920886707993656
Iteration 60, Training loss = 0.34720483594215834
Iteration 70, Training loss = 0.33179674182946867
Iteration 80, Training loss = 0.3210642605733413
Iteration 90, Training loss = 0.318608614974297
Iteration 100, Training loss = 0.3112892402479282
Iteration 110, Training loss = 0.30842418839725166
Iteration 120, Training loss = 0.3054139925023684
Iteration 130, Training loss = 0.3032771586798705
Iteration 140, Training loss = 0.30001529384977543
Iteration 150, Training loss = 0.3031096565895356
Iteration 160, Training loss = 0.29831599888320154
Iteration 170, Training loss = 0.2967475327448203
Iteration 180, Training loss = 0.29400719759555966
Iteration 190, Training loss = 0.29270640889612526
Iteration 200, Training loss = 0.29166554444684434
Iteration 210, Training loss = 0.2940072650806262
Iteration 220, Training loss = 0.2914663964452652
Iteration 230, Training loss = 0.2871051145574221
Iteration 240, Training loss = 0.2866791975326263
Iteration 250, Training loss = 0.2862058707441275
Iteration 260, Training loss = 0.2847124869720294
Iteration 270, Training loss = 0.2847658060491085
Iteration 280, Training loss = 0.2818627836039433
Iteration 290, Training loss = 0.2819344759560548
Model training time: 51.156996726989746
Device: cuda
Iteration 0, Training loss = 0.9526069290362872
Iteration 10, Training loss = 0.69960017903493
Iteration 20, Training loss = 0.4527088922376816
Iteration 30, Training loss = 0.38862063793035656
Iteration 40, Training loss = 0.3571861288868464
Iteration 50, Training loss = 0.3407068203848142
Iteration 60, Training loss = 0.3314410410821438
Iteration 70, Training loss = 0.3240155675090276
Iteration 80, Training loss = 0.31653302148557627
Iteration 90, Training loss = 0.31179726560815024
Iteration 100, Training loss = 0.3099407543643163
Iteration 110, Training loss = 0.3068519988312171
Iteration 120, Training loss = 0.30723754330896413
Iteration 130, Training loss = 0.3018656299950985
Iteration 140, Training loss = 0.29890297252971393
Iteration 150, Training loss = 0.30169951958725083
Iteration 160, Training loss = 0.2962499735160516
Iteration 170, Training loss = 0.29322496013572585
Iteration 180, Training loss = 0.29399587572194064
Iteration 190, Training loss = 0.2889931181875559
Iteration 200, Training loss = 0.28915336871376407
Iteration 210, Training loss = 0.2851517421121781
Iteration 220, Training loss = 0.28539389472168225
Iteration 230, Training loss = 0.2836599169442287
Iteration 240, Training loss = 0.2801892760281379
Iteration 250, Training loss = 0.2798312658873888
Iteration 260, Training loss = 0.2790001702423279
Iteration 270, Training loss = 0.2760631808867821
Iteration 280, Training loss = 0.27581105510202736
Iteration 290, Training loss = 0.2739772874002273
Model training time: 50.62113952636719
Device: cuda
Iteration 0, Training loss = 1.0555724106155908
Iteration 10, Training loss = 0.9663450735119673
Iteration 20, Training loss = 0.864246116234706
Iteration 30, Training loss = 0.5766916836683567
Iteration 40, Training loss = 0.3985737275618773
Iteration 50, Training loss = 0.36720285922862017
Iteration 60, Training loss = 0.3502777417978415
Iteration 70, Training loss = 0.3398682349003278
Iteration 80, Training loss = 0.3311634079481547
Iteration 90, Training loss = 0.32584774107314074
Iteration 100, Training loss = 0.3192877064530666
Iteration 110, Training loss = 0.31826119354138005
Iteration 120, Training loss = 0.31072177408406365
Iteration 130, Training loss = 0.30775322020053864
Iteration 140, Training loss = 0.30764567092634165
Iteration 150, Training loss = 0.3023536360034576
Iteration 160, Training loss = 0.29995085384983283
Iteration 170, Training loss = 0.30068306467280936
Iteration 180, Training loss = 0.29734687249247843
Iteration 190, Training loss = 0.2954184190871624
Iteration 200, Training loss = 0.29544839549523133
Iteration 210, Training loss = 0.2934504228715713
Iteration 220, Training loss = 0.29060545649666053
Iteration 230, Training loss = 0.290309436332721
Iteration 240, Training loss = 0.28688093532736486
Iteration 250, Training loss = 0.28637735760555816
Iteration 260, Training loss = 0.28903355162877303
Iteration 270, Training loss = 0.2847224667381782
Iteration 280, Training loss = 0.2848736553524549
Iteration 290, Training loss = 0.2818516514335687
Model training time: 50.723538875579834
Device: cuda
Iteration 0, Training loss = 0.9927668256255296
Iteration 10, Training loss = 0.8829701095819473
Iteration 20, Training loss = 0.6501759929725757
Iteration 30, Training loss = 0.4356090392057712
Iteration 40, Training loss = 0.36869255653940713
Iteration 50, Training loss = 0.34071488592487115
Iteration 60, Training loss = 0.3277292757366712
Iteration 70, Training loss = 0.3196633435212649
Iteration 80, Training loss = 0.3152601058379962
Iteration 90, Training loss = 0.3109308277758268
Iteration 100, Training loss = 0.3072461811109231
Iteration 110, Training loss = 0.3045749284613591
Iteration 120, Training loss = 0.3021031351616749
Iteration 130, Training loss = 0.30071987655873483
Iteration 140, Training loss = 0.2996532288021766
Iteration 150, Training loss = 0.2984797815577342
Iteration 160, Training loss = 0.29668360762298107
Iteration 170, Training loss = 0.29768237185019714
Iteration 180, Training loss = 0.2977299446669909
Iteration 190, Training loss = 0.2929958633791942
Iteration 200, Training loss = 0.29078621331315774
Iteration 210, Training loss = 0.29137027722138625
Iteration 220, Training loss = 0.29136102102123773
Iteration 230, Training loss = 0.2878768706264404
Iteration 240, Training loss = 0.2878772732443534
Iteration 250, Training loss = 0.28640313618458235
Iteration 260, Training loss = 0.2857526026379604
Iteration 270, Training loss = 0.2869907058775425
Iteration 280, Training loss = 0.2842706825870734
Iteration 290, Training loss = 0.28340122719796806
Model training time: 50.23543906211853
Device: cuda
Iteration 0, Training loss = 1.0762499903257077
Iteration 10, Training loss = 0.9817217920835202
Iteration 20, Training loss = 0.9261734118828406
Iteration 30, Training loss = 0.7918121774609272
Iteration 40, Training loss = 0.6077706604622878
Iteration 50, Training loss = 0.49303276063157964
Iteration 60, Training loss = 0.4334141272478379
Iteration 70, Training loss = 0.3883442213902107
Iteration 80, Training loss = 0.35931349201844287
Iteration 90, Training loss = 0.340513573672909
Iteration 100, Training loss = 0.32893322250590873
Iteration 110, Training loss = 0.32100608916236806
Iteration 120, Training loss = 0.3166982035797376
Iteration 130, Training loss = 0.3135031650845821
Iteration 140, Training loss = 0.3108932361579858
Iteration 150, Training loss = 0.3085376381014402
Iteration 160, Training loss = 0.3093738770828797
Iteration 170, Training loss = 0.30552084658008355
Iteration 180, Training loss = 0.3050747777406986
Iteration 190, Training loss = 0.30200404301285744
Iteration 200, Training loss = 0.30150849816317743
Iteration 210, Training loss = 0.29986145046467966
Iteration 220, Training loss = 0.2978299053815695
Iteration 230, Training loss = 0.2968726381659508
Iteration 240, Training loss = 0.2959576885287578
Iteration 250, Training loss = 0.2952357835781116
Iteration 260, Training loss = 0.29270763824192375
Iteration 270, Training loss = 0.2938938529159014
Iteration 280, Training loss = 0.2906475057109044
Iteration 290, Training loss = 0.2892008072768266
Model training time: 51.56672716140747
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.701284831819626
Iteration 10, Training loss = 0.26322950351123625
Iteration 20, Training loss = 0.2498872376118715
Iteration 30, Training loss = 0.2427553918499213
Iteration 40, Training loss = 0.2325479043647647
Iteration 50, Training loss = 0.22479430256554714
Iteration 60, Training loss = 0.2206572827954705
Iteration 70, Training loss = 0.21509513151473725
Iteration 80, Training loss = 0.21241255935568076
Iteration 90, Training loss = 0.2122241067341887
Model training time: 18.503273010253906
Device: cuda
Iteration 0, Training loss = 0.6578418268607213
Iteration 10, Training loss = 0.2585139089765457
Iteration 20, Training loss = 0.24211669039840883
Iteration 30, Training loss = 0.2320043180281153
Iteration 40, Training loss = 0.22799002250226644
Iteration 50, Training loss = 0.22495036514905783
Iteration 60, Training loss = 0.22100583299134785
Iteration 70, Training loss = 0.21939987073150966
Iteration 80, Training loss = 0.21528034599927756
Iteration 90, Training loss = 0.2163659923065167
Model training time: 18.80242419242859
Device: cuda
Iteration 0, Training loss = 0.6266642960791404
Iteration 10, Training loss = 0.25495896259179485
Iteration 20, Training loss = 0.24263785707835966
Iteration 30, Training loss = 0.2318008063504329
Iteration 40, Training loss = 0.22542656470949834
Iteration 50, Training loss = 0.22034018916579393
Iteration 60, Training loss = 0.2182137332856655
Iteration 70, Training loss = 0.21273012129733196
Iteration 80, Training loss = 0.21127702904721865
Iteration 90, Training loss = 0.20805575469365487
Model training time: 18.793293952941895
Device: cuda
Iteration 0, Training loss = 0.5902969519106241
Iteration 10, Training loss = 0.25754087790846825
Iteration 20, Training loss = 0.24150232025063956
Iteration 30, Training loss = 0.23430862958328083
Iteration 40, Training loss = 0.23094780349101013
Iteration 50, Training loss = 0.22687609407764214
Iteration 60, Training loss = 0.2240405846387148
Iteration 70, Training loss = 0.2233172913726706
Iteration 80, Training loss = 0.21967955535421005
Iteration 90, Training loss = 0.21679184270592836
Model training time: 18.933769941329956
Device: cuda
Iteration 0, Training loss = 0.6965691031744847
Iteration 10, Training loss = 0.2551332792410484
Iteration 20, Training loss = 0.24007406615867063
Iteration 30, Training loss = 0.23217659515257066
Iteration 40, Training loss = 0.2248692962412651
Iteration 50, Training loss = 0.21793179393101197
Iteration 60, Training loss = 0.21586992133122224
Iteration 70, Training loss = 0.21358013998430508
Iteration 80, Training loss = 0.21056663853904375
Iteration 90, Training loss = 0.2107552678252642
Model training time: 18.746553659439087
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0192155201847737
Iteration 10, Training loss = 0.6360374955603709
Iteration 20, Training loss = 0.3813703548736297
Iteration 30, Training loss = 0.34838419335965926
Iteration 40, Training loss = 0.33121015847875523
Iteration 50, Training loss = 0.3208641682106715
Iteration 60, Training loss = 0.31349103811841744
Iteration 70, Training loss = 0.30777330610614556
Iteration 80, Training loss = 0.3070399295538664
Iteration 90, Training loss = 0.2991716512120687
Model training time: 16.453044176101685
Device: cuda
Iteration 0, Training loss = 1.012189788887134
Iteration 10, Training loss = 0.6474564955211602
Iteration 20, Training loss = 0.38216958538844037
Iteration 30, Training loss = 0.3459334565469852
Iteration 40, Training loss = 0.32846821214144045
Iteration 50, Training loss = 0.3159711047147329
Iteration 60, Training loss = 0.30943144098497355
Iteration 70, Training loss = 0.30355986623236764
Iteration 80, Training loss = 0.30122746598835176
Iteration 90, Training loss = 0.29621818045584053
Model training time: 16.369407653808594
Device: cuda
Iteration 0, Training loss = 1.0169915333390236
Iteration 10, Training loss = 0.5768346370985875
Iteration 20, Training loss = 0.40623980961166894
Iteration 30, Training loss = 0.36614887172786087
Iteration 40, Training loss = 0.3468156187580182
Iteration 50, Training loss = 0.3378089458610003
Iteration 60, Training loss = 0.33162671929368603
Iteration 70, Training loss = 0.32742417532090956
Iteration 80, Training loss = 0.32390890614344525
Iteration 90, Training loss = 0.3200459097726987
Model training time: 16.7438862323761
Device: cuda
Iteration 0, Training loss = 0.9739165019530517
Iteration 10, Training loss = 0.5685945617464873
Iteration 20, Training loss = 0.38884607874430144
Iteration 30, Training loss = 0.3486663687687654
Iteration 40, Training loss = 0.33026604640942353
Iteration 50, Training loss = 0.32095396819596106
Iteration 60, Training loss = 0.3125587488309695
Iteration 70, Training loss = 0.3076591325493959
Iteration 80, Training loss = 0.30002278777269215
Iteration 90, Training loss = 0.2960778083652258
Model training time: 16.50317883491516
Device: cuda
Iteration 0, Training loss = 1.03743671052731
Iteration 10, Training loss = 0.9850548482858218
Iteration 20, Training loss = 0.7214770099291434
Iteration 30, Training loss = 0.4099394097351111
Iteration 40, Training loss = 0.3499292784298842
Iteration 50, Training loss = 0.3287500830797049
Iteration 60, Training loss = 0.3160433838000664
Iteration 70, Training loss = 0.31017851929825085
Iteration 80, Training loss = 0.3060668178189259
Iteration 90, Training loss = 0.29902670503808904
Model training time: 16.486799955368042
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7356153725144955
Iteration 10, Training loss = 0.2717506733651345
Iteration 20, Training loss = 0.2597340735105368
Iteration 30, Training loss = 0.25343533934882057
Iteration 40, Training loss = 0.24934228781897289
Iteration 50, Training loss = 0.24352742330386087
Iteration 60, Training loss = 0.24085442779155877
Iteration 70, Training loss = 0.23843962921259496
Iteration 80, Training loss = 0.23544761767754188
Iteration 90, Training loss = 0.23589659295976162
Model training time: 18.793092966079712
Device: cuda
Iteration 0, Training loss = 0.6144381197026143
Iteration 10, Training loss = 0.2705181000324396
Iteration 20, Training loss = 0.2524953892884346
Iteration 30, Training loss = 0.24565341051381367
Iteration 40, Training loss = 0.24004019094774356
Iteration 50, Training loss = 0.23474230574300656
Iteration 60, Training loss = 0.2318142308638646
Iteration 70, Training loss = 0.22578400652855635
Iteration 80, Training loss = 0.22316329797300008
Iteration 90, Training loss = 0.22221798721987468
Model training time: 18.460886240005493
Device: cuda
Iteration 0, Training loss = 0.7008418612755262
Iteration 10, Training loss = 0.27758808720570344
Iteration 20, Training loss = 0.257148632994638
Iteration 30, Training loss = 0.25195100029500633
Iteration 40, Training loss = 0.2456715492388377
Iteration 50, Training loss = 0.23950948473066092
Iteration 60, Training loss = 0.23668727794518837
Iteration 70, Training loss = 0.23120717360423163
Iteration 80, Training loss = 0.23028195520433095
Iteration 90, Training loss = 0.22760941073871577
Model training time: 18.81845498085022
Device: cuda
Iteration 0, Training loss = 0.642350723537115
Iteration 10, Training loss = 0.2688646293603457
Iteration 20, Training loss = 0.2531275236262725
Iteration 30, Training loss = 0.24807201110972807
Iteration 40, Training loss = 0.2411908063894281
Iteration 50, Training loss = 0.23687338270246983
Iteration 60, Training loss = 0.23341452659895787
Iteration 70, Training loss = 0.22985721866671854
Iteration 80, Training loss = 0.23188420600042894
Iteration 90, Training loss = 0.22914091777056456
Model training time: 19.09476947784424
Device: cuda
Iteration 0, Training loss = 1.0805604073863764
Iteration 10, Training loss = 0.36244516561810786
Iteration 20, Training loss = 0.26844678131433636
Iteration 30, Training loss = 0.24913781293882772
Iteration 40, Training loss = 0.24182986109875715
Iteration 50, Training loss = 0.23756843490096238
Iteration 60, Training loss = 0.23510475198809916
Iteration 70, Training loss = 0.23211215092585638
Iteration 80, Training loss = 0.2308597654963915
Iteration 90, Training loss = 0.23191606110105148
Model training time: 18.779396057128906
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1033298602470984
Iteration 10, Training loss = 0.7127717613027647
Iteration 20, Training loss = 0.47524110629008365
Iteration 30, Training loss = 0.41159665455611855
Iteration 40, Training loss = 0.3720564048450727
Iteration 50, Training loss = 0.3480038599899182
Iteration 60, Training loss = 0.33597487211227417
Iteration 70, Training loss = 0.3313020517906317
Iteration 80, Training loss = 0.32367313681886745
Iteration 90, Training loss = 0.3190016435602537
Model training time: 16.51492476463318
Device: cuda
Iteration 0, Training loss = 1.1423163677637393
Iteration 10, Training loss = 0.8167596958004512
Iteration 20, Training loss = 0.467987053669416
Iteration 30, Training loss = 0.3953182321901505
Iteration 40, Training loss = 0.3594301433230822
Iteration 50, Training loss = 0.34446111565025955
Iteration 60, Training loss = 0.3343835031756988
Iteration 70, Training loss = 0.32208293160566914
Iteration 80, Training loss = 0.3175316070421384
Iteration 90, Training loss = 0.3101324883217995
Model training time: 16.869273900985718
Device: cuda
Iteration 0, Training loss = 1.0441451410834606
Iteration 10, Training loss = 0.8388245377976161
Iteration 20, Training loss = 0.5218175592330786
Iteration 30, Training loss = 0.42215690733148503
Iteration 40, Training loss = 0.3828173203823658
Iteration 50, Training loss = 0.3566401040611359
Iteration 60, Training loss = 0.3361901914557585
Iteration 70, Training loss = 0.32006256611874473
Iteration 80, Training loss = 0.3099937661049458
Iteration 90, Training loss = 0.30215434027979005
Model training time: 16.34968066215515
Device: cuda
Iteration 0, Training loss = 1.1220222459389613
Iteration 10, Training loss = 1.0020969561659372
Iteration 20, Training loss = 0.9948729815391394
Iteration 30, Training loss = 0.9133604862369024
Iteration 40, Training loss = 0.4680307579155152
Iteration 50, Training loss = 0.3826335215797791
Iteration 60, Training loss = 0.3467123365172973
Iteration 70, Training loss = 0.3324019960485972
Iteration 80, Training loss = 0.3233376033604145
Iteration 90, Training loss = 0.3190159973903344
Model training time: 16.393404722213745
Device: cuda
Iteration 0, Training loss = 1.1064073076614966
Iteration 10, Training loss = 0.6753161331781974
Iteration 20, Training loss = 0.46869229138470614
Iteration 30, Training loss = 0.40059937135531354
Iteration 40, Training loss = 0.36496532951983124
Iteration 50, Training loss = 0.34106299768273646
Iteration 60, Training loss = 0.32466354292745775
Iteration 70, Training loss = 0.31344008503051907
Iteration 80, Training loss = 0.301171529178436
Iteration 90, Training loss = 0.29385835046951586
Model training time: 16.890963077545166
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6705520250476323
Iteration 10, Training loss = 0.26336787239863324
Iteration 20, Training loss = 0.24302453160859072
Iteration 30, Training loss = 0.23839514043468696
Iteration 40, Training loss = 0.2331573493205584
Iteration 50, Training loss = 0.2295099736119692
Iteration 60, Training loss = 0.2282518529548095
Iteration 70, Training loss = 0.22316996989628443
Iteration 80, Training loss = 0.22323588740367156
Iteration 90, Training loss = 0.2249132446371592
Model training time: 19.106948614120483
Device: cuda
Iteration 0, Training loss = 0.8036284386538542
Iteration 10, Training loss = 0.2658807351612128
Iteration 20, Training loss = 0.25373556407598347
Iteration 30, Training loss = 0.24927814834966108
Iteration 40, Training loss = 0.24233118702585882
Iteration 50, Training loss = 0.23982038793082422
Iteration 60, Training loss = 0.23548701706414038
Iteration 70, Training loss = 0.23456773090247923
Iteration 80, Training loss = 0.23415456869854376
Iteration 90, Training loss = 0.22975142715642086
Model training time: 19.126726627349854
Device: cuda
Iteration 0, Training loss = 0.7148802360663047
Iteration 10, Training loss = 0.26646993080010783
Iteration 20, Training loss = 0.2527783044542258
Iteration 30, Training loss = 0.2453246542897362
Iteration 40, Training loss = 0.24060430735922778
Iteration 50, Training loss = 0.2359646951349882
Iteration 60, Training loss = 0.23205402421836668
Iteration 70, Training loss = 0.22989458060608461
Iteration 80, Training loss = 0.22521045445822752
Iteration 90, Training loss = 0.22256549261510372
Model training time: 19.250394582748413
Device: cuda
Iteration 0, Training loss = 0.6798334127435317
Iteration 10, Training loss = 0.2659615229528684
Iteration 20, Training loss = 0.2536773704565488
Iteration 30, Training loss = 0.24533039431732434
Iteration 40, Training loss = 0.23508911637159494
Iteration 50, Training loss = 0.23380211783716312
Iteration 60, Training loss = 0.22779092429062495
Iteration 70, Training loss = 0.22556778186788926
Iteration 80, Training loss = 0.22230481843535715
Iteration 90, Training loss = 0.21849345892237929
Model training time: 18.856701850891113
Device: cuda
Iteration 0, Training loss = 0.6820295037558446
Iteration 10, Training loss = 0.25517907179892063
Iteration 20, Training loss = 0.24519754502062613
Iteration 30, Training loss = 0.23780793223816615
Iteration 40, Training loss = 0.2328265943349554
Iteration 50, Training loss = 0.22787138891334718
Iteration 60, Training loss = 0.2232729997485876
Iteration 70, Training loss = 0.21961558767809317
Iteration 80, Training loss = 0.22230169263023597
Iteration 90, Training loss = 0.21492974667881543
Model training time: 18.72898244857788
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9903046391331233
Iteration 10, Training loss = 0.5769690656318114
Iteration 20, Training loss = 0.37457859315551245
Iteration 30, Training loss = 0.3371410377037067
Iteration 40, Training loss = 0.32413426972925663
Iteration 50, Training loss = 0.31477139689601386
Iteration 60, Training loss = 0.3096600849754535
Iteration 70, Training loss = 0.3050694749332391
Iteration 80, Training loss = 0.3019218437660199
Iteration 90, Training loss = 0.30016637665147966
Model training time: 17.008138179779053
Device: cuda
Iteration 0, Training loss = 1.0679107583486116
Iteration 10, Training loss = 0.8346315702566733
Iteration 20, Training loss = 0.4095523323004062
Iteration 30, Training loss = 0.35849811810140425
Iteration 40, Training loss = 0.3338272717709725
Iteration 50, Training loss = 0.3208923219488217
Iteration 60, Training loss = 0.31188800730384314
Iteration 70, Training loss = 0.30827572827155775
Iteration 80, Training loss = 0.3004236559455211
Iteration 90, Training loss = 0.2965211146152936
Model training time: 16.769131898880005
Device: cuda
Iteration 0, Training loss = 1.0663370805291028
Iteration 10, Training loss = 0.8123127944194354
Iteration 20, Training loss = 0.43488559843255925
Iteration 30, Training loss = 0.3695499004366306
Iteration 40, Training loss = 0.3397367009176658
Iteration 50, Training loss = 0.32849876296061736
Iteration 60, Training loss = 0.32047470816626
Iteration 70, Training loss = 0.3137412569843806
Iteration 80, Training loss = 0.3079121526903831
Iteration 90, Training loss = 0.30469627320193327
Model training time: 16.717134952545166
Device: cuda
Iteration 0, Training loss = 0.9931274193983811
Iteration 10, Training loss = 0.4504064259907374
Iteration 20, Training loss = 0.3551764101363145
Iteration 30, Training loss = 0.3312798242729444
Iteration 40, Training loss = 0.3179853373708633
Iteration 50, Training loss = 0.30809725104616237
Iteration 60, Training loss = 0.30094591485193145
Iteration 70, Training loss = 0.2952861805947927
Iteration 80, Training loss = 0.28817297675861764
Iteration 90, Training loss = 0.2833612993932687
Model training time: 16.968621730804443
Device: cuda
Iteration 0, Training loss = 1.0856735511468008
Iteration 10, Training loss = 0.6034946390069448
Iteration 20, Training loss = 0.41133002965496135
Iteration 30, Training loss = 0.3677118717191311
Iteration 40, Training loss = 0.3416964417466751
Iteration 50, Training loss = 0.3278593522711442
Iteration 60, Training loss = 0.31672021460074645
Iteration 70, Training loss = 0.3101532644090744
Iteration 80, Training loss = 0.30336356062728626
Iteration 90, Training loss = 0.29711184894236237
Model training time: 17.029996156692505
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6352348935145599
Iteration 10, Training loss = 0.261050570039795
Iteration 20, Training loss = 0.2498091971501708
Iteration 30, Training loss = 0.23953259019897535
Iteration 40, Training loss = 0.23662885923225147
Iteration 50, Training loss = 0.23208516657065886
Iteration 60, Training loss = 0.23003621958196163
Iteration 70, Training loss = 0.22753554453643468
Iteration 80, Training loss = 0.22619557882157657
Iteration 90, Training loss = 0.22362844216135833
Iteration 100, Training loss = 0.22450193189657652
Iteration 110, Training loss = 0.22227090367904076
Iteration 120, Training loss = 0.22152214024502498
Iteration 130, Training loss = 0.2208525322091121
Iteration 140, Training loss = 0.22019481881020161
Iteration 150, Training loss = 0.22014899824101192
Iteration 160, Training loss = 0.2190346964276754
Iteration 170, Training loss = 0.21990958271691433
Iteration 180, Training loss = 0.21835627893988901
Iteration 190, Training loss = 0.21585069632587525
Model training time: 37.70335555076599
Device: cuda
Iteration 0, Training loss = 0.6181832990394189
Iteration 10, Training loss = 0.26510334158172977
Iteration 20, Training loss = 0.2484417215992625
Iteration 30, Training loss = 0.24136096282074085
Iteration 40, Training loss = 0.23199655574101669
Iteration 50, Training loss = 0.22809916963944069
Iteration 60, Training loss = 0.22273694824140805
Iteration 70, Training loss = 0.22060164006856772
Iteration 80, Training loss = 0.2175408362124402
Iteration 90, Training loss = 0.21531416067423728
Iteration 100, Training loss = 0.2133961385832383
Iteration 110, Training loss = 0.21255189997072405
Iteration 120, Training loss = 0.21146335510107186
Iteration 130, Training loss = 0.21195868588984013
Iteration 140, Training loss = 0.2086704823260124
Iteration 150, Training loss = 0.21030780751831257
Iteration 160, Training loss = 0.20848623992731938
Iteration 170, Training loss = 0.2086508425955589
Iteration 180, Training loss = 0.20881376745036015
Iteration 190, Training loss = 0.20582394860684872
Model training time: 37.85013127326965
Device: cuda
Iteration 0, Training loss = 0.6221811556472228
Iteration 10, Training loss = 0.24805940307963353
Iteration 20, Training loss = 0.23663012153254107
Iteration 30, Training loss = 0.22717623197688505
Iteration 40, Training loss = 0.22198118240787432
Iteration 50, Training loss = 0.21891888947441027
Iteration 60, Training loss = 0.2149664179350321
Iteration 70, Training loss = 0.21128229481669572
Iteration 80, Training loss = 0.20828434753303343
Iteration 90, Training loss = 0.20810695759092385
Iteration 100, Training loss = 0.20485390163958073
Iteration 110, Training loss = 0.20360344720001405
Iteration 120, Training loss = 0.20359974368833578
Iteration 130, Training loss = 0.20165357359040242
Iteration 140, Training loss = 0.20256931821887308
Iteration 150, Training loss = 0.20077752687323552
Iteration 160, Training loss = 0.20013951474370864
Iteration 170, Training loss = 0.19999029394239187
Iteration 180, Training loss = 0.20197422795284253
Iteration 190, Training loss = 0.19909979531971309
Model training time: 37.92313313484192
Device: cuda
Iteration 0, Training loss = 0.7035868663627368
Iteration 10, Training loss = 0.25341906484503013
Iteration 20, Training loss = 0.23729104992861932
Iteration 30, Training loss = 0.23004479190477958
Iteration 40, Training loss = 0.2237086556135462
Iteration 50, Training loss = 0.21605472968747982
Iteration 60, Training loss = 0.21535901481715533
Iteration 70, Training loss = 0.21030813469909704
Iteration 80, Training loss = 0.20965985566950762
Iteration 90, Training loss = 0.20653412627199522
Iteration 100, Training loss = 0.2045659818328344
Iteration 110, Training loss = 0.20235608546779707
Iteration 120, Training loss = 0.203643506177916
Iteration 130, Training loss = 0.20131394954828116
Iteration 140, Training loss = 0.20200709009972903
Iteration 150, Training loss = 0.19991555981911147
Iteration 160, Training loss = 0.20043771076374328
Iteration 170, Training loss = 0.19857309099573356
Iteration 180, Training loss = 0.2003384860089192
Iteration 190, Training loss = 0.1983080395998863
Model training time: 37.4982476234436
Device: cuda
Iteration 0, Training loss = 0.7060812575599322
Iteration 10, Training loss = 0.2500262488252841
Iteration 20, Training loss = 0.23295724119704503
Iteration 30, Training loss = 0.22459912909051546
Iteration 40, Training loss = 0.22068255953490734
Iteration 50, Training loss = 0.21674761978479531
Iteration 60, Training loss = 0.2125756063570197
Iteration 70, Training loss = 0.212046574586286
Iteration 80, Training loss = 0.20925199032689518
Iteration 90, Training loss = 0.20515606824595195
Iteration 100, Training loss = 0.20330889150500298
Iteration 110, Training loss = 0.20136146447979486
Iteration 120, Training loss = 0.20201627322687551
Iteration 130, Training loss = 0.1985929740879398
Iteration 140, Training loss = 0.2003362083521027
Iteration 150, Training loss = 0.1965647594859967
Iteration 160, Training loss = 0.19785857386887074
Iteration 170, Training loss = 0.19652675085056287
Iteration 180, Training loss = 0.1948642308752124
Iteration 190, Training loss = 0.19546620590755573
Model training time: 37.43312931060791
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.04312641116289
Iteration 10, Training loss = 0.8286229349099673
Iteration 20, Training loss = 0.4094874407522954
Iteration 30, Training loss = 0.361772569326254
Iteration 40, Training loss = 0.3424217728181527
Iteration 50, Training loss = 0.3276188066945626
Iteration 60, Training loss = 0.3176099256827281
Iteration 70, Training loss = 0.3095347722275899
Iteration 80, Training loss = 0.3030795490799042
Iteration 90, Training loss = 0.2995917871594429
Iteration 100, Training loss = 0.2919158965635758
Iteration 110, Training loss = 0.2884458081366924
Iteration 120, Training loss = 0.284887020691083
Iteration 130, Training loss = 0.28443469379383785
Iteration 140, Training loss = 0.28082161052868915
Iteration 150, Training loss = 0.2766357578623753
Iteration 160, Training loss = 0.27901934488461566
Iteration 170, Training loss = 0.2736337245083772
Iteration 180, Training loss = 0.27284619856912357
Iteration 190, Training loss = 0.2712441493685429
Model training time: 33.288146018981934
Device: cuda
Iteration 0, Training loss = 0.9710129539553936
Iteration 10, Training loss = 0.5290351954217141
Iteration 20, Training loss = 0.36010577687277245
Iteration 30, Training loss = 0.3319992566337952
Iteration 40, Training loss = 0.31890337068874103
Iteration 50, Training loss = 0.3086077601004105
Iteration 60, Training loss = 0.30446711225578416
Iteration 70, Training loss = 0.30031383925905597
Iteration 80, Training loss = 0.2956669580621215
Iteration 90, Training loss = 0.29337883239182144
Iteration 100, Training loss = 0.2906107186124875
Iteration 110, Training loss = 0.28808615270715493
Iteration 120, Training loss = 0.28511078326174844
Iteration 130, Training loss = 0.2821845644368575
Iteration 140, Training loss = 0.2784017456265596
Iteration 150, Training loss = 0.2773463806280723
Iteration 160, Training loss = 0.2763683619694068
Iteration 170, Training loss = 0.2720187274882427
Iteration 180, Training loss = 0.270478080241726
Iteration 190, Training loss = 0.2700586960865901
Model training time: 33.33950066566467
Device: cuda
Iteration 0, Training loss = 1.0355941166098301
Iteration 10, Training loss = 0.563016318071347
Iteration 20, Training loss = 0.3598967526967709
Iteration 30, Training loss = 0.3328335573180364
Iteration 40, Training loss = 0.3277156473352359
Iteration 50, Training loss = 0.31518195813091904
Iteration 60, Training loss = 0.3087388464751152
Iteration 70, Training loss = 0.3038950920678102
Iteration 80, Training loss = 0.3010497899869314
Iteration 90, Training loss = 0.296755204263788
Iteration 100, Training loss = 0.2939454640906591
Iteration 110, Training loss = 0.28828000062360215
Iteration 120, Training loss = 0.2880143490261756
Iteration 130, Training loss = 0.28400817427497643
Iteration 140, Training loss = 0.2810877889681321
Iteration 150, Training loss = 0.27874185144901276
Iteration 160, Training loss = 0.27698069252073765
Iteration 170, Training loss = 0.2743608167824837
Iteration 180, Training loss = 0.272237056436447
Iteration 190, Training loss = 0.26952173174000704
Model training time: 33.11501955986023
Device: cuda
Iteration 0, Training loss = 1.008332463984306
Iteration 10, Training loss = 0.5481996051967144
Iteration 20, Training loss = 0.394124239969712
Iteration 30, Training loss = 0.3477496011899068
Iteration 40, Training loss = 0.3278606300457166
Iteration 50, Training loss = 0.31937907894070333
Iteration 60, Training loss = 0.31249981387876546
Iteration 70, Training loss = 0.30627148412168026
Iteration 80, Training loss = 0.30206158184088194
Iteration 90, Training loss = 0.2987307539353004
Iteration 100, Training loss = 0.29434903682424474
Iteration 110, Training loss = 0.29102587098112476
Iteration 120, Training loss = 0.2880359270538275
Iteration 130, Training loss = 0.2859101647940966
Iteration 140, Training loss = 0.28232385275455624
Iteration 150, Training loss = 0.2819495524924535
Iteration 160, Training loss = 0.27929378773730534
Iteration 170, Training loss = 0.2770570096774743
Iteration 180, Training loss = 0.2748038103947273
Iteration 190, Training loss = 0.27541014494804233
Model training time: 33.47496747970581
Device: cuda
Iteration 0, Training loss = 0.9905220734385344
Iteration 10, Training loss = 0.5695434023554509
Iteration 20, Training loss = 0.3520564850992881
Iteration 30, Training loss = 0.32657772531876195
Iteration 40, Training loss = 0.31629380641075283
Iteration 50, Training loss = 0.3073132555358685
Iteration 60, Training loss = 0.3034882862120867
Iteration 70, Training loss = 0.2993791200793706
Iteration 80, Training loss = 0.2946315400588971
Iteration 90, Training loss = 0.2910320106893778
Iteration 100, Training loss = 0.2875701875354235
Iteration 110, Training loss = 0.28320359281049323
Iteration 120, Training loss = 0.281237694936303
Iteration 130, Training loss = 0.277960530983714
Iteration 140, Training loss = 0.27626057943472493
Iteration 150, Training loss = 0.27629532392781514
Iteration 160, Training loss = 0.271977587388112
Iteration 170, Training loss = 0.2713606965083342
Iteration 180, Training loss = 0.2692318842388116
Iteration 190, Training loss = 0.26904440098083937
Model training time: 33.277916431427
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7099099617737991
Iteration 10, Training loss = 0.2619838703137178
Iteration 20, Training loss = 0.2530173071875022
Iteration 30, Training loss = 0.2440964479285937
Iteration 40, Training loss = 0.24061943805561617
Iteration 50, Training loss = 0.23676602757320955
Iteration 60, Training loss = 0.23449601154201305
Iteration 70, Training loss = 0.23239837644191888
Iteration 80, Training loss = 0.231429961653283
Iteration 90, Training loss = 0.23048387467861176
Iteration 100, Training loss = 0.23128415644168854
Iteration 110, Training loss = 0.2296974276407407
Iteration 120, Training loss = 0.23002043044051299
Iteration 130, Training loss = 0.22859270927997735
Iteration 140, Training loss = 0.2270887177437544
Iteration 150, Training loss = 0.22701129131019115
Iteration 160, Training loss = 0.22810429520905018
Iteration 170, Training loss = 0.2280640800555165
Iteration 180, Training loss = 0.22687596469544447
Iteration 190, Training loss = 0.22559675789223269
Model training time: 37.238075256347656
Device: cuda
Iteration 0, Training loss = 0.7965565696358681
Iteration 10, Training loss = 0.2648439771280839
Iteration 20, Training loss = 0.2527336933864997
Iteration 30, Training loss = 0.2504244693913139
Iteration 40, Training loss = 0.24682388153786844
Iteration 50, Training loss = 0.24340478889644146
Iteration 60, Training loss = 0.24052796691942674
Iteration 70, Training loss = 0.23634849352618822
Iteration 80, Training loss = 0.23612857137161952
Iteration 90, Training loss = 0.23319661846527687
Iteration 100, Training loss = 0.2337310818525461
Iteration 110, Training loss = 0.2298793179484514
Iteration 120, Training loss = 0.23076146368223888
Iteration 130, Training loss = 0.22772255554222143
Iteration 140, Training loss = 0.22784436594408292
Iteration 150, Training loss = 0.22694004527651346
Iteration 160, Training loss = 0.22613359042085135
Iteration 170, Training loss = 0.22152035461308864
Iteration 180, Training loss = 0.22291456958135733
Iteration 190, Training loss = 0.21831703365135652
Model training time: 37.3047513961792
Device: cuda
Iteration 0, Training loss = 0.7662123097823217
Iteration 10, Training loss = 0.2674966294031877
Iteration 20, Training loss = 0.2519566135910841
Iteration 30, Training loss = 0.24304699152708054
Iteration 40, Training loss = 0.23622060767733133
Iteration 50, Training loss = 0.2347992561184443
Iteration 60, Training loss = 0.22886261386940113
Iteration 70, Training loss = 0.22506775248509187
Iteration 80, Training loss = 0.22420147218956396
Iteration 90, Training loss = 0.22204732472220293
Iteration 100, Training loss = 0.22136525379923674
Iteration 110, Training loss = 0.21872008720842692
Iteration 120, Training loss = 0.21878073020623282
Iteration 130, Training loss = 0.2171077410905407
Iteration 140, Training loss = 0.21632913356790176
Iteration 150, Training loss = 0.21573583889179504
Iteration 160, Training loss = 0.21591878596406716
Iteration 170, Training loss = 0.2167014400832928
Iteration 180, Training loss = 0.21674124194452396
Iteration 190, Training loss = 0.21526066299814445
Model training time: 37.172234296798706
Device: cuda
Iteration 0, Training loss = 0.6849403633521154
Iteration 10, Training loss = 0.2592617030470417
Iteration 20, Training loss = 0.24922816211787555
Iteration 30, Training loss = 0.24491551847985157
Iteration 40, Training loss = 0.24004377553669307
Iteration 50, Training loss = 0.23761532548815012
Iteration 60, Training loss = 0.2351702325619184
Iteration 70, Training loss = 0.23051049510160318
Iteration 80, Training loss = 0.2293923098880511
Iteration 90, Training loss = 0.22880799172875974
Iteration 100, Training loss = 0.2281766624118273
Iteration 110, Training loss = 0.22553412401332304
Iteration 120, Training loss = 0.22480577559998402
Iteration 130, Training loss = 0.2245557429985358
Iteration 140, Training loss = 0.22309479912599692
Iteration 150, Training loss = 0.2244168772147252
Iteration 160, Training loss = 0.22108291705640462
Iteration 170, Training loss = 0.2210925417737319
Iteration 180, Training loss = 0.21951897184436137
Iteration 190, Training loss = 0.21950267169338006
Model training time: 37.13671088218689
Device: cuda
Iteration 0, Training loss = 0.7756592760292383
Iteration 10, Training loss = 0.26273930130096584
Iteration 20, Training loss = 0.24898198433220387
Iteration 30, Training loss = 0.24178375246433112
Iteration 40, Training loss = 0.2358959406041182
Iteration 50, Training loss = 0.23380024229677823
Iteration 60, Training loss = 0.2296578371180938
Iteration 70, Training loss = 0.22532057375288927
Iteration 80, Training loss = 0.2221058915154292
Iteration 90, Training loss = 0.22468524196973214
Iteration 100, Training loss = 0.22155187665843046
Iteration 110, Training loss = 0.2197939371690154
Iteration 120, Training loss = 0.21753820633658996
Iteration 130, Training loss = 0.21565278418935263
Iteration 140, Training loss = 0.21655903097528678
Iteration 150, Training loss = 0.2143581327624046
Iteration 160, Training loss = 0.21608516108244658
Iteration 170, Training loss = 0.21567229174364072
Iteration 180, Training loss = 0.21329897620643562
Iteration 190, Training loss = 0.21637900818425876
Model training time: 37.67489528656006
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0720757154318004
Iteration 10, Training loss = 0.8902257405794584
Iteration 20, Training loss = 0.5916685863183095
Iteration 30, Training loss = 0.47346265169863516
Iteration 40, Training loss = 0.419749608120093
Iteration 50, Training loss = 0.3898180526896165
Iteration 60, Training loss = 0.3727713738783048
Iteration 70, Training loss = 0.3531900532543659
Iteration 80, Training loss = 0.34380170330405235
Iteration 90, Training loss = 0.3332761308321586
Iteration 100, Training loss = 0.32090181742723173
Iteration 110, Training loss = 0.3126813335200915
Iteration 120, Training loss = 0.3055212835852916
Iteration 130, Training loss = 0.30095194108211076
Iteration 140, Training loss = 0.29407165543391156
Iteration 150, Training loss = 0.2915454771942817
Iteration 160, Training loss = 0.2857760600745678
Iteration 170, Training loss = 0.2830470675745836
Iteration 180, Training loss = 0.2797339282070215
Iteration 190, Training loss = 0.2778580920914045
Model training time: 33.22129583358765
Device: cuda
Iteration 0, Training loss = 1.094732800928446
Iteration 10, Training loss = 0.5489319757773325
Iteration 20, Training loss = 0.3902297909729756
Iteration 30, Training loss = 0.3510180853593808
Iteration 40, Training loss = 0.3354280102424897
Iteration 50, Training loss = 0.32322813083346075
Iteration 60, Training loss = 0.31799943573199785
Iteration 70, Training loss = 0.3138627146299069
Iteration 80, Training loss = 0.3091709977732255
Iteration 90, Training loss = 0.3068724214457549
Iteration 100, Training loss = 0.30341293648458445
Iteration 110, Training loss = 0.29939728359190315
Iteration 120, Training loss = 0.29772681914843047
Iteration 130, Training loss = 0.29685609157268816
Iteration 140, Training loss = 0.29428740003361153
Iteration 150, Training loss = 0.29306839635738957
Iteration 160, Training loss = 0.29003355738062125
Iteration 170, Training loss = 0.2884210368188528
Iteration 180, Training loss = 0.28531452956107944
Iteration 190, Training loss = 0.2864046704310637
Model training time: 33.17961025238037
Device: cuda
Iteration 0, Training loss = 1.0490469153110797
Iteration 10, Training loss = 0.8623274026008753
Iteration 20, Training loss = 0.5478406012631379
Iteration 30, Training loss = 0.4498305306411706
Iteration 40, Training loss = 0.406114613207487
Iteration 50, Training loss = 0.377700156174027
Iteration 60, Training loss = 0.35548381378444344
Iteration 70, Training loss = 0.34012332658928174
Iteration 80, Training loss = 0.32704673155855674
Iteration 90, Training loss = 0.31870848413270253
Iteration 100, Training loss = 0.31046752946881145
Iteration 110, Training loss = 0.3105567412880751
Iteration 120, Training loss = 0.30020506479419196
Iteration 130, Training loss = 0.296209813740391
Iteration 140, Training loss = 0.29065086205418295
Iteration 150, Training loss = 0.2874810318820752
Iteration 160, Training loss = 0.28529952753048676
Iteration 170, Training loss = 0.2808779295390615
Iteration 180, Training loss = 0.2816563301648085
Iteration 190, Training loss = 0.27777301319516623
Model training time: 33.289631843566895
Device: cuda
Iteration 0, Training loss = 1.1137913923997145
Iteration 10, Training loss = 0.49235716605415714
Iteration 20, Training loss = 0.3616821820346209
Iteration 30, Training loss = 0.3378623491869523
Iteration 40, Training loss = 0.32587510719895363
Iteration 50, Training loss = 0.3197293251466293
Iteration 60, Training loss = 0.3104807804696835
Iteration 70, Training loss = 0.30503274285449433
Iteration 80, Training loss = 0.30002734896082145
Iteration 90, Training loss = 0.29768192395567894
Iteration 100, Training loss = 0.2936757258497752
Iteration 110, Training loss = 0.28750105732335496
Iteration 120, Training loss = 0.2845762327599984
Iteration 130, Training loss = 0.2816291404171632
Iteration 140, Training loss = 0.280854306398676
Iteration 150, Training loss = 0.2787471376359463
Iteration 160, Training loss = 0.27731783931645065
Iteration 170, Training loss = 0.27360353977061236
Iteration 180, Training loss = 0.2732280845252367
Iteration 190, Training loss = 0.2701501099822613
Model training time: 33.081836462020874
Device: cuda
Iteration 0, Training loss = 1.1402710062953143
Iteration 10, Training loss = 0.5023814015663587
Iteration 20, Training loss = 0.3571680085017131
Iteration 30, Training loss = 0.3363386646199685
Iteration 40, Training loss = 0.32920541972495043
Iteration 50, Training loss = 0.32411174347194344
Iteration 60, Training loss = 0.3146005386056808
Iteration 70, Training loss = 0.3144073494924949
Iteration 80, Training loss = 0.3088956670119212
Iteration 90, Training loss = 0.30487678285974723
Iteration 100, Training loss = 0.301465367898345
Iteration 110, Training loss = 0.29818376082067305
Iteration 120, Training loss = 0.29660358007710713
Iteration 130, Training loss = 0.29234911859608614
Iteration 140, Training loss = 0.2902148934797599
Iteration 150, Training loss = 0.2880444767383429
Iteration 160, Training loss = 0.2869507375244911
Iteration 170, Training loss = 0.28417776181147647
Iteration 180, Training loss = 0.2819501045518197
Iteration 190, Training loss = 0.27953142300248146
Model training time: 33.384915590286255
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7211807711193194
Iteration 10, Training loss = 0.25693419403754747
Iteration 20, Training loss = 0.24637280261287323
Iteration 30, Training loss = 0.24189233822891346
Iteration 40, Training loss = 0.23775370619617975
Iteration 50, Training loss = 0.23619802095569098
Iteration 60, Training loss = 0.23380567384167358
Iteration 70, Training loss = 0.23345208168029785
Iteration 80, Training loss = 0.23104617801996377
Iteration 90, Training loss = 0.23026367040494314
Iteration 100, Training loss = 0.22892311516289526
Iteration 110, Training loss = 0.22978221209576496
Iteration 120, Training loss = 0.22568658373963374
Iteration 130, Training loss = 0.22672888930313861
Iteration 140, Training loss = 0.2269985227344128
Iteration 150, Training loss = 0.22786546749277756
Iteration 160, Training loss = 0.22411012678192213
Iteration 170, Training loss = 0.22460713158719814
Iteration 180, Training loss = 0.2226460833962147
Iteration 190, Training loss = 0.2224824634165718
Model training time: 37.592427015304565
Device: cuda
Iteration 0, Training loss = 0.7557609693075602
Iteration 10, Training loss = 0.2630072936702233
Iteration 20, Training loss = 0.24412776300540337
Iteration 30, Training loss = 0.23641353007405996
Iteration 40, Training loss = 0.23213905769471938
Iteration 50, Training loss = 0.2274620495020197
Iteration 60, Training loss = 0.22422504288932452
Iteration 70, Training loss = 0.21881841595929402
Iteration 80, Training loss = 0.21652435461202493
Iteration 90, Training loss = 0.2155822075616855
Iteration 100, Training loss = 0.2134380626420562
Iteration 110, Training loss = 0.212229379094564
Iteration 120, Training loss = 0.2119960175970426
Iteration 130, Training loss = 0.21178463094223005
Iteration 140, Training loss = 0.21091671660542488
Iteration 150, Training loss = 0.21049900625187618
Iteration 160, Training loss = 0.21183768709978232
Iteration 170, Training loss = 0.2083376764964599
Iteration 180, Training loss = 0.2095045712418281
Iteration 190, Training loss = 0.20785753830121115
Model training time: 37.641382932662964
Device: cuda
Iteration 0, Training loss = 0.7140255753810589
Iteration 10, Training loss = 0.2560998760163784
Iteration 20, Training loss = 0.242824986577034
Iteration 30, Training loss = 0.23594926627209553
Iteration 40, Training loss = 0.23095610895409033
Iteration 50, Training loss = 0.2294351521592874
Iteration 60, Training loss = 0.2239547619023002
Iteration 70, Training loss = 0.22231711798275894
Iteration 80, Training loss = 0.22062323650775048
Iteration 90, Training loss = 0.21945789442039454
Iteration 100, Training loss = 0.21979486082608884
Iteration 110, Training loss = 0.21522471638253102
Iteration 120, Training loss = 0.2137340850984821
Iteration 130, Training loss = 0.21276907035364553
Iteration 140, Training loss = 0.21085272306719652
Iteration 150, Training loss = 0.21006273220364863
Iteration 160, Training loss = 0.21071250564776933
Iteration 170, Training loss = 0.2076294022397353
Iteration 180, Training loss = 0.20484069971224436
Iteration 190, Training loss = 0.2062701006205036
Model training time: 37.41432166099548
Device: cuda
Iteration 0, Training loss = 0.961185194265384
Iteration 10, Training loss = 0.27275121154693455
Iteration 20, Training loss = 0.24851410850309408
Iteration 30, Training loss = 0.23853190529804963
Iteration 40, Training loss = 0.23154790355609015
Iteration 50, Training loss = 0.22668400034308434
Iteration 60, Training loss = 0.22684595762537077
Iteration 70, Training loss = 0.22425989613223535
Iteration 80, Training loss = 0.22055302990170625
Iteration 90, Training loss = 0.2212361405388667
Iteration 100, Training loss = 0.2170288612206395
Iteration 110, Training loss = 0.217185187440079
Iteration 120, Training loss = 0.21572044276847288
Iteration 130, Training loss = 0.21589146783718696
Iteration 140, Training loss = 0.21768191392318562
Iteration 150, Training loss = 0.21627179977412408
Iteration 160, Training loss = 0.21486297421730483
Iteration 170, Training loss = 0.21397168275255424
Iteration 180, Training loss = 0.2138065263055838
Iteration 190, Training loss = 0.21041718809507215
Model training time: 41.11149501800537
Device: cuda
Iteration 0, Training loss = 0.7360171036651502
Iteration 10, Training loss = 0.2593823242932558
Iteration 20, Training loss = 0.2473618439756907
Iteration 30, Training loss = 0.23846456709389502
Iteration 40, Training loss = 0.23418390278059703
Iteration 50, Training loss = 0.23345426119004303
Iteration 60, Training loss = 0.2279401819150035
Iteration 70, Training loss = 0.22681055633494487
Iteration 80, Training loss = 0.22345866155452454
Iteration 90, Training loss = 0.22273750567378905
Iteration 100, Training loss = 0.22067614845358408
Iteration 110, Training loss = 0.21850663559654585
Iteration 120, Training loss = 0.21684238995210484
Iteration 130, Training loss = 0.2171090324051105
Iteration 140, Training loss = 0.2169394099081938
Iteration 150, Training loss = 0.21250230124076971
Iteration 160, Training loss = 0.2119388858286234
Iteration 170, Training loss = 0.2119025459083227
Iteration 180, Training loss = 0.2114965497301175
Iteration 190, Training loss = 0.20991930601975092
Model training time: 41.87876558303833
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0316733471476114
Iteration 10, Training loss = 0.7086675324692175
Iteration 20, Training loss = 0.40264576874100244
Iteration 30, Training loss = 0.36056041660217136
Iteration 40, Training loss = 0.3429296420743832
Iteration 50, Training loss = 0.3327505188779189
Iteration 60, Training loss = 0.3253000519023492
Iteration 70, Training loss = 0.32081216086561865
Iteration 80, Training loss = 0.3189177316828416
Iteration 90, Training loss = 0.31156258714886814
Iteration 100, Training loss = 0.3081100080162287
Iteration 110, Training loss = 0.30403798087858236
Iteration 120, Training loss = 0.30073720302719337
Iteration 130, Training loss = 0.2986286978882093
Iteration 140, Training loss = 0.29588463859489333
Iteration 150, Training loss = 0.29615994084339875
Iteration 160, Training loss = 0.2914706709293219
Iteration 170, Training loss = 0.2918923253623339
Iteration 180, Training loss = 0.28757695538493305
Iteration 190, Training loss = 0.2850841017296681
Model training time: 36.391016244888306
Device: cuda
Iteration 0, Training loss = 1.0292359974521856
Iteration 10, Training loss = 0.5515837488839259
Iteration 20, Training loss = 0.35116892866790295
Iteration 30, Training loss = 0.32986584606652075
Iteration 40, Training loss = 0.31850606052634806
Iteration 50, Training loss = 0.3113066873584802
Iteration 60, Training loss = 0.3053741478002988
Iteration 70, Training loss = 0.3021067538513587
Iteration 80, Training loss = 0.2926525832512058
Iteration 90, Training loss = 0.2882150807059728
Iteration 100, Training loss = 0.28410040214657784
Iteration 110, Training loss = 0.2812626933535704
Iteration 120, Training loss = 0.27830715319858146
Iteration 130, Training loss = 0.27549453638494015
Iteration 140, Training loss = 0.27582603664352345
Iteration 150, Training loss = 0.270785082418185
Iteration 160, Training loss = 0.26967833663981694
Iteration 170, Training loss = 0.26892780526899374
Iteration 180, Training loss = 0.26784932885605556
Iteration 190, Training loss = 0.2653802252876071
Model training time: 34.31086492538452
Device: cuda
Iteration 0, Training loss = 1.076378691654939
Iteration 10, Training loss = 0.5673515719289963
Iteration 20, Training loss = 0.364405263788425
Iteration 30, Training loss = 0.34069276257203174
Iteration 40, Training loss = 0.3305066872674685
Iteration 50, Training loss = 0.32285774026352626
Iteration 60, Training loss = 0.3171597531495186
Iteration 70, Training loss = 0.31372992422145146
Iteration 80, Training loss = 0.30991241708397865
Iteration 90, Training loss = 0.3063060880567019
Iteration 100, Training loss = 0.299732470096877
Iteration 110, Training loss = 0.2971991471086557
Iteration 120, Training loss = 0.29257219886550534
Iteration 130, Training loss = 0.2931673996723615
Iteration 140, Training loss = 0.2879555629423031
Iteration 150, Training loss = 0.28443341472974193
Iteration 160, Training loss = 0.28115496397591555
Iteration 170, Training loss = 0.2795059136473216
Iteration 180, Training loss = 0.27649162981945735
Iteration 190, Training loss = 0.2743572654345861
Model training time: 33.59463882446289
Device: cuda
Iteration 0, Training loss = 1.0026618253726225
Iteration 10, Training loss = 0.6692886226452314
Iteration 20, Training loss = 0.4509958942922262
Iteration 30, Training loss = 0.39776063638810927
Iteration 40, Training loss = 0.36393921363812226
Iteration 50, Training loss = 0.3437311521802957
Iteration 60, Training loss = 0.33070064350389516
Iteration 70, Training loss = 0.3208268852188037
Iteration 80, Training loss = 0.3136009330359789
Iteration 90, Training loss = 0.31037250261467236
Iteration 100, Training loss = 0.3043972819757003
Iteration 110, Training loss = 0.2994600091702663
Iteration 120, Training loss = 0.2947497565585833
Iteration 130, Training loss = 0.292782752129894
Iteration 140, Training loss = 0.2877968197258619
Iteration 150, Training loss = 0.2843567244708538
Iteration 160, Training loss = 0.28253150516404557
Iteration 170, Training loss = 0.2789426011821398
Iteration 180, Training loss = 0.27601483234992397
Iteration 190, Training loss = 0.274213830438944
Model training time: 33.25545310974121
Device: cuda
Iteration 0, Training loss = 0.9867377367157203
Iteration 10, Training loss = 0.801612077997281
Iteration 20, Training loss = 0.3968634754419327
Iteration 30, Training loss = 0.3468776709185197
Iteration 40, Training loss = 0.3259923174404181
Iteration 50, Training loss = 0.3147605462716176
Iteration 60, Training loss = 0.3116026705560776
Iteration 70, Training loss = 0.30425792330732715
Iteration 80, Training loss = 0.3021468831082949
Iteration 90, Training loss = 0.2996537540681087
Iteration 100, Training loss = 0.29225466400384903
Iteration 110, Training loss = 0.28908433937109435
Iteration 120, Training loss = 0.28717445180966306
Iteration 130, Training loss = 0.2829469366428944
Iteration 140, Training loss = 0.2828202521285185
Iteration 150, Training loss = 0.2787014607053537
Iteration 160, Training loss = 0.2772454774150482
Iteration 170, Training loss = 0.2754983360377642
Iteration 180, Training loss = 0.2735753576629437
Iteration 190, Training loss = 0.27216643639481986
Model training time: 33.594539642333984
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6491531875844185
Iteration 10, Training loss = 0.25086958032961076
Iteration 20, Training loss = 0.23567497651450908
Iteration 30, Training loss = 0.22481942850236708
Iteration 40, Training loss = 0.21595573067091978
Iteration 50, Training loss = 0.21039863575536472
Iteration 60, Training loss = 0.20740271560274637
Iteration 70, Training loss = 0.20771138713910028
Iteration 80, Training loss = 0.20159854453343612
Iteration 90, Training loss = 0.19986413576855108
Iteration 100, Training loss = 0.2035088210295026
Iteration 110, Training loss = 0.19673935123361075
Iteration 120, Training loss = 0.19858105670517454
Iteration 130, Training loss = 0.19717691208307558
Iteration 140, Training loss = 0.19729521139883077
Iteration 150, Training loss = 0.19624547903927472
Iteration 160, Training loss = 0.1958169432786795
Iteration 170, Training loss = 0.19685473164113668
Iteration 180, Training loss = 0.19478187717210788
Iteration 190, Training loss = 0.19364527584268496
Iteration 200, Training loss = 0.19537225208030298
Iteration 210, Training loss = 0.19448157915702233
Iteration 220, Training loss = 0.19452434649261144
Iteration 230, Training loss = 0.19447238435252354
Iteration 240, Training loss = 0.19310837561407915
Iteration 250, Training loss = 0.1928813477548269
Iteration 260, Training loss = 0.1934655621791115
Iteration 270, Training loss = 0.192182786261233
Iteration 280, Training loss = 0.1952443544824536
Iteration 290, Training loss = 0.19147006823466375
Model training time: 56.28714561462402
Device: cuda
Iteration 0, Training loss = 0.6523255149905498
Iteration 10, Training loss = 0.25296727381646633
Iteration 20, Training loss = 0.24515468197373244
Iteration 30, Training loss = 0.2394599771270385
Iteration 40, Training loss = 0.23036180236018622
Iteration 50, Training loss = 0.2238466557688438
Iteration 60, Training loss = 0.22076792914706928
Iteration 70, Training loss = 0.21999381582897443
Iteration 80, Training loss = 0.21534026392663902
Iteration 90, Training loss = 0.21172936948446128
Iteration 100, Training loss = 0.21009334004842317
Iteration 110, Training loss = 0.20740924789928472
Iteration 120, Training loss = 0.20518489525868341
Iteration 130, Training loss = 0.2038277154071973
Iteration 140, Training loss = 0.20319185835810807
Iteration 150, Training loss = 0.20133799873292446
Iteration 160, Training loss = 0.2002677032007621
Iteration 170, Training loss = 0.19991352975081939
Iteration 180, Training loss = 0.19714713619592097
Iteration 190, Training loss = 0.1959708585188939
Iteration 200, Training loss = 0.1958004696151385
Iteration 210, Training loss = 0.196440542403322
Iteration 220, Training loss = 0.19373846261833721
Iteration 230, Training loss = 0.19190636871812436
Iteration 240, Training loss = 0.19285095690821225
Iteration 250, Training loss = 0.19368648607856953
Iteration 260, Training loss = 0.19318524260933584
Iteration 270, Training loss = 0.19118767668708012
Iteration 280, Training loss = 0.19190201320900366
Iteration 290, Training loss = 0.19018534771524942
Model training time: 55.677643060684204
Device: cuda
Iteration 0, Training loss = 0.7338659322032561
Iteration 10, Training loss = 0.24990793116963828
Iteration 20, Training loss = 0.2358919968112157
Iteration 30, Training loss = 0.23119370080530643
Iteration 40, Training loss = 0.2239145701034711
Iteration 50, Training loss = 0.21930723895247167
Iteration 60, Training loss = 0.21740986636051765
Iteration 70, Training loss = 0.21382560430524442
Iteration 80, Training loss = 0.21429185691074684
Iteration 90, Training loss = 0.21354326290579942
Iteration 100, Training loss = 0.21169751818076923
Iteration 110, Training loss = 0.21152339245264346
Iteration 120, Training loss = 0.2124854184400577
Iteration 130, Training loss = 0.20941781338590842
Iteration 140, Training loss = 0.20883136170987898
Iteration 150, Training loss = 0.21109455931358612
Iteration 160, Training loss = 0.21016688802494451
Iteration 170, Training loss = 0.20837706162665898
Iteration 180, Training loss = 0.20925538363651588
Iteration 190, Training loss = 0.208178634612033
Iteration 200, Training loss = 0.2093808906725966
Iteration 210, Training loss = 0.20894738995971587
Iteration 220, Training loss = 0.20580738890342987
Iteration 230, Training loss = 0.20844662877229544
Iteration 240, Training loss = 0.20601066767882842
Iteration 250, Training loss = 0.20334190868127805
Iteration 260, Training loss = 0.20498786780696648
Iteration 270, Training loss = 0.2031957436209688
Iteration 280, Training loss = 0.20570734343849695
Iteration 290, Training loss = 0.20442469317752582
Model training time: 55.82287263870239
Device: cuda
Iteration 0, Training loss = 0.726634849436008
Iteration 10, Training loss = 0.2510281757881435
Iteration 20, Training loss = 0.2414627394710596
Iteration 30, Training loss = 0.23472512183854213
Iteration 40, Training loss = 0.2256240747295893
Iteration 50, Training loss = 0.22000468679918692
Iteration 60, Training loss = 0.21405652921216992
Iteration 70, Training loss = 0.2134872003673361
Iteration 80, Training loss = 0.20989702856884554
Iteration 90, Training loss = 0.20619552081020978
Iteration 100, Training loss = 0.2042675675967565
Iteration 110, Training loss = 0.20355636884386724
Iteration 120, Training loss = 0.20234975104148573
Iteration 130, Training loss = 0.2015362004391276
Iteration 140, Training loss = 0.20066141244024038
Iteration 150, Training loss = 0.19971454652169576
Iteration 160, Training loss = 0.19961759304771057
Iteration 170, Training loss = 0.2010966888987101
Iteration 180, Training loss = 0.20013303424303347
Iteration 190, Training loss = 0.20056663152690118
Iteration 200, Training loss = 0.19773008860647678
Iteration 210, Training loss = 0.19710178666103345
Iteration 220, Training loss = 0.19700117258784863
Iteration 230, Training loss = 0.19572616798373368
Iteration 240, Training loss = 0.1970451487801396
Iteration 250, Training loss = 0.19626322066268095
Iteration 260, Training loss = 0.19692593010572287
Iteration 270, Training loss = 0.196140311873303
Iteration 280, Training loss = 0.19588731021548694
Iteration 290, Training loss = 0.19384190970315382
Model training time: 56.86908769607544
Device: cuda
Iteration 0, Training loss = 0.6303633889899805
Iteration 10, Training loss = 0.25254291919274974
Iteration 20, Training loss = 0.23685869159033665
Iteration 30, Training loss = 0.22676855533455426
Iteration 40, Training loss = 0.2226873473622478
Iteration 50, Training loss = 0.21873047174169466
Iteration 60, Training loss = 0.21593033006558052
Iteration 70, Training loss = 0.21323123309188163
Iteration 80, Training loss = 0.21296680704332316
Iteration 90, Training loss = 0.2110378434165166
Iteration 100, Training loss = 0.209473449163712
Iteration 110, Training loss = 0.20356342131988361
Iteration 120, Training loss = 0.20409014849708632
Iteration 130, Training loss = 0.20312657866340417
Iteration 140, Training loss = 0.2025975760502311
Iteration 150, Training loss = 0.2022373348187942
Iteration 160, Training loss = 0.2020726242604164
Iteration 170, Training loss = 0.20166596526709887
Iteration 180, Training loss = 0.2013698062644555
Iteration 190, Training loss = 0.19972971776643625
Iteration 200, Training loss = 0.1986956808429498
Iteration 210, Training loss = 0.2027040280831548
Iteration 220, Training loss = 0.19915245767109668
Iteration 230, Training loss = 0.19980816399821869
Iteration 240, Training loss = 0.20009068285043424
Iteration 250, Training loss = 0.2001129092935186
Iteration 260, Training loss = 0.19916853308677673
Iteration 270, Training loss = 0.19901305766632923
Iteration 280, Training loss = 0.19816836370871618
Iteration 290, Training loss = 0.19923332813554084
Model training time: 56.505205392837524
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1628298203532512
Iteration 10, Training loss = 0.5847977010103372
Iteration 20, Training loss = 0.37630891613662243
Iteration 30, Training loss = 0.34039350828299153
Iteration 40, Training loss = 0.3219860683266933
Iteration 50, Training loss = 0.31189327156887603
Iteration 60, Training loss = 0.30682889013909376
Iteration 70, Training loss = 0.30130261168456995
Iteration 80, Training loss = 0.2964187846160852
Iteration 90, Training loss = 0.2936351186094376
Iteration 100, Training loss = 0.29063208043002164
Iteration 110, Training loss = 0.2852921157788772
Iteration 120, Training loss = 0.2828903663903475
Iteration 130, Training loss = 0.27984213915008765
Iteration 140, Training loss = 0.27855888496224696
Iteration 150, Training loss = 0.2768218010090865
Iteration 160, Training loss = 0.27535780820135886
Iteration 170, Training loss = 0.2709518994849462
Iteration 180, Training loss = 0.27023479328132594
Iteration 190, Training loss = 0.26941392140892834
Iteration 200, Training loss = 0.26681345987778443
Iteration 210, Training loss = 0.26670923384909445
Iteration 220, Training loss = 0.26471222593234134
Iteration 230, Training loss = 0.26379373841560805
Iteration 240, Training loss = 0.2630013438084951
Iteration 250, Training loss = 0.262283085200649
Iteration 260, Training loss = 0.26159671364495385
Iteration 270, Training loss = 0.26279925268429977
Iteration 280, Training loss = 0.2594461292028427
Iteration 290, Training loss = 0.25988908756810886
Model training time: 49.616650104522705
Device: cuda
Iteration 0, Training loss = 1.0551193912441914
Iteration 10, Training loss = 0.7026576187748176
Iteration 20, Training loss = 0.4036670681089163
Iteration 30, Training loss = 0.3608939488633321
Iteration 40, Training loss = 0.3366252506295076
Iteration 50, Training loss = 0.3219888691718762
Iteration 60, Training loss = 0.30957578924986034
Iteration 70, Training loss = 0.3034850141176811
Iteration 80, Training loss = 0.29806656748629534
Iteration 90, Training loss = 0.2926733490939324
Iteration 100, Training loss = 0.2896967719380672
Iteration 110, Training loss = 0.2863052278183974
Iteration 120, Training loss = 0.2827794305407084
Iteration 130, Training loss = 0.2768913915810677
Iteration 140, Training loss = 0.2751788034175451
Iteration 150, Training loss = 0.27317847383136934
Iteration 160, Training loss = 0.2702323996390288
Iteration 170, Training loss = 0.26817170817118424
Iteration 180, Training loss = 0.2661336955542748
Iteration 190, Training loss = 0.2663484366180805
Iteration 200, Training loss = 0.2645572307877816
Iteration 210, Training loss = 0.26243518923337644
Iteration 220, Training loss = 0.26418057162887776
Iteration 230, Training loss = 0.26109255420473904
Iteration 240, Training loss = 0.26057067800026673
Iteration 250, Training loss = 0.25877803922272646
Iteration 260, Training loss = 0.25821653423974145
Iteration 270, Training loss = 0.257547909537187
Iteration 280, Training loss = 0.25754493618240726
Iteration 290, Training loss = 0.2570596102338571
Model training time: 50.29552388191223
Device: cuda
Iteration 0, Training loss = 1.0087070545324912
Iteration 10, Training loss = 0.6415889346255705
Iteration 20, Training loss = 0.410568644794134
Iteration 30, Training loss = 0.3654268366069748
Iteration 40, Training loss = 0.34502565760452014
Iteration 50, Training loss = 0.33314415411307263
Iteration 60, Training loss = 0.32838961854577065
Iteration 70, Training loss = 0.320996491668316
Iteration 80, Training loss = 0.31527675373049885
Iteration 90, Training loss = 0.31325986766471314
Iteration 100, Training loss = 0.30745557165489745
Iteration 110, Training loss = 0.30350471287965775
Iteration 120, Training loss = 0.29977367116281617
Iteration 130, Training loss = 0.29723682498129517
Iteration 140, Training loss = 0.29449541365297943
Iteration 150, Training loss = 0.29038912444733656
Iteration 160, Training loss = 0.2886791858248986
Iteration 170, Training loss = 0.28678228419560653
Iteration 180, Training loss = 0.2853691027714656
Iteration 190, Training loss = 0.2815592005275763
Iteration 200, Training loss = 0.2807528581470251
Iteration 210, Training loss = 0.27819940705712026
Iteration 220, Training loss = 0.2767686714919714
Iteration 230, Training loss = 0.2753153079404281
Iteration 240, Training loss = 0.27452730244168866
Iteration 250, Training loss = 0.27272936357901645
Iteration 260, Training loss = 0.2712544735807639
Iteration 270, Training loss = 0.27178446031533754
Iteration 280, Training loss = 0.26920566163383997
Iteration 290, Training loss = 0.2675685303715559
Model training time: 49.9850378036499
Device: cuda
Iteration 0, Training loss = 0.9403240646307285
Iteration 10, Training loss = 0.48624167390740836
Iteration 20, Training loss = 0.3977957238944677
Iteration 30, Training loss = 0.36035553896083283
Iteration 40, Training loss = 0.3426022670016839
Iteration 50, Training loss = 0.333272813174587
Iteration 60, Training loss = 0.3227130498450536
Iteration 70, Training loss = 0.31741208339539856
Iteration 80, Training loss = 0.3114527671669538
Iteration 90, Training loss = 0.3070496409558333
Iteration 100, Training loss = 0.30273288015562755
Iteration 110, Training loss = 0.29866766471129197
Iteration 120, Training loss = 0.2952307604539853
Iteration 130, Training loss = 0.29086473684471387
Iteration 140, Training loss = 0.2876688617353256
Iteration 150, Training loss = 0.282503466336773
Iteration 160, Training loss = 0.2801073293846387
Iteration 170, Training loss = 0.2765189412121589
Iteration 180, Training loss = 0.2752792744968946
Iteration 190, Training loss = 0.2735536897984835
Iteration 200, Training loss = 0.27254424444757974
Iteration 210, Training loss = 0.2688138032188782
Iteration 220, Training loss = 0.26635580180356133
Iteration 230, Training loss = 0.2679370448279839
Iteration 240, Training loss = 0.263409075112297
Iteration 250, Training loss = 0.2623983403811088
Iteration 260, Training loss = 0.26266688552613443
Iteration 270, Training loss = 0.26162032783031464
Iteration 280, Training loss = 0.2601238924723405
Iteration 290, Training loss = 0.2596323616229571
Model training time: 49.940309286117554
Device: cuda
Iteration 0, Training loss = 0.9968361026392534
Iteration 10, Training loss = 0.5381383440242364
Iteration 20, Training loss = 0.3756515019788192
Iteration 30, Training loss = 0.3427050354389044
Iteration 40, Training loss = 0.3308484127315191
Iteration 50, Training loss = 0.32016449980437756
Iteration 60, Training loss = 0.3168828609184577
Iteration 70, Training loss = 0.3114416331339341
Iteration 80, Training loss = 0.30424376476842624
Iteration 90, Training loss = 0.30185289222460526
Iteration 100, Training loss = 0.30131234830388653
Iteration 110, Training loss = 0.29328650322098
Iteration 120, Training loss = 0.2911125382838341
Iteration 130, Training loss = 0.28734402716732943
Iteration 140, Training loss = 0.2883159299022876
Iteration 150, Training loss = 0.2815895054775935
Iteration 160, Training loss = 0.2796757092269567
Iteration 170, Training loss = 0.27581600572627324
Iteration 180, Training loss = 0.2743092719465494
Iteration 190, Training loss = 0.2735367769805285
Iteration 200, Training loss = 0.2706461391196801
Iteration 210, Training loss = 0.2698030617947762
Iteration 220, Training loss = 0.26945008581074387
Iteration 230, Training loss = 0.266033497137519
Iteration 240, Training loss = 0.2658204724295781
Iteration 250, Training loss = 0.2629056671777597
Iteration 260, Training loss = 0.26178121380507946
Iteration 270, Training loss = 0.2607210947630497
Iteration 280, Training loss = 0.26029235325180566
Iteration 290, Training loss = 0.2594174540673311
Model training time: 51.33221888542175
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8245543224307207
Iteration 10, Training loss = 0.26646452812621224
Iteration 20, Training loss = 0.2539188257203652
Iteration 30, Training loss = 0.2495779420893926
Iteration 40, Training loss = 0.2454241719096899
Iteration 50, Training loss = 0.2406432025421124
Iteration 60, Training loss = 0.23685224139346525
Iteration 70, Training loss = 0.2316853250735081
Iteration 80, Training loss = 0.2290513519770824
Iteration 90, Training loss = 0.2275957612750622
Iteration 100, Training loss = 0.22543513896660164
Iteration 110, Training loss = 0.2256099461362912
Iteration 120, Training loss = 0.22249827533960342
Iteration 130, Training loss = 0.22262580334567106
Iteration 140, Training loss = 0.21954839225285327
Iteration 150, Training loss = 0.2194840802023044
Iteration 160, Training loss = 0.21909690490708902
Iteration 170, Training loss = 0.21987202641769096
Iteration 180, Training loss = 0.21852409911270326
Iteration 190, Training loss = 0.21787877624424604
Iteration 200, Training loss = 0.2162256993783208
Iteration 210, Training loss = 0.2180786533997609
Iteration 220, Training loss = 0.21667884175594038
Iteration 230, Training loss = 0.21490531113858408
Iteration 240, Training loss = 0.21389935430712426
Iteration 250, Training loss = 0.21737279642659885
Iteration 260, Training loss = 0.21355367724138957
Iteration 270, Training loss = 0.21347985645899406
Iteration 280, Training loss = 0.2130813320668844
Iteration 290, Training loss = 0.21302564069628716
Model training time: 57.21824789047241
Device: cuda
Iteration 0, Training loss = 0.9027429495293361
Iteration 10, Training loss = 0.2742711915992774
Iteration 20, Training loss = 0.25353323854506016
Iteration 30, Training loss = 0.24549935915722296
Iteration 40, Training loss = 0.24402393429325178
Iteration 50, Training loss = 0.2406723781560476
Iteration 60, Training loss = 0.23863615210239703
Iteration 70, Training loss = 0.23852596460626677
Iteration 80, Training loss = 0.23703639030169982
Iteration 90, Training loss = 0.2360030679175487
Iteration 100, Training loss = 0.2341105195765312
Iteration 110, Training loss = 0.23635922888150582
Iteration 120, Training loss = 0.23482547428172368
Iteration 130, Training loss = 0.23242185580042693
Iteration 140, Training loss = 0.2328670806466387
Iteration 150, Training loss = 0.22871950784554848
Iteration 160, Training loss = 0.229591144177203
Iteration 170, Training loss = 0.22907219970455536
Iteration 180, Training loss = 0.22872043393838865
Iteration 190, Training loss = 0.22677389804560405
Iteration 200, Training loss = 0.22706339637247416
Iteration 210, Training loss = 0.22857841319189623
Iteration 220, Training loss = 0.22632405906915665
Iteration 230, Training loss = 0.2241620302486878
Iteration 240, Training loss = 0.22689915706331915
Iteration 250, Training loss = 0.22299145584782729
Iteration 260, Training loss = 0.22535254448079145
Iteration 270, Training loss = 0.2208807425430188
Iteration 280, Training loss = 0.2209359171012273
Iteration 290, Training loss = 0.2196086711035325
Model training time: 55.87099528312683
Device: cuda
Iteration 0, Training loss = 0.7894332474814012
Iteration 10, Training loss = 0.2764752937050966
Iteration 20, Training loss = 0.25854041857215077
Iteration 30, Training loss = 0.2503107740328862
Iteration 40, Training loss = 0.241922652348876
Iteration 50, Training loss = 0.23879285916112936
Iteration 60, Training loss = 0.23289835997498953
Iteration 70, Training loss = 0.22903299603897792
Iteration 80, Training loss = 0.2322060102596879
Iteration 90, Training loss = 0.22576509752812293
Iteration 100, Training loss = 0.22592160134361342
Iteration 110, Training loss = 0.2274841144680977
Iteration 120, Training loss = 0.22213804557059819
Iteration 130, Training loss = 0.22094040335370943
Iteration 140, Training loss = 0.21820670989556953
Iteration 150, Training loss = 0.21695123247515696
Iteration 160, Training loss = 0.21630613240771568
Iteration 170, Training loss = 0.2137253943544168
Iteration 180, Training loss = 0.21391246023659521
Iteration 190, Training loss = 0.2148172247868318
Iteration 200, Training loss = 0.21278279604246983
Iteration 210, Training loss = 0.20954781319372928
Iteration 220, Training loss = 0.2086218764575628
Iteration 230, Training loss = 0.20769099669101146
Iteration 240, Training loss = 0.20731945987790823
Iteration 250, Training loss = 0.2073255479335785
Iteration 260, Training loss = 0.20592673760480606
Iteration 270, Training loss = 0.20818913298157546
Iteration 280, Training loss = 0.2079076116474775
Iteration 290, Training loss = 0.20668050059332296
Model training time: 56.892035484313965
Device: cuda
Iteration 0, Training loss = 0.7453523880014052
Iteration 10, Training loss = 0.266245529628717
Iteration 20, Training loss = 0.2515132288233592
Iteration 30, Training loss = 0.242387444855502
Iteration 40, Training loss = 0.23941223595577937
Iteration 50, Training loss = 0.23395875268257582
Iteration 60, Training loss = 0.2311215618482003
Iteration 70, Training loss = 0.23068413109733507
Iteration 80, Training loss = 0.2276876146117082
Iteration 90, Training loss = 0.22610103295972714
Iteration 100, Training loss = 0.22552356396157008
Iteration 110, Training loss = 0.22278837132482573
Iteration 120, Training loss = 0.22227632025113472
Iteration 130, Training loss = 0.22052246375152698
Iteration 140, Training loss = 0.2203033039203057
Iteration 150, Training loss = 0.21857703083123153
Iteration 160, Training loss = 0.2157920069562701
Iteration 170, Training loss = 0.21396425273269415
Iteration 180, Training loss = 0.21313055590368235
Iteration 190, Training loss = 0.2126252927745764
Iteration 200, Training loss = 0.21191368280695036
Iteration 210, Training loss = 0.21255251600478703
Iteration 220, Training loss = 0.2082696739011086
Iteration 230, Training loss = 0.20906818063499835
Iteration 240, Training loss = 0.209599415341822
Iteration 250, Training loss = 0.20822743956859296
Iteration 260, Training loss = 0.2069739864136164
Iteration 270, Training loss = 0.20574722840235785
Iteration 280, Training loss = 0.20760381780564785
Iteration 290, Training loss = 0.2054407702615628
Model training time: 57.30709385871887
Device: cuda
Iteration 0, Training loss = 0.8273401509683865
Iteration 10, Training loss = 0.2643262680190114
Iteration 20, Training loss = 0.255746269885164
Iteration 30, Training loss = 0.2494314731313632
Iteration 40, Training loss = 0.24464147122433552
Iteration 50, Training loss = 0.24109396030409977
Iteration 60, Training loss = 0.2399975719073644
Iteration 70, Training loss = 0.23881471142745936
Iteration 80, Training loss = 0.2351894536270545
Iteration 90, Training loss = 0.23309348308696196
Iteration 100, Training loss = 0.23075559821266395
Iteration 110, Training loss = 0.23061822218677172
Iteration 120, Training loss = 0.23168856321045986
Iteration 130, Training loss = 0.22979898951374567
Iteration 140, Training loss = 0.22842946376364964
Iteration 150, Training loss = 0.22958102693351415
Iteration 160, Training loss = 0.22678972608768022
Iteration 170, Training loss = 0.22681428929074451
Iteration 180, Training loss = 0.22886131668033508
Iteration 190, Training loss = 0.22590259849452055
Iteration 200, Training loss = 0.22740614543167445
Iteration 210, Training loss = 0.22543536828687558
Iteration 220, Training loss = 0.22581078140781477
Iteration 230, Training loss = 0.22418988625017497
Iteration 240, Training loss = 0.22367571867429292
Iteration 250, Training loss = 0.222896831898162
Iteration 260, Training loss = 0.22211778436142665
Iteration 270, Training loss = 0.22150463467607132
Iteration 280, Training loss = 0.22166606482978052
Iteration 290, Training loss = 0.22158069299677244
Model training time: 56.25748348236084
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9999715261734449
Iteration 10, Training loss = 0.7743360337156516
Iteration 20, Training loss = 0.39237075800505966
Iteration 30, Training loss = 0.35470055129665595
Iteration 40, Training loss = 0.33669150706667167
Iteration 50, Training loss = 0.3279419941111253
Iteration 60, Training loss = 0.3199275154620409
Iteration 70, Training loss = 0.3124922743210426
Iteration 80, Training loss = 0.3096969246577758
Iteration 90, Training loss = 0.3037382500389448
Iteration 100, Training loss = 0.3012184285773681
Iteration 110, Training loss = 0.2971535445405887
Iteration 120, Training loss = 0.295036938184729
Iteration 130, Training loss = 0.29141185690577215
Iteration 140, Training loss = 0.29156312776299625
Iteration 150, Training loss = 0.2865009027031752
Iteration 160, Training loss = 0.2836649893568112
Iteration 170, Training loss = 0.28291720080261046
Iteration 180, Training loss = 0.27878432685079485
Iteration 190, Training loss = 0.27802905125113636
Iteration 200, Training loss = 0.2765081893079556
Iteration 210, Training loss = 0.2745556372862596
Iteration 220, Training loss = 0.2722847544803069
Iteration 230, Training loss = 0.276414207684306
Iteration 240, Training loss = 0.2695138848458345
Iteration 250, Training loss = 0.27180026242366206
Iteration 260, Training loss = 0.26844303109324896
Iteration 270, Training loss = 0.26729699911979526
Iteration 280, Training loss = 0.26670904142352253
Iteration 290, Training loss = 0.26555635708455855
Model training time: 49.8924446105957
Device: cuda
Iteration 0, Training loss = 1.059946751365295
Iteration 10, Training loss = 0.465241057654986
Iteration 20, Training loss = 0.37100857438949436
Iteration 30, Training loss = 0.3453356663768108
Iteration 40, Training loss = 0.33369057797468626
Iteration 50, Training loss = 0.3247085971614489
Iteration 60, Training loss = 0.3206216243024056
Iteration 70, Training loss = 0.31445854171537435
Iteration 80, Training loss = 0.3102108919276641
Iteration 90, Training loss = 0.3064812903220837
Iteration 100, Training loss = 0.3044009032444312
Iteration 110, Training loss = 0.30135194011605704
Iteration 120, Training loss = 0.30188784862940127
Iteration 130, Training loss = 0.2958173781919938
Iteration 140, Training loss = 0.29866066875939185
Iteration 150, Training loss = 0.2923403231856915
Iteration 160, Training loss = 0.29117459459946704
Iteration 170, Training loss = 0.28768079971464783
Iteration 180, Training loss = 0.2874018888060863
Iteration 190, Training loss = 0.28499706304417205
Iteration 200, Training loss = 0.2825413164324485
Iteration 210, Training loss = 0.2811367723804254
Iteration 220, Training loss = 0.2794325085213551
Iteration 230, Training loss = 0.2775431784013143
Iteration 240, Training loss = 0.2765267605964954
Iteration 250, Training loss = 0.27262566654154885
Iteration 260, Training loss = 0.27171241936202234
Iteration 270, Training loss = 0.27050830896657246
Iteration 280, Training loss = 0.2684599800178638
Iteration 290, Training loss = 0.26780032366514206
Model training time: 49.832911014556885
Device: cuda
Iteration 0, Training loss = 1.042260898420444
Iteration 10, Training loss = 0.921207460646446
Iteration 20, Training loss = 0.43969124584243846
Iteration 30, Training loss = 0.3645904545600598
Iteration 40, Training loss = 0.3384677111529387
Iteration 50, Training loss = 0.3271300305540745
Iteration 60, Training loss = 0.3182905006867189
Iteration 70, Training loss = 0.3109475655051378
Iteration 80, Training loss = 0.3045301021864781
Iteration 90, Training loss = 0.3011886772628014
Iteration 100, Training loss = 0.2964120985796818
Iteration 110, Training loss = 0.29614042949218017
Iteration 120, Training loss = 0.291903708416682
Iteration 130, Training loss = 0.28692088104211366
Iteration 140, Training loss = 0.2836145100971827
Iteration 150, Training loss = 0.28226094793241757
Iteration 160, Training loss = 0.2784391842209376
Iteration 170, Training loss = 0.27688903719759905
Iteration 180, Training loss = 0.2733429635946567
Iteration 190, Training loss = 0.2703943454588835
Iteration 200, Training loss = 0.2698979437924348
Iteration 210, Training loss = 0.2669261053491097
Iteration 220, Training loss = 0.2653605735932405
Iteration 230, Training loss = 0.2643245333948961
Iteration 240, Training loss = 0.2635579635030948
Iteration 250, Training loss = 0.2616315812159043
Iteration 260, Training loss = 0.26131636248185086
Iteration 270, Training loss = 0.26157878224666303
Iteration 280, Training loss = 0.25988236241615736
Iteration 290, Training loss = 0.2590427157970575
Model training time: 49.601776361465454
Device: cuda
Iteration 0, Training loss = 0.983502810391096
Iteration 10, Training loss = 0.6375124159340675
Iteration 20, Training loss = 0.42660895362496376
Iteration 30, Training loss = 0.3814760704453175
Iteration 40, Training loss = 0.3551822811938249
Iteration 50, Training loss = 0.3393519873229357
Iteration 60, Training loss = 0.3265782053080889
Iteration 70, Training loss = 0.31659114518417764
Iteration 80, Training loss = 0.3091049575461791
Iteration 90, Training loss = 0.30333580105350566
Iteration 100, Training loss = 0.3001360987814573
Iteration 110, Training loss = 0.2951718596025155
Iteration 120, Training loss = 0.2917968683804457
Iteration 130, Training loss = 0.2888725134902276
Iteration 140, Training loss = 0.2863917707537229
Iteration 150, Training loss = 0.2856303652605185
Iteration 160, Training loss = 0.2828248683363199
Iteration 170, Training loss = 0.28140669530973983
Iteration 180, Training loss = 0.2802108527662662
Iteration 190, Training loss = 0.27816569346647996
Iteration 200, Training loss = 0.276942618477803
Iteration 210, Training loss = 0.2768485127733304
Iteration 220, Training loss = 0.27686845410901767
Iteration 230, Training loss = 0.2751365468765681
Iteration 240, Training loss = 0.2732724550251777
Iteration 250, Training loss = 0.2720415239723829
Iteration 260, Training loss = 0.27066889362266433
Iteration 270, Training loss = 0.2721929603184645
Iteration 280, Training loss = 0.2683726759770742
Iteration 290, Training loss = 0.2681655942534025
Model training time: 49.417030572891235
Device: cuda
Iteration 0, Training loss = 1.1068351870545974
Iteration 10, Training loss = 0.6729069185944704
Iteration 20, Training loss = 0.366070524431192
Iteration 30, Training loss = 0.3390562075835008
Iteration 40, Training loss = 0.3279702008630221
Iteration 50, Training loss = 0.3240888952635802
Iteration 60, Training loss = 0.3138395150980124
Iteration 70, Training loss = 0.3083738565731507
Iteration 80, Training loss = 0.30509991556979144
Iteration 90, Training loss = 0.2991798078784576
Iteration 100, Training loss = 0.2955985638098075
Iteration 110, Training loss = 0.29219246412125915
Iteration 120, Training loss = 0.29130999084848624
Iteration 130, Training loss = 0.2861638580663846
Iteration 140, Training loss = 0.28301060600922656
Iteration 150, Training loss = 0.2821222380376779
Iteration 160, Training loss = 0.2798837903313912
Iteration 170, Training loss = 0.2768147113518073
Iteration 180, Training loss = 0.27559416397259784
Iteration 190, Training loss = 0.27328939435000604
Iteration 200, Training loss = 0.27438264454786593
Iteration 210, Training loss = 0.27013269238747084
Iteration 220, Training loss = 0.26980379266807664
Iteration 230, Training loss = 0.26760173914954066
Iteration 240, Training loss = 0.26691203313664746
Iteration 250, Training loss = 0.26765745935531765
Iteration 260, Training loss = 0.26590309535654694
Iteration 270, Training loss = 0.2675992665955654
Iteration 280, Training loss = 0.26435239082918716
Iteration 290, Training loss = 0.26453307643532753
Model training time: 50.378273487091064
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6249599830748943
Iteration 10, Training loss = 0.25887107404951865
Iteration 20, Training loss = 0.24433233646246102
Iteration 30, Training loss = 0.2368092124995131
Iteration 40, Training loss = 0.230851535900281
Iteration 50, Training loss = 0.226889035736139
Iteration 60, Training loss = 0.2250915734240642
Iteration 70, Training loss = 0.22363445091132933
Iteration 80, Training loss = 0.2219064197001549
Iteration 90, Training loss = 0.21825957154998413
Iteration 100, Training loss = 0.21703137679455373
Iteration 110, Training loss = 0.21528892691891927
Iteration 120, Training loss = 0.21201064022114643
Iteration 130, Training loss = 0.21171206923631522
Iteration 140, Training loss = 0.21119698831954828
Iteration 150, Training loss = 0.20909941662102938
Iteration 160, Training loss = 0.21200509111468607
Iteration 170, Training loss = 0.20800060664231962
Iteration 180, Training loss = 0.20735973241524056
Iteration 190, Training loss = 0.20770083704533485
Iteration 200, Training loss = 0.2077753680686538
Iteration 210, Training loss = 0.20562733423251373
Iteration 220, Training loss = 0.207986965488929
Iteration 230, Training loss = 0.20549898859686577
Iteration 240, Training loss = 0.20505172859590787
Iteration 250, Training loss = 0.20552693543812403
Iteration 260, Training loss = 0.20585934875103143
Iteration 270, Training loss = 0.20668475945981649
Iteration 280, Training loss = 0.20633283243156397
Iteration 290, Training loss = 0.2040420901030302
Model training time: 56.34187316894531
Device: cuda
Iteration 0, Training loss = 0.8341889762534545
Iteration 10, Training loss = 0.2637458724471239
Iteration 20, Training loss = 0.24955750170808572
Iteration 30, Training loss = 0.2446258095307992
Iteration 40, Training loss = 0.23504600659585917
Iteration 50, Training loss = 0.2299850286486057
Iteration 60, Training loss = 0.22559886382749447
Iteration 70, Training loss = 0.22518009635118338
Iteration 80, Training loss = 0.22341956148067346
Iteration 90, Training loss = 0.21697884382536778
Iteration 100, Training loss = 0.21549734974709842
Iteration 110, Training loss = 0.2179203133743543
Iteration 120, Training loss = 0.21234812365415004
Iteration 130, Training loss = 0.21082303257515797
Iteration 140, Training loss = 0.20824701009461513
Iteration 150, Training loss = 0.20708418300805184
Iteration 160, Training loss = 0.2072019587055995
Iteration 170, Training loss = 0.20728449423152667
Iteration 180, Training loss = 0.20636929062983164
Iteration 190, Training loss = 0.2055701888524569
Iteration 200, Training loss = 0.20287485392047808
Iteration 210, Training loss = 0.20487141874260628
Iteration 220, Training loss = 0.20313026831270412
Iteration 230, Training loss = 0.20431316257096255
Iteration 240, Training loss = 0.20265302083526665
Iteration 250, Training loss = 0.20221966340278202
Iteration 260, Training loss = 0.2011842681095004
Iteration 270, Training loss = 0.20274999035665622
Iteration 280, Training loss = 0.20159790113281745
Iteration 290, Training loss = 0.2017475519186029
Model training time: 56.02121138572693
Device: cuda
Iteration 0, Training loss = 0.7104955702446974
Iteration 10, Training loss = 0.2558186042767305
Iteration 20, Training loss = 0.24696494810856306
Iteration 30, Training loss = 0.23964355585093683
Iteration 40, Training loss = 0.23649623116048482
Iteration 50, Training loss = 0.2364503156680327
Iteration 60, Training loss = 0.2371741417222298
Iteration 70, Training loss = 0.23199949270257583
Iteration 80, Training loss = 0.22983429709879252
Iteration 90, Training loss = 0.22780060632011065
Iteration 100, Training loss = 0.22677702786257634
Iteration 110, Training loss = 0.22819679474028257
Iteration 120, Training loss = 0.22850591460099587
Iteration 130, Training loss = 0.22581340917027914
Iteration 140, Training loss = 0.22263014703415906
Iteration 150, Training loss = 0.22221521230844352
Iteration 160, Training loss = 0.21833771295272386
Iteration 170, Training loss = 0.21690658322320536
Iteration 180, Training loss = 0.2149852873900762
Iteration 190, Training loss = 0.2136418649640221
Iteration 200, Training loss = 0.21270706189366487
Iteration 210, Training loss = 0.21335020947914857
Iteration 220, Training loss = 0.21089508155217537
Iteration 230, Training loss = 0.2101387192423527
Iteration 240, Training loss = 0.20898294133635667
Iteration 250, Training loss = 0.2098869475750969
Iteration 260, Training loss = 0.2070546466857195
Iteration 270, Training loss = 0.2088083838327573
Iteration 280, Training loss = 0.20701755167773137
Iteration 290, Training loss = 0.20970877367430008
Model training time: 55.68669867515564
Device: cuda
Iteration 0, Training loss = 0.6898749307371103
Iteration 10, Training loss = 0.26005210612828916
Iteration 20, Training loss = 0.24546822525847417
Iteration 30, Training loss = 0.23643001240606493
Iteration 40, Training loss = 0.22924469182124504
Iteration 50, Training loss = 0.22445074411538932
Iteration 60, Training loss = 0.21957438704199517
Iteration 70, Training loss = 0.21849514816242915
Iteration 80, Training loss = 0.21407953907664007
Iteration 90, Training loss = 0.21280861982645896
Iteration 100, Training loss = 0.2109736889744034
Iteration 110, Training loss = 0.20952290669083595
Iteration 120, Training loss = 0.20923104961044514
Iteration 130, Training loss = 0.20893610655688322
Iteration 140, Training loss = 0.20952720352663443
Iteration 150, Training loss = 0.20703987777233124
Iteration 160, Training loss = 0.20692017251768938
Iteration 170, Training loss = 0.20677568677526253
Iteration 180, Training loss = 0.20637511311528775
Iteration 190, Training loss = 0.20663079991936684
Iteration 200, Training loss = 0.20543834316329315
Iteration 210, Training loss = 0.205869887310725
Iteration 220, Training loss = 0.20460551117475217
Iteration 230, Training loss = 0.20496917888522148
Iteration 240, Training loss = 0.20489404601259872
Iteration 250, Training loss = 0.20403257270271963
Iteration 260, Training loss = 0.2058624682518152
Iteration 270, Training loss = 0.20509749156637833
Iteration 280, Training loss = 0.20577714635202518
Iteration 290, Training loss = 0.2038267315723575
Model training time: 56.13016057014465
Device: cuda
Iteration 0, Training loss = 0.7663687562140135
Iteration 10, Training loss = 0.27034929466362184
Iteration 20, Training loss = 0.26039997961085576
Iteration 30, Training loss = 0.25572690238746315
Iteration 40, Training loss = 0.2508298230285828
Iteration 50, Training loss = 0.24502786836371973
Iteration 60, Training loss = 0.24222439341247082
Iteration 70, Training loss = 0.23908222968188617
Iteration 80, Training loss = 0.23283684798158133
Iteration 90, Training loss = 0.22912642846886927
Iteration 100, Training loss = 0.22629611251445916
Iteration 110, Training loss = 0.22410859869649777
Iteration 120, Training loss = 0.22155290702357888
Iteration 130, Training loss = 0.2238475577905774
Iteration 140, Training loss = 0.2221880413305301
Iteration 150, Training loss = 0.21982087818189308
Iteration 160, Training loss = 0.21760882559017494
Iteration 170, Training loss = 0.21707237196656373
Iteration 180, Training loss = 0.21466346360886326
Iteration 190, Training loss = 0.21607240948539513
Iteration 200, Training loss = 0.2167986546857999
Iteration 210, Training loss = 0.21342781773553446
Iteration 220, Training loss = 0.2157759012367863
Iteration 230, Training loss = 0.21388810873031616
Iteration 240, Training loss = 0.2146565640488496
Iteration 250, Training loss = 0.21213912892226988
Iteration 260, Training loss = 0.21521967878708473
Iteration 270, Training loss = 0.21349622029811144
Iteration 280, Training loss = 0.2157130505029972
Iteration 290, Training loss = 0.21081368911724824
Model training time: 56.50204634666443
{'activation_functions': ['relu', 'relu'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0347740707489161
Iteration 10, Training loss = 0.878419480071618
Iteration 20, Training loss = 0.4351390895362084
Iteration 30, Training loss = 0.3712180635103813
Iteration 40, Training loss = 0.33436232576003444
Iteration 50, Training loss = 0.31839624505776626
Iteration 60, Training loss = 0.3092829815756816
Iteration 70, Training loss = 0.30273239973645943
Iteration 80, Training loss = 0.29658715412593806
Iteration 90, Training loss = 0.2923629448677485
Iteration 100, Training loss = 0.29179485915945125
Iteration 110, Training loss = 0.2866533672293791
Iteration 120, Training loss = 0.2835100546765786
Iteration 130, Training loss = 0.28371936168808204
Iteration 140, Training loss = 0.2800692738248752
Iteration 150, Training loss = 0.2792532346569575
Iteration 160, Training loss = 0.27768515050411224
Iteration 170, Training loss = 0.2768890844801298
Iteration 180, Training loss = 0.2748229479273924
Iteration 190, Training loss = 0.27467337995767593
Iteration 200, Training loss = 0.2735979717511397
Iteration 210, Training loss = 0.2710866416589572
Iteration 220, Training loss = 0.2728002227556247
Iteration 230, Training loss = 0.26998025092941064
Iteration 240, Training loss = 0.2688458709476086
Iteration 250, Training loss = 0.27070148733372873
Iteration 260, Training loss = 0.26810105646458954
Iteration 270, Training loss = 0.26719623001722187
Iteration 280, Training loss = 0.2665863171792947
Iteration 290, Training loss = 0.2682160905633981
Model training time: 49.71716856956482
Device: cuda
Iteration 0, Training loss = 1.0227552468960102
Iteration 10, Training loss = 0.6136329182638571
Iteration 20, Training loss = 0.3901938847624339
Iteration 30, Training loss = 0.35380729903968483
Iteration 40, Training loss = 0.34030953823373866
Iteration 50, Training loss = 0.3301213854350723
Iteration 60, Training loss = 0.32715436744575316
Iteration 70, Training loss = 0.3187732348361841
Iteration 80, Training loss = 0.312572641728016
Iteration 90, Training loss = 0.3068060211550731
Iteration 100, Training loss = 0.30206418968737125
Iteration 110, Training loss = 0.2973345287430745
Iteration 120, Training loss = 0.29453004925296855
Iteration 130, Training loss = 0.2899166986059684
Iteration 140, Training loss = 0.2878826935417377
Iteration 150, Training loss = 0.28423096089122385
Iteration 160, Training loss = 0.2796755521200024
Iteration 170, Training loss = 0.27928776351305157
Iteration 180, Training loss = 0.2749690804630518
Iteration 190, Training loss = 0.2726724977390124
Iteration 200, Training loss = 0.2710211141608082
Iteration 210, Training loss = 0.270653251845103
Iteration 220, Training loss = 0.2686697534070565
Iteration 230, Training loss = 0.26781713919570815
Iteration 240, Training loss = 0.2669199216537751
Iteration 250, Training loss = 0.26618413197306484
Iteration 260, Training loss = 0.26469715469731736
Iteration 270, Training loss = 0.26492539850565106
Iteration 280, Training loss = 0.26440882983689123
Iteration 290, Training loss = 0.26243105874611783
Model training time: 50.02245545387268
Device: cuda
Iteration 0, Training loss = 0.9707568133106599
Iteration 10, Training loss = 0.42375715248859847
Iteration 20, Training loss = 0.3609574346874769
Iteration 30, Training loss = 0.3425269816070795
Iteration 40, Training loss = 0.3238079065027145
Iteration 50, Training loss = 0.31477557280315804
Iteration 60, Training loss = 0.3083314017321055
Iteration 70, Training loss = 0.3066606945716418
Iteration 80, Training loss = 0.29924034900390184
Iteration 90, Training loss = 0.29650652480240053
Iteration 100, Training loss = 0.29257937893271446
Iteration 110, Training loss = 0.2915378301762618
Iteration 120, Training loss = 0.2863661307268418
Iteration 130, Training loss = 0.28292755137842435
Iteration 140, Training loss = 0.2817033679726032
Iteration 150, Training loss = 0.2777948494140918
Iteration 160, Training loss = 0.27668619284836143
Iteration 170, Training loss = 0.2743601015267464
Iteration 180, Training loss = 0.27293346120187867
Iteration 190, Training loss = 0.2720355296937319
Iteration 200, Training loss = 0.27108092391147065
Iteration 210, Training loss = 0.26767542127233285
Iteration 220, Training loss = 0.26787387164166343
Iteration 230, Training loss = 0.26451181290814507
Iteration 240, Training loss = 0.26429039813005006
Iteration 250, Training loss = 0.26328735870237535
Iteration 260, Training loss = 0.26192997510616595
Iteration 270, Training loss = 0.26109568086954266
Iteration 280, Training loss = 0.2607180771346276
Iteration 290, Training loss = 0.2594158509029792
Model training time: 50.48699641227722
Device: cuda
Iteration 0, Training loss = 1.0820259113724415
Iteration 10, Training loss = 0.7396528276686485
Iteration 20, Training loss = 0.39037878152269584
Iteration 30, Training loss = 0.3497120663523674
Iteration 40, Training loss = 0.33428086139834845
Iteration 50, Training loss = 0.31773957653114426
Iteration 60, Training loss = 0.3084858428113736
Iteration 70, Training loss = 0.3039831779897213
Iteration 80, Training loss = 0.29682657251564354
Iteration 90, Training loss = 0.2957734080174795
Iteration 100, Training loss = 0.28836520517674774
Iteration 110, Training loss = 0.28691694957132524
Iteration 120, Training loss = 0.28109010151372504
Iteration 130, Training loss = 0.28098044573114467
Iteration 140, Training loss = 0.2768994585539286
Iteration 150, Training loss = 0.2735998865503531
Iteration 160, Training loss = 0.2707581929862499
Iteration 170, Training loss = 0.2684875732431045
Iteration 180, Training loss = 0.2666919305204199
Iteration 190, Training loss = 0.2664638471144896
Iteration 200, Training loss = 0.26625612578712976
Iteration 210, Training loss = 0.2671456371362393
Iteration 220, Training loss = 0.26249060321312684
Iteration 230, Training loss = 0.2608823167303434
Iteration 240, Training loss = 0.2618805614228432
Iteration 250, Training loss = 0.2591143770573231
Iteration 260, Training loss = 0.2580784775603276
Iteration 270, Training loss = 0.2593580756623011
Iteration 280, Training loss = 0.2571952801484328
Iteration 290, Training loss = 0.25631173384877354
Model training time: 49.759658336639404
Device: cuda
Iteration 0, Training loss = 1.072179604608279
Iteration 10, Training loss = 0.690279168005173
Iteration 20, Training loss = 0.37376191510030854
Iteration 30, Training loss = 0.3496023276104377
Iteration 40, Training loss = 0.33551959292246747
Iteration 50, Training loss = 0.3275818068247575
Iteration 60, Training loss = 0.3235837029149899
Iteration 70, Training loss = 0.3170523829758167
Iteration 80, Training loss = 0.31100644543766975
Iteration 90, Training loss = 0.30878439416679054
Iteration 100, Training loss = 0.30498816588750255
Iteration 110, Training loss = 0.3061381425135411
Iteration 120, Training loss = 0.29989562871364445
Iteration 130, Training loss = 0.2993903753276055
Iteration 140, Training loss = 0.2973505449123107
Iteration 150, Training loss = 0.2936843865765975
Iteration 160, Training loss = 0.2910179624047417
Iteration 170, Training loss = 0.29151208211596197
Iteration 180, Training loss = 0.28733184417852986
Iteration 190, Training loss = 0.2851760024921252
Iteration 200, Training loss = 0.28391753781873447
Iteration 210, Training loss = 0.2805921767766659
Iteration 220, Training loss = 0.2827617686528426
Iteration 230, Training loss = 0.27765060932590413
Iteration 240, Training loss = 0.27491513152535146
Iteration 250, Training loss = 0.27633927447291523
Iteration 260, Training loss = 0.2732454084146481
Iteration 270, Training loss = 0.27264565659257084
Iteration 280, Training loss = 0.2728554907326515
Iteration 290, Training loss = 0.2717443761917261
Model training time: 50.129387855529785
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9767013547512201
Iteration 10, Training loss = 0.28516401235873884
Iteration 20, Training loss = 0.2582506104730643
Iteration 30, Training loss = 0.24696123943879053
Iteration 40, Training loss = 0.23896137023201355
Iteration 50, Training loss = 0.2348011932693995
Iteration 60, Training loss = 0.23118176425878817
Iteration 70, Training loss = 0.2295337262061926
Iteration 80, Training loss = 0.22691052922835717
Iteration 90, Training loss = 0.2243924711186152
Model training time: 13.959972620010376
Device: cuda
Iteration 0, Training loss = 0.8533345655753062
Iteration 10, Training loss = 0.28627306251571727
Iteration 20, Training loss = 0.26091860263393474
Iteration 30, Training loss = 0.2515563325813183
Iteration 40, Training loss = 0.2466027450103026
Iteration 50, Training loss = 0.242481757241946
Iteration 60, Training loss = 0.23879844838610062
Iteration 70, Training loss = 0.23606011300132826
Iteration 80, Training loss = 0.23388888056461626
Iteration 90, Training loss = 0.23125499143050268
Model training time: 14.069926261901855
Device: cuda
Iteration 0, Training loss = 1.0071189667169864
Iteration 10, Training loss = 0.2957742351752061
Iteration 20, Training loss = 0.26824176053588206
Iteration 30, Training loss = 0.25600719537872535
Iteration 40, Training loss = 0.24964939946165451
Iteration 50, Training loss = 0.2453146937948007
Iteration 60, Training loss = 0.2425357745243953
Iteration 70, Training loss = 0.23871402814984322
Iteration 80, Training loss = 0.23634808510541916
Iteration 90, Training loss = 0.234120879895412
Model training time: 15.507604598999023
Device: cuda
Iteration 0, Training loss = 0.9301056483617196
Iteration 10, Training loss = 0.28007602519713914
Iteration 20, Training loss = 0.25445406110240865
Iteration 30, Training loss = 0.24523959423487002
Iteration 40, Training loss = 0.23717711120843887
Iteration 50, Training loss = 0.2344541194347235
Iteration 60, Training loss = 0.2282361161823456
Iteration 70, Training loss = 0.22576927909484276
Iteration 80, Training loss = 0.22257624022089517
Iteration 90, Training loss = 0.2202450939669059
Model training time: 15.830225706100464
Device: cuda
Iteration 0, Training loss = 0.9617031033222492
Iteration 10, Training loss = 0.28199530192292654
Iteration 20, Training loss = 0.25841047070347345
Iteration 30, Training loss = 0.24797338218643114
Iteration 40, Training loss = 0.24174809226622948
Iteration 50, Training loss = 0.23746712505817413
Iteration 60, Training loss = 0.23479733587457582
Iteration 70, Training loss = 0.23186825817594162
Iteration 80, Training loss = 0.22898485855414316
Iteration 90, Training loss = 0.2276260698070893
Model training time: 15.482283115386963
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9910607509888135
Iteration 10, Training loss = 0.9389901149731416
Iteration 20, Training loss = 0.8585358090125598
Iteration 30, Training loss = 0.7315079810527655
Iteration 40, Training loss = 0.5909583694659747
Iteration 50, Training loss = 0.502116858959198
Iteration 60, Training loss = 0.46027162212591904
Iteration 70, Training loss = 0.43485699937893796
Iteration 80, Training loss = 0.4175030537522756
Iteration 90, Training loss = 0.40242923165743166
Model training time: 14.26922082901001
Device: cuda
Iteration 0, Training loss = 1.0461345326441984
Iteration 10, Training loss = 0.9831916506473835
Iteration 20, Training loss = 0.930137024475978
Iteration 30, Training loss = 0.8528209122327658
Iteration 40, Training loss = 0.7382024549520932
Iteration 50, Training loss = 0.6036403356836393
Iteration 60, Training loss = 0.4960216839726155
Iteration 70, Training loss = 0.4286138037076363
Iteration 80, Training loss = 0.3886216420393724
Iteration 90, Training loss = 0.3660699432859054
Model training time: 14.316834211349487
Device: cuda
Iteration 0, Training loss = 1.007507738012534
Iteration 10, Training loss = 0.8849878414319112
Iteration 20, Training loss = 0.7020203138773258
Iteration 30, Training loss = 0.5010825103292098
Iteration 40, Training loss = 0.40031453795157945
Iteration 50, Training loss = 0.3718384931293818
Iteration 60, Training loss = 0.3586866265306106
Iteration 70, Training loss = 0.3507286923436018
Iteration 80, Training loss = 0.3432201508146066
Iteration 90, Training loss = 0.33868576700870806
Model training time: 14.041590929031372
Device: cuda
Iteration 0, Training loss = 0.9920759155200078
Iteration 10, Training loss = 0.9629463805602148
Iteration 20, Training loss = 0.9293311410225354
Iteration 30, Training loss = 0.863059361393635
Iteration 40, Training loss = 0.7465055699531848
Iteration 50, Training loss = 0.581210077382051
Iteration 60, Training loss = 0.46327834175183225
Iteration 70, Training loss = 0.42201688713752306
Iteration 80, Training loss = 0.4043701858474658
Iteration 90, Training loss = 0.3927291522805507
Model training time: 14.116976261138916
Device: cuda
Iteration 0, Training loss = 1.0406198421349893
Iteration 10, Training loss = 0.9905111067570173
Iteration 20, Training loss = 0.9662117407872126
Iteration 30, Training loss = 0.9355884205836517
Iteration 40, Training loss = 0.8851841206733997
Iteration 50, Training loss = 0.7994556163366024
Iteration 60, Training loss = 0.664297433426747
Iteration 70, Training loss = 0.506172521183124
Iteration 80, Training loss = 0.412518988435085
Iteration 90, Training loss = 0.3787158756301953
Model training time: 13.948901653289795
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9705758816920794
Iteration 10, Training loss = 0.29328436862963897
Iteration 20, Training loss = 0.26791522995783734
Iteration 30, Training loss = 0.2566623707803396
Iteration 40, Training loss = 0.25176369971953905
Iteration 50, Training loss = 0.24885061755776405
Iteration 60, Training loss = 0.24588033585594252
Iteration 70, Training loss = 0.24421140522910997
Iteration 80, Training loss = 0.2408986191910047
Iteration 90, Training loss = 0.23899962667089242
Model training time: 14.939488649368286
Device: cuda
Iteration 0, Training loss = 1.100336151627394
Iteration 10, Training loss = 0.28953572591909993
Iteration 20, Training loss = 0.2670283718751027
Iteration 30, Training loss = 0.2552533599619682
Iteration 40, Training loss = 0.2498477236009561
Iteration 50, Training loss = 0.24789087732250875
Iteration 60, Training loss = 0.24261357406010994
Iteration 70, Training loss = 0.24029388966468665
Iteration 80, Training loss = 0.23792618226546508
Iteration 90, Training loss = 0.23530599761467713
Model training time: 14.449269771575928
Device: cuda
Iteration 0, Training loss = 0.9896004417767892
Iteration 10, Training loss = 0.3080841726981677
Iteration 20, Training loss = 0.2821895649226812
Iteration 30, Training loss = 0.2660577188317592
Iteration 40, Training loss = 0.2575649501612553
Iteration 50, Training loss = 0.25072612728063876
Iteration 60, Training loss = 0.24507346061559823
Iteration 70, Training loss = 0.24206038019978082
Iteration 80, Training loss = 0.2387018842765918
Iteration 90, Training loss = 0.23524940013885498
Model training time: 14.027010202407837
Device: cuda
Iteration 0, Training loss = 1.1425976283275163
Iteration 10, Training loss = 0.30163228483154225
Iteration 20, Training loss = 0.265391762726582
Iteration 30, Training loss = 0.2542733507087597
Iteration 40, Training loss = 0.24913656826202685
Iteration 50, Training loss = 0.24779674267539611
Iteration 60, Training loss = 0.24458739476708266
Iteration 70, Training loss = 0.2430077653664809
Iteration 80, Training loss = 0.24046776415063784
Iteration 90, Training loss = 0.23904206231236458
Model training time: 14.042177438735962
Device: cuda
Iteration 0, Training loss = 1.016249804542615
Iteration 10, Training loss = 0.2920476198196411
Iteration 20, Training loss = 0.2674346804045714
Iteration 30, Training loss = 0.2579060738476423
Iteration 40, Training loss = 0.2507965871347831
Iteration 50, Training loss = 0.24812933458731726
Iteration 60, Training loss = 0.24503415851638868
Iteration 70, Training loss = 0.2427321162361365
Iteration 80, Training loss = 0.2413040975538584
Iteration 90, Training loss = 0.23989600000473169
Model training time: 14.379356861114502
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9975901807730014
Iteration 10, Training loss = 0.8821134017064021
Iteration 20, Training loss = 0.7288014625127499
Iteration 30, Training loss = 0.568549428994839
Iteration 40, Training loss = 0.4741107981938582
Iteration 50, Training loss = 0.43188320444180417
Iteration 60, Training loss = 0.40846024453639984
Iteration 70, Training loss = 0.392111908357877
Iteration 80, Training loss = 0.37934200981488597
Iteration 90, Training loss = 0.3681355651754599
Model training time: 13.073127269744873
Device: cuda
Iteration 0, Training loss = 1.1391180833944907
Iteration 10, Training loss = 1.0118965689952557
Iteration 20, Training loss = 0.9742060487086956
Iteration 30, Training loss = 0.9271231798025278
Iteration 40, Training loss = 0.8564910636498377
Iteration 50, Training loss = 0.7408457123316251
Iteration 60, Training loss = 0.592705236031459
Iteration 70, Training loss = 0.47511778943813765
Iteration 80, Training loss = 0.4233948731651673
Iteration 90, Training loss = 0.3995329081439055
Model training time: 12.9199538230896
Device: cuda
Iteration 0, Training loss = 1.0339428358353102
Iteration 10, Training loss = 0.9707951625952354
Iteration 20, Training loss = 0.922136698777859
Iteration 30, Training loss = 0.856303850045571
Iteration 40, Training loss = 0.7671675590368418
Iteration 50, Training loss = 0.670801932995136
Iteration 60, Training loss = 0.5965641656747231
Iteration 70, Training loss = 0.5450630377118404
Iteration 80, Training loss = 0.5079016972046632
Iteration 90, Training loss = 0.4812622047387637
Model training time: 13.000381469726562
Device: cuda
Iteration 0, Training loss = 0.958947805257944
Iteration 10, Training loss = 0.8590760059081591
Iteration 20, Training loss = 0.7035320022931466
Iteration 30, Training loss = 0.555355140222953
Iteration 40, Training loss = 0.47765664412425113
Iteration 50, Training loss = 0.4382358199128738
Iteration 60, Training loss = 0.4151648420553941
Iteration 70, Training loss = 0.3984249056531833
Iteration 80, Training loss = 0.3851971993079552
Iteration 90, Training loss = 0.375611438201024
Model training time: 13.082510471343994
Device: cuda
Iteration 0, Training loss = 0.959591255738185
Iteration 10, Training loss = 0.8453463109639975
Iteration 20, Training loss = 0.686684257709063
Iteration 30, Training loss = 0.513621595616524
Iteration 40, Training loss = 0.41212812123390347
Iteration 50, Training loss = 0.37513228620474154
Iteration 60, Training loss = 0.359041823217502
Iteration 70, Training loss = 0.34865818745814836
Iteration 80, Training loss = 0.34206703821053874
Iteration 90, Training loss = 0.33634483126493603
Model training time: 13.064324617385864
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9791669421471082
Iteration 10, Training loss = 0.45226145363771
Iteration 20, Training loss = 0.35570349945471835
Iteration 30, Training loss = 0.30277268731823337
Iteration 40, Training loss = 0.27477198810531545
Iteration 50, Training loss = 0.2596994927869393
Iteration 60, Training loss = 0.2509105463440602
Iteration 70, Training loss = 0.2443536098759908
Iteration 80, Training loss = 0.2398624594968099
Iteration 90, Training loss = 0.23811944316212946
Model training time: 14.076512575149536
Device: cuda
Iteration 0, Training loss = 0.987429100733537
Iteration 10, Training loss = 0.28038003095067465
Iteration 20, Training loss = 0.25714986570752585
Iteration 30, Training loss = 0.24729948347577682
Iteration 40, Training loss = 0.2423922631603021
Iteration 50, Training loss = 0.23785232523312935
Iteration 60, Training loss = 0.23545144183131364
Iteration 70, Training loss = 0.2349695783968155
Iteration 80, Training loss = 0.2334952148107382
Iteration 90, Training loss = 0.2316261254824125
Model training time: 13.874887943267822
Device: cuda
Iteration 0, Training loss = 1.0099682475511844
Iteration 10, Training loss = 0.29987977330501264
Iteration 20, Training loss = 0.2742163928655478
Iteration 30, Training loss = 0.2597640228386109
Iteration 40, Training loss = 0.253457124130084
Iteration 50, Training loss = 0.2497548506810115
Iteration 60, Training loss = 0.24579802413399404
Iteration 70, Training loss = 0.24214956107047889
Iteration 80, Training loss = 0.23960896122914094
Iteration 90, Training loss = 0.23637427074404863
Model training time: 13.86851167678833
Device: cuda
Iteration 0, Training loss = 0.962154514514483
Iteration 10, Training loss = 0.29342499948464906
Iteration 20, Training loss = 0.2713168690410944
Iteration 30, Training loss = 0.2591100690456537
Iteration 40, Training loss = 0.25160741863342434
Iteration 50, Training loss = 0.24857094912574842
Iteration 60, Training loss = 0.24545201057424912
Iteration 70, Training loss = 0.2424492102402907
Iteration 80, Training loss = 0.24144348186942247
Iteration 90, Training loss = 0.23983977563106096
Model training time: 14.241406679153442
Device: cuda
Iteration 0, Training loss = 0.9739198764929404
Iteration 10, Training loss = 0.28644897358921856
Iteration 20, Training loss = 0.2612776100062407
Iteration 30, Training loss = 0.24969918510088554
Iteration 40, Training loss = 0.24525345689975298
Iteration 50, Training loss = 0.24227744656113478
Iteration 60, Training loss = 0.23823377747948354
Iteration 70, Training loss = 0.2356356167449401
Iteration 80, Training loss = 0.23482381724394286
Iteration 90, Training loss = 0.23025212264977968
Model training time: 14.247438192367554
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.072241866817841
Iteration 10, Training loss = 0.9826730959690534
Iteration 20, Training loss = 0.9377345798107294
Iteration 30, Training loss = 0.8820451704355387
Iteration 40, Training loss = 0.8089131884849988
Iteration 50, Training loss = 0.7340664726037246
Iteration 60, Training loss = 0.6655423159782703
Iteration 70, Training loss = 0.6075433876651984
Iteration 80, Training loss = 0.5619713572355417
Iteration 90, Training loss = 0.5308474170473906
Model training time: 12.950350046157837
Device: cuda
Iteration 0, Training loss = 1.1030240781032121
Iteration 10, Training loss = 1.01504636154725
Iteration 20, Training loss = 0.9825400480857263
Iteration 30, Training loss = 0.9419096215413167
Iteration 40, Training loss = 0.8830939325002524
Iteration 50, Training loss = 0.7811220700924213
Iteration 60, Training loss = 0.6447519138455391
Iteration 70, Training loss = 0.5116605689892402
Iteration 80, Training loss = 0.43421284567851287
Iteration 90, Training loss = 0.4022849809664946
Model training time: 12.854729652404785
Device: cuda
Iteration 0, Training loss = 1.0308269823972995
Iteration 10, Training loss = 0.9315248246376331
Iteration 20, Training loss = 0.8608659895566794
Iteration 30, Training loss = 0.7570466135556881
Iteration 40, Training loss = 0.6237061241498361
Iteration 50, Training loss = 0.4967144842331226
Iteration 60, Training loss = 0.42707586173827833
Iteration 70, Training loss = 0.39599779821359193
Iteration 80, Training loss = 0.3773684576153755
Iteration 90, Training loss = 0.36728643282101703
Model training time: 12.949641942977905
Device: cuda
Iteration 0, Training loss = 0.9652899824655973
Iteration 10, Training loss = 0.9060400483699945
Iteration 20, Training loss = 0.8207097087915127
Iteration 30, Training loss = 0.6998290453965847
Iteration 40, Training loss = 0.5619971362444071
Iteration 50, Training loss = 0.4604150899327718
Iteration 60, Training loss = 0.4076087732727711
Iteration 70, Training loss = 0.38260313352713216
Iteration 80, Training loss = 0.36685208403147185
Iteration 90, Training loss = 0.35508132955202687
Model training time: 13.040064573287964
Device: cuda
Iteration 0, Training loss = 1.0137313501192973
Iteration 10, Training loss = 0.9638764778008828
Iteration 20, Training loss = 0.9115240573883057
Iteration 30, Training loss = 0.8234770676264396
Iteration 40, Training loss = 0.6677250415086746
Iteration 50, Training loss = 0.502792819761313
Iteration 60, Training loss = 0.42215899378061295
Iteration 70, Training loss = 0.3912115612855324
Iteration 80, Training loss = 0.37503106548235965
Iteration 90, Training loss = 0.3634648793018781
Model training time: 13.216512441635132
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0087943753370872
Iteration 10, Training loss = 0.29287252489190835
Iteration 20, Training loss = 0.26824311081033486
Iteration 30, Training loss = 0.25685340194747996
Iteration 40, Training loss = 0.2477498438495856
Iteration 50, Training loss = 0.24073323951317713
Iteration 60, Training loss = 0.23666256207686204
Iteration 70, Training loss = 0.23383765094555342
Iteration 80, Training loss = 0.23139442446140143
Iteration 90, Training loss = 0.22872876118008906
Iteration 100, Training loss = 0.22587814611884263
Iteration 110, Training loss = 0.22519764504753625
Iteration 120, Training loss = 0.22316981135652617
Iteration 130, Training loss = 0.22140544624282762
Iteration 140, Training loss = 0.22023761100493944
Iteration 150, Training loss = 0.22006080786769205
Iteration 160, Training loss = 0.21971920591134292
Iteration 170, Training loss = 0.21826718661647576
Iteration 180, Training loss = 0.21729597649895227
Iteration 190, Training loss = 0.2175707837136892
Model training time: 28.484517335891724
Device: cuda
Iteration 0, Training loss = 0.9787940314182868
Iteration 10, Training loss = 0.29155451632463014
Iteration 20, Training loss = 0.2654983839736535
Iteration 30, Training loss = 0.25285983830690384
Iteration 40, Training loss = 0.24578741278785926
Iteration 50, Training loss = 0.24086766518079317
Iteration 60, Training loss = 0.23705454887105867
Iteration 70, Training loss = 0.2341777771138228
Iteration 80, Training loss = 0.2314268218783232
Iteration 90, Training loss = 0.22996656281443742
Iteration 100, Training loss = 0.22638653619931295
Iteration 110, Training loss = 0.22488210121026406
Iteration 120, Training loss = 0.22189545000974947
Iteration 130, Training loss = 0.22013591745725045
Iteration 140, Training loss = 0.2189537716599611
Iteration 150, Training loss = 0.21672244627888387
Iteration 160, Training loss = 0.21530320380742735
Iteration 170, Training loss = 0.21502357148207152
Iteration 180, Training loss = 0.21345951207555258
Iteration 190, Training loss = 0.21203478225148642
Model training time: 28.333736181259155
Device: cuda
Iteration 0, Training loss = 0.9543481102356544
Iteration 10, Training loss = 0.28805961975684535
Iteration 20, Training loss = 0.2657318782921021
Iteration 30, Training loss = 0.2513905494258954
Iteration 40, Training loss = 0.24233013047621801
Iteration 50, Training loss = 0.2363380302603428
Iteration 60, Training loss = 0.23172071662086707
Iteration 70, Training loss = 0.2273546507438788
Iteration 80, Training loss = 0.2257328345798529
Iteration 90, Training loss = 0.22312626672478822
Iteration 100, Training loss = 0.2202966267672869
Iteration 110, Training loss = 0.21846028388692781
Iteration 120, Training loss = 0.2163086312894638
Iteration 130, Training loss = 0.21359321131156042
Iteration 140, Training loss = 0.21342006841531166
Iteration 150, Training loss = 0.21082619377053702
Iteration 160, Training loss = 0.20954239569031274
Iteration 170, Training loss = 0.20856263373906797
Iteration 180, Training loss = 0.20730005032741106
Iteration 190, Training loss = 0.20685164057291472
Model training time: 28.282676219940186
Device: cuda
Iteration 0, Training loss = 0.816029280424118
Iteration 10, Training loss = 0.2786963776900218
Iteration 20, Training loss = 0.2563622066607842
Iteration 30, Training loss = 0.24653286658800566
Iteration 40, Training loss = 0.24121738540438506
Iteration 50, Training loss = 0.23738929027548203
Iteration 60, Training loss = 0.23336848960472986
Iteration 70, Training loss = 0.23095344952665842
Iteration 80, Training loss = 0.22732739952894357
Iteration 90, Training loss = 0.22283049013752204
Iteration 100, Training loss = 0.21979906180730233
Iteration 110, Training loss = 0.21681525701513657
Iteration 120, Training loss = 0.2139261648632013
Iteration 130, Training loss = 0.2109842048241542
Iteration 140, Training loss = 0.210264748105636
Iteration 150, Training loss = 0.20724100719850796
Iteration 160, Training loss = 0.20596417842003015
Iteration 170, Training loss = 0.20342583696429545
Iteration 180, Training loss = 0.2032640498991196
Iteration 190, Training loss = 0.20159204246906134
Model training time: 28.47593641281128
Device: cuda
Iteration 0, Training loss = 0.9438576514904315
Iteration 10, Training loss = 0.292825876806791
Iteration 20, Training loss = 0.2669968195259571
Iteration 30, Training loss = 0.25217502650160056
Iteration 40, Training loss = 0.24513103067874908
Iteration 50, Training loss = 0.23956621954074273
Iteration 60, Training loss = 0.23688300584371275
Iteration 70, Training loss = 0.2350466216985996
Iteration 80, Training loss = 0.23257359260549912
Iteration 90, Training loss = 0.23036463386737382
Iteration 100, Training loss = 0.23032163914579612
Iteration 110, Training loss = 0.22822157408182436
Iteration 120, Training loss = 0.22783674013156158
Iteration 130, Training loss = 0.2258865566780934
Iteration 140, Training loss = 0.22484648428284204
Iteration 150, Training loss = 0.224410126129022
Iteration 160, Training loss = 0.22267472858612353
Iteration 170, Training loss = 0.22203188991317382
Iteration 180, Training loss = 0.22031329199671745
Iteration 190, Training loss = 0.21938417851924896
Model training time: 27.946600675582886
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0270977581922824
Iteration 10, Training loss = 0.9611735768043078
Iteration 20, Training loss = 0.9026061583023804
Iteration 30, Training loss = 0.802054532445394
Iteration 40, Training loss = 0.6585288426050773
Iteration 50, Training loss = 0.548896115559798
Iteration 60, Training loss = 0.48975106099477184
Iteration 70, Training loss = 0.4542867776293021
Iteration 80, Training loss = 0.4286950717751796
Iteration 90, Training loss = 0.409853305381078
Iteration 100, Training loss = 0.3946720310128652
Iteration 110, Training loss = 0.3803962337282988
Iteration 120, Training loss = 0.3702177537175325
Iteration 130, Training loss = 0.36064593264689815
Iteration 140, Training loss = 0.35273407915463817
Iteration 150, Training loss = 0.34758288155381495
Iteration 160, Training loss = 0.3425186133155456
Iteration 170, Training loss = 0.3389524691379987
Iteration 180, Training loss = 0.33550790410775405
Iteration 190, Training loss = 0.33317985041783404
Model training time: 26.033649444580078
Device: cuda
Iteration 0, Training loss = 1.0460673627945094
Iteration 10, Training loss = 0.9707297568137829
Iteration 20, Training loss = 0.9201954339559262
Iteration 30, Training loss = 0.8626577292497342
Iteration 40, Training loss = 0.7853855146811559
Iteration 50, Training loss = 0.696440515609888
Iteration 60, Training loss = 0.6130420066989385
Iteration 70, Training loss = 0.5482218242608584
Iteration 80, Training loss = 0.5010918780015066
Iteration 90, Training loss = 0.4685022790844624
Iteration 100, Training loss = 0.44337864449390996
Iteration 110, Training loss = 0.42011045607236713
Iteration 120, Training loss = 0.40075046912981915
Iteration 130, Training loss = 0.38324811653448987
Iteration 140, Training loss = 0.36864931738147366
Iteration 150, Training loss = 0.35501793686013955
Iteration 160, Training loss = 0.3453474256854791
Iteration 170, Training loss = 0.3380495527615914
Iteration 180, Training loss = 0.33114487735124737
Iteration 190, Training loss = 0.3255918100476265
Model training time: 25.65611696243286
Device: cuda
Iteration 0, Training loss = 1.0843599495979457
Iteration 10, Training loss = 0.9754758603297747
Iteration 20, Training loss = 0.9090733700073682
Iteration 30, Training loss = 0.8093887097560443
Iteration 40, Training loss = 0.6484216583462862
Iteration 50, Training loss = 0.48076503494611156
Iteration 60, Training loss = 0.40435806891092885
Iteration 70, Training loss = 0.3818871482060506
Iteration 80, Training loss = 0.3723802652496558
Iteration 90, Training loss = 0.3653807800549727
Iteration 100, Training loss = 0.35878790284578616
Iteration 110, Training loss = 0.35326339247135013
Iteration 120, Training loss = 0.3487351115506429
Iteration 130, Training loss = 0.3448816239833832
Iteration 140, Training loss = 0.3412381187081337
Iteration 150, Training loss = 0.3378773515041058
Iteration 160, Training loss = 0.33564327055445087
Iteration 170, Training loss = 0.3318519408886249
Iteration 180, Training loss = 0.3296131497392288
Iteration 190, Training loss = 0.32690814338051355
Model training time: 25.898494243621826
Device: cuda
Iteration 0, Training loss = 1.0986034721136093
Iteration 10, Training loss = 0.971892957503979
Iteration 20, Training loss = 0.9023459966366107
Iteration 30, Training loss = 0.7924154469600091
Iteration 40, Training loss = 0.61884538600078
Iteration 50, Training loss = 0.4587310724533521
Iteration 60, Training loss = 0.3959369206657776
Iteration 70, Training loss = 0.37759889031832033
Iteration 80, Training loss = 0.36776813578147155
Iteration 90, Training loss = 0.35893425803918105
Iteration 100, Training loss = 0.3530347691132472
Iteration 110, Training loss = 0.34702762961387634
Iteration 120, Training loss = 0.34226155624939847
Iteration 130, Training loss = 0.3374534295155452
Iteration 140, Training loss = 0.33370234588017833
Iteration 150, Training loss = 0.33011863839167815
Iteration 160, Training loss = 0.32657611198150194
Iteration 170, Training loss = 0.32360194394221675
Iteration 180, Training loss = 0.3208710574186765
Iteration 190, Training loss = 0.3178838657645079
Model training time: 26.63004732131958
Device: cuda
Iteration 0, Training loss = 1.020909967330786
Iteration 10, Training loss = 0.9525994188510455
Iteration 20, Training loss = 0.8766622600647119
Iteration 30, Training loss = 0.755177072607554
Iteration 40, Training loss = 0.5790049645763177
Iteration 50, Training loss = 0.4409663350536273
Iteration 60, Training loss = 0.3884211893265064
Iteration 70, Training loss = 0.3679122792986723
Iteration 80, Training loss = 0.3552876395674852
Iteration 90, Training loss = 0.3477653442667081
Iteration 100, Training loss = 0.34238438594799775
Iteration 110, Training loss = 0.3388009787752078
Iteration 120, Training loss = 0.3337066892821055
Iteration 130, Training loss = 0.33074561019356435
Iteration 140, Training loss = 0.3282643542266809
Iteration 150, Training loss = 0.3263378558823696
Iteration 160, Training loss = 0.32474501259051836
Iteration 170, Training loss = 0.32189806980582386
Iteration 180, Training loss = 0.32028303811183345
Iteration 190, Training loss = 0.31886717619804233
Model training time: 26.809011220932007
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9721692811984283
Iteration 10, Training loss = 0.3048733022923653
Iteration 20, Training loss = 0.27951789217499584
Iteration 30, Training loss = 0.26510944819221127
Iteration 40, Training loss = 0.2565477997637712
Iteration 50, Training loss = 0.25143827583927375
Iteration 60, Training loss = 0.249320820833628
Iteration 70, Training loss = 0.2475914662847152
Iteration 80, Training loss = 0.2446222692154921
Iteration 90, Training loss = 0.24356322534955466
Iteration 100, Training loss = 0.2410306535088099
Iteration 110, Training loss = 0.24050436827998894
Iteration 120, Training loss = 0.23853603435250428
Iteration 130, Training loss = 0.23763665958092764
Iteration 140, Training loss = 0.23556658167105454
Iteration 150, Training loss = 0.2338059922823539
Iteration 160, Training loss = 0.23103636159346655
Iteration 170, Training loss = 0.229643674997183
Iteration 180, Training loss = 0.22795919156991518
Iteration 190, Training loss = 0.22725622384594038
Model training time: 29.05812692642212
Device: cuda
Iteration 0, Training loss = 0.9748621663221946
Iteration 10, Training loss = 0.4755807083386641
Iteration 20, Training loss = 0.2732362231382957
Iteration 30, Training loss = 0.26523337914393497
Iteration 40, Training loss = 0.26095260364504963
Iteration 50, Training loss = 0.25866196820369136
Iteration 60, Training loss = 0.25625386289679086
Iteration 70, Training loss = 0.2543611472042707
Iteration 80, Training loss = 0.251399397276915
Iteration 90, Training loss = 0.25016967923595357
Iteration 100, Training loss = 0.24818089002600083
Iteration 110, Training loss = 0.24748552762545073
Iteration 120, Training loss = 0.24582851506196535
Iteration 130, Training loss = 0.24609180224629548
Iteration 140, Training loss = 0.24344247550918505
Iteration 150, Training loss = 0.2425287924706936
Iteration 160, Training loss = 0.24150299395506197
Iteration 170, Training loss = 0.2414057541352052
Iteration 180, Training loss = 0.24030931666493416
Iteration 190, Training loss = 0.24018926803882307
Model training time: 28.83838653564453
Device: cuda
Iteration 0, Training loss = 1.1193194503967578
Iteration 10, Training loss = 0.29392025533777016
Iteration 20, Training loss = 0.26609943692500776
Iteration 30, Training loss = 0.2571083834538093
Iteration 40, Training loss = 0.24976359107173407
Iteration 50, Training loss = 0.24618487919752413
Iteration 60, Training loss = 0.24384754580947068
Iteration 70, Training loss = 0.24125837162137032
Iteration 80, Training loss = 0.239278520242526
Iteration 90, Training loss = 0.23823585217961898
Iteration 100, Training loss = 0.23656572286899274
Iteration 110, Training loss = 0.23427256454642004
Iteration 120, Training loss = 0.23315161781815383
Iteration 130, Training loss = 0.23176883820157784
Iteration 140, Training loss = 0.22821226372168615
Iteration 150, Training loss = 0.22728119589961493
Iteration 160, Training loss = 0.22404590077125108
Iteration 170, Training loss = 0.22198447298545104
Iteration 180, Training loss = 0.21899965348152015
Iteration 190, Training loss = 0.2208451064160237
Model training time: 29.041675329208374
Device: cuda
Iteration 0, Training loss = 0.9177579398338611
Iteration 10, Training loss = 0.28130268993285984
Iteration 20, Training loss = 0.26502615528611034
Iteration 30, Training loss = 0.25779296858952594
Iteration 40, Training loss = 0.2544646942271636
Iteration 50, Training loss = 0.25103841750667644
Iteration 60, Training loss = 0.24918756977869913
Iteration 70, Training loss = 0.24784219580201003
Iteration 80, Training loss = 0.24565029230255347
Iteration 90, Training loss = 0.24231868122632688
Iteration 100, Training loss = 0.2401130273938179
Iteration 110, Training loss = 0.23772951473410314
Iteration 120, Training loss = 0.23664027309188476
Iteration 130, Training loss = 0.2354165344284131
Iteration 140, Training loss = 0.23348280042409897
Iteration 150, Training loss = 0.2315812036395073
Iteration 160, Training loss = 0.2306204822200995
Iteration 170, Training loss = 0.22868942784575316
Iteration 180, Training loss = 0.22759666437139878
Iteration 190, Training loss = 0.2260064700475106
Model training time: 29.304412841796875
Device: cuda
Iteration 0, Training loss = 1.0549990855730498
Iteration 10, Training loss = 0.29785623716620296
Iteration 20, Training loss = 0.2674559778892077
Iteration 30, Training loss = 0.2560578908484716
Iteration 40, Training loss = 0.25257205533293575
Iteration 50, Training loss = 0.2473820187151432
Iteration 60, Training loss = 0.24490937390006506
Iteration 70, Training loss = 0.2431973499747423
Iteration 80, Training loss = 0.2417652546786345
Iteration 90, Training loss = 0.24011589844639486
Iteration 100, Training loss = 0.2398921179656799
Iteration 110, Training loss = 0.23775726528121874
Iteration 120, Training loss = 0.23742837458848953
Iteration 130, Training loss = 0.2363253247279387
Iteration 140, Training loss = 0.2348290171760779
Iteration 150, Training loss = 0.23333810585049483
Iteration 160, Training loss = 0.23322937809503996
Iteration 170, Training loss = 0.2327838488496267
Iteration 180, Training loss = 0.23224631496346915
Iteration 190, Training loss = 0.23206529794977263
Model training time: 28.87313723564148
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.141069110769492
Iteration 10, Training loss = 0.9856478411417741
Iteration 20, Training loss = 0.9467605616037662
Iteration 30, Training loss = 0.8928310229228094
Iteration 40, Training loss = 0.8007329862851363
Iteration 50, Training loss = 0.664475509753594
Iteration 60, Training loss = 0.5489005297422409
Iteration 70, Training loss = 0.48838633413498217
Iteration 80, Training loss = 0.4570294584219272
Iteration 90, Training loss = 0.4359733536839485
Iteration 100, Training loss = 0.41889747919944614
Iteration 110, Training loss = 0.4052802433188145
Iteration 120, Training loss = 0.393107254917805
Iteration 130, Training loss = 0.3823028906033589
Iteration 140, Training loss = 0.37358542818289536
Iteration 150, Training loss = 0.36505643278360367
Iteration 160, Training loss = 0.3597440112095613
Iteration 170, Training loss = 0.3535956299075714
Iteration 180, Training loss = 0.34891242591234356
Iteration 190, Training loss = 0.3449742031785158
Model training time: 26.511674404144287
Device: cuda
Iteration 0, Training loss = 1.2511157737328455
Iteration 10, Training loss = 1.0159518798956504
Iteration 20, Training loss = 0.9893648830743936
Iteration 30, Training loss = 0.9739452130519427
Iteration 40, Training loss = 0.9454761009949905
Iteration 50, Training loss = 0.8758498120766419
Iteration 60, Training loss = 0.7788712015518775
Iteration 70, Training loss = 0.6713491999186002
Iteration 80, Training loss = 0.5754814646564997
Iteration 90, Training loss = 0.5023445022793916
Iteration 100, Training loss = 0.45258212089538574
Iteration 110, Training loss = 0.41891088222081846
Iteration 120, Training loss = 0.3946964265062259
Iteration 130, Training loss = 0.3762234627054288
Iteration 140, Training loss = 0.36212370888544965
Iteration 150, Training loss = 0.3533265848572438
Iteration 160, Training loss = 0.3454835775953073
Iteration 170, Training loss = 0.3388049043715
Iteration 180, Training loss = 0.334225866657037
Iteration 190, Training loss = 0.3316948310686992
Model training time: 26.787631511688232
Device: cuda
Iteration 0, Training loss = 0.9759259120776103
Iteration 10, Training loss = 0.9015131821999183
Iteration 20, Training loss = 0.8151582594101245
Iteration 30, Training loss = 0.7123614744498179
Iteration 40, Training loss = 0.6261097135452124
Iteration 50, Training loss = 0.5630548659425515
Iteration 60, Training loss = 0.5197559274159945
Iteration 70, Training loss = 0.4881074640613336
Iteration 80, Training loss = 0.462751641869545
Iteration 90, Training loss = 0.44204504501361114
Iteration 100, Training loss = 0.42600469635083127
Iteration 110, Training loss = 0.40728033219392484
Iteration 120, Training loss = 0.3944779708981514
Iteration 130, Training loss = 0.3832150250673294
Iteration 140, Training loss = 0.37272668457948244
Iteration 150, Training loss = 0.36545430696927583
Iteration 160, Training loss = 0.35856151351561916
Iteration 170, Training loss = 0.3523589756626349
Iteration 180, Training loss = 0.3480963655389272
Iteration 190, Training loss = 0.34499704006772774
Model training time: 26.945348501205444
Device: cuda
Iteration 0, Training loss = 1.12628220136349
Iteration 10, Training loss = 0.9845187492095507
Iteration 20, Training loss = 0.953012812596101
Iteration 30, Training loss = 0.9144242394428986
Iteration 40, Training loss = 0.8505216527443665
Iteration 50, Training loss = 0.7512954805906003
Iteration 60, Training loss = 0.6236463091694392
Iteration 70, Training loss = 0.5127209029518641
Iteration 80, Training loss = 0.4565806480554434
Iteration 90, Training loss = 0.43252400251535267
Iteration 100, Training loss = 0.4170912246291454
Iteration 110, Training loss = 0.4051627224454513
Iteration 120, Training loss = 0.3933445060482392
Iteration 130, Training loss = 0.3827423218351144
Iteration 140, Training loss = 0.37511305568309933
Iteration 150, Training loss = 0.36650375849925554
Iteration 160, Training loss = 0.35943983094050336
Iteration 170, Training loss = 0.35458605965742696
Iteration 180, Training loss = 0.34817256090732723
Iteration 190, Training loss = 0.3439235297533182
Model training time: 26.97245502471924
Device: cuda
Iteration 0, Training loss = 1.0816822350025177
Iteration 10, Training loss = 0.927115716613256
Iteration 20, Training loss = 0.8050668457379708
Iteration 30, Training loss = 0.6343377725436137
Iteration 40, Training loss = 0.5018668312292832
Iteration 50, Training loss = 0.4400966786421262
Iteration 60, Training loss = 0.4092694985178801
Iteration 70, Training loss = 0.3907679640329801
Iteration 80, Training loss = 0.3764919678752239
Iteration 90, Training loss = 0.366035344508978
Iteration 100, Training loss = 0.35802429112104267
Iteration 110, Training loss = 0.35025931837467045
Iteration 120, Training loss = 0.3442704792206104
Iteration 130, Training loss = 0.3396342322230339
Iteration 140, Training loss = 0.3367218552873685
Iteration 150, Training loss = 0.3341294644543758
Iteration 160, Training loss = 0.3304453930602624
Iteration 170, Training loss = 0.32855728899057096
Iteration 180, Training loss = 0.3263114238014588
Iteration 190, Training loss = 0.32488193993385023
Model training time: 26.621219635009766
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0165990808835397
Iteration 10, Training loss = 0.2917702839924739
Iteration 20, Training loss = 0.2669338836119725
Iteration 30, Training loss = 0.25414149491832805
Iteration 40, Training loss = 0.24803563035451448
Iteration 50, Training loss = 0.24475722530713448
Iteration 60, Training loss = 0.2427877144744763
Iteration 70, Training loss = 0.24004105421213004
Iteration 80, Training loss = 0.23722380170455346
Iteration 90, Training loss = 0.23733400868681762
Iteration 100, Training loss = 0.23542975577024314
Iteration 110, Training loss = 0.23399458624995673
Iteration 120, Training loss = 0.23313511793430036
Iteration 130, Training loss = 0.23220946439183676
Iteration 140, Training loss = 0.23092631135995573
Iteration 150, Training loss = 0.22961146069260743
Iteration 160, Training loss = 0.22837203368544579
Iteration 170, Training loss = 0.22765366704418108
Iteration 180, Training loss = 0.2284362682929406
Iteration 190, Training loss = 0.22611089767171785
Model training time: 28.78302025794983
Device: cuda
Iteration 0, Training loss = 0.9739327006615125
Iteration 10, Training loss = 0.2881210452088943
Iteration 20, Training loss = 0.2642594850980319
Iteration 30, Training loss = 0.25291347618286425
Iteration 40, Training loss = 0.24701701964323336
Iteration 50, Training loss = 0.24061282695485994
Iteration 60, Training loss = 0.2376155171256799
Iteration 70, Training loss = 0.2347054212139203
Iteration 80, Training loss = 0.2318169935964621
Iteration 90, Training loss = 0.22902704058931425
Iteration 100, Training loss = 0.22567991969677118
Iteration 110, Training loss = 0.22466408747893113
Iteration 120, Training loss = 0.22274165256665304
Iteration 130, Training loss = 0.2215841830923007
Iteration 140, Training loss = 0.22159668172781283
Iteration 150, Training loss = 0.21961622226696748
Iteration 160, Training loss = 0.218937347141596
Iteration 170, Training loss = 0.21780336590913627
Iteration 180, Training loss = 0.21745515385499367
Iteration 190, Training loss = 0.2175706375676852
Model training time: 28.449726343154907
Device: cuda
Iteration 0, Training loss = 0.9400513080450205
Iteration 10, Training loss = 0.29085484662881267
Iteration 20, Training loss = 0.270637206446666
Iteration 30, Training loss = 0.25896815898326725
Iteration 40, Training loss = 0.2515234379814221
Iteration 50, Training loss = 0.2457663333759858
Iteration 60, Training loss = 0.2428721682383464
Iteration 70, Training loss = 0.23933309459915528
Iteration 80, Training loss = 0.23716132973249143
Iteration 90, Training loss = 0.23404032765672758
Iteration 100, Training loss = 0.23248654002180466
Iteration 110, Training loss = 0.22917777815690407
Iteration 120, Training loss = 0.22611650853202894
Iteration 130, Training loss = 0.2242730540724901
Iteration 140, Training loss = 0.22168607408037552
Iteration 150, Training loss = 0.22154255354633698
Iteration 160, Training loss = 0.2194343229325918
Iteration 170, Training loss = 0.2187173733344445
Iteration 180, Training loss = 0.21789906775722137
Iteration 190, Training loss = 0.21786485182551238
Model training time: 28.42135453224182
Device: cuda
Iteration 0, Training loss = 1.015304644520466
Iteration 10, Training loss = 0.2848837392834517
Iteration 20, Training loss = 0.2651524583880718
Iteration 30, Training loss = 0.25281139033345074
Iteration 40, Training loss = 0.24637940468696448
Iteration 50, Training loss = 0.23972081994781128
Iteration 60, Training loss = 0.23635097650381234
Iteration 70, Training loss = 0.2325803769322542
Iteration 80, Training loss = 0.22934933809133676
Iteration 90, Training loss = 0.226869165037687
Iteration 100, Training loss = 0.22407233371184423
Iteration 110, Training loss = 0.22177241742610931
Iteration 120, Training loss = 0.22020135074853897
Iteration 130, Training loss = 0.21878216587580168
Iteration 140, Training loss = 0.21753985090897635
Iteration 150, Training loss = 0.2160026511320701
Iteration 160, Training loss = 0.21584032877133444
Iteration 170, Training loss = 0.2142362121779185
Iteration 180, Training loss = 0.2131905693274278
Iteration 190, Training loss = 0.2114577001103988
Model training time: 29.085264205932617
Device: cuda
Iteration 0, Training loss = 1.101883548956651
Iteration 10, Training loss = 0.29645580827043605
Iteration 20, Training loss = 0.2679723882331298
Iteration 30, Training loss = 0.25618413663827455
Iteration 40, Training loss = 0.24985409957858232
Iteration 50, Training loss = 0.24543131744632354
Iteration 60, Training loss = 0.24304705686294115
Iteration 70, Training loss = 0.23988215825878656
Iteration 80, Training loss = 0.237271359620186
Iteration 90, Training loss = 0.2362464308165587
Iteration 100, Training loss = 0.23487798439768645
Iteration 110, Training loss = 0.23285093244451743
Iteration 120, Training loss = 0.23139553201886323
Iteration 130, Training loss = 0.23084969302782646
Iteration 140, Training loss = 0.22887358241356337
Iteration 150, Training loss = 0.22841704063690627
Iteration 160, Training loss = 0.22881733855375877
Iteration 170, Training loss = 0.22798708568398768
Iteration 180, Training loss = 0.2268621136362736
Iteration 190, Training loss = 0.22695128820263422
Model training time: 28.78457474708557
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9238454218094165
Iteration 10, Training loss = 0.8097666490536469
Iteration 20, Training loss = 0.6554830693281614
Iteration 30, Training loss = 0.5248442183320339
Iteration 40, Training loss = 0.4554104724755654
Iteration 50, Training loss = 0.4189880375678723
Iteration 60, Training loss = 0.3938825067419272
Iteration 70, Training loss = 0.3757837156836803
Iteration 80, Training loss = 0.3619172676251485
Iteration 90, Training loss = 0.35218219000559586
Iteration 100, Training loss = 0.3449095132259222
Iteration 110, Training loss = 0.338917115273384
Iteration 120, Training loss = 0.33465809002518654
Iteration 130, Training loss = 0.3315495026226227
Iteration 140, Training loss = 0.32830053023420847
Iteration 150, Training loss = 0.32602040469646454
Iteration 160, Training loss = 0.3245825655758381
Iteration 170, Training loss = 0.32303625316574025
Iteration 180, Training loss = 0.3206203938103639
Iteration 190, Training loss = 0.3195232579914423
Model training time: 26.63422656059265
Device: cuda
Iteration 0, Training loss = 1.1056118951394007
Iteration 10, Training loss = 0.9650155592423219
Iteration 20, Training loss = 0.8784665018320084
Iteration 30, Training loss = 0.7503233047632071
Iteration 40, Training loss = 0.5877267162387187
Iteration 50, Training loss = 0.47760281482568157
Iteration 60, Training loss = 0.43154216213868213
Iteration 70, Training loss = 0.4024522006511688
Iteration 80, Training loss = 0.38215426871409786
Iteration 90, Training loss = 0.3679624807376128
Iteration 100, Training loss = 0.3568110460272202
Iteration 110, Training loss = 0.34844249658859694
Iteration 120, Training loss = 0.341947466135025
Iteration 130, Training loss = 0.33848459789386165
Iteration 140, Training loss = 0.33459141105413437
Iteration 150, Training loss = 0.3320814070220177
Iteration 160, Training loss = 0.33023634323706996
Iteration 170, Training loss = 0.3291770870295855
Iteration 180, Training loss = 0.32798834202381283
Iteration 190, Training loss = 0.32474277884914327
Model training time: 26.470232248306274
Device: cuda
Iteration 0, Training loss = 1.0713591965345235
Iteration 10, Training loss = 0.9603462826747161
Iteration 20, Training loss = 0.8932371105139072
Iteration 30, Training loss = 0.7892110966719114
Iteration 40, Training loss = 0.654637234715315
Iteration 50, Training loss = 0.5514605263104806
Iteration 60, Training loss = 0.4928101599216461
Iteration 70, Training loss = 0.45643080599032915
Iteration 80, Training loss = 0.43208000522393447
Iteration 90, Training loss = 0.4134434060408519
Iteration 100, Training loss = 0.3977892736975963
Iteration 110, Training loss = 0.38683317601680756
Iteration 120, Training loss = 0.3766246438026428
Iteration 130, Training loss = 0.36778045617617094
Iteration 140, Training loss = 0.36196514104421323
Iteration 150, Training loss = 0.35635540548425454
Iteration 160, Training loss = 0.3532752718489904
Iteration 170, Training loss = 0.34796423751574296
Iteration 180, Training loss = 0.34503849939658093
Iteration 190, Training loss = 0.3419742011106931
Model training time: 26.15360379219055
Device: cuda
Iteration 0, Training loss = 1.0180916018210924
Iteration 10, Training loss = 0.8879179496031541
Iteration 20, Training loss = 0.732563309944593
Iteration 30, Training loss = 0.5563587660972888
Iteration 40, Training loss = 0.45539649518636555
Iteration 50, Training loss = 0.415654718875885
Iteration 60, Training loss = 0.392065197802507
Iteration 70, Training loss = 0.37613444660718626
Iteration 80, Training loss = 0.3642219454050064
Iteration 90, Training loss = 0.35396379232406616
Iteration 100, Training loss = 0.3455352301780994
Iteration 110, Training loss = 0.3392905117227481
Iteration 120, Training loss = 0.33406498111211336
Iteration 130, Training loss = 0.32935264190802205
Iteration 140, Training loss = 0.32485300216537255
Iteration 150, Training loss = 0.322003977229962
Iteration 160, Training loss = 0.31920449005869717
Iteration 170, Training loss = 0.31782810676556367
Iteration 180, Training loss = 0.3147396164444777
Iteration 190, Training loss = 0.3132741396816877
Model training time: 26.25645613670349
Device: cuda
Iteration 0, Training loss = 1.0974210856052546
Iteration 10, Training loss = 1.0061709777666972
Iteration 20, Training loss = 0.9660004961949128
Iteration 30, Training loss = 0.9151507169008255
Iteration 40, Training loss = 0.8342048434110788
Iteration 50, Training loss = 0.7229343561025766
Iteration 60, Training loss = 0.5872266710950778
Iteration 70, Training loss = 0.4837201008429894
Iteration 80, Training loss = 0.4288822292135312
Iteration 90, Training loss = 0.40392032953409046
Iteration 100, Training loss = 0.3868480594112323
Iteration 110, Training loss = 0.37403452998170483
Iteration 120, Training loss = 0.3641291054395529
Iteration 130, Training loss = 0.3561091491809258
Iteration 140, Training loss = 0.34799747398266423
Iteration 150, Training loss = 0.3419421888314761
Iteration 160, Training loss = 0.3373124639575298
Iteration 170, Training loss = 0.3313388322981504
Iteration 180, Training loss = 0.3278892573255759
Iteration 190, Training loss = 0.3248436227440834
Model training time: 26.03906512260437
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9541069200405707
Iteration 10, Training loss = 0.29098225127045924
Iteration 20, Training loss = 0.2719370909035206
Iteration 30, Training loss = 0.26342349957961303
Iteration 40, Training loss = 0.2589826712814661
Iteration 50, Training loss = 0.2548242681301557
Iteration 60, Training loss = 0.25085082621528554
Iteration 70, Training loss = 0.24690067424223974
Iteration 80, Training loss = 0.24354053346010354
Iteration 90, Training loss = 0.23937841390187925
Iteration 100, Training loss = 0.236827537130851
Iteration 110, Training loss = 0.2360412893960109
Iteration 120, Training loss = 0.23278879947387254
Iteration 130, Training loss = 0.23160663762917885
Iteration 140, Training loss = 0.22981438499230605
Iteration 150, Training loss = 0.2287402726136721
Iteration 160, Training loss = 0.2277613987143223
Iteration 170, Training loss = 0.22507177150020233
Iteration 180, Training loss = 0.22498163093741125
Iteration 190, Training loss = 0.2236066168317428
Iteration 200, Training loss = 0.22327289587030044
Iteration 210, Training loss = 0.2222164674447133
Iteration 220, Training loss = 0.22249697292080292
Iteration 230, Training loss = 0.22095225837368232
Iteration 240, Training loss = 0.21996077780540174
Iteration 250, Training loss = 0.21859645041135642
Iteration 260, Training loss = 0.21788387946211374
Iteration 270, Training loss = 0.21746888785408094
Iteration 280, Training loss = 0.2149349476855535
Iteration 290, Training loss = 0.2148792093189863
Model training time: 41.91566300392151
Device: cuda
Iteration 0, Training loss = 0.9538698437122198
Iteration 10, Training loss = 0.30293669580267024
Iteration 20, Training loss = 0.27379509090231013
Iteration 30, Training loss = 0.26124724688438267
Iteration 40, Training loss = 0.25413105722803336
Iteration 50, Training loss = 0.24676659846535096
Iteration 60, Training loss = 0.24281896851383722
Iteration 70, Training loss = 0.2374209208557239
Iteration 80, Training loss = 0.23581668820518714
Iteration 90, Training loss = 0.23163795585815722
Iteration 100, Training loss = 0.23022747555604348
Iteration 110, Training loss = 0.22743090328115684
Iteration 120, Training loss = 0.2264748840377881
Iteration 130, Training loss = 0.2256593079521106
Iteration 140, Training loss = 0.22498246310995176
Iteration 150, Training loss = 0.22239229283653772
Iteration 160, Training loss = 0.22077695308969572
Iteration 170, Training loss = 0.21935776305886415
Iteration 180, Training loss = 0.21843808918045118
Iteration 190, Training loss = 0.21713332573954874
Iteration 200, Training loss = 0.216420655640272
Iteration 210, Training loss = 0.21570108401087615
Iteration 220, Training loss = 0.217205233585376
Iteration 230, Training loss = 0.2141499358874101
Iteration 240, Training loss = 0.21500115010600823
Iteration 250, Training loss = 0.213551614433527
Iteration 260, Training loss = 0.21243087431559196
Iteration 270, Training loss = 0.21285324056561178
Iteration 280, Training loss = 0.21109372062178758
Iteration 290, Training loss = 0.21017213638585347
Model training time: 42.13559889793396
Device: cuda
Iteration 0, Training loss = 0.9552606951731902
Iteration 10, Training loss = 0.2855565840235123
Iteration 20, Training loss = 0.2627466472868736
Iteration 30, Training loss = 0.24955783039331436
Iteration 40, Training loss = 0.24245328771380278
Iteration 50, Training loss = 0.23744799597905233
Iteration 60, Training loss = 0.2346502382021684
Iteration 70, Training loss = 0.2309039493019764
Iteration 80, Training loss = 0.22782387441167465
Iteration 90, Training loss = 0.22430295755083746
Iteration 100, Training loss = 0.2213825640770105
Iteration 110, Training loss = 0.2194625400006771
Iteration 120, Training loss = 0.21799692855431482
Iteration 130, Training loss = 0.21702529747898763
Iteration 140, Training loss = 0.2143725795814624
Iteration 150, Training loss = 0.21473671180697587
Iteration 160, Training loss = 0.21299905215318388
Iteration 170, Training loss = 0.211485106498003
Iteration 180, Training loss = 0.2108561981182832
Iteration 190, Training loss = 0.20977910894614
Iteration 200, Training loss = 0.20892915989343935
Iteration 210, Training loss = 0.20917512677036798
Iteration 220, Training loss = 0.20878158767636007
Iteration 230, Training loss = 0.20750077154773933
Iteration 240, Training loss = 0.2067698065478068
Iteration 250, Training loss = 0.2064020404448876
Iteration 260, Training loss = 0.2055579538528736
Iteration 270, Training loss = 0.20609028866657844
Iteration 280, Training loss = 0.20489820150228646
Iteration 290, Training loss = 0.2036004805794129
Model training time: 42.414888858795166
Device: cuda
Iteration 0, Training loss = 1.012778809437385
Iteration 10, Training loss = 0.2878264506848959
Iteration 20, Training loss = 0.26132546136012447
Iteration 30, Training loss = 0.24851649397840866
Iteration 40, Training loss = 0.24075954751326487
Iteration 50, Training loss = 0.2361932829595529
Iteration 60, Training loss = 0.23319356773908323
Iteration 70, Training loss = 0.23101058057867563
Iteration 80, Training loss = 0.22662508974854761
Iteration 90, Training loss = 0.22515730559825897
Iteration 100, Training loss = 0.22249698037138352
Iteration 110, Training loss = 0.22111008430902773
Iteration 120, Training loss = 0.2192314676940441
Iteration 130, Training loss = 0.21648642650017372
Iteration 140, Training loss = 0.2153535341987243
Iteration 150, Training loss = 0.21456306112500337
Iteration 160, Training loss = 0.2131719024708638
Iteration 170, Training loss = 0.21087211685685012
Iteration 180, Training loss = 0.21012748405337334
Iteration 190, Training loss = 0.2094669138582853
Iteration 200, Training loss = 0.20826682219138512
Iteration 210, Training loss = 0.207390908151865
Iteration 220, Training loss = 0.20722454958237135
Iteration 230, Training loss = 0.2063458854189286
Iteration 240, Training loss = 0.20570988122087258
Iteration 250, Training loss = 0.20407726472386947
Iteration 260, Training loss = 0.20502488744946626
Iteration 270, Training loss = 0.20433715960154167
Iteration 280, Training loss = 0.20322933735755774
Iteration 290, Training loss = 0.2022798236172933
Model training time: 41.966614961624146
Device: cuda
Iteration 0, Training loss = 0.9602395399258687
Iteration 10, Training loss = 0.2809994868361033
Iteration 20, Training loss = 0.2618742963442436
Iteration 30, Training loss = 0.24808253519810164
Iteration 40, Training loss = 0.24161366172708
Iteration 50, Training loss = 0.23709752410650253
Iteration 60, Training loss = 0.23313213827518317
Iteration 70, Training loss = 0.22999825758429673
Iteration 80, Training loss = 0.22812520282772872
Iteration 90, Training loss = 0.22460783101045168
Iteration 100, Training loss = 0.22259188185517603
Iteration 110, Training loss = 0.22021867535435236
Iteration 120, Training loss = 0.21838658532271019
Iteration 130, Training loss = 0.21596120297908783
Iteration 140, Training loss = 0.21471791771742013
Iteration 150, Training loss = 0.21234574942634657
Iteration 160, Training loss = 0.21118128271057054
Iteration 170, Training loss = 0.21026038378477097
Iteration 180, Training loss = 0.2091262202996474
Iteration 190, Training loss = 0.20768261844149002
Iteration 200, Training loss = 0.2068150249811319
Iteration 210, Training loss = 0.20659012548052347
Iteration 220, Training loss = 0.20551562280609056
Iteration 230, Training loss = 0.20531180415015954
Iteration 240, Training loss = 0.20349914036118066
Iteration 250, Training loss = 0.20312074313943201
Iteration 260, Training loss = 0.20320516652785814
Iteration 270, Training loss = 0.20111124532727095
Iteration 280, Training loss = 0.20119357395630616
Iteration 290, Training loss = 0.20113457939945734
Model training time: 41.788047313690186
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1060216438311796
Iteration 10, Training loss = 0.9707866689333549
Iteration 20, Training loss = 0.9166796207427979
Iteration 30, Training loss = 0.8431650881583874
Iteration 40, Training loss = 0.7253218453664046
Iteration 50, Training loss = 0.5714234830095217
Iteration 60, Training loss = 0.45479675038502765
Iteration 70, Training loss = 0.4092761278152466
Iteration 80, Training loss = 0.3893161599452679
Iteration 90, Training loss = 0.3764299968114266
Iteration 100, Training loss = 0.366162135050847
Iteration 110, Training loss = 0.35704533755779266
Iteration 120, Training loss = 0.34984514518426013
Iteration 130, Training loss = 0.3443728519173769
Iteration 140, Training loss = 0.3381702321080061
Iteration 150, Training loss = 0.33358146565464825
Iteration 160, Training loss = 0.32934769414938414
Iteration 170, Training loss = 0.325355415733961
Iteration 180, Training loss = 0.32271321289814436
Iteration 190, Training loss = 0.319093403334801
Iteration 200, Training loss = 0.31609258915369326
Iteration 210, Training loss = 0.31545289491231626
Iteration 220, Training loss = 0.31187202953375304
Iteration 230, Training loss = 0.30966454199873483
Iteration 240, Training loss = 0.30784830565635973
Iteration 250, Training loss = 0.30705750447053176
Iteration 260, Training loss = 0.30421038229878133
Iteration 270, Training loss = 0.30286391337330526
Iteration 280, Training loss = 0.3015272875244801
Iteration 290, Training loss = 0.30027883929701954
Model training time: 38.825145959854126
Device: cuda
Iteration 0, Training loss = 1.0048895203150237
Iteration 10, Training loss = 0.9477610576611298
Iteration 20, Training loss = 0.87642119710262
Iteration 30, Training loss = 0.7575929474372131
Iteration 40, Training loss = 0.600878717807623
Iteration 50, Training loss = 0.489742453281696
Iteration 60, Training loss = 0.4330541015817569
Iteration 70, Training loss = 0.40532280332767046
Iteration 80, Training loss = 0.38885364394921523
Iteration 90, Training loss = 0.3778299488700353
Iteration 100, Training loss = 0.37097151978657794
Iteration 110, Training loss = 0.36369338918190736
Iteration 120, Training loss = 0.35966239783626336
Iteration 130, Training loss = 0.35650744633032727
Iteration 140, Training loss = 0.3520663265998547
Iteration 150, Training loss = 0.3486255699625382
Iteration 160, Training loss = 0.3461416776363666
Iteration 170, Training loss = 0.3440479137576543
Iteration 180, Training loss = 0.34085751267579883
Iteration 190, Training loss = 0.3385114844601888
Iteration 200, Training loss = 0.3372889020695136
Iteration 210, Training loss = 0.33479893551423
Iteration 220, Training loss = 0.33390891494659275
Iteration 230, Training loss = 0.3312424131884025
Iteration 240, Training loss = 0.3289109909763703
Iteration 250, Training loss = 0.3292840395409327
Iteration 260, Training loss = 0.32614666395462477
Iteration 270, Training loss = 0.3256034134672238
Iteration 280, Training loss = 0.32334912912203717
Iteration 290, Training loss = 0.32154249485868674
Model training time: 39.14183449745178
Device: cuda
Iteration 0, Training loss = 1.0244889075939472
Iteration 10, Training loss = 0.966137075653443
Iteration 20, Training loss = 0.9100283739658502
Iteration 30, Training loss = 0.8083551766780707
Iteration 40, Training loss = 0.6493330276929415
Iteration 50, Training loss = 0.4698102130339696
Iteration 60, Training loss = 0.38333238661289215
Iteration 70, Training loss = 0.36121353850914883
Iteration 80, Training loss = 0.3515273306805354
Iteration 90, Training loss = 0.3444495149529897
Iteration 100, Training loss = 0.33985181525349617
Iteration 110, Training loss = 0.33540434247026074
Iteration 120, Training loss = 0.3319893043774825
Iteration 130, Training loss = 0.3293535408492272
Iteration 140, Training loss = 0.3265381251963285
Iteration 150, Training loss = 0.32553796384197015
Iteration 160, Training loss = 0.3230220225568001
Iteration 170, Training loss = 0.3206297348325069
Iteration 180, Training loss = 0.318600601874865
Iteration 190, Training loss = 0.31818262602274233
Iteration 200, Training loss = 0.3165086651077637
Iteration 210, Training loss = 0.31401467552551854
Iteration 220, Training loss = 0.3134640363546518
Iteration 230, Training loss = 0.31283809502537435
Iteration 240, Training loss = 0.3116483146754595
Iteration 250, Training loss = 0.31078582171064156
Iteration 260, Training loss = 0.3086780579044269
Iteration 270, Training loss = 0.3072807745864758
Iteration 280, Training loss = 0.3060812746676115
Iteration 290, Training loss = 0.3052992680324958
Model training time: 39.06151604652405
Device: cuda
Iteration 0, Training loss = 1.0140934620912259
Iteration 10, Training loss = 0.9325536168538607
Iteration 20, Training loss = 0.8555120069247025
Iteration 30, Training loss = 0.719785587145732
Iteration 40, Training loss = 0.5561189433703055
Iteration 50, Training loss = 0.4511409098139176
Iteration 60, Training loss = 0.4029950946569443
Iteration 70, Training loss = 0.3787083184489837
Iteration 80, Training loss = 0.36495663913396686
Iteration 90, Training loss = 0.35312555042596966
Iteration 100, Training loss = 0.34430804218237215
Iteration 110, Training loss = 0.3383594143849153
Iteration 120, Training loss = 0.33334421538389647
Iteration 130, Training loss = 0.33006831688376576
Iteration 140, Training loss = 0.32752887732707536
Iteration 150, Training loss = 0.3225911405797188
Iteration 160, Training loss = 0.32072808507543343
Iteration 170, Training loss = 0.31811080758388227
Iteration 180, Training loss = 0.31549363984511447
Iteration 190, Training loss = 0.31361349328206134
Iteration 200, Training loss = 0.3101337875884313
Iteration 210, Training loss = 0.3081977851688862
Iteration 220, Training loss = 0.30694065902095574
Iteration 230, Training loss = 0.3057228633417533
Iteration 240, Training loss = 0.3024241445729366
Iteration 250, Training loss = 0.30139161159212774
Iteration 260, Training loss = 0.2997654295311524
Iteration 270, Training loss = 0.29827132563178355
Iteration 280, Training loss = 0.29728272786507237
Iteration 290, Training loss = 0.29586922692564815
Model training time: 39.33501625061035
Device: cuda
Iteration 0, Training loss = 0.9751684849078839
Iteration 10, Training loss = 0.9458860903978348
Iteration 20, Training loss = 0.9003829944592255
Iteration 30, Training loss = 0.8199698294584568
Iteration 40, Training loss = 0.6865438692844831
Iteration 50, Training loss = 0.5356818695481007
Iteration 60, Training loss = 0.4291325962314239
Iteration 70, Training loss = 0.38712127850605893
Iteration 80, Training loss = 0.3697037983399171
Iteration 90, Training loss = 0.3595282882452011
Iteration 100, Training loss = 0.3513311199270762
Iteration 110, Training loss = 0.3447437802186379
Iteration 120, Training loss = 0.3396401247726037
Iteration 130, Training loss = 0.3341024351807741
Iteration 140, Training loss = 0.3298488035798073
Iteration 150, Training loss = 0.326949344518093
Iteration 160, Training loss = 0.3236385007890371
Iteration 170, Training loss = 0.32250098482920575
Iteration 180, Training loss = 0.3197182964247007
Iteration 190, Training loss = 0.3169695892586158
Iteration 200, Training loss = 0.3145493770448061
Iteration 210, Training loss = 0.3139239139854908
Iteration 220, Training loss = 0.31155460528456247
Iteration 230, Training loss = 0.310901968811567
Iteration 240, Training loss = 0.3088138255362327
Iteration 250, Training loss = 0.3081157889503699
Iteration 260, Training loss = 0.30636038086735284
Iteration 270, Training loss = 0.30475270175016844
Iteration 280, Training loss = 0.3037758862169889
Iteration 290, Training loss = 0.30215725417320544
Model training time: 39.020684003829956
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9071265653922007
Iteration 10, Training loss = 0.30101328343153
Iteration 20, Training loss = 0.27602400372807795
Iteration 30, Training loss = 0.2647235147081889
Iteration 40, Training loss = 0.25929907909952676
Iteration 50, Training loss = 0.25460498894636446
Iteration 60, Training loss = 0.25037267288336384
Iteration 70, Training loss = 0.2468478920368048
Iteration 80, Training loss = 0.24480171501636505
Iteration 90, Training loss = 0.24159998102830008
Iteration 100, Training loss = 0.23949848817518124
Iteration 110, Training loss = 0.23772137010326752
Iteration 120, Training loss = 0.23634588632446069
Iteration 130, Training loss = 0.23472912294360307
Iteration 140, Training loss = 0.2329786651982711
Iteration 150, Training loss = 0.23272184597758147
Iteration 160, Training loss = 0.23053802607151178
Iteration 170, Training loss = 0.22820671533162779
Iteration 180, Training loss = 0.22629433268537888
Iteration 190, Training loss = 0.22592482200035682
Iteration 200, Training loss = 0.22320132759901193
Iteration 210, Training loss = 0.22126072673843458
Iteration 220, Training loss = 0.22039821199499643
Iteration 230, Training loss = 0.2194489730665317
Iteration 240, Training loss = 0.2185448264846435
Iteration 250, Training loss = 0.2171488687969171
Iteration 260, Training loss = 0.21639547439721915
Iteration 270, Training loss = 0.21608996849793655
Iteration 280, Training loss = 0.2156748462181825
Iteration 290, Training loss = 0.21643786046367425
Model training time: 42.40564322471619
Device: cuda
Iteration 0, Training loss = 1.0829946650908544
Iteration 10, Training loss = 0.30919756568395174
Iteration 20, Training loss = 0.2729565601509351
Iteration 30, Training loss = 0.2602764284954621
Iteration 40, Training loss = 0.25446369040470856
Iteration 50, Training loss = 0.2508583071713264
Iteration 60, Training loss = 0.2484687460729709
Iteration 70, Training loss = 0.24606675568681496
Iteration 80, Training loss = 0.24394253899271673
Iteration 90, Training loss = 0.242077214213518
Iteration 100, Training loss = 0.24067941393989783
Iteration 110, Training loss = 0.23954990190955308
Iteration 120, Training loss = 0.23841742827342108
Iteration 130, Training loss = 0.2378679568377825
Iteration 140, Training loss = 0.2362620899310479
Iteration 150, Training loss = 0.23782366829422805
Iteration 160, Training loss = 0.23513466434983107
Iteration 170, Training loss = 0.2347131735430314
Iteration 180, Training loss = 0.2341992943905867
Iteration 190, Training loss = 0.2324411725768676
Iteration 200, Training loss = 0.232101905804414
Iteration 210, Training loss = 0.231348505960061
Iteration 220, Training loss = 0.2315286062657833
Iteration 230, Training loss = 0.230571265404041
Iteration 240, Training loss = 0.23102994721669418
Iteration 250, Training loss = 0.23060287707127058
Iteration 260, Training loss = 0.23074204474687576
Iteration 270, Training loss = 0.22918436532983413
Iteration 280, Training loss = 0.22828728676988527
Iteration 290, Training loss = 0.22917276248335838
Model training time: 42.559110164642334
Device: cuda
Iteration 0, Training loss = 1.0602831634191365
Iteration 10, Training loss = 0.3021950784784097
Iteration 20, Training loss = 0.2807066704218204
Iteration 30, Training loss = 0.2688776824909907
Iteration 40, Training loss = 0.26177101782881296
Iteration 50, Training loss = 0.25564616546034813
Iteration 60, Training loss = 0.2532161861084975
Iteration 70, Training loss = 0.2500852163021381
Iteration 80, Training loss = 0.25003770366311073
Iteration 90, Training loss = 0.24900349544791076
Iteration 100, Training loss = 0.24697537548266923
Iteration 110, Training loss = 0.24543787739597833
Iteration 120, Training loss = 0.24399196585783592
Iteration 130, Training loss = 0.2440156234571567
Iteration 140, Training loss = 0.24218833159941894
Iteration 150, Training loss = 0.24164332478092268
Iteration 160, Training loss = 0.2407210979324121
Iteration 170, Training loss = 0.23938000259491113
Iteration 180, Training loss = 0.239448820742277
Iteration 190, Training loss = 0.2393188327550888
Iteration 200, Training loss = 0.23782822690331018
Iteration 210, Training loss = 0.23666810903411645
Iteration 220, Training loss = 0.23658517748117447
Iteration 230, Training loss = 0.2350329321164351
Iteration 240, Training loss = 0.23413431014005953
Iteration 250, Training loss = 0.23471644291510949
Iteration 260, Training loss = 0.23358438278620058
Iteration 270, Training loss = 0.2342099928512023
Iteration 280, Training loss = 0.23264604147810203
Iteration 290, Training loss = 0.2328687092432609
Model training time: 42.16882395744324
Device: cuda
Iteration 0, Training loss = 1.1682703231389706
Iteration 10, Training loss = 0.30206528306007385
Iteration 20, Training loss = 0.27126319591815656
Iteration 30, Training loss = 0.26065973622294575
Iteration 40, Training loss = 0.255803519143508
Iteration 50, Training loss = 0.25306880416778416
Iteration 60, Training loss = 0.2507500886343993
Iteration 70, Training loss = 0.24841271684719965
Iteration 80, Training loss = 0.24616986656418213
Iteration 90, Training loss = 0.24487921624229506
Iteration 100, Training loss = 0.2441921099447287
Iteration 110, Training loss = 0.24210945545480803
Iteration 120, Training loss = 0.24078292199052298
Iteration 130, Training loss = 0.23924623114558366
Iteration 140, Training loss = 0.23791733699349257
Iteration 150, Training loss = 0.23613805696368217
Iteration 160, Training loss = 0.235253816900345
Iteration 170, Training loss = 0.23371258177436316
Iteration 180, Training loss = 0.23207021991793925
Iteration 190, Training loss = 0.23194181374632394
Iteration 200, Training loss = 0.23054469290834206
Iteration 210, Training loss = 0.22985013144520614
Iteration 220, Training loss = 0.22876131821137208
Iteration 230, Training loss = 0.22834821245991266
Iteration 240, Training loss = 0.22986441306196725
Iteration 250, Training loss = 0.22679351528103536
Iteration 260, Training loss = 0.22645287616894796
Iteration 270, Training loss = 0.2242678475494568
Iteration 280, Training loss = 0.22480893794160622
Iteration 290, Training loss = 0.2233610861003399
Model training time: 42.60431623458862
Device: cuda
Iteration 0, Training loss = 1.0050199719575734
Iteration 10, Training loss = 0.3007876148017553
Iteration 20, Training loss = 0.2764253771075836
Iteration 30, Training loss = 0.263384756560509
Iteration 40, Training loss = 0.25844384271364945
Iteration 50, Training loss = 0.2533962566118974
Iteration 60, Training loss = 0.2510102912783623
Iteration 70, Training loss = 0.24865410734827703
Iteration 80, Training loss = 0.24475636075322443
Iteration 90, Training loss = 0.2423085464307895
Iteration 100, Training loss = 0.2403926502626676
Iteration 110, Training loss = 0.23779309970828202
Iteration 120, Training loss = 0.23694866064649361
Iteration 130, Training loss = 0.2352829102713328
Iteration 140, Training loss = 0.2349995942069934
Iteration 150, Training loss = 0.23294278864677137
Iteration 160, Training loss = 0.2326193219767167
Iteration 170, Training loss = 0.23157456603187782
Iteration 180, Training loss = 0.23138507541555625
Iteration 190, Training loss = 0.2285574654547068
Iteration 200, Training loss = 0.2289683987888006
Iteration 210, Training loss = 0.22635242285636756
Iteration 220, Training loss = 0.22537745420749372
Iteration 230, Training loss = 0.2253856988480458
Iteration 240, Training loss = 0.2239705748282946
Iteration 250, Training loss = 0.223523715367684
Iteration 260, Training loss = 0.2226784277993899
Iteration 270, Training loss = 0.22153784477939972
Iteration 280, Training loss = 0.22139210970355913
Iteration 290, Training loss = 0.22096793869367012
Model training time: 43.883355140686035
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1253725336148188
Iteration 10, Training loss = 0.9747340530157089
Iteration 20, Training loss = 0.9311106548859522
Iteration 30, Training loss = 0.8611738727642939
Iteration 40, Training loss = 0.7454680468027408
Iteration 50, Training loss = 0.601716684607359
Iteration 60, Training loss = 0.494107906634991
Iteration 70, Training loss = 0.43721715876689327
Iteration 80, Training loss = 0.4095316007733345
Iteration 90, Training loss = 0.39170169773010105
Iteration 100, Training loss = 0.3779101245678388
Iteration 110, Training loss = 0.36808403180195737
Iteration 120, Training loss = 0.36035537948975194
Iteration 130, Training loss = 0.35442478782855547
Iteration 140, Training loss = 0.3490454015823511
Iteration 150, Training loss = 0.345695427690561
Iteration 160, Training loss = 0.3415209691111858
Iteration 170, Training loss = 0.3394310729434857
Iteration 180, Training loss = 0.33722467433947784
Iteration 190, Training loss = 0.33471423444839626
Iteration 200, Training loss = 0.3342493480214706
Iteration 210, Training loss = 0.3331340178847313
Iteration 220, Training loss = 0.33051148945322406
Iteration 230, Training loss = 0.3295405183274012
Iteration 240, Training loss = 0.3282755229335565
Iteration 250, Training loss = 0.32681960440599
Iteration 260, Training loss = 0.32605313796263474
Iteration 270, Training loss = 0.32487378355402213
Iteration 280, Training loss = 0.3235705987765239
Iteration 290, Training loss = 0.3225717573211743
Model training time: 43.37548065185547
Device: cuda
Iteration 0, Training loss = 1.2334775924682617
Iteration 10, Training loss = 1.0150295186501284
Iteration 20, Training loss = 0.9866795195983007
Iteration 30, Training loss = 0.9605500205205038
Iteration 40, Training loss = 0.9271931396080897
Iteration 50, Training loss = 0.858102859212802
Iteration 60, Training loss = 0.7324452446057246
Iteration 70, Training loss = 0.5517154324513215
Iteration 80, Training loss = 0.4144396220262234
Iteration 90, Training loss = 0.36956868378015667
Iteration 100, Training loss = 0.3560903871861788
Iteration 110, Training loss = 0.35069009088552916
Iteration 120, Training loss = 0.3439549525769857
Iteration 130, Training loss = 0.34055300572743785
Iteration 140, Training loss = 0.33613862842321396
Iteration 150, Training loss = 0.3324920174020987
Iteration 160, Training loss = 0.32868012155477816
Iteration 170, Training loss = 0.32684993801208645
Iteration 180, Training loss = 0.3254813397159943
Iteration 190, Training loss = 0.3223088670235414
Iteration 200, Training loss = 0.31994073494122577
Iteration 210, Training loss = 0.318575698022659
Iteration 220, Training loss = 0.3171344760518808
Iteration 230, Training loss = 0.31639283093122333
Iteration 240, Training loss = 0.31477496371819425
Iteration 250, Training loss = 0.31234882284815496
Iteration 260, Training loss = 0.3113200372228256
Iteration 270, Training loss = 0.30940649715753704
Iteration 280, Training loss = 0.3087467852120216
Iteration 290, Training loss = 0.3067953600906409
Model training time: 42.728976011276245
Device: cuda
Iteration 0, Training loss = 1.0561157728617008
Iteration 10, Training loss = 0.9699564828322484
Iteration 20, Training loss = 0.9280982865737035
Iteration 30, Training loss = 0.8693313976893058
Iteration 40, Training loss = 0.776442363858223
Iteration 50, Training loss = 0.6368608314257401
Iteration 60, Training loss = 0.49738889588759494
Iteration 70, Training loss = 0.4140938279720453
Iteration 80, Training loss = 0.38241546486432737
Iteration 90, Training loss = 0.37038396528133977
Iteration 100, Training loss = 0.3616872475697444
Iteration 110, Training loss = 0.35550844038908297
Iteration 120, Training loss = 0.3511175060501465
Iteration 130, Training loss = 0.3456446611537383
Iteration 140, Training loss = 0.34189758679041493
Iteration 150, Training loss = 0.33870696133145917
Iteration 160, Training loss = 0.3352544428064273
Iteration 170, Training loss = 0.3326207178716476
Iteration 180, Training loss = 0.33044451532455593
Iteration 190, Training loss = 0.3279502116716825
Iteration 200, Training loss = 0.32637554607712305
Iteration 210, Training loss = 0.32385382142204505
Iteration 220, Training loss = 0.323002633280479
Iteration 230, Training loss = 0.32043018679206187
Iteration 240, Training loss = 0.3184050926222251
Iteration 250, Training loss = 0.31700058166797346
Iteration 260, Training loss = 0.3165270072909502
Iteration 270, Training loss = 0.3151675852445456
Iteration 280, Training loss = 0.3133157789707184
Iteration 290, Training loss = 0.31168292462825775
Model training time: 41.43082022666931
Device: cuda
Iteration 0, Training loss = 0.996957405255391
Iteration 10, Training loss = 0.989861459686206
Iteration 20, Training loss = 0.9662154259590002
Iteration 30, Training loss = 0.9031418710947037
Iteration 40, Training loss = 0.7965728159134204
Iteration 50, Training loss = 0.661077082157135
Iteration 60, Training loss = 0.558469441647713
Iteration 70, Training loss = 0.4933131549220819
Iteration 80, Training loss = 0.44730925445373243
Iteration 90, Training loss = 0.4204356217613587
Iteration 100, Training loss = 0.40145194358550584
Iteration 110, Training loss = 0.38502389708390605
Iteration 120, Training loss = 0.37254691353211034
Iteration 130, Training loss = 0.3612689570738719
Iteration 140, Training loss = 0.35383491905835956
Iteration 150, Training loss = 0.3462649959211166
Iteration 160, Training loss = 0.34187905203837615
Iteration 170, Training loss = 0.3361781881405757
Iteration 180, Training loss = 0.3335135682271077
Iteration 190, Training loss = 0.32973220915748525
Iteration 200, Training loss = 0.32882871163579136
Iteration 210, Training loss = 0.32525160622138244
Iteration 220, Training loss = 0.32270198229413766
Iteration 230, Training loss = 0.3212922579393937
Iteration 240, Training loss = 0.3199575423048093
Iteration 250, Training loss = 0.3184827695099207
Iteration 260, Training loss = 0.3170650237454818
Iteration 270, Training loss = 0.31622881671557057
Iteration 280, Training loss = 0.3139362893998623
Iteration 290, Training loss = 0.3129326712626677
Model training time: 39.55416655540466
Device: cuda
Iteration 0, Training loss = 1.0110172228171275
Iteration 10, Training loss = 0.990721230323498
Iteration 20, Training loss = 0.9778669121173712
Iteration 30, Training loss = 0.9591512175706717
Iteration 40, Training loss = 0.9237567690702585
Iteration 50, Training loss = 0.8552316117745179
Iteration 60, Training loss = 0.7388778340358001
Iteration 70, Training loss = 0.5793517936880772
Iteration 80, Training loss = 0.4568713318843108
Iteration 90, Training loss = 0.40739940393429536
Iteration 100, Training loss = 0.38722688876665556
Iteration 110, Training loss = 0.3734958114532324
Iteration 120, Training loss = 0.3632650180504872
Iteration 130, Training loss = 0.3540765264859566
Iteration 140, Training loss = 0.344134577191793
Iteration 150, Training loss = 0.3370515698423752
Iteration 160, Training loss = 0.33165297428002727
Iteration 170, Training loss = 0.3266186943420997
Iteration 180, Training loss = 0.32276802166150165
Iteration 190, Training loss = 0.3201640345729314
Iteration 200, Training loss = 0.31711041726745093
Iteration 210, Training loss = 0.3139153632980127
Iteration 220, Training loss = 0.3124967443828399
Iteration 230, Training loss = 0.30995577201247215
Iteration 240, Training loss = 0.3087508985056327
Iteration 250, Training loss = 0.30662229800453555
Iteration 260, Training loss = 0.3055397687623134
Iteration 270, Training loss = 0.30511266795488506
Iteration 280, Training loss = 0.3023323084299381
Iteration 290, Training loss = 0.3010484255277194
Model training time: 39.63076090812683
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9665174243541864
Iteration 10, Training loss = 0.298105103465227
Iteration 20, Training loss = 0.27355959352392417
Iteration 30, Training loss = 0.2618122315750672
Iteration 40, Training loss = 0.2542241502266664
Iteration 50, Training loss = 0.24993599148897025
Iteration 60, Training loss = 0.245005368326719
Iteration 70, Training loss = 0.24276260659098625
Iteration 80, Training loss = 0.23975748740709746
Iteration 90, Training loss = 0.23811980881370032
Iteration 100, Training loss = 0.2373705325791469
Iteration 110, Training loss = 0.2353837424172805
Iteration 120, Training loss = 0.23535534624870008
Iteration 130, Training loss = 0.23452810742534125
Iteration 140, Training loss = 0.2335978949872347
Iteration 150, Training loss = 0.22991023499232072
Iteration 160, Training loss = 0.23026091003647217
Iteration 170, Training loss = 0.22890327039819497
Iteration 180, Training loss = 0.22728034108877182
Iteration 190, Training loss = 0.22756358637259558
Iteration 200, Training loss = 0.22522908678421608
Iteration 210, Training loss = 0.22342243131536704
Iteration 220, Training loss = 0.22305433423473284
Iteration 230, Training loss = 0.2216574366276081
Iteration 240, Training loss = 0.2216956879083927
Iteration 250, Training loss = 0.22018626131690466
Iteration 260, Training loss = 0.2202690143424731
Iteration 270, Training loss = 0.21940883019795784
Iteration 280, Training loss = 0.21869575547484252
Iteration 290, Training loss = 0.21932853013277054
Model training time: 42.26250743865967
Device: cuda
Iteration 0, Training loss = 0.9859221566181916
Iteration 10, Training loss = 0.2961159127836044
Iteration 20, Training loss = 0.26830433767575484
Iteration 30, Training loss = 0.2564680576324463
Iteration 40, Training loss = 0.24881791695952415
Iteration 50, Training loss = 0.24320906314712304
Iteration 60, Training loss = 0.24113702344206664
Iteration 70, Training loss = 0.2384661937562319
Iteration 80, Training loss = 0.23630020423577383
Iteration 90, Training loss = 0.23523148607749206
Iteration 100, Training loss = 0.23346787633804175
Iteration 110, Training loss = 0.2325463489844249
Iteration 120, Training loss = 0.23126002458425668
Iteration 130, Training loss = 0.22978960894621336
Iteration 140, Training loss = 0.22904973849654198
Iteration 150, Training loss = 0.22738284388413796
Iteration 160, Training loss = 0.22689048458750433
Iteration 170, Training loss = 0.2248922397310917
Iteration 180, Training loss = 0.22497013555123255
Iteration 190, Training loss = 0.22299593687057495
Iteration 200, Training loss = 0.22121286334899756
Iteration 210, Training loss = 0.22034308257011267
Iteration 220, Training loss = 0.21918714304382986
Iteration 230, Training loss = 0.21728703265006727
Iteration 240, Training loss = 0.2173677238707359
Iteration 250, Training loss = 0.21695460808964875
Iteration 260, Training loss = 0.2151776790045775
Iteration 270, Training loss = 0.21554469001980928
Iteration 280, Training loss = 0.2129778890655591
Iteration 290, Training loss = 0.2132888215665634
Model training time: 42.06229543685913
Device: cuda
Iteration 0, Training loss = 0.9718792449969512
Iteration 10, Training loss = 0.28174579773957914
Iteration 20, Training loss = 0.2620303126481863
Iteration 30, Training loss = 0.2542139400656407
Iteration 40, Training loss = 0.24984207233557335
Iteration 50, Training loss = 0.24761589501912779
Iteration 60, Training loss = 0.24404570173758727
Iteration 70, Training loss = 0.24024256261495444
Iteration 80, Training loss = 0.23784079574621642
Iteration 90, Training loss = 0.23541462822602346
Iteration 100, Training loss = 0.2328360341489315
Iteration 110, Training loss = 0.23118720748103583
Iteration 120, Training loss = 0.22812307683321145
Iteration 130, Training loss = 0.22641715951837027
Iteration 140, Training loss = 0.22412659715001398
Iteration 150, Training loss = 0.22161389514803886
Iteration 160, Training loss = 0.22026847818723091
Iteration 170, Training loss = 0.21978890179441526
Iteration 180, Training loss = 0.2177064951795798
Iteration 190, Training loss = 0.21885751503018233
Iteration 200, Training loss = 0.21633499545546678
Iteration 210, Training loss = 0.21577571447078997
Iteration 220, Training loss = 0.2148755923486673
Iteration 230, Training loss = 0.21405399361482033
Iteration 240, Training loss = 0.2133419499374353
Iteration 250, Training loss = 0.21243202227812547
Iteration 260, Training loss = 0.21285500492040926
Iteration 270, Training loss = 0.21100408526567313
Iteration 280, Training loss = 0.21075847143164048
Iteration 290, Training loss = 0.2107705373603564
Model training time: 42.38474726676941
Device: cuda
Iteration 0, Training loss = 0.9052038628321427
Iteration 10, Training loss = 0.2910797100227613
Iteration 20, Training loss = 0.2726871775319943
Iteration 30, Training loss = 0.25885413787685907
Iteration 40, Training loss = 0.2504474208332025
Iteration 50, Training loss = 0.24397028180269095
Iteration 60, Training loss = 0.2411245801127874
Iteration 70, Training loss = 0.23679433189905608
Iteration 80, Training loss = 0.23493818451578802
Iteration 90, Training loss = 0.2319233907529941
Iteration 100, Training loss = 0.22893287814580476
Iteration 110, Training loss = 0.22665936270585427
Iteration 120, Training loss = 0.2240769648208068
Iteration 130, Training loss = 0.22200344617550188
Iteration 140, Training loss = 0.22001823840232995
Iteration 150, Training loss = 0.21629143936129716
Iteration 160, Training loss = 0.21467456966638565
Iteration 170, Training loss = 0.2123635935668762
Iteration 180, Training loss = 0.2106633994441766
Iteration 190, Training loss = 0.209076404858094
Iteration 200, Training loss = 0.20824800030543253
Iteration 210, Training loss = 0.20752026369938484
Iteration 220, Training loss = 0.20719236593980056
Iteration 230, Training loss = 0.20533756596537736
Iteration 240, Training loss = 0.20529552635092002
Iteration 250, Training loss = 0.20435592503501818
Iteration 260, Training loss = 0.20401292494856393
Iteration 270, Training loss = 0.20261487937890565
Iteration 280, Training loss = 0.202236497344879
Iteration 290, Training loss = 0.20226344609489808
Model training time: 41.79811120033264
Device: cuda
Iteration 0, Training loss = 0.9700748496330701
Iteration 10, Training loss = 0.2902124744768326
Iteration 20, Training loss = 0.2664811502282436
Iteration 30, Training loss = 0.2553107268535174
Iteration 40, Training loss = 0.24977211109720743
Iteration 50, Training loss = 0.24472823595771423
Iteration 60, Training loss = 0.24198775050731805
Iteration 70, Training loss = 0.23929758828419906
Iteration 80, Training loss = 0.236871263728692
Iteration 90, Training loss = 0.23493979699336565
Iteration 100, Training loss = 0.23262485747153944
Iteration 110, Training loss = 0.23188812830127203
Iteration 120, Training loss = 0.22984502808405802
Iteration 130, Training loss = 0.22958200940719017
Iteration 140, Training loss = 0.22759912392267814
Iteration 150, Training loss = 0.22692267768658125
Iteration 160, Training loss = 0.2259518398115268
Iteration 170, Training loss = 0.22508559232720962
Iteration 180, Training loss = 0.2236920824417701
Iteration 190, Training loss = 0.22249670126117194
Iteration 200, Training loss = 0.22302129836036608
Iteration 210, Training loss = 0.22195459472445342
Iteration 220, Training loss = 0.22189433356890312
Iteration 230, Training loss = 0.2211332957331951
Iteration 240, Training loss = 0.22031488384191805
Iteration 250, Training loss = 0.22021617453831893
Iteration 260, Training loss = 0.21910083666443825
Iteration 270, Training loss = 0.21978640126494262
Iteration 280, Training loss = 0.21982757088083488
Iteration 290, Training loss = 0.21774966413011917
Model training time: 42.03684687614441
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0564402318917787
Iteration 10, Training loss = 0.9869634016202047
Iteration 20, Training loss = 0.9590271997910279
Iteration 30, Training loss = 0.9220096491850339
Iteration 40, Training loss = 0.871911156636018
Iteration 50, Training loss = 0.7911225328078637
Iteration 60, Training loss = 0.677470285158891
Iteration 70, Training loss = 0.5559564679861069
Iteration 80, Training loss = 0.4590558696251649
Iteration 90, Training loss = 0.40428895503282547
Iteration 100, Training loss = 0.3743266850136794
Iteration 110, Training loss = 0.35888362389344436
Iteration 120, Training loss = 0.34826319779341036
Iteration 130, Training loss = 0.34075006441428113
Iteration 140, Training loss = 0.3358862165075082
Iteration 150, Training loss = 0.33179236203432083
Iteration 160, Training loss = 0.3291040100157261
Iteration 170, Training loss = 0.3267113119363785
Iteration 180, Training loss = 0.3247338453164467
Iteration 190, Training loss = 0.32405340757507545
Iteration 200, Training loss = 0.3220853885779014
Iteration 210, Training loss = 0.31990443685880077
Iteration 220, Training loss = 0.3191443294859849
Iteration 230, Training loss = 0.31739601550193935
Iteration 240, Training loss = 0.3164394030777308
Iteration 250, Training loss = 0.315482160792901
Iteration 260, Training loss = 0.31378746376587796
Iteration 270, Training loss = 0.3136184396078953
Iteration 280, Training loss = 0.3134195300248953
Iteration 290, Training loss = 0.31124865492949116
Model training time: 38.80143165588379
Device: cuda
Iteration 0, Training loss = 1.015597016765521
Iteration 10, Training loss = 0.9716594631855304
Iteration 20, Training loss = 0.9172773280969033
Iteration 30, Training loss = 0.8412843358058196
Iteration 40, Training loss = 0.73869565817026
Iteration 50, Training loss = 0.6312868067851434
Iteration 60, Training loss = 0.5532505879035363
Iteration 70, Training loss = 0.5054956221809754
Iteration 80, Training loss = 0.47502241455591643
Iteration 90, Training loss = 0.45432026798908526
Iteration 100, Training loss = 0.4361979244993283
Iteration 110, Training loss = 0.42221859441353726
Iteration 120, Training loss = 0.41073910433512467
Iteration 130, Training loss = 0.39997078478336334
Iteration 140, Training loss = 0.3898047117086557
Iteration 150, Training loss = 0.3824410312450849
Iteration 160, Training loss = 0.3757666466900936
Iteration 170, Training loss = 0.36839612000263655
Iteration 180, Training loss = 0.36431998587571657
Iteration 190, Training loss = 0.3565617659344123
Iteration 200, Training loss = 0.35266229338370836
Iteration 210, Training loss = 0.34807091836745924
Iteration 220, Training loss = 0.34337039635731625
Iteration 230, Training loss = 0.34019465257342046
Iteration 240, Training loss = 0.3368600228658089
Iteration 250, Training loss = 0.3342348549228448
Iteration 260, Training loss = 0.33048439427064014
Iteration 270, Training loss = 0.32875213714746326
Iteration 280, Training loss = 0.32547603805477804
Iteration 290, Training loss = 0.3230989111157564
Model training time: 39.19465637207031
Device: cuda
Iteration 0, Training loss = 1.0010946691036224
Iteration 10, Training loss = 0.9463801383972168
Iteration 20, Training loss = 0.8874034858666934
Iteration 30, Training loss = 0.7832537866555728
Iteration 40, Training loss = 0.6415149030777124
Iteration 50, Training loss = 0.5202324344561651
Iteration 60, Training loss = 0.45653525854532534
Iteration 70, Training loss = 0.42642162109796816
Iteration 80, Training loss = 0.4076642474302879
Iteration 90, Training loss = 0.3942324049197711
Iteration 100, Training loss = 0.38422647003944105
Iteration 110, Training loss = 0.37810557106366527
Iteration 120, Training loss = 0.37055179877923083
Iteration 130, Training loss = 0.3651243797861613
Iteration 140, Training loss = 0.3603481621696399
Iteration 150, Training loss = 0.3564334557606624
Iteration 160, Training loss = 0.35174983739852905
Iteration 170, Training loss = 0.34753355441185146
Iteration 180, Training loss = 0.34469452385719007
Iteration 190, Training loss = 0.34137263836768955
Iteration 200, Training loss = 0.3404261561540457
Iteration 210, Training loss = 0.3360685373728092
Iteration 220, Training loss = 0.333932843632423
Iteration 230, Training loss = 0.33132260378736716
Iteration 240, Training loss = 0.3300714105940782
Iteration 250, Training loss = 0.3281227172567294
Iteration 260, Training loss = 0.3271864325954364
Iteration 270, Training loss = 0.3251035508628075
Iteration 280, Training loss = 0.3237282410264015
Iteration 290, Training loss = 0.32283887897546476
Model training time: 38.63663959503174
Device: cuda
Iteration 0, Training loss = 0.9758645055385736
Iteration 10, Training loss = 0.9129472420765803
Iteration 20, Training loss = 0.8542666572790879
Iteration 30, Training loss = 0.782635686489252
Iteration 40, Training loss = 0.7010388511877793
Iteration 50, Training loss = 0.62463939591096
Iteration 60, Training loss = 0.5660616228213677
Iteration 70, Training loss = 0.524446505193527
Iteration 80, Training loss = 0.4934376386495737
Iteration 90, Training loss = 0.4697255618297137
Iteration 100, Training loss = 0.45090549840376926
Iteration 110, Training loss = 0.43504517754683125
Iteration 120, Training loss = 0.4217747484262173
Iteration 130, Training loss = 0.4103644753877933
Iteration 140, Training loss = 0.39982387251578844
Iteration 150, Training loss = 0.39175088245135087
Iteration 160, Training loss = 0.38278268850766695
Iteration 170, Training loss = 0.37650299760011524
Iteration 180, Training loss = 0.37069065754230207
Iteration 190, Training loss = 0.3640559556392523
Iteration 200, Training loss = 0.3580199886973088
Iteration 210, Training loss = 0.3542279475010358
Iteration 220, Training loss = 0.3487331351408592
Iteration 230, Training loss = 0.3448147980066446
Iteration 240, Training loss = 0.3403382358642725
Iteration 250, Training loss = 0.3368776752971686
Iteration 260, Training loss = 0.3335167645261838
Iteration 270, Training loss = 0.33087483077095103
Iteration 280, Training loss = 0.32765844894143253
Iteration 290, Training loss = 0.3242283996481162
Model training time: 38.54285526275635
Device: cuda
Iteration 0, Training loss = 1.1401973997171109
Iteration 10, Training loss = 1.0188577427313878
Iteration 20, Training loss = 0.9939963622735097
Iteration 30, Training loss = 0.9724782074873264
Iteration 40, Training loss = 0.9427753606667886
Iteration 50, Training loss = 0.8888258120188346
Iteration 60, Training loss = 0.7929277133483154
Iteration 70, Training loss = 0.6437718065885397
Iteration 80, Training loss = 0.4905262944790033
Iteration 90, Training loss = 0.40928618609905243
Iteration 100, Training loss = 0.3830903751345781
Iteration 110, Training loss = 0.37187709602025837
Iteration 120, Training loss = 0.36268158428944075
Iteration 130, Training loss = 0.3541310547062984
Iteration 140, Training loss = 0.3485826924443245
Iteration 150, Training loss = 0.34249257697508884
Iteration 160, Training loss = 0.3376565876488502
Iteration 170, Training loss = 0.3324989854143216
Iteration 180, Training loss = 0.3293322084041742
Iteration 190, Training loss = 0.3239489103165957
Iteration 200, Training loss = 0.3205171044056232
Iteration 210, Training loss = 0.317718652005379
Iteration 220, Training loss = 0.3157176510072671
Iteration 230, Training loss = 0.31231147423386574
Iteration 240, Training loss = 0.310285072773695
Iteration 250, Training loss = 0.3082643532409118
Iteration 260, Training loss = 0.30735172216708845
Iteration 270, Training loss = 0.30560383401238
Iteration 280, Training loss = 0.3041483120849499
Iteration 290, Training loss = 0.3032840175124315
Model training time: 38.74723482131958
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9296811142793069
Iteration 10, Training loss = 0.2676577436236235
Iteration 20, Training loss = 0.25480658732927763
Iteration 30, Training loss = 0.24516269335379967
Iteration 40, Training loss = 0.23844554762427622
Iteration 50, Training loss = 0.2321274306338567
Iteration 60, Training loss = 0.22755713990101448
Iteration 70, Training loss = 0.22119107498572424
Iteration 80, Training loss = 0.21778252376959875
Iteration 90, Training loss = 0.21603330511313218
Model training time: 14.169522523880005
Device: cuda
Iteration 0, Training loss = 0.9013117460104135
Iteration 10, Training loss = 0.27299530374316067
Iteration 20, Training loss = 0.2518310607052766
Iteration 30, Training loss = 0.24373937570131743
Iteration 40, Training loss = 0.2363744484117398
Iteration 50, Training loss = 0.23132415459706232
Iteration 60, Training loss = 0.2286496231189141
Iteration 70, Training loss = 0.22591426481421178
Iteration 80, Training loss = 0.22425758695373169
Iteration 90, Training loss = 0.22182660807783788
Model training time: 14.11276388168335
Device: cuda
Iteration 0, Training loss = 0.882697622363384
Iteration 10, Training loss = 0.26522454590751576
Iteration 20, Training loss = 0.24735360793196237
Iteration 30, Training loss = 0.2371479573731239
Iteration 40, Training loss = 0.23183293669269636
Iteration 50, Training loss = 0.22639751978791678
Iteration 60, Training loss = 0.2236758112334288
Iteration 70, Training loss = 0.22316015970248443
Iteration 80, Training loss = 0.22103660295789057
Iteration 90, Training loss = 0.21693454252985808
Model training time: 14.242612838745117
Device: cuda
Iteration 0, Training loss = 0.7970740124583244
Iteration 10, Training loss = 0.2674024417423285
Iteration 20, Training loss = 0.24888743087649345
Iteration 30, Training loss = 0.2395787603006913
Iteration 40, Training loss = 0.23452077720027703
Iteration 50, Training loss = 0.2301048834163409
Iteration 60, Training loss = 0.22741093429235312
Iteration 70, Training loss = 0.22388160257385328
Iteration 80, Training loss = 0.22146649529727605
Iteration 90, Training loss = 0.22029074682639196
Model training time: 14.532340049743652
Device: cuda
Iteration 0, Training loss = 0.7369840036217983
Iteration 10, Training loss = 0.26249171736148685
Iteration 20, Training loss = 0.2466729893707312
Iteration 30, Training loss = 0.2403142357674929
Iteration 40, Training loss = 0.23147782482779944
Iteration 50, Training loss = 0.22655849330700362
Iteration 60, Training loss = 0.2199061449903708
Iteration 70, Training loss = 0.21707041160418436
Iteration 80, Training loss = 0.21475074554865176
Iteration 90, Training loss = 0.2117032600710025
Model training time: 14.245416641235352
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.077486358009852
Iteration 10, Training loss = 0.9536777012623273
Iteration 20, Training loss = 0.8119471302399268
Iteration 30, Training loss = 0.5960225170621505
Iteration 40, Training loss = 0.4793208029407721
Iteration 50, Training loss = 0.4213342930261905
Iteration 60, Training loss = 0.3819181890441821
Iteration 70, Training loss = 0.3589171844606216
Iteration 80, Training loss = 0.3468705644974342
Iteration 90, Training loss = 0.33906162043030447
Model training time: 12.940863132476807
Device: cuda
Iteration 0, Training loss = 1.046262923341531
Iteration 10, Training loss = 0.9150126943221459
Iteration 20, Training loss = 0.6883664079583608
Iteration 30, Training loss = 0.4268588807720404
Iteration 40, Training loss = 0.37809188262774396
Iteration 50, Training loss = 0.35845948039339137
Iteration 60, Training loss = 0.34508663702469605
Iteration 70, Training loss = 0.3363986731721805
Iteration 80, Training loss = 0.32900016325024456
Iteration 90, Training loss = 0.3238284344283434
Model training time: 13.048993349075317
Device: cuda
Iteration 0, Training loss = 1.0480795353651047
Iteration 10, Training loss = 0.9698611612503345
Iteration 20, Training loss = 0.8696696380010018
Iteration 30, Training loss = 0.5886300177528307
Iteration 40, Training loss = 0.4050703954238158
Iteration 50, Training loss = 0.3651312285890946
Iteration 60, Training loss = 0.3477041692687915
Iteration 70, Training loss = 0.33721771693000424
Iteration 80, Training loss = 0.32941929079019105
Iteration 90, Training loss = 0.32458718235676104
Model training time: 13.097031354904175
Device: cuda
Iteration 0, Training loss = 1.0117301184397478
Iteration 10, Training loss = 0.771357747224661
Iteration 20, Training loss = 0.4262535319878505
Iteration 30, Training loss = 0.35877946534982097
Iteration 40, Training loss = 0.3429138883948326
Iteration 50, Training loss = 0.3334437995575942
Iteration 60, Training loss = 0.32783677027775693
Iteration 70, Training loss = 0.32115446776151657
Iteration 80, Training loss = 0.31761771411849904
Iteration 90, Training loss = 0.31398380261201125
Model training time: 13.086646318435669
Device: cuda
Iteration 0, Training loss = 1.096505900988212
Iteration 10, Training loss = 0.9410111892681855
Iteration 20, Training loss = 0.7123571897928531
Iteration 30, Training loss = 0.4147641377953383
Iteration 40, Training loss = 0.37132526819522566
Iteration 50, Training loss = 0.3541156414609689
Iteration 60, Training loss = 0.34237236185715747
Iteration 70, Training loss = 0.332238114797152
Iteration 80, Training loss = 0.32515683942116225
Iteration 90, Training loss = 0.31966144419633424
Model training time: 13.04355263710022
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1340287683101802
Iteration 10, Training loss = 0.27037686482071877
Iteration 20, Training loss = 0.253218970906276
Iteration 30, Training loss = 0.245989466802432
Iteration 40, Training loss = 0.24102263897657394
Iteration 50, Training loss = 0.2337738235409443
Iteration 60, Training loss = 0.23128886606830817
Iteration 70, Training loss = 0.2259519286453724
Iteration 80, Training loss = 0.2233904325045072
Iteration 90, Training loss = 0.2218515253984011
Model training time: 14.341921091079712
Device: cuda
Iteration 0, Training loss = 1.0573152257845952
Iteration 10, Training loss = 0.28726857184217525
Iteration 20, Training loss = 0.26675283307066333
Iteration 30, Training loss = 0.2571828654752328
Iteration 40, Training loss = 0.24873919412493706
Iteration 50, Training loss = 0.24346861214591906
Iteration 60, Training loss = 0.23900580836030152
Iteration 70, Training loss = 0.23634289663571578
Iteration 80, Training loss = 0.23286338179157332
Iteration 90, Training loss = 0.23092239980514234
Model training time: 13.855533599853516
Device: cuda
Iteration 0, Training loss = 0.9357535999554855
Iteration 10, Training loss = 0.2762312940680064
Iteration 20, Training loss = 0.2568716246348161
Iteration 30, Training loss = 0.2502139061689377
Iteration 40, Training loss = 0.24520890701275605
Iteration 50, Training loss = 0.24247779238682526
Iteration 60, Training loss = 0.23667196241708902
Iteration 70, Training loss = 0.23222194268153265
Iteration 80, Training loss = 0.23029770960028356
Iteration 90, Training loss = 0.22790712891862944
Model training time: 14.033196926116943
Device: cuda
Iteration 0, Training loss = 0.9110384686635091
Iteration 10, Training loss = 0.2708187037362502
Iteration 20, Training loss = 0.2553067316229527
Iteration 30, Training loss = 0.24921933819468206
Iteration 40, Training loss = 0.24429194829784906
Iteration 50, Training loss = 0.23992725347097105
Iteration 60, Training loss = 0.23775239231494757
Iteration 70, Training loss = 0.23568283594571626
Iteration 80, Training loss = 0.2322408135693807
Iteration 90, Training loss = 0.23113430577975053
Model training time: 14.021772146224976
Device: cuda
Iteration 0, Training loss = 0.9254601242450567
Iteration 10, Training loss = 0.3624490476571597
Iteration 20, Training loss = 0.2947559683368756
Iteration 30, Training loss = 0.267866578239661
Iteration 40, Training loss = 0.259670990304305
Iteration 50, Training loss = 0.2524540089070797
Iteration 60, Training loss = 0.24887429636258346
Iteration 70, Training loss = 0.2491159851734455
Iteration 80, Training loss = 0.24725124727074915
Iteration 90, Training loss = 0.24713580625561568
Model training time: 13.975317001342773
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9848517603599108
Iteration 10, Training loss = 0.5994447802121823
Iteration 20, Training loss = 0.4247230188204692
Iteration 30, Training loss = 0.3813895909832074
Iteration 40, Training loss = 0.3551390463343033
Iteration 50, Training loss = 0.33897611785393494
Iteration 60, Training loss = 0.32819185807154727
Iteration 70, Training loss = 0.32194842340854496
Iteration 80, Training loss = 0.31658197423586476
Iteration 90, Training loss = 0.31324989062089187
Model training time: 13.200582504272461
Device: cuda
Iteration 0, Training loss = 1.0057772718943083
Iteration 10, Training loss = 0.9968861330014008
Iteration 20, Training loss = 0.991722332743498
Iteration 30, Training loss = 0.9727210310789255
Iteration 40, Training loss = 0.887271346954199
Iteration 50, Training loss = 0.5668826103210449
Iteration 60, Training loss = 0.4053732855961873
Iteration 70, Training loss = 0.37314271468382615
Iteration 80, Training loss = 0.35520884222709215
Iteration 90, Training loss = 0.3443215730098578
Model training time: 13.232373237609863
Device: cuda
Iteration 0, Training loss = 1.174494363940679
Iteration 10, Training loss = 0.9464464393945841
Iteration 20, Training loss = 0.8017312735319138
Iteration 30, Training loss = 0.5912816054545916
Iteration 40, Training loss = 0.5108706515568954
Iteration 50, Training loss = 0.4648757084057881
Iteration 60, Training loss = 0.43243607649436366
Iteration 70, Training loss = 0.40760451784500706
Iteration 80, Training loss = 0.3879388037782449
Iteration 90, Training loss = 0.3715341165661812
Model training time: 13.038737535476685
Device: cuda
Iteration 0, Training loss = 1.2370310471608088
Iteration 10, Training loss = 1.0033428760675283
Iteration 20, Training loss = 0.9928015183943969
Iteration 30, Training loss = 0.9830202334202253
Iteration 40, Training loss = 0.9503722546192316
Iteration 50, Training loss = 0.860009239270137
Iteration 60, Training loss = 0.6967819367463772
Iteration 70, Training loss = 0.5664372289410005
Iteration 80, Training loss = 0.5037669625419837
Iteration 90, Training loss = 0.46351300523831296
Model training time: 12.83111023902893
Device: cuda
Iteration 0, Training loss = 0.9846899795990723
Iteration 10, Training loss = 0.8393326080762423
Iteration 20, Training loss = 0.5965821324632719
Iteration 30, Training loss = 0.4385450998177895
Iteration 40, Training loss = 0.3823530295720467
Iteration 50, Training loss = 0.3523626218621547
Iteration 60, Training loss = 0.3370240611525682
Iteration 70, Training loss = 0.3279105353240783
Iteration 80, Training loss = 0.3210857522029143
Iteration 90, Training loss = 0.3168174581458935
Model training time: 12.978259801864624
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.860273275237817
Iteration 10, Training loss = 0.2707314451153462
Iteration 20, Training loss = 0.2526027032962212
Iteration 30, Training loss = 0.2457851652915661
Iteration 40, Training loss = 0.24108657928613517
Iteration 50, Training loss = 0.2364774469572764
Iteration 60, Training loss = 0.23320477857039526
Iteration 70, Training loss = 0.23085732012987137
Iteration 80, Training loss = 0.22784261548748383
Iteration 90, Training loss = 0.22601676101867968
Model training time: 13.939299583435059
Device: cuda
Iteration 0, Training loss = 0.8201204274709408
Iteration 10, Training loss = 0.26530990778253627
Iteration 20, Training loss = 0.2482155724786795
Iteration 30, Training loss = 0.24037758117684951
Iteration 40, Training loss = 0.23445406212256506
Iteration 50, Training loss = 0.23325124268348402
Iteration 60, Training loss = 0.2299205852815738
Iteration 70, Training loss = 0.2278859282915409
Iteration 80, Training loss = 0.22385302615853456
Iteration 90, Training loss = 0.22193822064078772
Model training time: 13.938419818878174
Device: cuda
Iteration 0, Training loss = 0.996666303047767
Iteration 10, Training loss = 0.2725362637295173
Iteration 20, Training loss = 0.25097191591675466
Iteration 30, Training loss = 0.24438057295404947
Iteration 40, Training loss = 0.24037585923304924
Iteration 50, Training loss = 0.2353412792659723
Iteration 60, Training loss = 0.2300186985387252
Iteration 70, Training loss = 0.2259317756845401
Iteration 80, Training loss = 0.22018756087009722
Iteration 90, Training loss = 0.21786063995498878
Model training time: 14.055696964263916
Device: cuda
Iteration 0, Training loss = 0.8925467213759055
Iteration 10, Training loss = 0.27064691054133266
Iteration 20, Training loss = 0.24838111033806434
Iteration 30, Training loss = 0.23950691435199517
Iteration 40, Training loss = 0.23423596328267685
Iteration 50, Training loss = 0.23002720366303736
Iteration 60, Training loss = 0.22613922993724161
Iteration 70, Training loss = 0.22329685751062173
Iteration 80, Training loss = 0.2226030408189847
Iteration 90, Training loss = 0.22060101450635836
Model training time: 13.80423092842102
Device: cuda
Iteration 0, Training loss = 0.887020359818752
Iteration 10, Training loss = 0.26909324096945614
Iteration 20, Training loss = 0.24815780239609572
Iteration 30, Training loss = 0.24086341376488024
Iteration 40, Training loss = 0.231450250515571
Iteration 50, Training loss = 0.22781222227674264
Iteration 60, Training loss = 0.22639208229688498
Iteration 70, Training loss = 0.2217352602344293
Iteration 80, Training loss = 0.2196373145740766
Iteration 90, Training loss = 0.21683159470558167
Model training time: 13.95907711982727
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0865691533455482
Iteration 10, Training loss = 0.9841130352937258
Iteration 20, Training loss = 0.9341008135905633
Iteration 30, Training loss = 0.7590501617926818
Iteration 40, Training loss = 0.5412454799963877
Iteration 50, Training loss = 0.470679518695061
Iteration 60, Training loss = 0.43137276573823047
Iteration 70, Training loss = 0.4009761598247748
Iteration 80, Training loss = 0.3783245596748132
Iteration 90, Training loss = 0.36117224452587277
Model training time: 12.854224443435669
Device: cuda
Iteration 0, Training loss = 1.0101242925112064
Iteration 10, Training loss = 0.8130600784833615
Iteration 20, Training loss = 0.5093664446702371
Iteration 30, Training loss = 0.39832614534176314
Iteration 40, Training loss = 0.37093774057351625
Iteration 50, Training loss = 0.35593628883361816
Iteration 60, Training loss = 0.34487236463106596
Iteration 70, Training loss = 0.33754496505627263
Iteration 80, Training loss = 0.3296878349322539
Iteration 90, Training loss = 0.324241276543874
Model training time: 12.85905647277832
Device: cuda
Iteration 0, Training loss = 0.9911795682632006
Iteration 10, Training loss = 0.854784254844372
Iteration 20, Training loss = 0.5378573571260159
Iteration 30, Training loss = 0.3905976059345099
Iteration 40, Training loss = 0.3671032700401086
Iteration 50, Training loss = 0.35136538400099826
Iteration 60, Training loss = 0.3418538458645344
Iteration 70, Training loss = 0.3338692933320999
Iteration 80, Training loss = 0.3284702581854967
Iteration 90, Training loss = 0.32420630351855206
Model training time: 13.21837854385376
Device: cuda
Iteration 0, Training loss = 0.9962331389005368
Iteration 10, Training loss = 0.8953772485256195
Iteration 20, Training loss = 0.6469650474878458
Iteration 30, Training loss = 0.4779485956980632
Iteration 40, Training loss = 0.41568900931339997
Iteration 50, Training loss = 0.3789266204604736
Iteration 60, Training loss = 0.3572769657923625
Iteration 70, Training loss = 0.343145980571325
Iteration 80, Training loss = 0.3349851266695903
Iteration 90, Training loss = 0.32858112626350844
Model training time: 12.986124992370605
Device: cuda
Iteration 0, Training loss = 1.0408849635949502
Iteration 10, Training loss = 0.9408927972500141
Iteration 20, Training loss = 0.7942469383661563
Iteration 30, Training loss = 0.5196369817623725
Iteration 40, Training loss = 0.4081501146921745
Iteration 50, Training loss = 0.37828928518753785
Iteration 60, Training loss = 0.360783960383672
Iteration 70, Training loss = 0.34753782932574934
Iteration 80, Training loss = 0.3372559372622233
Iteration 90, Training loss = 0.3297998538384071
Model training time: 12.917935371398926
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8721430416290576
Iteration 10, Training loss = 0.2697150409221649
Iteration 20, Training loss = 0.2566588813295731
Iteration 30, Training loss = 0.24832661277972734
Iteration 40, Training loss = 0.242472900220981
Iteration 50, Training loss = 0.23852878637038744
Iteration 60, Training loss = 0.23621613217087892
Iteration 70, Training loss = 0.23220295057846949
Iteration 80, Training loss = 0.22880137281922194
Iteration 90, Training loss = 0.22849006521014067
Iteration 100, Training loss = 0.22661694712363756
Iteration 110, Training loss = 0.2261098614678933
Iteration 120, Training loss = 0.224024616754972
Iteration 130, Training loss = 0.22220369342427987
Iteration 140, Training loss = 0.22113811855132764
Iteration 150, Training loss = 0.2213926464319229
Iteration 160, Training loss = 0.2182727994827124
Iteration 170, Training loss = 0.21773406433371398
Iteration 180, Training loss = 0.2168964150433357
Iteration 190, Training loss = 0.2171789943598784
Model training time: 28.116000175476074
Device: cuda
Iteration 0, Training loss = 0.7770001131754655
Iteration 10, Training loss = 0.26646392649182904
Iteration 20, Training loss = 0.24830590647000533
Iteration 30, Training loss = 0.23950177316482252
Iteration 40, Training loss = 0.2330802624615339
Iteration 50, Training loss = 0.22609962093142363
Iteration 60, Training loss = 0.21975019144324157
Iteration 70, Training loss = 0.21538885281636164
Iteration 80, Training loss = 0.21256828651978418
Iteration 90, Training loss = 0.20993368413585883
Iteration 100, Training loss = 0.2068961629500756
Iteration 110, Training loss = 0.20803818077995226
Iteration 120, Training loss = 0.2051736738246221
Iteration 130, Training loss = 0.20411138465771309
Iteration 140, Training loss = 0.20215507883291978
Iteration 150, Training loss = 0.2015125803076304
Iteration 160, Training loss = 0.20224800505317175
Iteration 170, Training loss = 0.19926278608349654
Iteration 180, Training loss = 0.19900379415888053
Iteration 190, Training loss = 0.1989580005980455
Model training time: 28.222193241119385
Device: cuda
Iteration 0, Training loss = 0.8476000244800861
Iteration 10, Training loss = 0.2717187080818873
Iteration 20, Training loss = 0.25543297082185745
Iteration 30, Training loss = 0.24543282934106314
Iteration 40, Training loss = 0.24064553414399809
Iteration 50, Training loss = 0.23409879895356986
Iteration 60, Training loss = 0.23136327129143935
Iteration 70, Training loss = 0.22518923993294054
Iteration 80, Training loss = 0.22392129640166575
Iteration 90, Training loss = 0.21915996791078493
Iteration 100, Training loss = 0.21758859747877488
Iteration 110, Training loss = 0.21607808453532365
Iteration 120, Training loss = 0.21529844890420252
Iteration 130, Training loss = 0.2145278132878817
Iteration 140, Training loss = 0.2125123759301809
Iteration 150, Training loss = 0.2116337614850356
Iteration 160, Training loss = 0.21088110770170504
Iteration 170, Training loss = 0.21042463269371253
Iteration 180, Training loss = 0.20631701069382521
Iteration 190, Training loss = 0.2074707173384153
Model training time: 28.032642364501953
Device: cuda
Iteration 0, Training loss = 0.8430032380498372
Iteration 10, Training loss = 0.26893040623802406
Iteration 20, Training loss = 0.2517997742845462
Iteration 30, Training loss = 0.2422783377651985
Iteration 40, Training loss = 0.23420163358633334
Iteration 50, Training loss = 0.22979971651847547
Iteration 60, Training loss = 0.22429025287811571
Iteration 70, Training loss = 0.2207120327422252
Iteration 80, Training loss = 0.21576704285465753
Iteration 90, Training loss = 0.214188279154209
Iteration 100, Training loss = 0.2126249702504048
Iteration 110, Training loss = 0.20998721655744773
Iteration 120, Training loss = 0.2079673558473587
Iteration 130, Training loss = 0.2049676927809532
Iteration 140, Training loss = 0.204347390968066
Iteration 150, Training loss = 0.20288361608982086
Iteration 160, Training loss = 0.20281524483401042
Iteration 170, Training loss = 0.1998669274437886
Iteration 180, Training loss = 0.19997831706244212
Iteration 190, Training loss = 0.1984774166574845
Model training time: 28.41245722770691
Device: cuda
Iteration 0, Training loss = 0.8634468294106997
Iteration 10, Training loss = 0.26279794854613453
Iteration 20, Training loss = 0.2433493807911873
Iteration 30, Training loss = 0.236012163070532
Iteration 40, Training loss = 0.23083748238591048
Iteration 50, Training loss = 0.22542062172522911
Iteration 60, Training loss = 0.22223704394239646
Iteration 70, Training loss = 0.21803366937316382
Iteration 80, Training loss = 0.21481633472901124
Iteration 90, Training loss = 0.21282105061870354
Iteration 100, Training loss = 0.20890620981271452
Iteration 110, Training loss = 0.20928895129607275
Iteration 120, Training loss = 0.20601697323413995
Iteration 130, Training loss = 0.20651565578121406
Iteration 140, Training loss = 0.20408451929688454
Iteration 150, Training loss = 0.20266984804318502
Iteration 160, Training loss = 0.2013000355890164
Iteration 170, Training loss = 0.20129202908048263
Iteration 180, Training loss = 0.19905815560084122
Iteration 190, Training loss = 0.20139778663332647
Model training time: 27.941391706466675
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1026994838164403
Iteration 10, Training loss = 0.9669502836007339
Iteration 20, Training loss = 0.8086265187997085
Iteration 30, Training loss = 0.4815509600134996
Iteration 40, Training loss = 0.3780108836407845
Iteration 50, Training loss = 0.35727639954823714
Iteration 60, Training loss = 0.34515027357981753
Iteration 70, Training loss = 0.3371163128087154
Iteration 80, Training loss = 0.3312426914389317
Iteration 90, Training loss = 0.3260406737144177
Iteration 100, Training loss = 0.3216093976337176
Iteration 110, Training loss = 0.3179494578104753
Iteration 120, Training loss = 0.31467626387110126
Iteration 130, Training loss = 0.3121707301873427
Iteration 140, Training loss = 0.30971654762442297
Iteration 150, Training loss = 0.30706448881671977
Iteration 160, Training loss = 0.30616210343746036
Iteration 170, Training loss = 0.30310743044202143
Iteration 180, Training loss = 0.30177144276408047
Iteration 190, Training loss = 0.2996134150486726
Model training time: 25.81285333633423
Device: cuda
Iteration 0, Training loss = 0.9952500623006088
Iteration 10, Training loss = 0.8887904401008899
Iteration 20, Training loss = 0.6667175728541154
Iteration 30, Training loss = 0.4748095887211653
Iteration 40, Training loss = 0.406090096785472
Iteration 50, Training loss = 0.3726387872145726
Iteration 60, Training loss = 0.3524890805666263
Iteration 70, Training loss = 0.34008676587389064
Iteration 80, Training loss = 0.33112024802428025
Iteration 90, Training loss = 0.32500716814628017
Iteration 100, Training loss = 0.32033458438057166
Iteration 110, Training loss = 0.31684373032588226
Iteration 120, Training loss = 0.312516905940496
Iteration 130, Training loss = 0.31057842620290244
Iteration 140, Training loss = 0.3082404569364511
Iteration 150, Training loss = 0.30574342980980873
Iteration 160, Training loss = 0.3022579146692386
Iteration 170, Training loss = 0.2996169744202724
Iteration 180, Training loss = 0.2976019809452387
Iteration 190, Training loss = 0.29493876804526037
Model training time: 25.914624214172363
Device: cuda
Iteration 0, Training loss = 0.9734932631254196
Iteration 10, Training loss = 0.7406148864672735
Iteration 20, Training loss = 0.4803857035361804
Iteration 30, Training loss = 0.4075783147261693
Iteration 40, Training loss = 0.37806349648879123
Iteration 50, Training loss = 0.3582224255570999
Iteration 60, Training loss = 0.3457398540698565
Iteration 70, Training loss = 0.3347627383012038
Iteration 80, Training loss = 0.3278065664836994
Iteration 90, Training loss = 0.32174263808589715
Iteration 100, Training loss = 0.3167024492644347
Iteration 110, Training loss = 0.3135152794420719
Iteration 120, Training loss = 0.3092993343105683
Iteration 130, Training loss = 0.30622102635411114
Iteration 140, Training loss = 0.30468202697542995
Iteration 150, Training loss = 0.3017404686946135
Iteration 160, Training loss = 0.29941465934881795
Iteration 170, Training loss = 0.29797866109472054
Iteration 180, Training loss = 0.2952112779021263
Iteration 190, Training loss = 0.29284925730182576
Model training time: 25.972649097442627
Device: cuda
Iteration 0, Training loss = 1.087617385845918
Iteration 10, Training loss = 0.9637716183295617
Iteration 20, Training loss = 0.8661794696862881
Iteration 30, Training loss = 0.6866401181771205
Iteration 40, Training loss = 0.5159067104642208
Iteration 50, Training loss = 0.4213259655695695
Iteration 60, Training loss = 0.37543706194712567
Iteration 70, Training loss = 0.3509413336332028
Iteration 80, Training loss = 0.3370932564139366
Iteration 90, Training loss = 0.3283812724626981
Iteration 100, Training loss = 0.3210492397730167
Iteration 110, Training loss = 0.3153877200988623
Iteration 120, Training loss = 0.3117730597463938
Iteration 130, Training loss = 0.3088575191795826
Iteration 140, Training loss = 0.3048361044090528
Iteration 150, Training loss = 0.3023669928885423
Iteration 160, Training loss = 0.29862804538928545
Iteration 170, Training loss = 0.29631219517726165
Iteration 180, Training loss = 0.295099110270922
Iteration 190, Training loss = 0.2922045069818313
Model training time: 25.76925253868103
Device: cuda
Iteration 0, Training loss = 0.9959584646500074
Iteration 10, Training loss = 0.9520487773876923
Iteration 20, Training loss = 0.8277660596829194
Iteration 30, Training loss = 0.5655495621837102
Iteration 40, Training loss = 0.45910864953811353
Iteration 50, Training loss = 0.4143394323495718
Iteration 60, Training loss = 0.3863589855340811
Iteration 70, Training loss = 0.36884719133377075
Iteration 80, Training loss = 0.35650092735886574
Iteration 90, Training loss = 0.3481208707850713
Iteration 100, Training loss = 0.34171587973833084
Iteration 110, Training loss = 0.33744665808402574
Iteration 120, Training loss = 0.33369953070695585
Iteration 130, Training loss = 0.3303148305186859
Iteration 140, Training loss = 0.3268351870087477
Iteration 150, Training loss = 0.32479144031038654
Iteration 160, Training loss = 0.32085047031824404
Iteration 170, Training loss = 0.3188569723413541
Iteration 180, Training loss = 0.31541421522314733
Iteration 190, Training loss = 0.3135084816469596
Model training time: 26.21191644668579
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9602173853379029
Iteration 10, Training loss = 0.281756397909843
Iteration 20, Training loss = 0.2631098610850481
Iteration 30, Training loss = 0.25862441899684757
Iteration 40, Training loss = 0.2548100661772948
Iteration 50, Training loss = 0.24931541400460097
Iteration 60, Training loss = 0.24557662869875246
Iteration 70, Training loss = 0.2428199204687889
Iteration 80, Training loss = 0.24182177220399564
Iteration 90, Training loss = 0.2374375548500281
Iteration 100, Training loss = 0.23557505240807167
Iteration 110, Training loss = 0.23398848794973814
Iteration 120, Training loss = 0.23185922635289338
Iteration 130, Training loss = 0.23195814398618844
Iteration 140, Training loss = 0.22777660506275985
Iteration 150, Training loss = 0.22567332192109182
Iteration 160, Training loss = 0.22409065526265365
Iteration 170, Training loss = 0.223541436287073
Iteration 180, Training loss = 0.22365512870825255
Iteration 190, Training loss = 0.22302414620151886
Model training time: 28.742677927017212
Device: cuda
Iteration 0, Training loss = 1.0173560082912445
Iteration 10, Training loss = 0.4524557080406409
Iteration 20, Training loss = 0.32940167589829517
Iteration 30, Training loss = 0.2824767822256455
Iteration 40, Training loss = 0.2608891986310482
Iteration 50, Training loss = 0.25041860485306155
Iteration 60, Training loss = 0.24444229614276153
Iteration 70, Training loss = 0.24045600140324006
Iteration 80, Training loss = 0.23845615352575594
Iteration 90, Training loss = 0.23683852415818435
Iteration 100, Training loss = 0.2357285959789386
Iteration 110, Training loss = 0.2350191789177748
Iteration 120, Training loss = 0.23433612946134347
Iteration 130, Training loss = 0.23514896258711815
Iteration 140, Training loss = 0.23289509891317442
Iteration 150, Training loss = 0.2317075990140438
Iteration 160, Training loss = 0.23120101254719955
Iteration 170, Training loss = 0.23205972978701958
Iteration 180, Training loss = 0.2320626019858397
Iteration 190, Training loss = 0.23010097100184515
Model training time: 28.541623830795288
Device: cuda
Iteration 0, Training loss = 0.9121627085483991
Iteration 10, Training loss = 0.27806714836221474
Iteration 20, Training loss = 0.2580439167527052
Iteration 30, Training loss = 0.2542351742203419
Iteration 40, Training loss = 0.2506619428212826
Iteration 50, Training loss = 0.24677718344789284
Iteration 60, Training loss = 0.2455824315547943
Iteration 70, Training loss = 0.24205139795174965
Iteration 80, Training loss = 0.23875158795943627
Iteration 90, Training loss = 0.23822493421343657
Iteration 100, Training loss = 0.2366506801201747
Iteration 110, Training loss = 0.2367357236261551
Iteration 120, Training loss = 0.2335865878714965
Iteration 130, Training loss = 0.23211459127756265
Iteration 140, Training loss = 0.2329272378522616
Iteration 150, Training loss = 0.23020063885129416
Iteration 160, Training loss = 0.22965589108375403
Iteration 170, Training loss = 0.22562547200001204
Iteration 180, Training loss = 0.2247680531671414
Iteration 190, Training loss = 0.22253426852134559
Model training time: 28.617367029190063
Device: cuda
Iteration 0, Training loss = 0.9041444395597165
Iteration 10, Training loss = 0.27685597997445327
Iteration 20, Training loss = 0.2586902007460594
Iteration 30, Training loss = 0.25322546466038776
Iteration 40, Training loss = 0.24629255326894614
Iteration 50, Training loss = 0.24344452269948447
Iteration 60, Training loss = 0.2415390550517119
Iteration 70, Training loss = 0.2397008492396428
Iteration 80, Training loss = 0.23847101170283097
Iteration 90, Training loss = 0.234743786832461
Iteration 100, Training loss = 0.23159715733849084
Iteration 110, Training loss = 0.22922610548826364
Iteration 120, Training loss = 0.2260790879909809
Iteration 130, Training loss = 0.2237124666571617
Iteration 140, Training loss = 0.22280560347896355
Iteration 150, Training loss = 0.22088390989945486
Iteration 160, Training loss = 0.2206251658499241
Iteration 170, Training loss = 0.2179432654610047
Iteration 180, Training loss = 0.2165297418832779
Iteration 190, Training loss = 0.21735839975567964
Model training time: 28.470591068267822
Device: cuda
Iteration 0, Training loss = 0.8705858794542459
Iteration 10, Training loss = 0.26687482687143177
Iteration 20, Training loss = 0.25292047657645667
Iteration 30, Training loss = 0.2467466862155841
Iteration 40, Training loss = 0.24069694028450891
Iteration 50, Training loss = 0.23742141116123933
Iteration 60, Training loss = 0.23802974705512708
Iteration 70, Training loss = 0.23374871623057586
Iteration 80, Training loss = 0.23085235030605242
Iteration 90, Training loss = 0.22602124598163825
Iteration 100, Training loss = 0.22440567412055457
Iteration 110, Training loss = 0.22168126616340417
Iteration 120, Training loss = 0.22031593981843728
Iteration 130, Training loss = 0.21773312957240984
Iteration 140, Training loss = 0.21723785881812757
Iteration 150, Training loss = 0.21499248164204451
Iteration 160, Training loss = 0.21443511488345954
Iteration 170, Training loss = 0.21458729958304992
Iteration 180, Training loss = 0.21196123183919832
Iteration 190, Training loss = 0.21170015575794074
Model training time: 27.745502710342407
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0267768559547572
Iteration 10, Training loss = 0.9145690088088696
Iteration 20, Training loss = 0.6346128015564039
Iteration 30, Training loss = 0.37944330332370907
Iteration 40, Training loss = 0.35158191277430606
Iteration 50, Training loss = 0.33981120643707424
Iteration 60, Training loss = 0.33053014885920745
Iteration 70, Training loss = 0.3240157901667632
Iteration 80, Training loss = 0.3199220657921754
Iteration 90, Training loss = 0.31580190618450826
Iteration 100, Training loss = 0.3119480466613403
Iteration 110, Training loss = 0.30978252910650694
Iteration 120, Training loss = 0.3063099378576645
Iteration 130, Training loss = 0.30400894610927653
Iteration 140, Training loss = 0.3016951018227981
Iteration 150, Training loss = 0.2999359197341479
Iteration 160, Training loss = 0.29757268726825714
Iteration 170, Training loss = 0.2962229610062562
Iteration 180, Training loss = 0.2945035039805449
Iteration 190, Training loss = 0.2923359919625979
Model training time: 25.987597465515137
Device: cuda
Iteration 0, Training loss = 1.041886949768433
Iteration 10, Training loss = 0.9026408321582354
Iteration 20, Training loss = 0.7160946394388492
Iteration 30, Training loss = 0.4945695554980865
Iteration 40, Training loss = 0.4155356998627002
Iteration 50, Training loss = 0.39193842388116396
Iteration 60, Training loss = 0.37630327160541827
Iteration 70, Training loss = 0.36466629287371266
Iteration 80, Training loss = 0.35546369736011213
Iteration 90, Training loss = 0.34780419216706204
Iteration 100, Training loss = 0.341731224495631
Iteration 110, Training loss = 0.33664480997965884
Iteration 120, Training loss = 0.33094089220349604
Iteration 130, Training loss = 0.3267873992713598
Iteration 140, Training loss = 0.32371341035916257
Iteration 150, Training loss = 0.3181470480675881
Iteration 160, Training loss = 0.31487859842868954
Iteration 170, Training loss = 0.31177834822581363
Iteration 180, Training loss = 0.3079810612476789
Iteration 190, Training loss = 0.30567178330742395
Model training time: 26.336689949035645
Device: cuda
Iteration 0, Training loss = 1.0659821377350733
Iteration 10, Training loss = 0.9052248505445627
Iteration 20, Training loss = 0.7411197916819499
Iteration 30, Training loss = 0.572773725940631
Iteration 40, Training loss = 0.49450775809012926
Iteration 50, Training loss = 0.4531500115990639
Iteration 60, Training loss = 0.424822422174307
Iteration 70, Training loss = 0.40474435285880017
Iteration 80, Training loss = 0.3870710701896594
Iteration 90, Training loss = 0.37232523870009643
Iteration 100, Training loss = 0.3606459183188585
Iteration 110, Training loss = 0.34957251554498303
Iteration 120, Training loss = 0.3416463944774408
Iteration 130, Training loss = 0.3340522268643746
Iteration 140, Training loss = 0.32719070808245587
Iteration 150, Training loss = 0.3214852807040398
Iteration 160, Training loss = 0.3163441273455436
Iteration 170, Training loss = 0.3114172128530649
Iteration 180, Training loss = 0.3063058208387632
Iteration 190, Training loss = 0.3026648756976311
Model training time: 26.0179283618927
Device: cuda
Iteration 0, Training loss = 1.1394308106257365
Iteration 10, Training loss = 1.0042972828333194
Iteration 20, Training loss = 0.9868514721210186
Iteration 30, Training loss = 0.9523201069006553
Iteration 40, Training loss = 0.8542627176413169
Iteration 50, Training loss = 0.5966729258115475
Iteration 60, Training loss = 0.40219629269379836
Iteration 70, Training loss = 0.3552159179861729
Iteration 80, Training loss = 0.33985240958057916
Iteration 90, Training loss = 0.3299970595309368
Iteration 100, Training loss = 0.3238408356331862
Iteration 110, Training loss = 0.32030827494767994
Iteration 120, Training loss = 0.316140557710941
Iteration 130, Training loss = 0.31344773133213705
Iteration 140, Training loss = 0.310730707186919
Iteration 150, Training loss = 0.3078525037719653
Iteration 160, Training loss = 0.3063571246770712
Iteration 170, Training loss = 0.3033063099361383
Iteration 180, Training loss = 0.30079641193151474
Iteration 190, Training loss = 0.2983347183236709
Model training time: 25.926685094833374
Device: cuda
Iteration 0, Training loss = 1.0531635697071369
Iteration 10, Training loss = 0.9806477553569354
Iteration 20, Training loss = 0.9021663172886922
Iteration 30, Training loss = 0.73510319682268
Iteration 40, Training loss = 0.5145664742359748
Iteration 50, Training loss = 0.40204107016324997
Iteration 60, Training loss = 0.3688437775350534
Iteration 70, Training loss = 0.35039397386404186
Iteration 80, Training loss = 0.34050561401706475
Iteration 90, Training loss = 0.33278454582278544
Iteration 100, Training loss = 0.3262858118575353
Iteration 110, Training loss = 0.32125325271716487
Iteration 120, Training loss = 0.3162591758255775
Iteration 130, Training loss = 0.31267485882227236
Iteration 140, Training loss = 0.3083880486396643
Iteration 150, Training loss = 0.3049679140631969
Iteration 160, Training loss = 0.3019060604274273
Iteration 170, Training loss = 0.2995140841947152
Iteration 180, Training loss = 0.29594969749450684
Iteration 190, Training loss = 0.2934156065950027
Model training time: 26.118167638778687
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9440833788651687
Iteration 10, Training loss = 0.2692330270432509
Iteration 20, Training loss = 0.2542708066220467
Iteration 30, Training loss = 0.2475254856623136
Iteration 40, Training loss = 0.24216620681377557
Iteration 50, Training loss = 0.23657475555172333
Iteration 60, Training loss = 0.23337294476536605
Iteration 70, Training loss = 0.2301458687736438
Iteration 80, Training loss = 0.227990882901045
Iteration 90, Training loss = 0.22614358451503974
Iteration 100, Training loss = 0.22350543508162865
Iteration 110, Training loss = 0.22064128116919443
Iteration 120, Training loss = 0.21895071864128113
Iteration 130, Training loss = 0.21782497517191446
Iteration 140, Training loss = 0.215810282012591
Iteration 150, Training loss = 0.2148972411568348
Iteration 160, Training loss = 0.21319476113869593
Iteration 170, Training loss = 0.2121557263800731
Iteration 180, Training loss = 0.21349696241892302
Iteration 190, Training loss = 0.2097455417880645
Model training time: 28.364460945129395
Device: cuda
Iteration 0, Training loss = 0.8977654656538596
Iteration 10, Training loss = 0.27134258643938947
Iteration 20, Training loss = 0.2532113091303752
Iteration 30, Training loss = 0.24636607903700608
Iteration 40, Training loss = 0.23870465990442497
Iteration 50, Training loss = 0.2361771149130968
Iteration 60, Training loss = 0.23050760936278564
Iteration 70, Training loss = 0.22932301891537812
Iteration 80, Training loss = 0.22688879187290484
Iteration 90, Training loss = 0.22584065365103576
Iteration 100, Training loss = 0.2235701224551751
Iteration 110, Training loss = 0.2220208432811957
Iteration 120, Training loss = 0.22069011342066985
Iteration 130, Training loss = 0.2188579013141302
Iteration 140, Training loss = 0.21805290705882585
Iteration 150, Training loss = 0.21658662133491957
Iteration 160, Training loss = 0.21641212873733962
Iteration 170, Training loss = 0.21769438024896842
Iteration 180, Training loss = 0.2149322944191786
Iteration 190, Training loss = 0.21467096215257278
Model training time: 28.07099962234497
Device: cuda
Iteration 0, Training loss = 0.8299368000947512
Iteration 10, Training loss = 0.27513879566238475
Iteration 20, Training loss = 0.2577421384362074
Iteration 30, Training loss = 0.24878918494169527
Iteration 40, Training loss = 0.23901885490004832
Iteration 50, Training loss = 0.2322601045553501
Iteration 60, Training loss = 0.2264886200428009
Iteration 70, Training loss = 0.2227532918063494
Iteration 80, Training loss = 0.22008980552737528
Iteration 90, Training loss = 0.21829824737058237
Iteration 100, Training loss = 0.21617388266783494
Iteration 110, Training loss = 0.21411641945059484
Iteration 120, Training loss = 0.21302579429287177
Iteration 130, Training loss = 0.21173514540378863
Iteration 140, Training loss = 0.2095843619452073
Iteration 150, Training loss = 0.20951151876495436
Iteration 160, Training loss = 0.20788289234042168
Iteration 170, Training loss = 0.20740671083331108
Iteration 180, Training loss = 0.20701240977415672
Iteration 190, Training loss = 0.20679984155755776
Model training time: 28.16149878501892
Device: cuda
Iteration 0, Training loss = 0.8836529954121664
Iteration 10, Training loss = 0.2682046443223953
Iteration 20, Training loss = 0.25354643166065216
Iteration 30, Training loss = 0.24470406942642653
Iteration 40, Training loss = 0.23783076067383474
Iteration 50, Training loss = 0.23546889367011878
Iteration 60, Training loss = 0.23228032141923904
Iteration 70, Training loss = 0.2279499081464914
Iteration 80, Training loss = 0.22571858964287317
Iteration 90, Training loss = 0.2241265022983918
Iteration 100, Training loss = 0.22090312007528085
Iteration 110, Training loss = 0.2206739390698763
Iteration 120, Training loss = 0.21834170073270798
Iteration 130, Training loss = 0.21631295520525712
Iteration 140, Training loss = 0.21524754682412514
Iteration 150, Training loss = 0.21572751007401025
Iteration 160, Training loss = 0.21362227058181396
Iteration 170, Training loss = 0.21246301067563203
Iteration 180, Training loss = 0.21111003739329484
Iteration 190, Training loss = 0.21123332816820878
Model training time: 27.940210580825806
Device: cuda
Iteration 0, Training loss = 0.7884764453539481
Iteration 10, Training loss = 0.27092599868774414
Iteration 20, Training loss = 0.25506830330078417
Iteration 30, Training loss = 0.24724305134553176
Iteration 40, Training loss = 0.24045612491094148
Iteration 50, Training loss = 0.23599612483611473
Iteration 60, Training loss = 0.23067937648067108
Iteration 70, Training loss = 0.22682593590938127
Iteration 80, Training loss = 0.22257103455754426
Iteration 90, Training loss = 0.22022292820306924
Iteration 100, Training loss = 0.21721066591831353
Iteration 110, Training loss = 0.21635097035994896
Iteration 120, Training loss = 0.21301182302144858
Iteration 130, Training loss = 0.21265055898290414
Iteration 140, Training loss = 0.21076986480217713
Iteration 150, Training loss = 0.21208366723014757
Iteration 160, Training loss = 0.21049143001437187
Iteration 170, Training loss = 0.20929112629248545
Iteration 180, Training loss = 0.20930122842009252
Iteration 190, Training loss = 0.20877958117769316
Model training time: 28.421941995620728
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0010660084394307
Iteration 10, Training loss = 0.8651184794994501
Iteration 20, Training loss = 0.527518627162163
Iteration 30, Training loss = 0.42897680000616956
Iteration 40, Training loss = 0.3983520412674317
Iteration 50, Training loss = 0.3750355037359091
Iteration 60, Training loss = 0.3574022857042459
Iteration 70, Training loss = 0.34522570211153764
Iteration 80, Training loss = 0.33568464792691743
Iteration 90, Training loss = 0.32841809867666316
Iteration 100, Training loss = 0.32462207743754756
Iteration 110, Training loss = 0.3195145594385954
Iteration 120, Training loss = 0.3172675213561608
Iteration 130, Training loss = 0.3136466406285763
Iteration 140, Training loss = 0.31195331937991655
Iteration 150, Training loss = 0.3097056872569598
Iteration 160, Training loss = 0.3074937124664967
Iteration 170, Training loss = 0.3052629160766418
Iteration 180, Training loss = 0.3045099019431151
Iteration 190, Training loss = 0.3037444376028501
Model training time: 26.17966079711914
Device: cuda
Iteration 0, Training loss = 1.049963619846564
Iteration 10, Training loss = 0.7482454432890966
Iteration 20, Training loss = 0.5297914474056318
Iteration 30, Training loss = 0.43375978905421037
Iteration 40, Training loss = 0.3879117008585196
Iteration 50, Training loss = 0.36183656408236575
Iteration 60, Training loss = 0.34593496672236
Iteration 70, Training loss = 0.3361826498921101
Iteration 80, Training loss = 0.3288236094208864
Iteration 90, Training loss = 0.32246319835002607
Iteration 100, Training loss = 0.31645756702010447
Iteration 110, Training loss = 0.3123264094957939
Iteration 120, Training loss = 0.308561485260725
Iteration 130, Training loss = 0.30562271321049106
Iteration 140, Training loss = 0.30119612612403357
Iteration 150, Training loss = 0.29845770018605083
Iteration 160, Training loss = 0.2955162230019386
Iteration 170, Training loss = 0.29327139115104306
Iteration 180, Training loss = 0.29139450278419715
Iteration 190, Training loss = 0.29040703664605433
Model training time: 25.82520055770874
Device: cuda
Iteration 0, Training loss = 1.0588881476567342
Iteration 10, Training loss = 0.9699924129706162
Iteration 20, Training loss = 0.874740406870842
Iteration 30, Training loss = 0.5913656239326184
Iteration 40, Training loss = 0.4258158178283618
Iteration 50, Training loss = 0.38583941700366825
Iteration 60, Training loss = 0.36182566159046614
Iteration 70, Training loss = 0.3459972458390089
Iteration 80, Training loss = 0.33566570196014184
Iteration 90, Training loss = 0.3280176199399508
Iteration 100, Training loss = 0.3219566938395684
Iteration 110, Training loss = 0.31787427686727965
Iteration 120, Training loss = 0.3146391955132668
Iteration 130, Training loss = 0.31073488037173563
Iteration 140, Training loss = 0.3078277583878774
Iteration 150, Training loss = 0.30460254933971626
Iteration 160, Training loss = 0.30248578494557965
Iteration 170, Training loss = 0.2998466881421896
Iteration 180, Training loss = 0.29779932676599574
Iteration 190, Training loss = 0.29543325648858
Model training time: 25.874333381652832
Device: cuda
Iteration 0, Training loss = 0.9934084507135245
Iteration 10, Training loss = 0.9265336738182948
Iteration 20, Training loss = 0.8078290189688022
Iteration 30, Training loss = 0.6172250194045213
Iteration 40, Training loss = 0.4430149828012173
Iteration 50, Training loss = 0.37574561341450763
Iteration 60, Training loss = 0.3511504356104594
Iteration 70, Training loss = 0.33562085834833294
Iteration 80, Training loss = 0.3264756632538942
Iteration 90, Training loss = 0.3192761465907097
Iteration 100, Training loss = 0.3129932590975211
Iteration 110, Training loss = 0.3077888199343131
Iteration 120, Training loss = 0.30430128138798934
Iteration 130, Training loss = 0.30073098895641476
Iteration 140, Training loss = 0.2990624692577582
Iteration 150, Training loss = 0.29623482204400575
Iteration 160, Training loss = 0.294502488982219
Iteration 170, Training loss = 0.2921581950325232
Iteration 180, Training loss = 0.29090310346621734
Iteration 190, Training loss = 0.2892170760494012
Model training time: 25.739136934280396
Device: cuda
Iteration 0, Training loss = 1.0438664738948529
Iteration 10, Training loss = 0.9386693915495505
Iteration 20, Training loss = 0.8139478976909931
Iteration 30, Training loss = 0.6061040140115298
Iteration 40, Training loss = 0.4357430304472263
Iteration 50, Training loss = 0.3838354117022111
Iteration 60, Training loss = 0.35974252625153613
Iteration 70, Training loss = 0.34397512960892457
Iteration 80, Training loss = 0.33310179297740644
Iteration 90, Training loss = 0.324515399738
Iteration 100, Training loss = 0.3197647479291146
Iteration 110, Training loss = 0.31543839178406274
Iteration 120, Training loss = 0.31157490152579087
Iteration 130, Training loss = 0.308637967476478
Iteration 140, Training loss = 0.3063364206598355
Iteration 150, Training loss = 0.3052686882706789
Iteration 160, Training loss = 0.3021214882341715
Iteration 170, Training loss = 0.3004496295291644
Iteration 180, Training loss = 0.2986646701510136
Iteration 190, Training loss = 0.29673550106011903
Model training time: 25.883651733398438
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8980439247993323
Iteration 10, Training loss = 0.2806936571231255
Iteration 20, Training loss = 0.262004525329058
Iteration 30, Training loss = 0.2513867224065157
Iteration 40, Training loss = 0.2469454720043219
Iteration 50, Training loss = 0.24108302106077856
Iteration 60, Training loss = 0.23709579241963533
Iteration 70, Training loss = 0.23390965880109713
Iteration 80, Training loss = 0.23040627258328292
Iteration 90, Training loss = 0.22683830530597612
Iteration 100, Training loss = 0.22551992965432313
Iteration 110, Training loss = 0.22381045955878037
Iteration 120, Training loss = 0.22126600232261878
Iteration 130, Training loss = 0.22110318736388132
Iteration 140, Training loss = 0.2178092825298126
Iteration 150, Training loss = 0.21710695297672197
Iteration 160, Training loss = 0.216303280626352
Iteration 170, Training loss = 0.2161920162347647
Iteration 180, Training loss = 0.21507035969541624
Iteration 190, Training loss = 0.21469367983249518
Iteration 200, Training loss = 0.21264139505533072
Iteration 210, Training loss = 0.21199228012791047
Iteration 220, Training loss = 0.21234067128254816
Iteration 230, Training loss = 0.21125359145494607
Iteration 240, Training loss = 0.211441073853236
Iteration 250, Training loss = 0.2105279561991875
Iteration 260, Training loss = 0.2088321650830599
Iteration 270, Training loss = 0.20809339330746576
Iteration 280, Training loss = 0.20735138683364943
Iteration 290, Training loss = 0.20963962691334578
Model training time: 45.71855163574219
Device: cuda
Iteration 0, Training loss = 0.8934479195338029
Iteration 10, Training loss = 0.26745720551564145
Iteration 20, Training loss = 0.2465830912383703
Iteration 30, Training loss = 0.24030574325185555
Iteration 40, Training loss = 0.23755482211709023
Iteration 50, Training loss = 0.2350706813427118
Iteration 60, Training loss = 0.2314381249822103
Iteration 70, Training loss = 0.22991312725039628
Iteration 80, Training loss = 0.22847943036602095
Iteration 90, Training loss = 0.2256040911261852
Iteration 100, Training loss = 0.22581790244350067
Iteration 110, Training loss = 0.22292744654875535
Iteration 120, Training loss = 0.22195468069269106
Iteration 130, Training loss = 0.22022272589114997
Iteration 140, Training loss = 0.22037837625696108
Iteration 150, Training loss = 0.2198456207720133
Iteration 160, Training loss = 0.21939092960495216
Iteration 170, Training loss = 0.2173728309571743
Iteration 180, Training loss = 0.2167354171665815
Iteration 190, Training loss = 0.21506118917694458
Iteration 200, Training loss = 0.21500767308932084
Iteration 210, Training loss = 0.21133752224537042
Iteration 220, Training loss = 0.21131404689871347
Iteration 230, Training loss = 0.2091206547159415
Iteration 240, Training loss = 0.20933904269566903
Iteration 250, Training loss = 0.20651115333804718
Iteration 260, Training loss = 0.20637318004782385
Iteration 270, Training loss = 0.2059406781425843
Iteration 280, Training loss = 0.2062425080400247
Iteration 290, Training loss = 0.20407742663071707
Model training time: 46.94326996803284
Device: cuda
Iteration 0, Training loss = 0.8260202476611505
Iteration 10, Training loss = 0.2630497735853379
Iteration 20, Training loss = 0.24393858932531798
Iteration 30, Training loss = 0.23662190311230147
Iteration 40, Training loss = 0.22926359652326658
Iteration 50, Training loss = 0.22318567564854255
Iteration 60, Training loss = 0.21857241340554678
Iteration 70, Training loss = 0.21703860306969056
Iteration 80, Training loss = 0.21279535714823467
Iteration 90, Training loss = 0.21217151530660117
Iteration 100, Training loss = 0.21004622859450486
Iteration 110, Training loss = 0.20896145357535437
Iteration 120, Training loss = 0.2072222247146643
Iteration 130, Training loss = 0.20535898781739748
Iteration 140, Training loss = 0.20565760422211427
Iteration 150, Training loss = 0.20388650736556604
Iteration 160, Training loss = 0.2024614722109758
Iteration 170, Training loss = 0.2026620154770521
Iteration 180, Training loss = 0.2012581699169599
Iteration 190, Training loss = 0.20078591601206705
Iteration 200, Training loss = 0.19980217430454034
Iteration 210, Training loss = 0.1997987857231727
Iteration 220, Training loss = 0.19858807053130406
Iteration 230, Training loss = 0.19827045099093363
Iteration 240, Training loss = 0.19771745428442955
Iteration 250, Training loss = 0.1975809155175319
Iteration 260, Training loss = 0.19670585113076064
Iteration 270, Training loss = 0.19654668581027251
Iteration 280, Training loss = 0.19571300280781892
Iteration 290, Training loss = 0.1943493913859129
Model training time: 44.6837956905365
Device: cuda
Iteration 0, Training loss = 0.9404636197365247
Iteration 10, Training loss = 0.2624942651734902
Iteration 20, Training loss = 0.2491340387899142
Iteration 30, Training loss = 0.24007544293999672
Iteration 40, Training loss = 0.2325944651204806
Iteration 50, Training loss = 0.229139272410136
Iteration 60, Training loss = 0.2245786445072064
Iteration 70, Training loss = 0.22252436698629305
Iteration 80, Training loss = 0.2207989156819307
Iteration 90, Training loss = 0.21835830549781138
Iteration 100, Training loss = 0.21685715134327227
Iteration 110, Training loss = 0.2157569324167875
Iteration 120, Training loss = 0.21466726150650245
Iteration 130, Training loss = 0.2144831562271485
Iteration 140, Training loss = 0.21396475142011276
Iteration 150, Training loss = 0.21298463843189752
Iteration 160, Training loss = 0.21236867257035696
Iteration 170, Training loss = 0.2131145762709471
Iteration 180, Training loss = 0.21251130247345337
Iteration 190, Training loss = 0.21111720743087622
Iteration 200, Training loss = 0.2112710175032799
Iteration 210, Training loss = 0.2104944770152752
Iteration 220, Training loss = 0.210743321535679
Iteration 230, Training loss = 0.20951126315272772
Iteration 240, Training loss = 0.2081884237436148
Iteration 250, Training loss = 0.2104280381821669
Iteration 260, Training loss = 0.209919076699477
Iteration 270, Training loss = 0.20895458156099686
Iteration 280, Training loss = 0.207393704698636
Iteration 290, Training loss = 0.20777242831312692
Model training time: 42.749945878982544
Device: cuda
Iteration 0, Training loss = 0.9808261348651006
Iteration 10, Training loss = 0.2602492570877075
Iteration 20, Training loss = 0.24248150621469206
Iteration 30, Training loss = 0.2342871671112684
Iteration 40, Training loss = 0.2276990617123934
Iteration 50, Training loss = 0.2244708423431103
Iteration 60, Training loss = 0.2211679105575268
Iteration 70, Training loss = 0.21766778826713562
Iteration 80, Training loss = 0.21664687074147737
Iteration 90, Training loss = 0.21386770631831425
Iteration 100, Training loss = 0.2138983100079573
Iteration 110, Training loss = 0.21192884158629638
Iteration 120, Training loss = 0.20936626797685257
Iteration 130, Training loss = 0.20715568243311003
Iteration 140, Training loss = 0.20625655353069305
Iteration 150, Training loss = 0.20484626522431007
Iteration 160, Training loss = 0.20377394614311364
Iteration 170, Training loss = 0.2024035084132965
Iteration 180, Training loss = 0.20322979872043317
Iteration 190, Training loss = 0.20324805436226037
Iteration 200, Training loss = 0.2001286448480991
Iteration 210, Training loss = 0.1989653677894519
Iteration 220, Training loss = 0.19753175916580054
Iteration 230, Training loss = 0.1972486964212014
Iteration 240, Training loss = 0.19763154536485672
Iteration 250, Training loss = 0.19627666029219443
Iteration 260, Training loss = 0.19532692890900832
Iteration 270, Training loss = 0.19630405994562003
Iteration 280, Training loss = 0.19519001360122973
Iteration 290, Training loss = 0.1961799908715945
Model training time: 42.54152512550354
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0546686282524695
Iteration 10, Training loss = 0.9293478887814742
Iteration 20, Training loss = 0.7654867676588205
Iteration 30, Training loss = 0.47572686637823397
Iteration 40, Training loss = 0.37366602569818497
Iteration 50, Training loss = 0.3437033874484209
Iteration 60, Training loss = 0.33185402762431365
Iteration 70, Training loss = 0.3247769262928229
Iteration 80, Training loss = 0.3185838263195295
Iteration 90, Training loss = 0.31422261549876285
Iteration 100, Training loss = 0.3121015183054484
Iteration 110, Training loss = 0.3075304455482043
Iteration 120, Training loss = 0.30518639345581716
Iteration 130, Training loss = 0.30143369705631184
Iteration 140, Training loss = 0.2995355776869334
Iteration 150, Training loss = 0.29722494603349614
Iteration 160, Training loss = 0.2948747016489506
Iteration 170, Training loss = 0.29235809898147214
Iteration 180, Training loss = 0.2908992122572202
Iteration 190, Training loss = 0.28871464242155737
Iteration 200, Training loss = 0.2873860652057024
Iteration 210, Training loss = 0.2851634776363006
Iteration 220, Training loss = 0.28400985724650896
Iteration 230, Training loss = 0.2820967421508752
Iteration 240, Training loss = 0.28110492487366384
Iteration 250, Training loss = 0.2804249794437335
Iteration 260, Training loss = 0.2783574370237497
Iteration 270, Training loss = 0.2783725866331504
Iteration 280, Training loss = 0.2767564748915342
Iteration 290, Training loss = 0.2755704911855551
Model training time: 38.49085831642151
Device: cuda
Iteration 0, Training loss = 1.0596473125311046
Iteration 10, Training loss = 0.9124586662420859
Iteration 20, Training loss = 0.6722912146494939
Iteration 30, Training loss = 0.4971704213665082
Iteration 40, Training loss = 0.43923500638741714
Iteration 50, Training loss = 0.4059293751533215
Iteration 60, Training loss = 0.3841850253251883
Iteration 70, Training loss = 0.36977598243034804
Iteration 80, Training loss = 0.35995742220145005
Iteration 90, Training loss = 0.3513215803183042
Iteration 100, Training loss = 0.3464045114815235
Iteration 110, Training loss = 0.3427163276534814
Iteration 120, Training loss = 0.33853916107462
Iteration 130, Training loss = 0.3363532487016458
Iteration 140, Training loss = 0.33399668307258534
Iteration 150, Training loss = 0.3320874528816113
Iteration 160, Training loss = 0.3291450177247708
Iteration 170, Training loss = 0.32746740545217806
Iteration 180, Training loss = 0.32521818635555416
Iteration 190, Training loss = 0.3237276922624845
Iteration 200, Training loss = 0.32176989488876784
Iteration 210, Training loss = 0.32002899279961217
Iteration 220, Training loss = 0.3199921539769723
Iteration 230, Training loss = 0.3168085211744675
Iteration 240, Training loss = 0.31590410923728574
Iteration 250, Training loss = 0.3135148802628884
Iteration 260, Training loss = 0.31365133134218365
Iteration 270, Training loss = 0.31111113927685297
Iteration 280, Training loss = 0.3103337557269977
Iteration 290, Training loss = 0.3081504347232672
Model training time: 39.01173996925354
Device: cuda
Iteration 0, Training loss = 0.9838657677173615
Iteration 10, Training loss = 0.9099989911684623
Iteration 20, Training loss = 0.6919354211825591
Iteration 30, Training loss = 0.42267340077803683
Iteration 40, Training loss = 0.3604590789629863
Iteration 50, Training loss = 0.34102663512413317
Iteration 60, Training loss = 0.3287363860469598
Iteration 70, Training loss = 0.3214740331929464
Iteration 80, Training loss = 0.31592446823532766
Iteration 90, Training loss = 0.31182750314474106
Iteration 100, Training loss = 0.3086802222980903
Iteration 110, Training loss = 0.30494205109201944
Iteration 120, Training loss = 0.30287449663648236
Iteration 130, Training loss = 0.2999407746470891
Iteration 140, Training loss = 0.29712615334070647
Iteration 150, Training loss = 0.29500453288738543
Iteration 160, Training loss = 0.293109186566793
Iteration 170, Training loss = 0.29067669121118694
Iteration 180, Training loss = 0.289431499174008
Iteration 190, Training loss = 0.28681164148908395
Iteration 200, Training loss = 0.2850949377394639
Iteration 210, Training loss = 0.283403893502859
Iteration 220, Training loss = 0.28199220133515507
Iteration 230, Training loss = 0.2802535521869476
Iteration 240, Training loss = 0.2787311767729429
Iteration 250, Training loss = 0.27757774694607806
Iteration 260, Training loss = 0.2763754011919865
Iteration 270, Training loss = 0.2755349716888024
Iteration 280, Training loss = 0.2741667399039635
Iteration 290, Training loss = 0.27279039615621936
Model training time: 39.029295444488525
Device: cuda
Iteration 0, Training loss = 1.0422444675977414
Iteration 10, Training loss = 0.9701569813948411
Iteration 20, Training loss = 0.8853475978741279
Iteration 30, Training loss = 0.6402184435954461
Iteration 40, Training loss = 0.4146447433875157
Iteration 50, Training loss = 0.36922889145521015
Iteration 60, Training loss = 0.3524659121265778
Iteration 70, Training loss = 0.3430345496879174
Iteration 80, Training loss = 0.33508048894313663
Iteration 90, Training loss = 0.32977272684757525
Iteration 100, Training loss = 0.32369692394366634
Iteration 110, Training loss = 0.31966111006645054
Iteration 120, Training loss = 0.31564590535484827
Iteration 130, Training loss = 0.31310132002601254
Iteration 140, Training loss = 0.3089128010548078
Iteration 150, Training loss = 0.3064809361329445
Iteration 160, Training loss = 0.3045927807688713
Iteration 170, Training loss = 0.30239436145012194
Iteration 180, Training loss = 0.29914432907333743
Iteration 190, Training loss = 0.2977003724529193
Iteration 200, Training loss = 0.29568277213435906
Iteration 210, Training loss = 0.2941462566646246
Iteration 220, Training loss = 0.29226756353790945
Iteration 230, Training loss = 0.2900450350000308
Iteration 240, Training loss = 0.28890274866269183
Iteration 250, Training loss = 0.28794242155093414
Iteration 260, Training loss = 0.28621753391165
Iteration 270, Training loss = 0.28503942690216577
Iteration 280, Training loss = 0.28262716044600195
Iteration 290, Training loss = 0.2818239643596686
Model training time: 40.06121039390564
Device: cuda
Iteration 0, Training loss = 1.0104373269356215
Iteration 10, Training loss = 0.7226620385280023
Iteration 20, Training loss = 0.42404694339403737
Iteration 30, Training loss = 0.36376205946390444
Iteration 40, Training loss = 0.3431853778087176
Iteration 50, Training loss = 0.3284369701376328
Iteration 60, Training loss = 0.31906991738539475
Iteration 70, Training loss = 0.31357032175247485
Iteration 80, Training loss = 0.3083737753331661
Iteration 90, Training loss = 0.3050614632666111
Iteration 100, Training loss = 0.30224740734467137
Iteration 110, Training loss = 0.2994808116211341
Iteration 120, Training loss = 0.296805174018328
Iteration 130, Training loss = 0.29507591747320616
Iteration 140, Training loss = 0.2928026183866538
Iteration 150, Training loss = 0.2900320363159363
Iteration 160, Training loss = 0.2894437559522115
Iteration 170, Training loss = 0.287039498010507
Iteration 180, Training loss = 0.28606164570038134
Iteration 190, Training loss = 0.2833643312064501
Iteration 200, Training loss = 0.28250526837431467
Iteration 210, Training loss = 0.28114911398062337
Iteration 220, Training loss = 0.2794651369062754
Iteration 230, Training loss = 0.2785681546307527
Iteration 240, Training loss = 0.2775547713614427
Iteration 250, Training loss = 0.2767982113246734
Iteration 260, Training loss = 0.275163954267135
Iteration 270, Training loss = 0.2744180754973338
Iteration 280, Training loss = 0.2734924652255498
Iteration 290, Training loss = 0.2720082144324596
Model training time: 39.71880507469177
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9637498305394099
Iteration 10, Training loss = 0.31088763217513377
Iteration 20, Training loss = 0.28265632774967414
Iteration 30, Training loss = 0.2725732160302309
Iteration 40, Training loss = 0.268217158718751
Iteration 50, Training loss = 0.2656167851617703
Iteration 60, Training loss = 0.26140783440608245
Iteration 70, Training loss = 0.26072613063913125
Iteration 80, Training loss = 0.26029073246396506
Iteration 90, Training loss = 0.2589416437997268
Iteration 100, Training loss = 0.2574304474087862
Iteration 110, Training loss = 0.25567009672522545
Iteration 120, Training loss = 0.25467247114731717
Iteration 130, Training loss = 0.25622015303144086
Iteration 140, Training loss = 0.2538016853997341
Iteration 150, Training loss = 0.2533203916480908
Iteration 160, Training loss = 0.2523573668530354
Iteration 170, Training loss = 0.2509805279282423
Iteration 180, Training loss = 0.24976269155740738
Iteration 190, Training loss = 0.2488445510657934
Iteration 200, Training loss = 0.24800353382642454
Iteration 210, Training loss = 0.24656725789491946
Iteration 220, Training loss = 0.24485721697027868
Iteration 230, Training loss = 0.24208668504769987
Iteration 240, Training loss = 0.24051342274133974
Iteration 250, Training loss = 0.2389300144635714
Iteration 260, Training loss = 0.23632664004197487
Iteration 270, Training loss = 0.2348104973252003
Iteration 280, Training loss = 0.23182559930361235
Iteration 290, Training loss = 0.23134609082570443
Model training time: 42.57133913040161
Device: cuda
Iteration 0, Training loss = 0.9153852531543145
Iteration 10, Training loss = 0.2859937170377144
Iteration 20, Training loss = 0.2592367332142133
Iteration 30, Training loss = 0.24761537376504678
Iteration 40, Training loss = 0.23906872306878751
Iteration 50, Training loss = 0.23388779851106498
Iteration 60, Training loss = 0.23050657688425139
Iteration 70, Training loss = 0.2265972987963603
Iteration 80, Training loss = 0.2220947083372336
Iteration 90, Training loss = 0.2212634370304071
Iteration 100, Training loss = 0.21644165309575888
Iteration 110, Training loss = 0.21429131151391909
Iteration 120, Training loss = 0.21167627807993156
Iteration 130, Training loss = 0.20923064763729388
Iteration 140, Training loss = 0.20879939083869642
Iteration 150, Training loss = 0.2090354232260814
Iteration 160, Training loss = 0.20712370712023515
Iteration 170, Training loss = 0.20735014917758796
Iteration 180, Training loss = 0.20755809144331858
Iteration 190, Training loss = 0.206139680571281
Iteration 200, Training loss = 0.20521081124360746
Iteration 210, Training loss = 0.204790398478508
Iteration 220, Training loss = 0.20538171925223792
Iteration 230, Training loss = 0.20507797942711756
Iteration 240, Training loss = 0.20365187210532334
Iteration 250, Training loss = 0.20448985065405184
Iteration 260, Training loss = 0.203357716019337
Iteration 270, Training loss = 0.20338177021879417
Iteration 280, Training loss = 0.202935664413067
Iteration 290, Training loss = 0.2040151500931153
Model training time: 42.38971567153931
Device: cuda
Iteration 0, Training loss = 0.9113541623720756
Iteration 10, Training loss = 0.3843123855499121
Iteration 20, Training loss = 0.29411176764048064
Iteration 30, Training loss = 0.26289986303219426
Iteration 40, Training loss = 0.24769574117202026
Iteration 50, Training loss = 0.24101876438810274
Iteration 60, Training loss = 0.23622912053878492
Iteration 70, Training loss = 0.23513203372175878
Iteration 80, Training loss = 0.23326389778118867
Iteration 90, Training loss = 0.2303118992310304
Iteration 100, Training loss = 0.22865120264200064
Iteration 110, Training loss = 0.22696826692957145
Iteration 120, Training loss = 0.22679497444858918
Iteration 130, Training loss = 0.22643681758871445
Iteration 140, Training loss = 0.2231231965124607
Iteration 150, Training loss = 0.22371430379840043
Iteration 160, Training loss = 0.22262747643085626
Iteration 170, Training loss = 0.22326299977990297
Iteration 180, Training loss = 0.22178387412658104
Iteration 190, Training loss = 0.22335146711422846
Iteration 200, Training loss = 0.22234925656364515
Iteration 210, Training loss = 0.22220201503772002
Iteration 220, Training loss = 0.22134748072578356
Iteration 230, Training loss = 0.22041668656926888
Iteration 240, Training loss = 0.21967691383682764
Iteration 250, Training loss = 0.22043976245018151
Iteration 260, Training loss = 0.22017773556021544
Iteration 270, Training loss = 0.21952201368717048
Iteration 280, Training loss = 0.21804989903019026
Iteration 290, Training loss = 0.21868703084496352
Model training time: 42.246283292770386
Device: cuda
Iteration 0, Training loss = 1.1489188545025313
Iteration 10, Training loss = 0.28060779262047547
Iteration 20, Training loss = 0.26566558102002513
Iteration 30, Training loss = 0.2557078107045247
Iteration 40, Training loss = 0.25271871141516244
Iteration 50, Training loss = 0.24846966507343146
Iteration 60, Training loss = 0.24622826020304972
Iteration 70, Training loss = 0.24452301831199572
Iteration 80, Training loss = 0.24242399260401726
Iteration 90, Training loss = 0.24124354009444898
Iteration 100, Training loss = 0.24151263357355043
Iteration 110, Training loss = 0.2409105017208136
Iteration 120, Training loss = 0.2401117980480194
Iteration 130, Training loss = 0.2394290640950203
Iteration 140, Training loss = 0.23845837723750335
Iteration 150, Training loss = 0.23802685995514578
Iteration 160, Training loss = 0.23640876779189476
Iteration 170, Training loss = 0.23604843851465446
Iteration 180, Training loss = 0.23566636729698914
Iteration 190, Training loss = 0.23473294853017881
Iteration 200, Training loss = 0.23502876580907747
Iteration 210, Training loss = 0.23414082338030523
Iteration 220, Training loss = 0.23481725328243697
Iteration 230, Training loss = 0.23251521931244776
Iteration 240, Training loss = 0.23211236813893685
Iteration 250, Training loss = 0.23348373747788942
Iteration 260, Training loss = 0.23216931378612152
Iteration 270, Training loss = 0.2317184149645842
Iteration 280, Training loss = 0.23167572944210127
Iteration 290, Training loss = 0.23100821014780265
Model training time: 41.892382860183716
Device: cuda
Iteration 0, Training loss = 1.0188160252112608
Iteration 10, Training loss = 0.26931528002023697
Iteration 20, Training loss = 0.2583131409035279
Iteration 30, Training loss = 0.25271065590473324
Iteration 40, Training loss = 0.24936426259004152
Iteration 50, Training loss = 0.24762574296731216
Iteration 60, Training loss = 0.2453389153457605
Iteration 70, Training loss = 0.24351221466293702
Iteration 80, Training loss = 0.24177048498621354
Iteration 90, Training loss = 0.23919823163977036
Iteration 100, Training loss = 0.23769233433099893
Iteration 110, Training loss = 0.2353483157662245
Iteration 120, Training loss = 0.2341039954469754
Iteration 130, Training loss = 0.23352640150831297
Iteration 140, Training loss = 0.23098980434812033
Iteration 150, Training loss = 0.22980024178440756
Iteration 160, Training loss = 0.2300367894080969
Iteration 170, Training loss = 0.22745941092188543
Iteration 180, Training loss = 0.22679915479742563
Iteration 190, Training loss = 0.226868658971328
Iteration 200, Training loss = 0.2240302932377045
Iteration 210, Training loss = 0.22394727457028168
Iteration 220, Training loss = 0.22243441354769927
Iteration 230, Training loss = 0.22218630663477457
Iteration 240, Training loss = 0.22216315825398153
Iteration 250, Training loss = 0.2206973869066972
Iteration 260, Training loss = 0.21925665008334014
Iteration 270, Training loss = 0.21868148875924256
Iteration 280, Training loss = 0.21870561173328987
Iteration 290, Training loss = 0.21775820650733435
Model training time: 41.72039818763733
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0335608881253462
Iteration 10, Training loss = 0.8063813333327954
Iteration 20, Training loss = 0.5095600801018568
Iteration 30, Training loss = 0.4106075396904579
Iteration 40, Training loss = 0.3784410512218109
Iteration 50, Training loss = 0.36064165314802754
Iteration 60, Training loss = 0.34876678769405073
Iteration 70, Training loss = 0.34155723051382947
Iteration 80, Training loss = 0.336309178517415
Iteration 90, Training loss = 0.3315401395352987
Iteration 100, Training loss = 0.3293461180650271
Iteration 110, Training loss = 0.3244254339772921
Iteration 120, Training loss = 0.32248515950945705
Iteration 130, Training loss = 0.31868552473875195
Iteration 140, Training loss = 0.3169146641515769
Iteration 150, Training loss = 0.31454631227713364
Iteration 160, Training loss = 0.3116846869771297
Iteration 170, Training loss = 0.30902013698449504
Iteration 180, Training loss = 0.30725537699002486
Iteration 190, Training loss = 0.306116850903401
Iteration 200, Training loss = 0.30356054925001585
Iteration 210, Training loss = 0.3012650517316965
Iteration 220, Training loss = 0.2999836653470993
Iteration 230, Training loss = 0.29755302088764995
Iteration 240, Training loss = 0.2966231064727673
Iteration 250, Training loss = 0.29451401789601034
Iteration 260, Training loss = 0.29327146221811956
Iteration 270, Training loss = 0.29196343943476677
Iteration 280, Training loss = 0.2907886677063428
Iteration 290, Training loss = 0.2892648921563075
Model training time: 38.73719000816345
Device: cuda
Iteration 0, Training loss = 1.0943169605273466
Iteration 10, Training loss = 0.9334331246522757
Iteration 20, Training loss = 0.7997243954585149
Iteration 30, Training loss = 0.6370216642434781
Iteration 40, Training loss = 0.49888743746739167
Iteration 50, Training loss = 0.41229997460658735
Iteration 60, Training loss = 0.3671766215792069
Iteration 70, Training loss = 0.34455012444120187
Iteration 80, Training loss = 0.33286267003187764
Iteration 90, Training loss = 0.32491167892630285
Iteration 100, Training loss = 0.3188307910011365
Iteration 110, Training loss = 0.31481292901130825
Iteration 120, Training loss = 0.30913146843130773
Iteration 130, Training loss = 0.3063609760541182
Iteration 140, Training loss = 0.30201215382951957
Iteration 150, Training loss = 0.29850150673435283
Iteration 160, Training loss = 0.29596860334277153
Iteration 170, Training loss = 0.2933289162241496
Iteration 180, Training loss = 0.2908247267970672
Iteration 190, Training loss = 0.28906735978447473
Iteration 200, Training loss = 0.28772340800899726
Iteration 210, Training loss = 0.28464050963521004
Iteration 220, Training loss = 0.28402528969141155
Iteration 230, Training loss = 0.28207776265648693
Iteration 240, Training loss = 0.2813656668250377
Iteration 250, Training loss = 0.280076148131719
Iteration 260, Training loss = 0.27902364902771437
Iteration 270, Training loss = 0.27811147530491537
Iteration 280, Training loss = 0.2770964104968768
Iteration 290, Training loss = 0.27611058549239087
Model training time: 39.15288519859314
Device: cuda
Iteration 0, Training loss = 1.0068730838023698
Iteration 10, Training loss = 0.9790135152064837
Iteration 20, Training loss = 0.8951640255176104
Iteration 30, Training loss = 0.6401570920760815
Iteration 40, Training loss = 0.48181357463965047
Iteration 50, Training loss = 0.4314244572932904
Iteration 60, Training loss = 0.4018186296407993
Iteration 70, Training loss = 0.3792756681258862
Iteration 80, Training loss = 0.3638119362294674
Iteration 90, Training loss = 0.353992335498333
Iteration 100, Training loss = 0.3465651164834316
Iteration 110, Training loss = 0.3396100499309026
Iteration 120, Training loss = 0.3351132735036887
Iteration 130, Training loss = 0.3316075810446189
Iteration 140, Training loss = 0.32895861422786343
Iteration 150, Training loss = 0.32493843654027355
Iteration 160, Training loss = 0.3246511481702328
Iteration 170, Training loss = 0.3215210758722745
Iteration 180, Training loss = 0.31905977771832394
Iteration 190, Training loss = 0.3171557637934501
Iteration 200, Training loss = 0.31630127275219333
Iteration 210, Training loss = 0.3127588377549098
Iteration 220, Training loss = 0.3122273525939538
Iteration 230, Training loss = 0.31012226383273417
Iteration 240, Training loss = 0.3076978735625744
Iteration 250, Training loss = 0.3056390311282415
Iteration 260, Training loss = 0.3045345751138834
Iteration 270, Training loss = 0.30290632895552194
Iteration 280, Training loss = 0.3011498786509037
Iteration 290, Training loss = 0.2998666582772365
Model training time: 40.36823582649231
Device: cuda
Iteration 0, Training loss = 1.07193869466965
Iteration 10, Training loss = 0.9819933863786551
Iteration 20, Training loss = 0.9265450697678786
Iteration 30, Training loss = 0.7846084305873284
Iteration 40, Training loss = 0.5839252798603132
Iteration 50, Training loss = 0.45514540718151975
Iteration 60, Training loss = 0.38854027596803814
Iteration 70, Training loss = 0.353114237005894
Iteration 80, Training loss = 0.3351551924760525
Iteration 90, Training loss = 0.32415433113391584
Iteration 100, Training loss = 0.31966759952215046
Iteration 110, Training loss = 0.314091632859065
Iteration 120, Training loss = 0.3112546116686784
Iteration 130, Training loss = 0.309116123100886
Iteration 140, Training loss = 0.30684515088796616
Iteration 150, Training loss = 0.3054350926898993
Iteration 160, Training loss = 0.303009502016581
Iteration 170, Training loss = 0.3016518153823339
Iteration 180, Training loss = 0.299493252657927
Iteration 190, Training loss = 0.298559094277712
Iteration 200, Training loss = 0.2975038762849111
Iteration 210, Training loss = 0.29462791320223075
Iteration 220, Training loss = 0.2932907176705507
Iteration 230, Training loss = 0.2916602629881639
Iteration 240, Training loss = 0.2907235739895931
Iteration 250, Training loss = 0.29016273812605786
Iteration 260, Training loss = 0.28772014293533105
Iteration 270, Training loss = 0.2871077496271867
Iteration 280, Training loss = 0.28501568419428974
Iteration 290, Training loss = 0.283695125235961
Model training time: 39.36993384361267
Device: cuda
Iteration 0, Training loss = 1.0367439607015023
Iteration 10, Training loss = 0.9729851702084908
Iteration 20, Training loss = 0.9148396540146607
Iteration 30, Training loss = 0.7812636311237628
Iteration 40, Training loss = 0.544768781043016
Iteration 50, Training loss = 0.39228905680087894
Iteration 60, Training loss = 0.3611961007118225
Iteration 70, Training loss = 0.3471667588903354
Iteration 80, Training loss = 0.34030599376330006
Iteration 90, Training loss = 0.3337639266481766
Iteration 100, Training loss = 0.3276696612055485
Iteration 110, Training loss = 0.3237057305299319
Iteration 120, Training loss = 0.3201607159123971
Iteration 130, Training loss = 0.31661527775801146
Iteration 140, Training loss = 0.3144346624612808
Iteration 150, Training loss = 0.31119448806230837
Iteration 160, Training loss = 0.3081773453607009
Iteration 170, Training loss = 0.3054069441098433
Iteration 180, Training loss = 0.3034914336525477
Iteration 190, Training loss = 0.30176471976133495
Iteration 200, Training loss = 0.29884367894667846
Iteration 210, Training loss = 0.297052800655365
Iteration 220, Training loss = 0.29556453973054886
Iteration 230, Training loss = 0.2944317643459027
Iteration 240, Training loss = 0.29274563835217404
Iteration 250, Training loss = 0.2906326920940326
Iteration 260, Training loss = 0.2900453516497062
Iteration 270, Training loss = 0.2886816936616714
Iteration 280, Training loss = 0.2866969991188783
Iteration 290, Training loss = 0.285939807502123
Model training time: 38.63463473320007
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9251991831339322
Iteration 10, Training loss = 0.27369071027407277
Iteration 20, Training loss = 0.25481273147922295
Iteration 30, Training loss = 0.2454521209001541
Iteration 40, Training loss = 0.23851691472988862
Iteration 50, Training loss = 0.2352590148265545
Iteration 60, Training loss = 0.23117373367914787
Iteration 70, Training loss = 0.22847191989421844
Iteration 80, Training loss = 0.22709441356934035
Iteration 90, Training loss = 0.2253191485427893
Iteration 100, Training loss = 0.22229216734950358
Iteration 110, Training loss = 0.22211157329953635
Iteration 120, Training loss = 0.22003552260307166
Iteration 130, Training loss = 0.21865165892701882
Iteration 140, Training loss = 0.2196142329619481
Iteration 150, Training loss = 0.2172118560052835
Iteration 160, Training loss = 0.2170094566849562
Iteration 170, Training loss = 0.21795129518096262
Iteration 180, Training loss = 0.21556488424539566
Iteration 190, Training loss = 0.21448282066446084
Iteration 200, Training loss = 0.21447243656103426
Iteration 210, Training loss = 0.21296445308969572
Iteration 220, Training loss = 0.21370404013074362
Iteration 230, Training loss = 0.21110848623972672
Iteration 240, Training loss = 0.21296686564500517
Iteration 250, Training loss = 0.21272861097867674
Iteration 260, Training loss = 0.20946219964669302
Iteration 270, Training loss = 0.20945283999809852
Iteration 280, Training loss = 0.2089554057098352
Iteration 290, Training loss = 0.20881048905161712
Model training time: 41.9362895488739
Device: cuda
Iteration 0, Training loss = 0.8717850056978372
Iteration 10, Training loss = 0.2772393791148296
Iteration 20, Training loss = 0.2581441347988752
Iteration 30, Training loss = 0.24747515928286773
Iteration 40, Training loss = 0.24195307235305125
Iteration 50, Training loss = 0.23923343792557716
Iteration 60, Training loss = 0.23617867151131997
Iteration 70, Training loss = 0.23453931567760614
Iteration 80, Training loss = 0.23315465106413916
Iteration 90, Training loss = 0.22975114704324648
Iteration 100, Training loss = 0.22756542380039507
Iteration 110, Training loss = 0.22546962562661904
Iteration 120, Training loss = 0.22529454758534065
Iteration 130, Training loss = 0.22292332695080683
Iteration 140, Training loss = 0.22240780695126608
Iteration 150, Training loss = 0.22109620817578757
Iteration 160, Training loss = 0.21900747401209977
Iteration 170, Training loss = 0.21761245366472465
Iteration 180, Training loss = 0.21701074191010916
Iteration 190, Training loss = 0.21628612451828444
Iteration 200, Training loss = 0.21475390029641298
Iteration 210, Training loss = 0.21446881385949942
Iteration 220, Training loss = 0.21364498969454032
Iteration 230, Training loss = 0.21324019678510153
Iteration 240, Training loss = 0.2122277359549816
Iteration 250, Training loss = 0.21186598791525915
Iteration 260, Training loss = 0.21210904877919418
Iteration 270, Training loss = 0.208423707061089
Iteration 280, Training loss = 0.20971870737580153
Iteration 290, Training loss = 0.20837173066460168
Model training time: 42.73338222503662
Device: cuda
Iteration 0, Training loss = 0.9059950984441317
Iteration 10, Training loss = 0.2804138924066837
Iteration 20, Training loss = 0.25525889774927724
Iteration 30, Training loss = 0.2455372068171318
Iteration 40, Training loss = 0.23888008267833635
Iteration 50, Training loss = 0.23259579619536033
Iteration 60, Training loss = 0.22866076689500076
Iteration 70, Training loss = 0.22649843331712943
Iteration 80, Training loss = 0.22543518273876265
Iteration 90, Training loss = 0.222288424292436
Iteration 100, Training loss = 0.21985573436205202
Iteration 110, Training loss = 0.21784692716140014
Iteration 120, Training loss = 0.21562711120798037
Iteration 130, Training loss = 0.2134649813748323
Iteration 140, Training loss = 0.21306412265850946
Iteration 150, Training loss = 0.21412983307471642
Iteration 160, Training loss = 0.21150144648093444
Iteration 170, Training loss = 0.2103512888917556
Iteration 180, Training loss = 0.20890259656768578
Iteration 190, Training loss = 0.20839193320045105
Iteration 200, Training loss = 0.20824254762667876
Iteration 210, Training loss = 0.20702202818714655
Iteration 220, Training loss = 0.20686383144213602
Iteration 230, Training loss = 0.20674866208663353
Iteration 240, Training loss = 0.20615313603327826
Iteration 250, Training loss = 0.20555534213781357
Iteration 260, Training loss = 0.2048224348288316
Iteration 270, Training loss = 0.20437249025473228
Iteration 280, Training loss = 0.2031613066792488
Iteration 290, Training loss = 0.2024416751586474
Model training time: 42.37380361557007
Device: cuda
Iteration 0, Training loss = 0.7617385886036433
Iteration 10, Training loss = 0.26854836740172827
Iteration 20, Training loss = 0.2488302714549578
Iteration 30, Training loss = 0.2403260016670594
Iteration 40, Training loss = 0.23518000256556731
Iteration 50, Training loss = 0.2286978432765374
Iteration 60, Training loss = 0.22449277971799558
Iteration 70, Training loss = 0.2197387573810724
Iteration 80, Training loss = 0.2168340912232032
Iteration 90, Training loss = 0.21364635601639748
Iteration 100, Training loss = 0.2113382016810087
Iteration 110, Training loss = 0.20919394464446947
Iteration 120, Training loss = 0.20767117119752443
Iteration 130, Training loss = 0.20637363510636184
Iteration 140, Training loss = 0.20664080175069663
Iteration 150, Training loss = 0.2046598708973481
Iteration 160, Training loss = 0.20395583802690873
Iteration 170, Training loss = 0.20642161942445314
Iteration 180, Training loss = 0.20411381039481896
Iteration 190, Training loss = 0.20518497205697572
Iteration 200, Training loss = 0.20296411732068428
Iteration 210, Training loss = 0.20270469383551523
Iteration 220, Training loss = 0.20184558687301782
Iteration 230, Training loss = 0.2011165120280706
Iteration 240, Training loss = 0.2022204571045362
Iteration 250, Training loss = 0.19987491776163763
Iteration 260, Training loss = 0.2013163578051787
Iteration 270, Training loss = 0.19924592742553124
Iteration 280, Training loss = 0.19960477317755038
Iteration 290, Training loss = 0.19864327919024688
Model training time: 42.40970206260681
Device: cuda
Iteration 0, Training loss = 0.8015013354328963
Iteration 10, Training loss = 0.267678777472331
Iteration 20, Training loss = 0.2527929234963197
Iteration 30, Training loss = 0.24297967954323843
Iteration 40, Training loss = 0.23586231756668824
Iteration 50, Training loss = 0.2305613005390534
Iteration 60, Training loss = 0.2258728934595218
Iteration 70, Training loss = 0.22272910521580622
Iteration 80, Training loss = 0.21944013753762612
Iteration 90, Training loss = 0.21666311730559057
Iteration 100, Training loss = 0.21724344990574396
Iteration 110, Training loss = 0.21463052584574774
Iteration 120, Training loss = 0.21277827884142214
Iteration 130, Training loss = 0.21080745498721415
Iteration 140, Training loss = 0.21101166565830892
Iteration 150, Training loss = 0.2107825124493012
Iteration 160, Training loss = 0.2091062283859803
Iteration 170, Training loss = 0.20703208819031715
Iteration 180, Training loss = 0.2065624394095861
Iteration 190, Training loss = 0.20736160415869492
Iteration 200, Training loss = 0.20676973318824401
Iteration 210, Training loss = 0.2060091750553021
Iteration 220, Training loss = 0.20422554044769362
Iteration 230, Training loss = 0.20344814085043395
Iteration 240, Training loss = 0.2039475254714489
Iteration 250, Training loss = 0.20255737551129782
Iteration 260, Training loss = 0.20326514885975763
Iteration 270, Training loss = 0.20155107201291964
Iteration 280, Training loss = 0.20103079137893823
Iteration 290, Training loss = 0.20127421531539696
Model training time: 42.634034156799316
{'activation_functions': ['relu', 'relu'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0265916196199565
Iteration 10, Training loss = 0.9435247790354949
Iteration 20, Training loss = 0.8394553443560233
Iteration 30, Training loss = 0.6402502896694037
Iteration 40, Training loss = 0.4457032256401502
Iteration 50, Training loss = 0.38383691242107976
Iteration 60, Training loss = 0.35879174447976625
Iteration 70, Training loss = 0.34554975709089863
Iteration 80, Training loss = 0.3365253840501492
Iteration 90, Training loss = 0.32971943742953813
Iteration 100, Training loss = 0.3252557882895836
Iteration 110, Training loss = 0.3227116299363283
Iteration 120, Training loss = 0.31733179923433524
Iteration 130, Training loss = 0.31484398532372254
Iteration 140, Training loss = 0.3118622793027988
Iteration 150, Training loss = 0.30953760187213236
Iteration 160, Training loss = 0.3073375133367685
Iteration 170, Training loss = 0.30553165393380016
Iteration 180, Training loss = 0.3032642820706734
Iteration 190, Training loss = 0.3006896949731387
Iteration 200, Training loss = 0.2987786918305434
Iteration 210, Training loss = 0.29630056940592253
Iteration 220, Training loss = 0.2951428695366933
Iteration 230, Training loss = 0.2935613181728583
Iteration 240, Training loss = 0.29185754691178983
Iteration 250, Training loss = 0.2910378575325012
Iteration 260, Training loss = 0.290264140527982
Iteration 270, Training loss = 0.2880655790750797
Iteration 280, Training loss = 0.28626990146361864
Iteration 290, Training loss = 0.2861559156041879
Model training time: 38.98169779777527
Device: cuda
Iteration 0, Training loss = 1.0104105999836555
Iteration 10, Training loss = 0.9050939553059064
Iteration 20, Training loss = 0.64075689132397
Iteration 30, Training loss = 0.44632207544950336
Iteration 40, Training loss = 0.39214364611185515
Iteration 50, Training loss = 0.3698382612604361
Iteration 60, Training loss = 0.35792321138657057
Iteration 70, Training loss = 0.34862312846458876
Iteration 80, Training loss = 0.34289328238138783
Iteration 90, Training loss = 0.33663976650971633
Iteration 100, Training loss = 0.33224559403382814
Iteration 110, Training loss = 0.32893670407625347
Iteration 120, Training loss = 0.32469531377920735
Iteration 130, Training loss = 0.32326497309483015
Iteration 140, Training loss = 0.31874144363861817
Iteration 150, Training loss = 0.31643329761349237
Iteration 160, Training loss = 0.313824084515755
Iteration 170, Training loss = 0.3123628892577611
Iteration 180, Training loss = 0.30975452753213734
Iteration 190, Training loss = 0.3079681791938268
Iteration 200, Training loss = 0.3049783113484199
Iteration 210, Training loss = 0.3036369151220872
Iteration 220, Training loss = 0.30125881416293293
Iteration 230, Training loss = 0.2990599638567521
Iteration 240, Training loss = 0.29759235278918195
Iteration 250, Training loss = 0.29561294271395755
Iteration 260, Training loss = 0.29311504014409506
Iteration 270, Training loss = 0.29171012571224797
Iteration 280, Training loss = 0.29060128273872227
Iteration 290, Training loss = 0.2883620565900436
Model training time: 39.05623936653137
Device: cuda
Iteration 0, Training loss = 1.0304952882803404
Iteration 10, Training loss = 0.9253489157328239
Iteration 20, Training loss = 0.68629373724644
Iteration 30, Training loss = 0.47743513607061827
Iteration 40, Training loss = 0.40791431757119984
Iteration 50, Training loss = 0.37897240026639056
Iteration 60, Training loss = 0.36003952301465547
Iteration 70, Training loss = 0.3489052143234473
Iteration 80, Training loss = 0.3405569104047922
Iteration 90, Training loss = 0.33431560116318554
Iteration 100, Training loss = 0.32933401545652974
Iteration 110, Training loss = 0.3244967386126518
Iteration 120, Training loss = 0.3208468917470712
Iteration 130, Training loss = 0.316858277297937
Iteration 140, Training loss = 0.3144424374286945
Iteration 150, Training loss = 0.3115228388745051
Iteration 160, Training loss = 0.30970531386824757
Iteration 170, Training loss = 0.30702080548955846
Iteration 180, Training loss = 0.3049095169855998
Iteration 190, Training loss = 0.30374787690547794
Iteration 200, Training loss = 0.302109978806514
Iteration 210, Training loss = 0.30131480728204435
Iteration 220, Training loss = 0.2983593705755014
Iteration 230, Training loss = 0.2973075397312641
Iteration 240, Training loss = 0.2959510815831331
Iteration 250, Training loss = 0.2952131388279108
Iteration 260, Training loss = 0.29393954001940215
Iteration 270, Training loss = 0.29199895979120183
Iteration 280, Training loss = 0.2905451509241874
Iteration 290, Training loss = 0.2890143979054231
Model training time: 39.047847509384155
Device: cuda
Iteration 0, Training loss = 1.1557211841528232
Iteration 10, Training loss = 0.8121941685676575
Iteration 20, Training loss = 0.49008923654372877
Iteration 30, Training loss = 0.39426392488754713
Iteration 40, Training loss = 0.3696673810482025
Iteration 50, Training loss = 0.3531531823369173
Iteration 60, Training loss = 0.33982694808107156
Iteration 70, Training loss = 0.3293125099287583
Iteration 80, Training loss = 0.321958219489226
Iteration 90, Training loss = 0.316328477114439
Iteration 100, Training loss = 0.3126425210099954
Iteration 110, Training loss = 0.30915084939736587
Iteration 120, Training loss = 0.3077266611731969
Iteration 130, Training loss = 0.30437495473485726
Iteration 140, Training loss = 0.3029366866327249
Iteration 150, Training loss = 0.30040110561710137
Iteration 160, Training loss = 0.29860092871464217
Iteration 170, Training loss = 0.29679314591563666
Iteration 180, Training loss = 0.29470967472745824
Iteration 190, Training loss = 0.29403456902274716
Iteration 200, Training loss = 0.2924764474424032
Iteration 210, Training loss = 0.29099371570807236
Iteration 220, Training loss = 0.28833935352472156
Iteration 230, Training loss = 0.28715783816117507
Iteration 240, Training loss = 0.28552727000071454
Iteration 250, Training loss = 0.28369486847749126
Iteration 260, Training loss = 0.2827559460241061
Iteration 270, Training loss = 0.28151786298705983
Iteration 280, Training loss = 0.27970883221580434
Iteration 290, Training loss = 0.27771469119649667
Model training time: 39.08420395851135
Device: cuda
Iteration 0, Training loss = 1.0310518672833076
Iteration 10, Training loss = 0.8923445917092837
Iteration 20, Training loss = 0.5720150969349421
Iteration 30, Training loss = 0.3810985552576872
Iteration 40, Training loss = 0.35951780757078755
Iteration 50, Training loss = 0.3475689056974191
Iteration 60, Training loss = 0.3407084331489526
Iteration 70, Training loss = 0.3337313197553158
Iteration 80, Training loss = 0.32867276210051316
Iteration 90, Training loss = 0.3243510462343693
Iteration 100, Training loss = 0.321007331689963
Iteration 110, Training loss = 0.31726061237546116
Iteration 120, Training loss = 0.3144870325922966
Iteration 130, Training loss = 0.3119449638403379
Iteration 140, Training loss = 0.30994878279475063
Iteration 150, Training loss = 0.3079995768001446
Iteration 160, Training loss = 0.30586595374804276
Iteration 170, Training loss = 0.30426613356058413
Iteration 180, Training loss = 0.30284976615355563
Iteration 190, Training loss = 0.30023453585230386
Iteration 200, Training loss = 0.2986310663131567
Iteration 210, Training loss = 0.2982752171273415
Iteration 220, Training loss = 0.2957624810246321
Iteration 230, Training loss = 0.29409600507754546
Iteration 240, Training loss = 0.2919757767365529
Iteration 250, Training loss = 0.2910091005838834
Iteration 260, Training loss = 0.28943896064391506
Iteration 270, Training loss = 0.28854775113555103
Iteration 280, Training loss = 0.2873703974943895
Iteration 290, Training loss = 0.2862041810384163
Model training time: 39.976706981658936
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8438848467052127
Iteration 10, Training loss = 0.29795138752561506
Iteration 20, Training loss = 0.2826820784417538
Iteration 30, Training loss = 0.2699519820132498
Iteration 40, Training loss = 0.2600718061225466
Iteration 50, Training loss = 0.25445388077102044
Iteration 60, Training loss = 0.2502846249692665
Iteration 70, Training loss = 0.24730337602390792
Iteration 80, Training loss = 0.24433684705352957
Iteration 90, Training loss = 0.24196498716570275
Model training time: 53.38885807991028
Device: cuda
Iteration 0, Training loss = 0.9094764926531702
Iteration 10, Training loss = 0.29826880993262905
Iteration 20, Training loss = 0.2824747118981641
Iteration 30, Training loss = 0.27279142642136633
Iteration 40, Training loss = 0.26814047574600064
Iteration 50, Training loss = 0.26378723877971455
Iteration 60, Training loss = 0.26117544599580994
Iteration 70, Training loss = 0.2576876005234499
Iteration 80, Training loss = 0.2552020313448248
Iteration 90, Training loss = 0.25314615764283094
Model training time: 51.008163928985596
Device: cuda
Iteration 0, Training loss = 0.8737052005082008
Iteration 10, Training loss = 0.2959877135858986
Iteration 20, Training loss = 0.2793789012740946
Iteration 30, Training loss = 0.26895941492648157
Iteration 40, Training loss = 0.26201602109407973
Iteration 50, Training loss = 0.2572813974670867
Iteration 60, Training loss = 0.25170384722673866
Iteration 70, Training loss = 0.24687170450176513
Iteration 80, Training loss = 0.24394548702398744
Iteration 90, Training loss = 0.24066656975855838
Model training time: 51.36328125
Device: cuda
Iteration 0, Training loss = 0.8404105603694916
Iteration 10, Training loss = 0.2940811834525831
Iteration 20, Training loss = 0.28020201957182506
Iteration 30, Training loss = 0.2706540990575751
Iteration 40, Training loss = 0.2642123751865461
Iteration 50, Training loss = 0.25921409829693326
Iteration 60, Training loss = 0.25483557031677073
Iteration 70, Training loss = 0.2509937995895924
Iteration 80, Training loss = 0.24820058211936788
Iteration 90, Training loss = 0.2449207420102332
Model training time: 51.13280248641968
Device: cuda
Iteration 0, Training loss = 0.8541235912221395
Iteration 10, Training loss = 0.28941370922052834
Iteration 20, Training loss = 0.27091404072236785
Iteration 30, Training loss = 0.2627851334524501
Iteration 40, Training loss = 0.25791371817594577
Iteration 50, Training loss = 0.2540612836654769
Iteration 60, Training loss = 0.2500878983418532
Iteration 70, Training loss = 0.24652347580211792
Iteration 80, Training loss = 0.2432507426268252
Iteration 90, Training loss = 0.24155744354603653
Model training time: 52.026450395584106
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0044671980434121
Iteration 10, Training loss = 0.985745222603149
Iteration 20, Training loss = 0.9598551098861648
Iteration 30, Training loss = 0.9006030871995137
Iteration 40, Training loss = 0.7546609114792387
Iteration 50, Training loss = 0.5303186927955895
Iteration 60, Training loss = 0.4169123618712437
Iteration 70, Training loss = 0.38886364114630884
Iteration 80, Training loss = 0.3737022131681442
Iteration 90, Training loss = 0.3622711339997033
Model training time: 43.90791344642639
Device: cuda
Iteration 0, Training loss = 1.1153530941748446
Iteration 10, Training loss = 0.9797511889484257
Iteration 20, Training loss = 0.9527393254038785
Iteration 30, Training loss = 0.8776531370298049
Iteration 40, Training loss = 0.6843129067386322
Iteration 50, Training loss = 0.4584811494367752
Iteration 60, Training loss = 0.3867056890961333
Iteration 70, Training loss = 0.36743561010672454
Iteration 80, Training loss = 0.35576343861099596
Iteration 90, Training loss = 0.347010090063184
Model training time: 42.947839975357056
Device: cuda
Iteration 0, Training loss = 1.0012196018101227
Iteration 10, Training loss = 0.9926695211026051
Iteration 20, Training loss = 0.9794769937997869
Iteration 30, Training loss = 0.9517830143396271
Iteration 40, Training loss = 0.8788863134008921
Iteration 50, Training loss = 0.6966549547763483
Iteration 60, Training loss = 0.4634633288328642
Iteration 70, Training loss = 0.3821982554099173
Iteration 80, Training loss = 0.3640049109844261
Iteration 90, Training loss = 0.35340166586218963
Model training time: 43.6093065738678
Device: cuda
Iteration 0, Training loss = 1.0691664240550764
Iteration 10, Training loss = 0.9893707804760691
Iteration 20, Training loss = 0.9740628307148561
Iteration 30, Training loss = 0.9405466662336492
Iteration 40, Training loss = 0.8560008070370764
Iteration 50, Training loss = 0.6675740630516994
Iteration 60, Training loss = 0.46398828627843836
Iteration 70, Training loss = 0.39320050764459097
Iteration 80, Training loss = 0.3710272712294761
Iteration 90, Training loss = 0.3571058451479919
Model training time: 43.42106318473816
Device: cuda
Iteration 0, Training loss = 1.0304339111717216
Iteration 10, Training loss = 0.9977763206416124
Iteration 20, Training loss = 0.9904612290512852
Iteration 30, Training loss = 0.9781325556897078
Iteration 40, Training loss = 0.9510674121593448
Iteration 50, Training loss = 0.8812370930254892
Iteration 60, Training loss = 0.7101991420147206
Iteration 70, Training loss = 0.478063416719148
Iteration 80, Training loss = 0.38259099423885345
Iteration 90, Training loss = 0.36125223084237906
Model training time: 44.79810833930969
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9620800647550865
Iteration 10, Training loss = 0.29811853232633406
Iteration 20, Training loss = 0.28058842167294346
Iteration 30, Training loss = 0.2724860258238079
Iteration 40, Training loss = 0.2677366980159831
Iteration 50, Training loss = 0.26423162602555667
Iteration 60, Training loss = 0.26182884668827633
Iteration 70, Training loss = 0.2599895295233761
Iteration 80, Training loss = 0.25793813993437237
Iteration 90, Training loss = 0.2568860897753776
Model training time: 52.276320457458496
Device: cuda
Iteration 0, Training loss = 0.8944009625449885
Iteration 10, Training loss = 0.2975154050578505
Iteration 20, Training loss = 0.2804110030537656
Iteration 30, Training loss = 0.2681790945135941
Iteration 40, Training loss = 0.26233934395502323
Iteration 50, Training loss = 0.25944062387322686
Iteration 60, Training loss = 0.2571472358184057
Iteration 70, Training loss = 0.25566127484993556
Iteration 80, Training loss = 0.25395129633657004
Iteration 90, Training loss = 0.25285075433534226
Model training time: 52.23170733451843
Device: cuda
Iteration 0, Training loss = 0.8527229755178779
Iteration 10, Training loss = 0.2967199863230056
Iteration 20, Training loss = 0.28167209590246256
Iteration 30, Training loss = 0.2702591183435253
Iteration 40, Training loss = 0.26239242671693497
Iteration 50, Training loss = 0.25638426540838893
Iteration 60, Training loss = 0.2521128321458872
Iteration 70, Training loss = 0.24865960023990555
Iteration 80, Training loss = 0.24605877412433486
Iteration 90, Training loss = 0.2441157908438193
Model training time: 52.10687446594238
Device: cuda
Iteration 0, Training loss = 0.9492066786162212
Iteration 10, Training loss = 0.2962653158868485
Iteration 20, Training loss = 0.27841710079163673
Iteration 30, Training loss = 0.2698262068860179
Iteration 40, Training loss = 0.2644548254317579
Iteration 50, Training loss = 0.2606100068217906
Iteration 60, Training loss = 0.257459815871196
Iteration 70, Training loss = 0.2549305168176679
Iteration 80, Training loss = 0.2529213864708062
Iteration 90, Training loss = 0.25082729465483755
Model training time: 56.77170467376709
Device: cuda
Iteration 0, Training loss = 1.0564438702119177
Iteration 10, Training loss = 0.30054818261550065
Iteration 20, Training loss = 0.283048128948229
Iteration 30, Training loss = 0.2703101026304698
Iteration 40, Training loss = 0.26254467005879767
Iteration 50, Training loss = 0.25808797729794686
Iteration 60, Training loss = 0.2541157158213435
Iteration 70, Training loss = 0.25083031888699414
Iteration 80, Training loss = 0.24784127004065756
Iteration 90, Training loss = 0.2452483349739062
Model training time: 55.12943124771118
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9969909877378773
Iteration 10, Training loss = 0.9935269165269976
Iteration 20, Training loss = 0.9881260583389181
Iteration 30, Training loss = 0.9775933334671556
Iteration 40, Training loss = 0.9562338306309236
Iteration 50, Training loss = 0.9057323705341856
Iteration 60, Training loss = 0.7815549263942617
Iteration 70, Training loss = 0.5589040048041586
Iteration 80, Training loss = 0.40825750799646676
Iteration 90, Training loss = 0.370073705009629
Model training time: 45.05005192756653
Device: cuda
Iteration 0, Training loss = 0.9978547903729409
Iteration 10, Training loss = 0.9883677340304303
Iteration 20, Training loss = 0.9741933215184015
Iteration 30, Training loss = 0.9425453620153247
Iteration 40, Training loss = 0.8681844034148763
Iteration 50, Training loss = 0.7018219160037814
Iteration 60, Training loss = 0.4908540796569704
Iteration 70, Training loss = 0.3966140583987386
Iteration 80, Training loss = 0.3721169508795184
Iteration 90, Training loss = 0.3593191635810723
Model training time: 45.48220348358154
Device: cuda
Iteration 0, Training loss = 1.005928670522953
Iteration 10, Training loss = 0.9931427925175673
Iteration 20, Training loss = 0.9752174711833566
Iteration 30, Training loss = 0.9354807329668549
Iteration 40, Training loss = 0.8315217094473343
Iteration 50, Training loss = 0.6113467883255522
Iteration 60, Training loss = 0.4246976924361097
Iteration 70, Training loss = 0.37313017162514656
Iteration 80, Training loss = 0.357527249878313
Iteration 90, Training loss = 0.3473121115067392
Model training time: 43.74329495429993
Device: cuda
Iteration 0, Training loss = 1.000982581414553
Iteration 10, Training loss = 0.9913584913959226
Iteration 20, Training loss = 0.9849225066187307
Iteration 30, Training loss = 0.9744094683818032
Iteration 40, Training loss = 0.9525791990410618
Iteration 50, Training loss = 0.9037998770397454
Iteration 60, Training loss = 0.7939901984055452
Iteration 70, Training loss = 0.6023261231746858
Iteration 80, Training loss = 0.4473440078860622
Iteration 90, Training loss = 0.3944861433552195
Model training time: 43.19363045692444
Device: cuda
Iteration 0, Training loss = 1.023773878451987
Iteration 10, Training loss = 0.994374470257586
Iteration 20, Training loss = 0.9865680741773102
Iteration 30, Training loss = 0.9716553460021862
Iteration 40, Training loss = 0.9355698128300776
Iteration 50, Training loss = 0.8396564310024206
Iteration 60, Training loss = 0.6257799194886667
Iteration 70, Training loss = 0.422539574867588
Iteration 80, Training loss = 0.36435829517916385
Iteration 90, Training loss = 0.3490293774095344
Model training time: 43.64098000526428
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9514090318512397
Iteration 10, Training loss = 0.2953267079072195
Iteration 20, Training loss = 0.27840739481385623
Iteration 30, Training loss = 0.2690063569757898
Iteration 40, Training loss = 0.26496796012285545
Iteration 50, Training loss = 0.26188142123219466
Iteration 60, Training loss = 0.25869637558160047
Iteration 70, Training loss = 0.25555436031726025
Iteration 80, Training loss = 0.25239713407024633
Iteration 90, Training loss = 0.24976028841142214
Model training time: 51.287630558013916
Device: cuda
Iteration 0, Training loss = 0.9177123733929226
Iteration 10, Training loss = 0.2948850631425225
Iteration 20, Training loss = 0.2772384184901997
Iteration 30, Training loss = 0.2675744889182271
Iteration 40, Training loss = 0.2617262498482376
Iteration 50, Training loss = 0.2580496010169856
Iteration 60, Training loss = 0.255671625961403
Iteration 70, Training loss = 0.2536549726478557
Iteration 80, Training loss = 0.25197866165753424
Iteration 90, Training loss = 0.25028847433969414
Model training time: 51.07327103614807
Device: cuda
Iteration 0, Training loss = 0.8483636188016388
Iteration 10, Training loss = 0.2952313396096518
Iteration 20, Training loss = 0.2815314777213494
Iteration 30, Training loss = 0.2710883231702786
Iteration 40, Training loss = 0.26497369951435495
Iteration 50, Training loss = 0.2607628230465526
Iteration 60, Training loss = 0.25678510565229534
Iteration 70, Training loss = 0.25356923330566206
Iteration 80, Training loss = 0.25055268937248293
Iteration 90, Training loss = 0.24890483107607245
Model training time: 50.88141059875488
Device: cuda
Iteration 0, Training loss = 0.8776740705274207
Iteration 10, Training loss = 0.2927649178819564
Iteration 20, Training loss = 0.2782671645500181
Iteration 30, Training loss = 0.2700719324461484
Iteration 40, Training loss = 0.2641427841208893
Iteration 50, Training loss = 0.26035426481295443
Iteration 60, Training loss = 0.25702530825758674
Iteration 70, Training loss = 0.2552307065729656
Iteration 80, Training loss = 0.25354158972135177
Iteration 90, Training loss = 0.2519757782287228
Model training time: 52.392889738082886
Device: cuda
Iteration 0, Training loss = 0.8593396112936172
Iteration 10, Training loss = 0.29333691674196694
Iteration 20, Training loss = 0.27791934770692056
Iteration 30, Training loss = 0.2694983612791101
Iteration 40, Training loss = 0.2646470608564035
Iteration 50, Training loss = 0.26082317179326
Iteration 60, Training loss = 0.2581148089674137
Iteration 70, Training loss = 0.2558020817400035
Iteration 80, Training loss = 0.2538342629773565
Iteration 90, Training loss = 0.25254475951339084
Model training time: 51.167378187179565
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0804081816794509
Iteration 10, Training loss = 0.9895193412286606
Iteration 20, Training loss = 0.9784227941573099
Iteration 30, Training loss = 0.9539485764994171
Iteration 40, Training loss = 0.8934103451109972
Iteration 50, Training loss = 0.7458172694534135
Iteration 60, Training loss = 0.518679257816033
Iteration 70, Training loss = 0.4015230930096878
Iteration 80, Training loss = 0.3747646403442572
Iteration 90, Training loss = 0.3625334678240319
Model training time: 45.869457721710205
Device: cuda
Iteration 0, Training loss = 0.9968679863373247
Iteration 10, Training loss = 0.9750968787196762
Iteration 20, Training loss = 0.9401911680403979
Iteration 30, Training loss = 0.8488899746472264
Iteration 40, Training loss = 0.6423646018955379
Iteration 50, Training loss = 0.44261287371052954
Iteration 60, Training loss = 0.3864897914032382
Iteration 70, Training loss = 0.36908290132916294
Iteration 80, Training loss = 0.35742916251571066
Iteration 90, Training loss = 0.3483123312778681
Model training time: 43.41216158866882
Device: cuda
Iteration 0, Training loss = 1.0735844809552948
Iteration 10, Training loss = 0.994022106431587
Iteration 20, Training loss = 0.9831413939847784
Iteration 30, Training loss = 0.9595064205494112
Iteration 40, Training loss = 0.897590014172524
Iteration 50, Training loss = 0.7367687211631285
Iteration 60, Training loss = 0.49842858776341914
Iteration 70, Training loss = 0.3972544404554021
Iteration 80, Training loss = 0.373971785511145
Iteration 90, Training loss = 0.3610370062273582
Model training time: 43.97409725189209
Device: cuda
Iteration 0, Training loss = 1.0326763562515342
Iteration 10, Training loss = 0.9886284393346338
Iteration 20, Training loss = 0.9724086424917632
Iteration 30, Training loss = 0.9342736606159164
Iteration 40, Training loss = 0.8284305272033082
Iteration 50, Training loss = 0.5920307655383542
Iteration 60, Training loss = 0.4029953991024604
Iteration 70, Training loss = 0.36468272434308513
Iteration 80, Training loss = 0.35384396903812165
Iteration 90, Training loss = 0.3460748683453761
Model training time: 43.33760380744934
Device: cuda
Iteration 0, Training loss = 0.9981950088505595
Iteration 10, Training loss = 0.9803972492495114
Iteration 20, Training loss = 0.9535627924356853
Iteration 30, Training loss = 0.886301232813057
Iteration 40, Training loss = 0.7153603873299051
Iteration 50, Training loss = 0.48101145346574575
Iteration 60, Training loss = 0.38989451733253194
Iteration 70, Training loss = 0.36840987840518535
Iteration 80, Training loss = 0.355944129406107
Iteration 90, Training loss = 0.3465618886612807
Model training time: 44.42310333251953
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9051212663367644
Iteration 10, Training loss = 0.29509733666014154
Iteration 20, Training loss = 0.27944706824154886
Iteration 30, Training loss = 0.27037909313062203
Iteration 40, Training loss = 0.2638259005459977
Iteration 50, Training loss = 0.25826120971911753
Iteration 60, Training loss = 0.25416970087021373
Iteration 70, Training loss = 0.24957878950793863
Iteration 80, Training loss = 0.24623825509308614
Iteration 90, Training loss = 0.24338495543736233
Iteration 100, Training loss = 0.24075575613080732
Iteration 110, Training loss = 0.23866514148997914
Iteration 120, Training loss = 0.23668320217230707
Iteration 130, Training loss = 0.23453203429970845
Iteration 140, Training loss = 0.23346643162480854
Iteration 150, Training loss = 0.23180582800081798
Iteration 160, Training loss = 0.23009889432145667
Iteration 170, Training loss = 0.22899612391110483
Iteration 180, Training loss = 0.22786483985820635
Iteration 190, Training loss = 0.22691301003856174
Model training time: 103.18322157859802
Device: cuda
Iteration 0, Training loss = 0.9073455396489427
Iteration 10, Training loss = 0.2967452580609853
Iteration 20, Training loss = 0.28152491787299694
Iteration 30, Training loss = 0.269964198042925
Iteration 40, Training loss = 0.263633005339066
Iteration 50, Training loss = 0.2593080336746667
Iteration 60, Training loss = 0.2559259338326951
Iteration 70, Training loss = 0.2533271628235789
Iteration 80, Training loss = 0.250766405253953
Iteration 90, Training loss = 0.2482086437377745
Iteration 100, Training loss = 0.24550446404311038
Iteration 110, Training loss = 0.24328985110322154
Iteration 120, Training loss = 0.24164117301744353
Iteration 130, Training loss = 0.23962446078477703
Iteration 140, Training loss = 0.23773701602624633
Iteration 150, Training loss = 0.23601855218554812
Iteration 160, Training loss = 0.23452537790049074
Iteration 170, Training loss = 0.2331673829719316
Iteration 180, Training loss = 0.2312235326819645
Iteration 190, Training loss = 0.2296297241083502
Model training time: 103.18504095077515
Device: cuda
Iteration 0, Training loss = 0.9224188905074002
Iteration 10, Training loss = 0.2929923759173539
Iteration 20, Training loss = 0.2774080533994312
Iteration 30, Training loss = 0.266918199601387
Iteration 40, Training loss = 0.26122584388778514
Iteration 50, Training loss = 0.2564100167436692
Iteration 60, Training loss = 0.2537112327847296
Iteration 70, Training loss = 0.2515714880953978
Iteration 80, Training loss = 0.24946325931689065
Iteration 90, Training loss = 0.24818777570785103
Iteration 100, Training loss = 0.24608256983554969
Iteration 110, Training loss = 0.24470467700399728
Iteration 120, Training loss = 0.24326345605942584
Iteration 130, Training loss = 0.2422915888503735
Iteration 140, Training loss = 0.24066075879277676
Iteration 150, Training loss = 0.2391807518304116
Iteration 160, Training loss = 0.23713700016892852
Iteration 170, Training loss = 0.23598449725937323
Iteration 180, Training loss = 0.23393422614142623
Iteration 190, Training loss = 0.23197324221308815
Model training time: 105.20172238349915
Device: cuda
Iteration 0, Training loss = 0.8974067222408174
Iteration 10, Training loss = 0.2920688629583354
Iteration 20, Training loss = 0.27881406732895764
Iteration 30, Training loss = 0.2706218822780302
Iteration 40, Training loss = 0.2652062722831315
Iteration 50, Training loss = 0.26071546614603036
Iteration 60, Training loss = 0.2573054340503406
Iteration 70, Training loss = 0.25438104808041895
Iteration 80, Training loss = 0.2515683265596845
Iteration 90, Training loss = 0.25005055635662404
Iteration 100, Training loss = 0.24815231378553276
Iteration 110, Training loss = 0.24565307466768757
Iteration 120, Training loss = 0.24423768948628308
Iteration 130, Training loss = 0.24176685356018618
Iteration 140, Training loss = 0.23972594038950906
Iteration 150, Training loss = 0.23761381946738638
Iteration 160, Training loss = 0.23549605752668426
Iteration 170, Training loss = 0.23309209913953452
Iteration 180, Training loss = 0.23120761298310671
Iteration 190, Training loss = 0.2297852412948597
Model training time: 102.36470079421997
Device: cuda
Iteration 0, Training loss = 0.8923417039702649
Iteration 10, Training loss = 0.293107565790054
Iteration 20, Training loss = 0.2762114489879504
Iteration 30, Training loss = 0.2675279072930103
Iteration 40, Training loss = 0.2609339507598854
Iteration 50, Training loss = 0.2572458286298389
Iteration 60, Training loss = 0.254380631154588
Iteration 70, Training loss = 0.2518281705912031
Iteration 80, Training loss = 0.24980405075858927
Iteration 90, Training loss = 0.2480994135538256
Iteration 100, Training loss = 0.24647559395136615
Iteration 110, Training loss = 0.24522586765791543
Iteration 120, Training loss = 0.24356195753661253
Iteration 130, Training loss = 0.24210745621174934
Iteration 140, Training loss = 0.24016670395617912
Iteration 150, Training loss = 0.23818715788045172
Iteration 160, Training loss = 0.23693044857994697
Iteration 170, Training loss = 0.23475662167871836
Iteration 180, Training loss = 0.23242144790854638
Iteration 190, Training loss = 0.23075174006292953
Model training time: 104.8036961555481
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0030462137146088
Iteration 10, Training loss = 0.9928616434696511
Iteration 20, Training loss = 0.9833251864944763
Iteration 30, Training loss = 0.9637917294629261
Iteration 40, Training loss = 0.9158863941924624
Iteration 50, Training loss = 0.7898013843606806
Iteration 60, Training loss = 0.5554805678692049
Iteration 70, Training loss = 0.4049202192926522
Iteration 80, Training loss = 0.37108562215621477
Iteration 90, Training loss = 0.3570532366525174
Iteration 100, Training loss = 0.34720902462415487
Iteration 110, Training loss = 0.34003656831982637
Iteration 120, Training loss = 0.3341315293607931
Iteration 130, Training loss = 0.32986213571727996
Iteration 140, Training loss = 0.326669728640494
Iteration 150, Training loss = 0.3241578736150813
Iteration 160, Training loss = 0.32200563845132224
Iteration 170, Training loss = 0.3203240858332297
Iteration 180, Training loss = 0.3190300972099454
Iteration 190, Training loss = 0.3177778365104019
Model training time: 86.696524143219
Device: cuda
Iteration 0, Training loss = 1.0816370107359805
Iteration 10, Training loss = 0.9904053108744125
Iteration 20, Training loss = 0.9784270629611489
Iteration 30, Training loss = 0.9532768650580261
Iteration 40, Training loss = 0.8894709368883553
Iteration 50, Training loss = 0.7279532968853634
Iteration 60, Training loss = 0.4960676877247508
Iteration 70, Training loss = 0.4022511897234305
Iteration 80, Training loss = 0.3799370531597091
Iteration 90, Training loss = 0.366309947092943
Iteration 100, Training loss = 0.3566827369948565
Iteration 110, Training loss = 0.3490459675614251
Iteration 120, Training loss = 0.34346027529773643
Iteration 130, Training loss = 0.33918240094516816
Iteration 140, Training loss = 0.33563917611903776
Iteration 150, Training loss = 0.33295960401435165
Iteration 160, Training loss = 0.3304631743065959
Iteration 170, Training loss = 0.3285172987756371
Iteration 180, Training loss = 0.32705392989783255
Iteration 190, Training loss = 0.3251464720860521
Model training time: 87.87911915779114
Device: cuda
Iteration 0, Training loss = 1.0344106643742568
Iteration 10, Training loss = 0.9873038199132638
Iteration 20, Training loss = 0.966689497954043
Iteration 30, Training loss = 0.9126133213753274
Iteration 40, Training loss = 0.7602999861967765
Iteration 50, Training loss = 0.4966228685136569
Iteration 60, Training loss = 0.3798016626883939
Iteration 70, Training loss = 0.35928726232369357
Iteration 80, Training loss = 0.3496564791761068
Iteration 90, Training loss = 0.3422293527362999
Iteration 100, Training loss = 0.33668691567517367
Iteration 110, Training loss = 0.3322963277667256
Iteration 120, Training loss = 0.3289190094205427
Iteration 130, Training loss = 0.32592522423290454
Iteration 140, Training loss = 0.3237181291994402
Iteration 150, Training loss = 0.32169011633924366
Iteration 160, Training loss = 0.3198476981092019
Iteration 170, Training loss = 0.31841222570248434
Iteration 180, Training loss = 0.3170143246434214
Iteration 190, Training loss = 0.315785451731295
Model training time: 85.80838298797607
Device: cuda
Iteration 0, Training loss = 1.115633049109369
Iteration 10, Training loss = 0.989519258027504
Iteration 20, Training loss = 0.9765722923359629
Iteration 30, Training loss = 0.9467196787096398
Iteration 40, Training loss = 0.8681129414146229
Iteration 50, Training loss = 0.6745992442164525
Iteration 60, Training loss = 0.4528877522625011
Iteration 70, Training loss = 0.38167563813217614
Iteration 80, Training loss = 0.3635008543296819
Iteration 90, Training loss = 0.35254981919004613
Iteration 100, Training loss = 0.3446086580542618
Iteration 110, Training loss = 0.3385013533309643
Iteration 120, Training loss = 0.3337518065518386
Iteration 130, Training loss = 0.3300617624426003
Iteration 140, Training loss = 0.32720258543335495
Iteration 150, Training loss = 0.32467957250503304
Iteration 160, Training loss = 0.32280998770824065
Iteration 170, Training loss = 0.3210423483578691
Iteration 180, Training loss = 0.3194389812125132
Iteration 190, Training loss = 0.31783850108595796
Model training time: 86.48774456977844
Device: cuda
Iteration 0, Training loss = 1.002506615535399
Iteration 10, Training loss = 0.9901892031220489
Iteration 20, Training loss = 0.9779025132950513
Iteration 30, Training loss = 0.9511994406328363
Iteration 40, Training loss = 0.8829087560892682
Iteration 50, Training loss = 0.7159639122122425
Iteration 60, Training loss = 0.4861777148076466
Iteration 70, Training loss = 0.39267184266022276
Iteration 80, Training loss = 0.37106418997482293
Iteration 90, Training loss = 0.3582516648506714
Iteration 100, Training loss = 0.34870562949567385
Iteration 110, Training loss = 0.34108984840190437
Iteration 120, Training loss = 0.3352866742317959
Iteration 130, Training loss = 0.3306209081310337
Iteration 140, Training loss = 0.32679541452312005
Iteration 150, Training loss = 0.323825026828498
Iteration 160, Training loss = 0.321245879277623
Iteration 170, Training loss = 0.319012684535605
Iteration 180, Training loss = 0.3172013196940861
Iteration 190, Training loss = 0.3155038843684566
Model training time: 88.16727590560913
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8894284335810682
Iteration 10, Training loss = 0.300365023332993
Iteration 20, Training loss = 0.28661056099229515
Iteration 30, Training loss = 0.27670465763342583
Iteration 40, Training loss = 0.26978342303640906
Iteration 50, Training loss = 0.2652382906751829
Iteration 60, Training loss = 0.2619316907144055
Iteration 70, Training loss = 0.2585179694628311
Iteration 80, Training loss = 0.2560199654997117
Iteration 90, Training loss = 0.25412320300539815
Iteration 100, Training loss = 0.2529226290580724
Iteration 110, Training loss = 0.25137345880702966
Iteration 120, Training loss = 0.24971097209840362
Iteration 130, Training loss = 0.2483482840166542
Iteration 140, Training loss = 0.24712303930275664
Iteration 150, Training loss = 0.24581637778308144
Iteration 160, Training loss = 0.24430911499131966
Iteration 170, Training loss = 0.2424170250032485
Iteration 180, Training loss = 0.24151244853368395
Iteration 190, Training loss = 0.24019980594768361
Model training time: 104.19001603126526
Device: cuda
Iteration 0, Training loss = 0.9270651304404326
Iteration 10, Training loss = 0.2946300898964988
Iteration 20, Training loss = 0.27465611422033054
Iteration 30, Training loss = 0.2660750982692109
Iteration 40, Training loss = 0.262628024935578
Iteration 50, Training loss = 0.2594682076724909
Iteration 60, Training loss = 0.2577443705560509
Iteration 70, Training loss = 0.2555572822437448
Iteration 80, Training loss = 0.2535273361833852
Iteration 90, Training loss = 0.2514534143609227
Iteration 100, Training loss = 0.24936635624482037
Iteration 110, Training loss = 0.24731555949111828
Iteration 120, Training loss = 0.24500140979813895
Iteration 130, Training loss = 0.24348545309886804
Iteration 140, Training loss = 0.2417838005100988
Iteration 150, Training loss = 0.24026884237804944
Iteration 160, Training loss = 0.239284409106355
Iteration 170, Training loss = 0.2377877264727836
Iteration 180, Training loss = 0.23682007519190018
Iteration 190, Training loss = 0.2358004753421496
Model training time: 113.70977425575256
Device: cuda
Iteration 0, Training loss = 0.9361063307624752
Iteration 10, Training loss = 0.29630681143545934
Iteration 20, Training loss = 0.27695346933949655
Iteration 30, Training loss = 0.2681945632178039
Iteration 40, Training loss = 0.2613382250359214
Iteration 50, Training loss = 0.2568289292457606
Iteration 60, Training loss = 0.25405284805678857
Iteration 70, Training loss = 0.25104171649234924
Iteration 80, Training loss = 0.24912365280614926
Iteration 90, Training loss = 0.2474268715079991
Iteration 100, Training loss = 0.24590673881242409
Iteration 110, Training loss = 0.24429516292224496
Iteration 120, Training loss = 0.24313921963042842
Iteration 130, Training loss = 0.2420363060173919
Iteration 140, Training loss = 0.2409695517136456
Iteration 150, Training loss = 0.2402982599492079
Iteration 160, Training loss = 0.23884603790668252
Iteration 170, Training loss = 0.23782487307420366
Iteration 180, Training loss = 0.23696277458356022
Iteration 190, Training loss = 0.23585790865682515
Model training time: 104.18208026885986
Device: cuda
Iteration 0, Training loss = 0.8959123802531429
Iteration 10, Training loss = 0.2943777358452575
Iteration 20, Training loss = 0.27842206163354416
Iteration 30, Training loss = 0.26753819825862857
Iteration 40, Training loss = 0.25902360172595007
Iteration 50, Training loss = 0.25307496588397543
Iteration 60, Training loss = 0.24856665705220174
Iteration 70, Training loss = 0.24545368538677836
Iteration 80, Training loss = 0.24226175234190778
Iteration 90, Training loss = 0.23997560204807264
Iteration 100, Training loss = 0.23827906059367315
Iteration 110, Training loss = 0.23652075319472005
Iteration 120, Training loss = 0.23452185704113496
Iteration 130, Training loss = 0.2330078698929228
Iteration 140, Training loss = 0.23153588638503383
Iteration 150, Training loss = 0.23037863867334823
Iteration 160, Training loss = 0.2292614982037221
Iteration 170, Training loss = 0.22802915875112173
Iteration 180, Training loss = 0.22730862141810088
Iteration 190, Training loss = 0.2260953937600802
Model training time: 105.07810544967651
Device: cuda
Iteration 0, Training loss = 0.9647379191389384
Iteration 10, Training loss = 0.29629978674664625
Iteration 20, Training loss = 0.2767677616517423
Iteration 30, Training loss = 0.2681560124345034
Iteration 40, Training loss = 0.26374799769019963
Iteration 50, Training loss = 0.26005087412226285
Iteration 60, Training loss = 0.2581430841192206
Iteration 70, Training loss = 0.2553699091798457
Iteration 80, Training loss = 0.25329448008508426
Iteration 90, Training loss = 0.25211182352128386
Iteration 100, Training loss = 0.2502840359223957
Iteration 110, Training loss = 0.24958485308989486
Iteration 120, Training loss = 0.24762010803450685
Iteration 130, Training loss = 0.24663488509146989
Iteration 140, Training loss = 0.24525330040703097
Iteration 150, Training loss = 0.24400944002964883
Iteration 160, Training loss = 0.24248440925383682
Iteration 170, Training loss = 0.24079223604958513
Iteration 180, Training loss = 0.2393619638955622
Iteration 190, Training loss = 0.23837412588893644
Model training time: 104.19458818435669
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0368470280424447
Iteration 10, Training loss = 0.9893235455269387
Iteration 20, Training loss = 0.9779687709294567
Iteration 30, Training loss = 0.9503029905999255
Iteration 40, Training loss = 0.8766616384428869
Iteration 50, Training loss = 0.6969085215223326
Iteration 60, Training loss = 0.4774905202896774
Iteration 70, Training loss = 0.39224124030541563
Iteration 80, Training loss = 0.36848138283586385
Iteration 90, Training loss = 0.354868324811753
Iteration 100, Training loss = 0.3446872448841827
Iteration 110, Training loss = 0.33686910441582774
Iteration 120, Training loss = 0.33085727680826305
Iteration 130, Training loss = 0.32636463553723644
Iteration 140, Training loss = 0.3227234112581387
Iteration 150, Training loss = 0.3199019253831221
Iteration 160, Training loss = 0.31745394247135295
Iteration 170, Training loss = 0.3154456916538047
Iteration 180, Training loss = 0.31373357720510725
Iteration 190, Training loss = 0.3123097806158712
Model training time: 89.22570538520813
Device: cuda
Iteration 0, Training loss = 1.0183775793697873
Iteration 10, Training loss = 0.9968947350401567
Iteration 20, Training loss = 0.9871713435390094
Iteration 30, Training loss = 0.969058400349236
Iteration 40, Training loss = 0.925776725819844
Iteration 50, Training loss = 0.8098241452299076
Iteration 60, Training loss = 0.582773668822307
Iteration 70, Training loss = 0.4236664557355945
Iteration 80, Training loss = 0.38426668231192856
Iteration 90, Training loss = 0.3686670084770309
Iteration 100, Training loss = 0.357783087644248
Iteration 110, Training loss = 0.34911100173111975
Iteration 120, Training loss = 0.3426273961982196
Iteration 130, Training loss = 0.33761852930951636
Iteration 140, Training loss = 0.3336040703602334
Iteration 150, Training loss = 0.3302545960063507
Iteration 160, Training loss = 0.3276119597310304
Iteration 170, Training loss = 0.32522739518713434
Iteration 180, Training loss = 0.3234135355015644
Iteration 190, Training loss = 0.3215969852363515
Model training time: 88.77577710151672
Device: cuda
Iteration 0, Training loss = 1.0098600450352953
Iteration 10, Training loss = 0.9964345648127088
Iteration 20, Training loss = 0.9917524256371413
Iteration 30, Training loss = 0.9843319181091271
Iteration 40, Training loss = 0.9684876241060493
Iteration 50, Training loss = 0.9318500589804846
Iteration 60, Training loss = 0.8352052948954031
Iteration 70, Training loss = 0.6281109643473175
Iteration 80, Training loss = 0.43928390351681096
Iteration 90, Training loss = 0.37949036038311573
Iteration 100, Training loss = 0.36161263940120725
Iteration 110, Training loss = 0.3509417992382881
Iteration 120, Training loss = 0.3426462048551938
Iteration 130, Training loss = 0.33635812543206295
Iteration 140, Training loss = 0.3314310407523093
Iteration 150, Training loss = 0.32759787582456346
Iteration 160, Training loss = 0.32435344249803855
Iteration 170, Training loss = 0.3217255579754169
Iteration 180, Training loss = 0.31978583960065543
Iteration 190, Training loss = 0.31780298344664654
Model training time: 88.8028666973114
Device: cuda
Iteration 0, Training loss = 1.0325150775563052
Iteration 10, Training loss = 0.9891664032208718
Iteration 20, Training loss = 0.9745757350332801
Iteration 30, Training loss = 0.9402421356402066
Iteration 40, Training loss = 0.8504030340808933
Iteration 50, Training loss = 0.6564580155559082
Iteration 60, Training loss = 0.45999315965550863
Iteration 70, Training loss = 0.3853677807795222
Iteration 80, Training loss = 0.3630667497329504
Iteration 90, Training loss = 0.35087225284905466
Iteration 100, Training loss = 0.3418349061549143
Iteration 110, Training loss = 0.3349433166964868
Iteration 120, Training loss = 0.3294375553799311
Iteration 130, Training loss = 0.32522801275547714
Iteration 140, Training loss = 0.32166534954497084
Iteration 150, Training loss = 0.3187238840661384
Iteration 160, Training loss = 0.3164091131877668
Iteration 170, Training loss = 0.3141205610287969
Iteration 180, Training loss = 0.3124586935842874
Iteration 190, Training loss = 0.3107653454523398
Model training time: 88.68794441223145
Device: cuda
Iteration 0, Training loss = 1.0111139758014218
Iteration 10, Training loss = 0.9973051335852025
Iteration 20, Training loss = 0.9877746445070457
Iteration 30, Training loss = 0.9692610390394132
Iteration 40, Training loss = 0.9236115707900853
Iteration 50, Training loss = 0.8005492775982863
Iteration 60, Training loss = 0.5715138576654198
Iteration 70, Training loss = 0.4265266884542262
Iteration 80, Training loss = 0.3865581616290256
Iteration 90, Training loss = 0.3679800891919517
Iteration 100, Training loss = 0.3544547871368561
Iteration 110, Training loss = 0.3440366592664118
Iteration 120, Training loss = 0.33594396618391065
Iteration 130, Training loss = 0.3299653768972392
Iteration 140, Training loss = 0.32517799082015963
Iteration 150, Training loss = 0.3217184357435305
Iteration 160, Training loss = 0.31865711501161353
Iteration 170, Training loss = 0.3164517850621849
Iteration 180, Training loss = 0.31446311525441256
Iteration 190, Training loss = 0.31297636852931165
Model training time: 88.94982552528381
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8610602699526863
Iteration 10, Training loss = 0.29761345523971044
Iteration 20, Training loss = 0.2813785048720334
Iteration 30, Training loss = 0.2702815631363929
Iteration 40, Training loss = 0.2633564481759764
Iteration 50, Training loss = 0.2577639345539684
Iteration 60, Training loss = 0.2531726409029441
Iteration 70, Training loss = 0.2490327485428884
Iteration 80, Training loss = 0.2463594985535012
Iteration 90, Training loss = 0.2435318283177461
Iteration 100, Training loss = 0.24233913836554233
Iteration 110, Training loss = 0.24025077841833198
Iteration 120, Training loss = 0.23864741176006002
Iteration 130, Training loss = 0.23739262156134366
Iteration 140, Training loss = 0.23586228031656067
Iteration 150, Training loss = 0.2344879905082123
Iteration 160, Training loss = 0.23303748849445624
Iteration 170, Training loss = 0.23174283626437475
Iteration 180, Training loss = 0.23039107435984993
Iteration 190, Training loss = 0.22941118222604942
Model training time: 105.83721256256104
Device: cuda
Iteration 0, Training loss = 0.8506879550348472
Iteration 10, Training loss = 0.2967472496571033
Iteration 20, Training loss = 0.2781534884440697
Iteration 30, Training loss = 0.26826780063007705
Iteration 40, Training loss = 0.2633453059679948
Iteration 50, Training loss = 0.26003242017065353
Iteration 60, Training loss = 0.25778460431365935
Iteration 70, Training loss = 0.2554505715466873
Iteration 80, Training loss = 0.2544142133567581
Iteration 90, Training loss = 0.25270743837656756
Iteration 100, Training loss = 0.2512642250330916
Iteration 110, Training loss = 0.25093388824428253
Iteration 120, Training loss = 0.24918476089871247
Iteration 130, Training loss = 0.24827974284243642
Iteration 140, Training loss = 0.24723056462969964
Iteration 150, Training loss = 0.24623049599892002
Iteration 160, Training loss = 0.24505836002474546
Iteration 170, Training loss = 0.24339168758715612
Iteration 180, Training loss = 0.24221649814676718
Iteration 190, Training loss = 0.24144370558042502
Model training time: 103.94724464416504
Device: cuda
Iteration 0, Training loss = 0.9151036208247446
Iteration 10, Training loss = 0.29683692522401095
Iteration 20, Training loss = 0.28230774555959653
Iteration 30, Training loss = 0.2710430687406161
Iteration 40, Training loss = 0.26348018552431474
Iteration 50, Training loss = 0.258406135346064
Iteration 60, Training loss = 0.25531339773470785
Iteration 70, Training loss = 0.2528717954965538
Iteration 80, Training loss = 0.25085351823605867
Iteration 90, Training loss = 0.24881983319292922
Iteration 100, Training loss = 0.24686432711004344
Iteration 110, Training loss = 0.2446167567343458
Iteration 120, Training loss = 0.24307324832923188
Iteration 130, Training loss = 0.2406545614701928
Iteration 140, Training loss = 0.23854127789669408
Iteration 150, Training loss = 0.23654296099418012
Iteration 160, Training loss = 0.23429659671053182
Iteration 170, Training loss = 0.2328014982634249
Iteration 180, Training loss = 0.23131994707352024
Iteration 190, Training loss = 0.2296773191478293
Model training time: 102.50529909133911
Device: cuda
Iteration 0, Training loss = 0.8965235930522475
Iteration 10, Training loss = 0.2957276460189219
Iteration 20, Training loss = 0.2837323258488865
Iteration 30, Training loss = 0.2732978755603691
Iteration 40, Training loss = 0.2657819856634729
Iteration 50, Training loss = 0.2612367255238875
Iteration 60, Training loss = 0.25779688710089743
Iteration 70, Training loss = 0.25506290244998425
Iteration 80, Training loss = 0.25238536026770786
Iteration 90, Training loss = 0.2503595406473693
Iteration 100, Training loss = 0.2479636569396924
Iteration 110, Training loss = 0.24556180788767829
Iteration 120, Training loss = 0.24351224056095536
Iteration 130, Training loss = 0.2412477237585093
Iteration 140, Training loss = 0.2398417308998743
Iteration 150, Training loss = 0.2381517777611788
Iteration 160, Training loss = 0.23678995252160703
Iteration 170, Training loss = 0.23534017579390987
Iteration 180, Training loss = 0.2345174100547668
Iteration 190, Training loss = 0.23341704551446235
Model training time: 102.04117202758789
Device: cuda
Iteration 0, Training loss = 0.8577844770422282
Iteration 10, Training loss = 0.29582716872775816
Iteration 20, Training loss = 0.2787617617455868
Iteration 30, Training loss = 0.2655257074563757
Iteration 40, Training loss = 0.258592718919022
Iteration 50, Training loss = 0.25431946414290557
Iteration 60, Training loss = 0.2508390714340002
Iteration 70, Training loss = 0.24773147124932407
Iteration 80, Training loss = 0.24564128964417783
Iteration 90, Training loss = 0.2432298654325072
Iteration 100, Training loss = 0.2417381052784712
Iteration 110, Training loss = 0.24083324976391712
Iteration 120, Training loss = 0.23898609076371782
Iteration 130, Training loss = 0.23754634765894592
Iteration 140, Training loss = 0.23698176741239232
Iteration 150, Training loss = 0.23531246890672472
Iteration 160, Training loss = 0.23425115053214982
Iteration 170, Training loss = 0.23320510822909796
Iteration 180, Training loss = 0.23181874960769175
Iteration 190, Training loss = 0.23117344148150368
Model training time: 104.38022804260254
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.024689179572297
Iteration 10, Training loss = 1.0008570010667852
Iteration 20, Training loss = 0.9948515297425573
Iteration 30, Training loss = 0.985089631909031
Iteration 40, Training loss = 0.9649481852031505
Iteration 50, Training loss = 0.9145244854270113
Iteration 60, Training loss = 0.78180546160183
Iteration 70, Training loss = 0.5384486783070368
Iteration 80, Training loss = 0.3969231888687928
Iteration 90, Training loss = 0.3698205075093678
Iteration 100, Training loss = 0.3592185233933874
Iteration 110, Training loss = 0.3508194714026936
Iteration 120, Training loss = 0.34395371442412637
Iteration 130, Training loss = 0.33851528497859296
Iteration 140, Training loss = 0.3340190275781957
Iteration 150, Training loss = 0.3303235515446986
Iteration 160, Training loss = 0.32716984538131416
Iteration 170, Training loss = 0.3247376228829273
Iteration 180, Training loss = 0.32260426853672935
Iteration 190, Training loss = 0.32067640170851863
Model training time: 86.0143129825592
Device: cuda
Iteration 0, Training loss = 1.052955887580322
Iteration 10, Training loss = 0.9894825189800586
Iteration 20, Training loss = 0.9756446384055851
Iteration 30, Training loss = 0.9461041062565173
Iteration 40, Training loss = 0.8725487620865173
Iteration 50, Training loss = 0.7053883964732542
Iteration 60, Training loss = 0.4892515364051154
Iteration 70, Training loss = 0.39704954809989534
Iteration 80, Training loss = 0.37343562616274373
Iteration 90, Training loss = 0.3604164905395115
Iteration 100, Training loss = 0.3507787800116631
Iteration 110, Training loss = 0.34342455844108477
Iteration 120, Training loss = 0.3376974142348218
Iteration 130, Training loss = 0.3331598382235728
Iteration 140, Training loss = 0.3296208977338477
Iteration 150, Training loss = 0.3268538415071173
Iteration 160, Training loss = 0.3242035542612503
Iteration 170, Training loss = 0.32227434304522545
Iteration 180, Training loss = 0.32049416880558534
Iteration 190, Training loss = 0.3189659665629592
Model training time: 85.07901930809021
Device: cuda
Iteration 0, Training loss = 1.0149372383122295
Iteration 10, Training loss = 0.9928528966684318
Iteration 20, Training loss = 0.984603582874626
Iteration 30, Training loss = 0.9686428265190586
Iteration 40, Training loss = 0.9345947754440815
Iteration 50, Training loss = 0.8585820187234994
Iteration 60, Training loss = 0.7109958303609714
Iteration 70, Training loss = 0.5258163594016151
Iteration 80, Training loss = 0.41897099232413865
Iteration 90, Training loss = 0.3851964995084606
Iteration 100, Training loss = 0.3697796867163649
Iteration 110, Training loss = 0.3586852902109051
Iteration 120, Training loss = 0.34986335502770566
Iteration 130, Training loss = 0.3426404029816461
Iteration 140, Training loss = 0.3368349972634281
Iteration 150, Training loss = 0.3319583583000786
Iteration 160, Training loss = 0.3280962597912507
Iteration 170, Training loss = 0.324691812726545
Iteration 180, Training loss = 0.3219182529006397
Iteration 190, Training loss = 0.31953527404956034
Model training time: 87.9146728515625
Device: cuda
Iteration 0, Training loss = 1.032304083463932
Iteration 10, Training loss = 1.0042317874062148
Iteration 20, Training loss = 0.9980418594062472
Iteration 30, Training loss = 0.990534693147022
Iteration 40, Training loss = 0.9775457511369599
Iteration 50, Training loss = 0.9468273815606466
Iteration 60, Training loss = 0.8636904528441209
Iteration 70, Training loss = 0.6621658313505298
Iteration 80, Training loss = 0.4434033000512504
Iteration 90, Training loss = 0.37613491787965303
Iteration 100, Training loss = 0.35975769191042273
Iteration 110, Training loss = 0.34943972331343204
Iteration 120, Training loss = 0.341893554022468
Iteration 130, Training loss = 0.3359337922627643
Iteration 140, Training loss = 0.3314724458656646
Iteration 150, Training loss = 0.32784458155998597
Iteration 160, Training loss = 0.3247900563528982
Iteration 170, Training loss = 0.322278444580824
Iteration 180, Training loss = 0.32027916393759176
Iteration 190, Training loss = 0.31850251357001197
Model training time: 87.83998966217041
Device: cuda
Iteration 0, Training loss = 0.9908730993115007
Iteration 10, Training loss = 0.9719172533574463
Iteration 20, Training loss = 0.9338065535912502
Iteration 30, Training loss = 0.8465302149262325
Iteration 40, Training loss = 0.6636834304788788
Iteration 50, Training loss = 0.4560435194946086
Iteration 60, Training loss = 0.3797732137017331
Iteration 70, Training loss = 0.36067475585975023
Iteration 80, Training loss = 0.3497084092899038
Iteration 90, Training loss = 0.34140390449949964
Iteration 100, Training loss = 0.33510684770403415
Iteration 110, Training loss = 0.3302417986149072
Iteration 120, Training loss = 0.32624350542306324
Iteration 130, Training loss = 0.32319401843207224
Iteration 140, Training loss = 0.32074625698114423
Iteration 150, Training loss = 0.31856061029304317
Iteration 160, Training loss = 0.31683416743977016
Iteration 170, Training loss = 0.3154227769692354
Iteration 180, Training loss = 0.31415699276234277
Iteration 190, Training loss = 0.31302602628818726
Model training time: 89.37139773368835
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8886469753400466
Iteration 10, Training loss = 0.2980537302611671
Iteration 20, Training loss = 0.2805364078528656
Iteration 30, Training loss = 0.2683452865510241
Iteration 40, Training loss = 0.26116409524373224
Iteration 50, Training loss = 0.2558979531862834
Iteration 60, Training loss = 0.251590686509742
Iteration 70, Training loss = 0.24871789501668753
Iteration 80, Training loss = 0.2465910213586782
Iteration 90, Training loss = 0.24531322498478844
Iteration 100, Training loss = 0.24322877827914519
Iteration 110, Training loss = 0.2410357347340041
Iteration 120, Training loss = 0.23957723432353564
Iteration 130, Training loss = 0.23776742503876835
Iteration 140, Training loss = 0.2363913714452748
Iteration 150, Training loss = 0.23434826789410293
Iteration 160, Training loss = 0.2322998225021449
Iteration 170, Training loss = 0.2304909917149359
Iteration 180, Training loss = 0.22872244196855993
Iteration 190, Training loss = 0.22707779732584665
Iteration 200, Training loss = 0.22528355626375854
Iteration 210, Training loss = 0.22369629380994502
Iteration 220, Training loss = 0.22204410739369312
Iteration 230, Training loss = 0.22100872665499371
Iteration 240, Training loss = 0.21981094936337367
Iteration 250, Training loss = 0.21847969948451687
Iteration 260, Training loss = 0.21759435706760635
Iteration 270, Training loss = 0.2161434918245161
Iteration 280, Training loss = 0.21553866960306722
Iteration 290, Training loss = 0.21434570451156568
Model training time: 161.15766882896423
Device: cuda
Iteration 0, Training loss = 0.8410587126420717
Iteration 10, Training loss = 0.29525526490324056
Iteration 20, Training loss = 0.2746090245936021
Iteration 30, Training loss = 0.26442877715566254
Iteration 40, Training loss = 0.25903866562730754
Iteration 50, Training loss = 0.2555165681762499
Iteration 60, Training loss = 0.2530752233168692
Iteration 70, Training loss = 0.25071320904657857
Iteration 80, Training loss = 0.24823311566584913
Iteration 90, Training loss = 0.2461637378496639
Iteration 100, Training loss = 0.24345424105627486
Iteration 110, Training loss = 0.24042260773101096
Iteration 120, Training loss = 0.2387159051008288
Iteration 130, Training loss = 0.23676428221364576
Iteration 140, Training loss = 0.23468560487320578
Iteration 150, Training loss = 0.23321750642817476
Iteration 160, Training loss = 0.23132867181164013
Iteration 170, Training loss = 0.23018793598466866
Iteration 180, Training loss = 0.22901760074185978
Iteration 190, Training loss = 0.22726344643724455
Iteration 200, Training loss = 0.22609925053598806
Iteration 210, Training loss = 0.2250028443827179
Iteration 220, Training loss = 0.223487904507297
Iteration 230, Training loss = 0.2226016714212825
Iteration 240, Training loss = 0.22085820589458105
Iteration 250, Training loss = 0.22009203000305352
Iteration 260, Training loss = 0.21863447588848145
Iteration 270, Training loss = 0.2176272316991417
Iteration 280, Training loss = 0.21657953951643108
Iteration 290, Training loss = 0.21583392461152978
Model training time: 153.33386850357056
Device: cuda
Iteration 0, Training loss = 1.0307808799836018
Iteration 10, Training loss = 0.2964383310866125
Iteration 20, Training loss = 0.2784454377309462
Iteration 30, Training loss = 0.2664375226701143
Iteration 40, Training loss = 0.2598650969077831
Iteration 50, Training loss = 0.2560274854266326
Iteration 60, Training loss = 0.2507035197920141
Iteration 70, Training loss = 0.2464431863702094
Iteration 80, Training loss = 0.24339320106526552
Iteration 90, Training loss = 0.24095923249715753
Iteration 100, Training loss = 0.23825300633726917
Iteration 110, Training loss = 0.23666353146692165
Iteration 120, Training loss = 0.23471937483722302
Iteration 130, Training loss = 0.2331283582288763
Iteration 140, Training loss = 0.23192733127542617
Iteration 150, Training loss = 0.23087030860254898
Iteration 160, Training loss = 0.2294690992782393
Iteration 170, Training loss = 0.22815784357361873
Iteration 180, Training loss = 0.2269248863181537
Iteration 190, Training loss = 0.22538262746380258
Iteration 200, Training loss = 0.22434470799839526
Iteration 210, Training loss = 0.22321858560129748
Iteration 220, Training loss = 0.2216828389602914
Iteration 230, Training loss = 0.2201267278620175
Iteration 240, Training loss = 0.2183918970080034
Iteration 250, Training loss = 0.21715238049951074
Iteration 260, Training loss = 0.216057017535765
Iteration 270, Training loss = 0.2151760518929716
Iteration 280, Training loss = 0.21431415739128723
Iteration 290, Training loss = 0.21340506997401432
Model training time: 150.94650793075562
Device: cuda
Iteration 0, Training loss = 0.8731631196439987
Iteration 10, Training loss = 0.29308683473896463
Iteration 20, Training loss = 0.2778997115664563
Iteration 30, Training loss = 0.26801303682113675
Iteration 40, Training loss = 0.2615546963333218
Iteration 50, Training loss = 0.25654383907523054
Iteration 60, Training loss = 0.2528925197194477
Iteration 70, Training loss = 0.2486709831485448
Iteration 80, Training loss = 0.24589172878796484
Iteration 90, Training loss = 0.24335871561098907
Iteration 100, Training loss = 0.24108589422544036
Iteration 110, Training loss = 0.23866461848808548
Iteration 120, Training loss = 0.23682135081969508
Iteration 130, Training loss = 0.23465210267908637
Iteration 140, Training loss = 0.23349532347903124
Iteration 150, Training loss = 0.23182879474925072
Iteration 160, Training loss = 0.23000489652481554
Iteration 170, Training loss = 0.22877458889786037
Iteration 180, Training loss = 0.22725596946391297
Iteration 190, Training loss = 0.2256322735988199
Iteration 200, Training loss = 0.22442687376792436
Iteration 210, Training loss = 0.22281375633818762
Iteration 220, Training loss = 0.22188410133159478
Iteration 230, Training loss = 0.22030409581544325
Iteration 240, Training loss = 0.2196670822330162
Iteration 250, Training loss = 0.21798506736322407
Iteration 260, Training loss = 0.21689211080171006
Iteration 270, Training loss = 0.21624602554208142
Iteration 280, Training loss = 0.21562740636975944
Iteration 290, Training loss = 0.21445811365800965
Model training time: 153.76964497566223
Device: cuda
Iteration 0, Training loss = 0.833277107367504
Iteration 10, Training loss = 0.29635692165925487
Iteration 20, Training loss = 0.2798827345737822
Iteration 30, Training loss = 0.2681932140854311
Iteration 40, Training loss = 0.26120633608492466
Iteration 50, Training loss = 0.2560510172646213
Iteration 60, Training loss = 0.25135395426943574
Iteration 70, Training loss = 0.24790900219547546
Iteration 80, Training loss = 0.2450666814212649
Iteration 90, Training loss = 0.24204509878490507
Iteration 100, Training loss = 0.24042688176649246
Iteration 110, Training loss = 0.23795185066741545
Iteration 120, Training loss = 0.23616039155831348
Iteration 130, Training loss = 0.23440895983656151
Iteration 140, Training loss = 0.23225094009975544
Iteration 150, Training loss = 0.23036446812907663
Iteration 160, Training loss = 0.22848854741627309
Iteration 170, Training loss = 0.2268565672271621
Iteration 180, Training loss = 0.2249495148496391
Iteration 190, Training loss = 0.22263030168435763
Iteration 200, Training loss = 0.22183143969886818
Iteration 210, Training loss = 0.22013994942241086
Iteration 220, Training loss = 0.21870212547354778
Iteration 230, Training loss = 0.21750951246256978
Iteration 240, Training loss = 0.21658091813542363
Iteration 250, Training loss = 0.215515619680538
Iteration 260, Training loss = 0.21455944942865188
Iteration 270, Training loss = 0.21378646157772144
Iteration 280, Training loss = 0.21263703296172995
Iteration 290, Training loss = 0.211629594606291
Model training time: 151.4503047466278
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9977471207879646
Iteration 10, Training loss = 0.9873618456122373
Iteration 20, Training loss = 0.9692177798499784
Iteration 30, Training loss = 0.926826783300312
Iteration 40, Training loss = 0.8123647767608448
Iteration 50, Training loss = 0.5756223470549895
Iteration 60, Training loss = 0.4120867017827946
Iteration 70, Training loss = 0.3769629384790148
Iteration 80, Training loss = 0.3633174551132228
Iteration 90, Training loss = 0.35370023108423476
Iteration 100, Training loss = 0.3464885635468342
Iteration 110, Training loss = 0.34079582545428244
Iteration 120, Training loss = 0.33636004095288224
Iteration 130, Training loss = 0.33310042170144744
Iteration 140, Training loss = 0.3298892898724096
Iteration 150, Training loss = 0.3276288541480358
Iteration 160, Training loss = 0.3255094639217305
Iteration 170, Training loss = 0.3238932683558787
Iteration 180, Training loss = 0.32233524706764893
Iteration 190, Training loss = 0.32098983197538383
Iteration 200, Training loss = 0.3196890768358263
Iteration 210, Training loss = 0.31867943368581536
Iteration 220, Training loss = 0.31768675933233476
Iteration 230, Training loss = 0.3168610304810522
Iteration 240, Training loss = 0.31598305869982835
Iteration 250, Training loss = 0.3151705499386672
Iteration 260, Training loss = 0.31427630150722247
Iteration 270, Training loss = 0.31349796192048535
Iteration 280, Training loss = 0.3129191344680567
Iteration 290, Training loss = 0.3121726316992942
Model training time: 130.6143205165863
Device: cuda
Iteration 0, Training loss = 1.0391319555751348
Iteration 10, Training loss = 0.9922430142940678
Iteration 20, Training loss = 0.9770057865262898
Iteration 30, Training loss = 0.9450652354710327
Iteration 40, Training loss = 0.8644937179423418
Iteration 50, Training loss = 0.6763426318728606
Iteration 60, Training loss = 0.46147867871254467
Iteration 70, Training loss = 0.3863450315111197
Iteration 80, Training loss = 0.36706251034075643
Iteration 90, Training loss = 0.35598382718193616
Iteration 100, Training loss = 0.3478898431645756
Iteration 110, Training loss = 0.34139949029207806
Iteration 120, Training loss = 0.33656556244046576
Iteration 130, Training loss = 0.33295044510402055
Iteration 140, Training loss = 0.32982697155660345
Iteration 150, Training loss = 0.32710521476970167
Iteration 160, Training loss = 0.32509597466658735
Iteration 170, Training loss = 0.32307699430941383
Iteration 180, Training loss = 0.3216402905544415
Iteration 190, Training loss = 0.3200998673752203
Iteration 200, Training loss = 0.3186614473708894
Iteration 210, Training loss = 0.31743739209798577
Iteration 220, Training loss = 0.31618953290560053
Iteration 230, Training loss = 0.31510833786128506
Iteration 240, Training loss = 0.3138908127426524
Iteration 250, Training loss = 0.3130139569970655
Iteration 260, Training loss = 0.3118825337283548
Iteration 270, Training loss = 0.31105109241049167
Iteration 280, Training loss = 0.3099428807130449
Iteration 290, Training loss = 0.3090107596634953
Model training time: 129.85743236541748
Device: cuda
Iteration 0, Training loss = 1.1160903204295596
Iteration 10, Training loss = 0.9894578358740264
Iteration 20, Training loss = 0.9724655801534076
Iteration 30, Training loss = 0.9309232318661115
Iteration 40, Training loss = 0.8201431720799452
Iteration 50, Training loss = 0.6002376357377586
Iteration 60, Training loss = 0.4327604040034458
Iteration 70, Training loss = 0.38429869171998693
Iteration 80, Training loss = 0.36557044691381385
Iteration 90, Training loss = 0.3531682515115484
Iteration 100, Training loss = 0.34365997604177884
Iteration 110, Training loss = 0.33644106473602337
Iteration 120, Training loss = 0.3309140958920229
Iteration 130, Training loss = 0.32661480655032266
Iteration 140, Training loss = 0.3229544104841373
Iteration 150, Training loss = 0.3202632833948724
Iteration 160, Training loss = 0.31814606678976565
Iteration 170, Training loss = 0.31626281939609285
Iteration 180, Training loss = 0.31466089726648955
Iteration 190, Training loss = 0.31311704095714604
Iteration 200, Training loss = 0.3116203928433954
Iteration 210, Training loss = 0.31034542010425076
Iteration 220, Training loss = 0.30939913440053746
Iteration 230, Training loss = 0.30846056278984424
Iteration 240, Training loss = 0.30712214703998614
Iteration 250, Training loss = 0.30615142049137095
Iteration 260, Training loss = 0.30536589150928123
Iteration 270, Training loss = 0.3044749350257705
Iteration 280, Training loss = 0.30376108347936637
Iteration 290, Training loss = 0.3028276458959891
Model training time: 131.22593879699707
Device: cuda
Iteration 0, Training loss = 1.005050984840416
Iteration 10, Training loss = 0.9960209227936031
Iteration 20, Training loss = 0.9857552229059233
Iteration 30, Training loss = 0.9644356479656321
Iteration 40, Training loss = 0.9100778616658135
Iteration 50, Training loss = 0.7593013646383262
Iteration 60, Training loss = 0.5060950906239179
Iteration 70, Training loss = 0.38802615681225683
Iteration 80, Training loss = 0.3647690575398487
Iteration 90, Training loss = 0.3536819720564108
Iteration 100, Training loss = 0.3456720477263229
Iteration 110, Training loss = 0.33903410033149234
Iteration 120, Training loss = 0.33401975019431285
Iteration 130, Training loss = 0.33003751697248757
Iteration 140, Training loss = 0.327039861541972
Iteration 150, Training loss = 0.32434891232855384
Iteration 160, Training loss = 0.32228365374535106
Iteration 170, Training loss = 0.32034833236771115
Iteration 180, Training loss = 0.31887465722624386
Iteration 190, Training loss = 0.3173636531786538
Iteration 200, Training loss = 0.3162073831617399
Iteration 210, Training loss = 0.31486937526351894
Iteration 220, Training loss = 0.3138996119504979
Iteration 230, Training loss = 0.3128042783561111
Iteration 240, Training loss = 0.31173797965627026
Iteration 250, Training loss = 0.31099866622874006
Iteration 260, Training loss = 0.30996386037539625
Iteration 270, Training loss = 0.3092016957465731
Iteration 280, Training loss = 0.3083851824697224
Iteration 290, Training loss = 0.30760337590954784
Model training time: 129.1026566028595
Device: cuda
Iteration 0, Training loss = 0.9959833318182689
Iteration 10, Training loss = 0.9885155547040426
Iteration 20, Training loss = 0.9749843901785465
Iteration 30, Training loss = 0.9457486242993981
Iteration 40, Training loss = 0.8710300133678585
Iteration 50, Training loss = 0.6951794243320715
Iteration 60, Training loss = 0.4731177018355515
Iteration 70, Training loss = 0.3852848907432025
Iteration 80, Training loss = 0.36239458104600053
Iteration 90, Training loss = 0.3498615481446499
Iteration 100, Training loss = 0.3409027884788721
Iteration 110, Training loss = 0.3342379560518207
Iteration 120, Training loss = 0.32923361893571895
Iteration 130, Training loss = 0.32568374833601726
Iteration 140, Training loss = 0.322765993527148
Iteration 150, Training loss = 0.32061279593960135
Iteration 160, Training loss = 0.3187419022219233
Iteration 170, Training loss = 0.3173342297715079
Iteration 180, Training loss = 0.3160907035698036
Iteration 190, Training loss = 0.31497825936529306
Iteration 200, Training loss = 0.31405972886027783
Iteration 210, Training loss = 0.31308027068220673
Iteration 220, Training loss = 0.3121989335657321
Iteration 230, Training loss = 0.3113089365474248
Iteration 240, Training loss = 0.3106426577518985
Iteration 250, Training loss = 0.3099437846145965
Iteration 260, Training loss = 0.3092574425059716
Iteration 270, Training loss = 0.30853801345132453
Iteration 280, Training loss = 0.30791579895894117
Iteration 290, Training loss = 0.3072387117522681
Model training time: 130.10133957862854
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9302222216389081
Iteration 10, Training loss = 0.297768194995117
Iteration 20, Training loss = 0.2820744186315063
Iteration 30, Training loss = 0.272979659603381
Iteration 40, Training loss = 0.2669452248416282
Iteration 50, Training loss = 0.2618588660336291
Iteration 60, Training loss = 0.25747224177993816
Iteration 70, Training loss = 0.25352709691691916
Iteration 80, Training loss = 0.2505710091234408
Iteration 90, Training loss = 0.24706829887686282
Iteration 100, Training loss = 0.244601963777952
Iteration 110, Training loss = 0.2422840464995502
Iteration 120, Training loss = 0.24076498088767395
Iteration 130, Training loss = 0.23895545915382538
Iteration 140, Training loss = 0.23762625293495002
Iteration 150, Training loss = 0.2364046946528749
Iteration 160, Training loss = 0.2352296628833683
Iteration 170, Training loss = 0.23490879260743214
Iteration 180, Training loss = 0.2340218975382336
Iteration 190, Training loss = 0.23285520000983093
Iteration 200, Training loss = 0.2316569068421752
Iteration 210, Training loss = 0.23136106530344228
Iteration 220, Training loss = 0.2305896673306426
Iteration 230, Training loss = 0.2296534187135627
Iteration 240, Training loss = 0.2298675101728474
Iteration 250, Training loss = 0.22895105267913232
Iteration 260, Training loss = 0.228122328965052
Iteration 270, Training loss = 0.22811408209237868
Iteration 280, Training loss = 0.22699806799539354
Iteration 290, Training loss = 0.22664617425051786
Model training time: 153.3735635280609
Device: cuda
Iteration 0, Training loss = 0.9790454055148811
Iteration 10, Training loss = 0.3026327473127236
Iteration 20, Training loss = 0.28304231828912985
Iteration 30, Training loss = 0.27309607784771167
Iteration 40, Training loss = 0.2671779968548052
Iteration 50, Training loss = 0.26303620735683975
Iteration 60, Training loss = 0.259568576031245
Iteration 70, Training loss = 0.25686161469027724
Iteration 80, Training loss = 0.255334976636613
Iteration 90, Training loss = 0.25363972709124083
Iteration 100, Training loss = 0.25292402081931187
Iteration 110, Training loss = 0.25211263467844114
Iteration 120, Training loss = 0.25038012283116795
Iteration 130, Training loss = 0.24974234378727528
Iteration 140, Training loss = 0.24881583178916508
Iteration 150, Training loss = 0.24793067740470387
Iteration 160, Training loss = 0.24706217602400457
Iteration 170, Training loss = 0.24633454963492712
Iteration 180, Training loss = 0.24552405383843776
Iteration 190, Training loss = 0.2447671068530106
Iteration 200, Training loss = 0.24359519527120105
Iteration 210, Training loss = 0.24260827673477353
Iteration 220, Training loss = 0.2415119453669316
Iteration 230, Training loss = 0.24055148123540254
Iteration 240, Training loss = 0.23943943508672946
Iteration 250, Training loss = 0.2386881293183233
Iteration 260, Training loss = 0.23776581217749068
Iteration 270, Training loss = 0.2364538458624706
Iteration 280, Training loss = 0.23579953262722234
Iteration 290, Training loss = 0.23504199559369618
Model training time: 153.5666046142578
Device: cuda
Iteration 0, Training loss = 0.9542136721258879
Iteration 10, Training loss = 0.2947965603114329
Iteration 20, Training loss = 0.2780607873280747
Iteration 30, Training loss = 0.26738589163193116
Iteration 40, Training loss = 0.2607618592156336
Iteration 50, Training loss = 0.2573496723326586
Iteration 60, Training loss = 0.25456064102436093
Iteration 70, Training loss = 0.25262118148962465
Iteration 80, Training loss = 0.2510403055995342
Iteration 90, Training loss = 0.24976447170496563
Iteration 100, Training loss = 0.24837101576511567
Iteration 110, Training loss = 0.24720431891321848
Iteration 120, Training loss = 0.24649247101375035
Iteration 130, Training loss = 0.24537384055428585
Iteration 140, Training loss = 0.24478760260646626
Iteration 150, Training loss = 0.2434280139211448
Iteration 160, Training loss = 0.24236873030373896
Iteration 170, Training loss = 0.24178416090712998
Iteration 180, Training loss = 0.24034675638265818
Iteration 190, Training loss = 0.23919977235086895
Iteration 200, Training loss = 0.23793685665358644
Iteration 210, Training loss = 0.23652639573769188
Iteration 220, Training loss = 0.2356333798812319
Iteration 230, Training loss = 0.23396656254518408
Iteration 240, Training loss = 0.2325732557883274
Iteration 250, Training loss = 0.2315419019982399
Iteration 260, Training loss = 0.2306953629357041
Iteration 270, Training loss = 0.23002499452009617
Iteration 280, Training loss = 0.2291684655098303
Iteration 290, Training loss = 0.22847732962007672
Model training time: 160.10000586509705
Device: cuda
Iteration 0, Training loss = 0.9069311532212228
Iteration 10, Training loss = 0.2949304454587851
Iteration 20, Training loss = 0.2783801514682412
Iteration 30, Training loss = 0.2681202107891621
Iteration 40, Training loss = 0.26160284267066464
Iteration 50, Training loss = 0.25730690650515636
Iteration 60, Training loss = 0.2539761534037371
Iteration 70, Training loss = 0.2514599277558973
Iteration 80, Training loss = 0.250274481644209
Iteration 90, Training loss = 0.24893564361059636
Iteration 100, Training loss = 0.24780603413431754
Iteration 110, Training loss = 0.24751174154062247
Iteration 120, Training loss = 0.2464200618155932
Iteration 130, Training loss = 0.24571745626213476
Iteration 140, Training loss = 0.24511689660263408
Iteration 150, Training loss = 0.24482889390453588
Iteration 160, Training loss = 0.24379794178733524
Iteration 170, Training loss = 0.24336490708856548
Iteration 180, Training loss = 0.24279221908593004
Iteration 190, Training loss = 0.2423798022633892
Iteration 200, Training loss = 0.2417493600028479
Iteration 210, Training loss = 0.24100819500681853
Iteration 220, Training loss = 0.24088160933579428
Iteration 230, Training loss = 0.24022076886877886
Iteration 240, Training loss = 0.2397317794888995
Iteration 250, Training loss = 0.23938270732219225
Iteration 260, Training loss = 0.23895236435763484
Iteration 270, Training loss = 0.23862067790426872
Iteration 280, Training loss = 0.23786366922153976
Iteration 290, Training loss = 0.23672427298370344
Model training time: 161.52641582489014
Device: cuda
Iteration 0, Training loss = 0.9442615145343846
Iteration 10, Training loss = 0.29471653146042376
Iteration 20, Training loss = 0.2785739472934178
Iteration 30, Training loss = 0.27080195227561216
Iteration 40, Training loss = 0.265586013042032
Iteration 50, Training loss = 0.2619906056482913
Iteration 60, Training loss = 0.2584678760704925
Iteration 70, Training loss = 0.25648737007855793
Iteration 80, Training loss = 0.25401838935604976
Iteration 90, Training loss = 0.25170746763162405
Iteration 100, Training loss = 0.2504851881586033
Iteration 110, Training loss = 0.24828044521750897
Iteration 120, Training loss = 0.2464775543380303
Iteration 130, Training loss = 0.24432875500139545
Iteration 140, Training loss = 0.24280086040641147
Iteration 150, Training loss = 0.24142572553445005
Iteration 160, Training loss = 0.24009268945556575
Iteration 170, Training loss = 0.2391623012162294
Iteration 180, Training loss = 0.23806347584320328
Iteration 190, Training loss = 0.23738546781332096
Iteration 200, Training loss = 0.23631958412705553
Iteration 210, Training loss = 0.23573519498759263
Iteration 220, Training loss = 0.23459246354155044
Iteration 230, Training loss = 0.2336640707138231
Iteration 240, Training loss = 0.23264475642155505
Iteration 250, Training loss = 0.2321663590342023
Iteration 260, Training loss = 0.23099109689202205
Iteration 270, Training loss = 0.23030851579318612
Iteration 280, Training loss = 0.22987231209008227
Iteration 290, Training loss = 0.22895420572695663
Model training time: 155.08607578277588
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.042582599504808
Iteration 10, Training loss = 1.0036652885539767
Iteration 20, Training loss = 0.9995348859785833
Iteration 30, Training loss = 0.9949734198700718
Iteration 40, Training loss = 0.9882488841727629
Iteration 50, Training loss = 0.9735354950728197
Iteration 60, Training loss = 0.9396588204847987
Iteration 70, Training loss = 0.8541234627762949
Iteration 80, Training loss = 0.6695086621199047
Iteration 90, Training loss = 0.4711840949754161
Iteration 100, Training loss = 0.3973470842217706
Iteration 110, Training loss = 0.3747739082531548
Iteration 120, Training loss = 0.361204508430444
Iteration 130, Training loss = 0.3508490486020903
Iteration 140, Training loss = 0.3426410089501746
Iteration 150, Training loss = 0.3362587648319852
Iteration 160, Training loss = 0.3313303162269384
Iteration 170, Training loss = 0.32720220076330636
Iteration 180, Training loss = 0.32391783883727493
Iteration 190, Training loss = 0.3214020937428636
Iteration 200, Training loss = 0.3189786501210769
Iteration 210, Training loss = 0.31712691197239457
Iteration 220, Training loss = 0.3157731468358571
Iteration 230, Training loss = 0.31404756974726555
Iteration 240, Training loss = 0.3129809072859062
Iteration 250, Training loss = 0.3118042835997323
Iteration 260, Training loss = 0.3107397540610004
Iteration 270, Training loss = 0.30979174976270946
Iteration 280, Training loss = 0.30892814237615385
Iteration 290, Training loss = 0.30805080120629896
Model training time: 132.95739316940308
Device: cuda
Iteration 0, Training loss = 1.0113089983746157
Iteration 10, Training loss = 0.9914030952257337
Iteration 20, Training loss = 0.9764301610916637
Iteration 30, Training loss = 0.9436461409558395
Iteration 40, Training loss = 0.8558145647476141
Iteration 50, Training loss = 0.6587636030615097
Iteration 60, Training loss = 0.4581541212289154
Iteration 70, Training loss = 0.38916590366034476
Iteration 80, Training loss = 0.36897560961309994
Iteration 90, Training loss = 0.3569230796999273
Iteration 100, Training loss = 0.34813509223031075
Iteration 110, Training loss = 0.3415149888497288
Iteration 120, Training loss = 0.33589355701684376
Iteration 130, Training loss = 0.3317116023769679
Iteration 140, Training loss = 0.3282912903562296
Iteration 150, Training loss = 0.32541419396100263
Iteration 160, Training loss = 0.3231248893475128
Iteration 170, Training loss = 0.32106634920359234
Iteration 180, Training loss = 0.31930943527969263
Iteration 190, Training loss = 0.3176708504124646
Iteration 200, Training loss = 0.31621343819266656
Iteration 210, Training loss = 0.31486632649676277
Iteration 220, Training loss = 0.31362793484553875
Iteration 230, Training loss = 0.31261592645189085
Iteration 240, Training loss = 0.3115637854066369
Iteration 250, Training loss = 0.3107285004731529
Iteration 260, Training loss = 0.30980931208006696
Iteration 270, Training loss = 0.3090821630693521
Iteration 280, Training loss = 0.3083110263359171
Iteration 290, Training loss = 0.3075631210179075
Model training time: 130.99141192436218
Device: cuda
Iteration 0, Training loss = 1.0284260354088235
Iteration 10, Training loss = 0.9802887058719885
Iteration 20, Training loss = 0.9427517568805316
Iteration 30, Training loss = 0.8517613543725187
Iteration 40, Training loss = 0.6484725206873896
Iteration 50, Training loss = 0.44571175861012274
Iteration 60, Training loss = 0.38305063340046214
Iteration 70, Training loss = 0.3641790841812083
Iteration 80, Training loss = 0.3523209891834502
Iteration 90, Training loss = 0.3434870527746025
Iteration 100, Training loss = 0.33682696091926706
Iteration 110, Training loss = 0.33149489717102515
Iteration 120, Training loss = 0.3274356463053613
Iteration 130, Training loss = 0.3241054659244801
Iteration 140, Training loss = 0.32154982589347597
Iteration 150, Training loss = 0.31905305901537795
Iteration 160, Training loss = 0.31720110869653
Iteration 170, Training loss = 0.31554377039321685
Iteration 180, Training loss = 0.31394434662549314
Iteration 190, Training loss = 0.3126853274375417
Iteration 200, Training loss = 0.31138788730122563
Iteration 210, Training loss = 0.3102890094133324
Iteration 220, Training loss = 0.30921474375173486
Iteration 230, Training loss = 0.3082588854410458
Iteration 240, Training loss = 0.3072986018102048
Iteration 250, Training loss = 0.30643928101507284
Iteration 260, Training loss = 0.30557310218600325
Iteration 270, Training loss = 0.304785134121956
Iteration 280, Training loss = 0.3037749088885709
Iteration 290, Training loss = 0.3031819188746355
Model training time: 129.65697956085205
Device: cuda
Iteration 0, Training loss = 1.054147540440571
Iteration 10, Training loss = 0.9905113714226221
Iteration 20, Training loss = 0.978455390393301
Iteration 30, Training loss = 0.9539033438622518
Iteration 40, Training loss = 0.8932503700689312
Iteration 50, Training loss = 0.7487348269030777
Iteration 60, Training loss = 0.5434285217566871
Iteration 70, Training loss = 0.4226088276714736
Iteration 80, Training loss = 0.3806572026476156
Iteration 90, Training loss = 0.3614008251135632
Iteration 100, Training loss = 0.3483617906492501
Iteration 110, Training loss = 0.3387422208419435
Iteration 120, Training loss = 0.33151376943755667
Iteration 130, Training loss = 0.3258300084467373
Iteration 140, Training loss = 0.3215992799249746
Iteration 150, Training loss = 0.3183170195317153
Iteration 160, Training loss = 0.31551180621180636
Iteration 170, Training loss = 0.31329983923827764
Iteration 180, Training loss = 0.3113427386409434
Iteration 190, Training loss = 0.30978212875402
Iteration 200, Training loss = 0.3083543067766448
Iteration 210, Training loss = 0.30710451999458216
Iteration 220, Training loss = 0.3058874455909752
Iteration 230, Training loss = 0.3048293876301578
Iteration 240, Training loss = 0.303696602276249
Iteration 250, Training loss = 0.3027902901822083
Iteration 260, Training loss = 0.30168818947622333
Iteration 270, Training loss = 0.3008059544294856
Iteration 280, Training loss = 0.29988069404773504
Iteration 290, Training loss = 0.2991517840300576
Model training time: 129.79199838638306
Device: cuda
Iteration 0, Training loss = 1.005767578579323
Iteration 10, Training loss = 0.9970667963310823
Iteration 20, Training loss = 0.9909013130907285
Iteration 30, Training loss = 0.9800293758764105
Iteration 40, Training loss = 0.9551348645808333
Iteration 50, Training loss = 0.8920338334240578
Iteration 60, Training loss = 0.7365090764030706
Iteration 70, Training loss = 0.5146580498674591
Iteration 80, Training loss = 0.4079609870477681
Iteration 90, Training loss = 0.37882431677698225
Iteration 100, Training loss = 0.3636087150139324
Iteration 110, Training loss = 0.35235624249853176
Iteration 120, Training loss = 0.3435216318211602
Iteration 130, Training loss = 0.33664883157242875
Iteration 140, Training loss = 0.33142965016367937
Iteration 150, Training loss = 0.3271462839033644
Iteration 160, Training loss = 0.32380652903933216
Iteration 170, Training loss = 0.3210196080673982
Iteration 180, Training loss = 0.3188084953308971
Iteration 190, Training loss = 0.31698176502892816
Iteration 200, Training loss = 0.3154028218609369
Iteration 210, Training loss = 0.3141002725205468
Iteration 220, Training loss = 0.3126993963526467
Iteration 230, Training loss = 0.3115115259519212
Iteration 240, Training loss = 0.3104782940472587
Iteration 250, Training loss = 0.3095031045828258
Iteration 260, Training loss = 0.3085719177963947
Iteration 270, Training loss = 0.3076573385886361
Iteration 280, Training loss = 0.30688782757910343
Iteration 290, Training loss = 0.30600964615333454
Model training time: 132.07272005081177
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9096174350518003
Iteration 10, Training loss = 0.29993713535278244
Iteration 20, Training loss = 0.28684371477105713
Iteration 30, Training loss = 0.27763653247105297
Iteration 40, Training loss = 0.27088259523125596
Iteration 50, Training loss = 0.2663668610209703
Iteration 60, Training loss = 0.261363041963762
Iteration 70, Training loss = 0.2577262462520426
Iteration 80, Training loss = 0.254295017795759
Iteration 90, Training loss = 0.25070716487582023
Iteration 100, Training loss = 0.2476306112604915
Iteration 110, Training loss = 0.24458824083678082
Iteration 120, Training loss = 0.24257829328931274
Iteration 130, Training loss = 0.24028302649753144
Iteration 140, Training loss = 0.2385850841535783
Iteration 150, Training loss = 0.23714067216863355
Iteration 160, Training loss = 0.2360149250156077
Iteration 170, Training loss = 0.23473279105981962
Iteration 180, Training loss = 0.23336738282160666
Iteration 190, Training loss = 0.23248161819310223
Iteration 200, Training loss = 0.23106698714395124
Iteration 210, Training loss = 0.23028810138275202
Iteration 220, Training loss = 0.22961665373340645
Iteration 230, Training loss = 0.228203191541442
Iteration 240, Training loss = 0.227908522720487
Iteration 250, Training loss = 0.22628998417253934
Iteration 260, Training loss = 0.22605942994283995
Iteration 270, Training loss = 0.22493974509091988
Iteration 280, Training loss = 0.22433326866017705
Iteration 290, Training loss = 0.2238031764837212
Model training time: 154.50920462608337
Device: cuda
Iteration 0, Training loss = 0.8717884903381292
Iteration 10, Training loss = 0.29883469937067053
Iteration 20, Training loss = 0.28428506324424
Iteration 30, Training loss = 0.27253688972885326
Iteration 40, Training loss = 0.2655886132044596
Iteration 50, Training loss = 0.2619789456700586
Iteration 60, Training loss = 0.25939644381548244
Iteration 70, Training loss = 0.25671139263861403
Iteration 80, Training loss = 0.2545874971021463
Iteration 90, Training loss = 0.25286851507627356
Iteration 100, Training loss = 0.2508315309232719
Iteration 110, Training loss = 0.24903684318715089
Iteration 120, Training loss = 0.2469441637613294
Iteration 130, Training loss = 0.24536278243142814
Iteration 140, Training loss = 0.24373971493170568
Iteration 150, Training loss = 0.24286906713434916
Iteration 160, Training loss = 0.24158482759577599
Iteration 170, Training loss = 0.23994803044394777
Iteration 180, Training loss = 0.23873843263482933
Iteration 190, Training loss = 0.23815161592302253
Iteration 200, Training loss = 0.23661987239959453
Iteration 210, Training loss = 0.23550065630284694
Iteration 220, Training loss = 0.2348197068270413
Iteration 230, Training loss = 0.23310968579304997
Iteration 240, Training loss = 0.2326829418828643
Iteration 250, Training loss = 0.23118119037396684
Iteration 260, Training loss = 0.23052123648094208
Iteration 270, Training loss = 0.22990473475280165
Iteration 280, Training loss = 0.2289965822390725
Iteration 290, Training loss = 0.22808752557122966
Model training time: 152.77779483795166
Device: cuda
Iteration 0, Training loss = 0.8899877017981781
Iteration 10, Training loss = 0.295283156503488
Iteration 20, Training loss = 0.2815725384663439
Iteration 30, Training loss = 0.27149873578663886
Iteration 40, Training loss = 0.26567910853367455
Iteration 50, Training loss = 0.26107635981342403
Iteration 60, Training loss = 0.2569703514762133
Iteration 70, Training loss = 0.2543772984359224
Iteration 80, Training loss = 0.2522076266585482
Iteration 90, Training loss = 0.24901719118939764
Iteration 100, Training loss = 0.24674919405478543
Iteration 110, Training loss = 0.24387466536776206
Iteration 120, Training loss = 0.24183074291118986
Iteration 130, Training loss = 0.2400230340497257
Iteration 140, Training loss = 0.23821103360873735
Iteration 150, Training loss = 0.2371418204600528
Iteration 160, Training loss = 0.23605538837918358
Iteration 170, Training loss = 0.2347172869428018
Iteration 180, Training loss = 0.23379583716320357
Iteration 190, Training loss = 0.2330321644592949
Iteration 200, Training loss = 0.2317872497525977
Iteration 210, Training loss = 0.23065089870054842
Iteration 220, Training loss = 0.22989248948004865
Iteration 230, Training loss = 0.22903186400346548
Iteration 240, Training loss = 0.2283020477029371
Iteration 250, Training loss = 0.22723107415163488
Iteration 260, Training loss = 0.22669032443377932
Iteration 270, Training loss = 0.22590106188998385
Iteration 280, Training loss = 0.22488960849242984
Iteration 290, Training loss = 0.22457977746033783
Model training time: 153.98713612556458
Device: cuda
Iteration 0, Training loss = 0.8985559339240446
Iteration 10, Training loss = 0.2931013772079211
Iteration 20, Training loss = 0.27574161149687687
Iteration 30, Training loss = 0.2652266083812887
Iteration 40, Training loss = 0.2583593620623284
Iteration 50, Training loss = 0.25266272078559127
Iteration 60, Training loss = 0.24873358070778212
Iteration 70, Training loss = 0.24582409582689368
Iteration 80, Training loss = 0.2431951041804676
Iteration 90, Training loss = 0.24116957939926706
Iteration 100, Training loss = 0.23905925647542783
Iteration 110, Training loss = 0.23736180276688884
Iteration 120, Training loss = 0.2360571510368196
Iteration 130, Training loss = 0.23487970936529284
Iteration 140, Training loss = 0.23405248391245814
Iteration 150, Training loss = 0.2327624767946562
Iteration 160, Training loss = 0.23174403020638532
Iteration 170, Training loss = 0.23106446777432074
Iteration 180, Training loss = 0.22992259956733943
Iteration 190, Training loss = 0.22918051925034558
Iteration 200, Training loss = 0.22777262211892277
Iteration 210, Training loss = 0.22727757886268612
Iteration 220, Training loss = 0.2268866401427306
Iteration 230, Training loss = 0.22571361979546328
Iteration 240, Training loss = 0.2251594606651665
Iteration 250, Training loss = 0.22529526615907725
Iteration 260, Training loss = 0.22351658257676094
Iteration 270, Training loss = 0.22285886956762171
Iteration 280, Training loss = 0.22182299117343476
Iteration 290, Training loss = 0.22135084522008608
Model training time: 154.63210725784302
Device: cuda
Iteration 0, Training loss = 0.877969500588447
Iteration 10, Training loss = 0.2974598357413352
Iteration 20, Training loss = 0.28351850562320785
Iteration 30, Training loss = 0.2707309927656056
Iteration 40, Training loss = 0.26309650560917636
Iteration 50, Training loss = 0.2580707082412145
Iteration 60, Training loss = 0.2537720176159037
Iteration 70, Training loss = 0.25065618096771886
Iteration 80, Training loss = 0.24780067072653597
Iteration 90, Training loss = 0.24568873762038082
Iteration 100, Training loss = 0.2436063896152067
Iteration 110, Training loss = 0.24174047642123614
Iteration 120, Training loss = 0.23984355182718423
Iteration 130, Training loss = 0.23872173384517503
Iteration 140, Training loss = 0.23721175152835489
Iteration 150, Training loss = 0.23579703727804718
Iteration 160, Training loss = 0.23491668659679538
Iteration 170, Training loss = 0.23423761048198613
Iteration 180, Training loss = 0.23312941083535732
Iteration 190, Training loss = 0.23244681068612646
Iteration 200, Training loss = 0.23133427354780006
Iteration 210, Training loss = 0.23078129628163563
Iteration 220, Training loss = 0.22995084014520517
Iteration 230, Training loss = 0.22996453134473818
Iteration 240, Training loss = 0.2286712360223327
Iteration 250, Training loss = 0.22814789356677354
Iteration 260, Training loss = 0.22768390173268377
Iteration 270, Training loss = 0.2269614105796121
Iteration 280, Training loss = 0.2267672025912033
Iteration 290, Training loss = 0.22608836785355724
Model training time: 155.76612734794617
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9886870927585528
Iteration 10, Training loss = 0.968162719476021
Iteration 20, Training loss = 0.9221176028107326
Iteration 30, Training loss = 0.8032230404329647
Iteration 40, Training loss = 0.5781900954159929
Iteration 50, Training loss = 0.4264962627365283
Iteration 60, Training loss = 0.3875954804928482
Iteration 70, Training loss = 0.37131460646018566
Iteration 80, Training loss = 0.35935038158593396
Iteration 90, Training loss = 0.3500191677569188
Iteration 100, Training loss = 0.34274175440211563
Iteration 110, Training loss = 0.3370017917335178
Iteration 120, Training loss = 0.3325662412128206
Iteration 130, Training loss = 0.32909877885124006
Iteration 140, Training loss = 0.32581407793281153
Iteration 150, Training loss = 0.3233125908503232
Iteration 160, Training loss = 0.32103526254903897
Iteration 170, Training loss = 0.31910008114489746
Iteration 180, Training loss = 0.31751182877411277
Iteration 190, Training loss = 0.3159994917220411
Iteration 200, Training loss = 0.3145724389765222
Iteration 210, Training loss = 0.31328653104007964
Iteration 220, Training loss = 0.3120788581146166
Iteration 230, Training loss = 0.31109778198794646
Iteration 240, Training loss = 0.31028363799572567
Iteration 250, Training loss = 0.30917290430380706
Iteration 260, Training loss = 0.30831883517578784
Iteration 270, Training loss = 0.30749979649487763
Iteration 280, Training loss = 0.30662108393110893
Iteration 290, Training loss = 0.3059631338022812
Model training time: 140.9204499721527
Device: cuda
Iteration 0, Training loss = 1.0048119598237424
Iteration 10, Training loss = 0.9878588757561136
Iteration 20, Training loss = 0.9652794941574263
Iteration 30, Training loss = 0.9144373041088298
Iteration 40, Training loss = 0.7916136470603019
Iteration 50, Training loss = 0.5772071397044757
Iteration 60, Training loss = 0.42033693919603243
Iteration 70, Training loss = 0.3775891027713226
Iteration 80, Training loss = 0.3633234526141215
Iteration 90, Training loss = 0.35346095235093744
Iteration 100, Training loss = 0.3459432150708561
Iteration 110, Training loss = 0.3397937064706269
Iteration 120, Training loss = 0.3350789696880172
Iteration 130, Training loss = 0.3312301857816394
Iteration 140, Training loss = 0.32791592718325285
Iteration 150, Training loss = 0.3254854621018394
Iteration 160, Training loss = 0.3232677780434525
Iteration 170, Training loss = 0.3213805607892122
Iteration 180, Training loss = 0.319967634091943
Iteration 190, Training loss = 0.31842357686948547
Iteration 200, Training loss = 0.31703001064119846
Iteration 210, Training loss = 0.31589143723249435
Iteration 220, Training loss = 0.3147978093159401
Iteration 230, Training loss = 0.3137311411178141
Iteration 240, Training loss = 0.3128681598867112
Iteration 250, Training loss = 0.31182669527089046
Iteration 260, Training loss = 0.31103459325310107
Iteration 270, Training loss = 0.31009467012873576
Iteration 280, Training loss = 0.3091876460983447
Iteration 290, Training loss = 0.3084140279427279
Model training time: 138.0340645313263
Device: cuda
Iteration 0, Training loss = 0.9984110961093163
Iteration 10, Training loss = 0.9751960042025217
Iteration 20, Training loss = 0.9321796728825742
Iteration 30, Training loss = 0.8092148166158875
Iteration 40, Training loss = 0.5501779658093002
Iteration 50, Training loss = 0.39465451547222047
Iteration 60, Training loss = 0.3685212607966786
Iteration 70, Training loss = 0.3585696546202999
Iteration 80, Training loss = 0.3509953806929958
Iteration 90, Training loss = 0.3450165848466444
Iteration 100, Training loss = 0.3401245436389856
Iteration 110, Training loss = 0.33589726071308657
Iteration 120, Training loss = 0.332574707814481
Iteration 130, Training loss = 0.3297960745581126
Iteration 140, Training loss = 0.3274707430319694
Iteration 150, Training loss = 0.3253656818957652
Iteration 160, Training loss = 0.32358558052027775
Iteration 170, Training loss = 0.3220774936365735
Iteration 180, Training loss = 0.32076808294285875
Iteration 190, Training loss = 0.3194341360620667
Iteration 200, Training loss = 0.3182864910393015
Iteration 210, Training loss = 0.3172901666409744
Iteration 220, Training loss = 0.3162340274482027
Iteration 230, Training loss = 0.3152416472393144
Iteration 240, Training loss = 0.31438470294487103
Iteration 250, Training loss = 0.31364095366174316
Iteration 260, Training loss = 0.31278692703630967
Iteration 270, Training loss = 0.31235702703853496
Iteration 280, Training loss = 0.3113094617682565
Iteration 290, Training loss = 0.3106701999866645
Model training time: 130.70300221443176
Device: cuda
Iteration 0, Training loss = 1.0323467748938693
Iteration 10, Training loss = 0.9941181658110954
Iteration 20, Training loss = 0.9841407732293912
Iteration 30, Training loss = 0.9630919044012018
Iteration 40, Training loss = 0.9131397225377634
Iteration 50, Training loss = 0.782775698288301
Iteration 60, Training loss = 0.5413161287801318
Iteration 70, Training loss = 0.4002672045268389
Iteration 80, Training loss = 0.3724284957165291
Iteration 90, Training loss = 0.36047391174924864
Iteration 100, Training loss = 0.35161960492988475
Iteration 110, Training loss = 0.34446755942796103
Iteration 120, Training loss = 0.3389419948470217
Iteration 130, Training loss = 0.33449668475126815
Iteration 140, Training loss = 0.33115809165464477
Iteration 150, Training loss = 0.3281589643646383
Iteration 160, Training loss = 0.32575609387049664
Iteration 170, Training loss = 0.32379526962451727
Iteration 180, Training loss = 0.32175832966987505
Iteration 190, Training loss = 0.3200916600104683
Iteration 200, Training loss = 0.3185856660363461
Iteration 210, Training loss = 0.3171459167438038
Iteration 220, Training loss = 0.3161167666388193
Iteration 230, Training loss = 0.31466549041412645
Iteration 240, Training loss = 0.313448183202426
Iteration 250, Training loss = 0.312243224092315
Iteration 260, Training loss = 0.3112953797199247
Iteration 270, Training loss = 0.31014418087845563
Iteration 280, Training loss = 0.30910588896519914
Iteration 290, Training loss = 0.30813010890244286
Model training time: 132.04558062553406
Device: cuda
Iteration 0, Training loss = 1.0417140437240462
Iteration 10, Training loss = 0.9862395260149284
Iteration 20, Training loss = 0.9704580660593711
Iteration 30, Training loss = 0.9329467602705551
Iteration 40, Training loss = 0.8359994251849288
Iteration 50, Training loss = 0.6282199172361711
Iteration 60, Training loss = 0.4446566124011472
Iteration 70, Training loss = 0.39386893801625644
Iteration 80, Training loss = 0.3769717935211145
Iteration 90, Training loss = 0.3647284523265991
Iteration 100, Training loss = 0.3546649615751629
Iteration 110, Training loss = 0.3467130234541674
Iteration 120, Training loss = 0.34006543841763215
Iteration 130, Training loss = 0.3345366723059742
Iteration 140, Training loss = 0.3300091278249935
Iteration 150, Training loss = 0.3262544334584229
Iteration 160, Training loss = 0.3231588074752551
Iteration 170, Training loss = 0.32052927262197106
Iteration 180, Training loss = 0.31813570158894355
Iteration 190, Training loss = 0.31616573582333457
Iteration 200, Training loss = 0.3144645787071951
Iteration 210, Training loss = 0.3127940819208616
Iteration 220, Training loss = 0.31142503678004907
Iteration 230, Training loss = 0.31032688051606494
Iteration 240, Training loss = 0.30904001091352096
Iteration 250, Training loss = 0.30795283411735486
Iteration 260, Training loss = 0.30679288295438156
Iteration 270, Training loss = 0.30573495852673027
Iteration 280, Training loss = 0.30501177943070346
Iteration 290, Training loss = 0.30405765490439557
Model training time: 132.8529372215271
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6591129974793579
Iteration 10, Training loss = 0.28168640009716117
Iteration 20, Training loss = 0.26483114484075193
Iteration 30, Training loss = 0.2546803771403165
Iteration 40, Training loss = 0.24871400070825442
Iteration 50, Training loss = 0.2447177845326232
Iteration 60, Training loss = 0.24076921629920134
Iteration 70, Training loss = 0.23667508349291638
Iteration 80, Training loss = 0.234594549631957
Iteration 90, Training loss = 0.2314367720419789
Model training time: 52.37184548377991
Device: cuda
Iteration 0, Training loss = 0.6574137484334571
Iteration 10, Training loss = 0.283245293861007
Iteration 20, Training loss = 0.2633955375117771
Iteration 30, Training loss = 0.2555668301808199
Iteration 40, Training loss = 0.24909698313720002
Iteration 50, Training loss = 0.24480891482088238
Iteration 60, Training loss = 0.24027969026103724
Iteration 70, Training loss = 0.23683338392228248
Iteration 80, Training loss = 0.23390968813229415
Iteration 90, Training loss = 0.23122736771949556
Model training time: 53.464038133621216
Device: cuda
Iteration 0, Training loss = 0.6140166782293712
Iteration 10, Training loss = 0.2786264948853569
Iteration 20, Training loss = 0.26273583303280085
Iteration 30, Training loss = 0.2549638875171578
Iteration 40, Training loss = 0.24899714752201885
Iteration 50, Training loss = 0.2433851935672991
Iteration 60, Training loss = 0.23961396082405895
Iteration 70, Training loss = 0.23498275683015657
Iteration 80, Training loss = 0.23130891849140278
Iteration 90, Training loss = 0.2283705144795898
Model training time: 52.69629263877869
Device: cuda
Iteration 0, Training loss = 0.686061477437435
Iteration 10, Training loss = 0.28400020669361004
Iteration 20, Training loss = 0.26462458838995084
Iteration 30, Training loss = 0.2563466309202208
Iteration 40, Training loss = 0.25129920821501617
Iteration 50, Training loss = 0.2462369010177132
Iteration 60, Training loss = 0.24119362168320732
Iteration 70, Training loss = 0.23689556040645512
Iteration 80, Training loss = 0.23404446698851505
Iteration 90, Training loss = 0.23093760433915742
Model training time: 51.93673586845398
Device: cuda
Iteration 0, Training loss = 0.6337533189078509
Iteration 10, Training loss = 0.2754190647320655
Iteration 20, Training loss = 0.259551123578381
Iteration 30, Training loss = 0.24898178882662378
Iteration 40, Training loss = 0.24237460071180403
Iteration 50, Training loss = 0.23801838138454184
Iteration 60, Training loss = 0.23446597204009212
Iteration 70, Training loss = 0.23112097730720302
Iteration 80, Training loss = 0.2280057484496015
Iteration 90, Training loss = 0.2254276757757687
Model training time: 52.8081738948822
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0138253628774647
Iteration 10, Training loss = 0.985278942466648
Iteration 20, Training loss = 0.9159227821064919
Iteration 30, Training loss = 0.5736702532658565
Iteration 40, Training loss = 0.38611413044155946
Iteration 50, Training loss = 0.356002989281465
Iteration 60, Training loss = 0.33923958485481526
Iteration 70, Training loss = 0.3285734017557728
Iteration 80, Training loss = 0.3219200351228148
Iteration 90, Training loss = 0.31699689594943065
Model training time: 42.94820809364319
Device: cuda
Iteration 0, Training loss = 1.0357600824307587
Iteration 10, Training loss = 0.945455956040514
Iteration 20, Training loss = 0.6728378233407369
Iteration 30, Training loss = 0.39272601280099834
Iteration 40, Training loss = 0.3608863274352602
Iteration 50, Training loss = 0.3445613667296728
Iteration 60, Training loss = 0.33410787275714965
Iteration 70, Training loss = 0.3274660178845789
Iteration 80, Training loss = 0.3229964096775644
Iteration 90, Training loss = 0.31940876308278365
Model training time: 44.35619354248047
Device: cuda
Iteration 0, Training loss = 1.002721729275678
Iteration 10, Training loss = 0.970394773575642
Iteration 20, Training loss = 0.8243087553227496
Iteration 30, Training loss = 0.4264620205175501
Iteration 40, Training loss = 0.3718413719614251
Iteration 50, Training loss = 0.3527418815555642
Iteration 60, Training loss = 0.340517136180372
Iteration 70, Training loss = 0.3322290308575076
Iteration 80, Training loss = 0.3262478780479466
Iteration 90, Training loss = 0.3220008765561529
Model training time: 43.47795224189758
Device: cuda
Iteration 0, Training loss = 0.9972682808126722
Iteration 10, Training loss = 0.9684496928935478
Iteration 20, Training loss = 0.8220129449633075
Iteration 30, Training loss = 0.4194143552684899
Iteration 40, Training loss = 0.3587761385677513
Iteration 50, Training loss = 0.3416846612824655
Iteration 60, Training loss = 0.33111435741402623
Iteration 70, Training loss = 0.3244966245448041
Iteration 80, Training loss = 0.32024549786677947
Iteration 90, Training loss = 0.3170928524640224
Model training time: 43.6791090965271
Device: cuda
Iteration 0, Training loss = 1.003574260354908
Iteration 10, Training loss = 0.9850649247735234
Iteration 20, Training loss = 0.9214570556223826
Iteration 30, Training loss = 0.5797291745501627
Iteration 40, Training loss = 0.3810138790789297
Iteration 50, Training loss = 0.3560643433153485
Iteration 60, Training loss = 0.34133772400476164
Iteration 70, Training loss = 0.33198654994404636
Iteration 80, Training loss = 0.32593941205061666
Iteration 90, Training loss = 0.3217261367033238
Model training time: 42.9864718914032
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7365979239452838
Iteration 10, Training loss = 0.2829510792007458
Iteration 20, Training loss = 0.2654765424009674
Iteration 30, Training loss = 0.25837248873154994
Iteration 40, Training loss = 0.2546941619411508
Iteration 50, Training loss = 0.25108198357912875
Iteration 60, Training loss = 0.24806805766812248
Iteration 70, Training loss = 0.24655818262703483
Iteration 80, Training loss = 0.24362909016323436
Iteration 90, Training loss = 0.24144151087029506
Model training time: 51.670103788375854
Device: cuda
Iteration 0, Training loss = 0.8114398891787263
Iteration 10, Training loss = 0.28265038935454073
Iteration 20, Training loss = 0.26574821925336456
Iteration 30, Training loss = 0.2593077063813048
Iteration 40, Training loss = 0.2529038638958919
Iteration 50, Training loss = 0.24794451724602581
Iteration 60, Training loss = 0.24487256630353144
Iteration 70, Training loss = 0.24148396667787583
Iteration 80, Training loss = 0.23830930491769572
Iteration 90, Training loss = 0.23529601935973757
Model training time: 53.568992376327515
Device: cuda
Iteration 0, Training loss = 0.7331517584639946
Iteration 10, Training loss = 0.2820432491365992
Iteration 20, Training loss = 0.2646686139969791
Iteration 30, Training loss = 0.25651192174408105
Iteration 40, Training loss = 0.25097212052879264
Iteration 50, Training loss = 0.24665349624491778
Iteration 60, Training loss = 0.24391378865511884
Iteration 70, Training loss = 0.2405828748610926
Iteration 80, Training loss = 0.23677842002407112
Iteration 90, Training loss = 0.23505964494574155
Model training time: 51.49278140068054
Device: cuda
Iteration 0, Training loss = 0.7975732955459244
Iteration 10, Training loss = 0.28006598979379016
Iteration 20, Training loss = 0.2599345754480247
Iteration 30, Training loss = 0.24993905320438864
Iteration 40, Training loss = 0.24332807822824967
Iteration 50, Training loss = 0.23930984046355286
Iteration 60, Training loss = 0.23492507807026475
Iteration 70, Training loss = 0.2330854434619227
Iteration 80, Training loss = 0.2303897292143496
Iteration 90, Training loss = 0.22887211149562936
Model training time: 51.594231367111206
Device: cuda
Iteration 0, Training loss = 0.7186659614918596
Iteration 10, Training loss = 0.2778270085033291
Iteration 20, Training loss = 0.25943653933144656
Iteration 30, Training loss = 0.2518669617414186
Iteration 40, Training loss = 0.24677297932977538
Iteration 50, Training loss = 0.24328559442236117
Iteration 60, Training loss = 0.24083396127632398
Iteration 70, Training loss = 0.23820420816432478
Iteration 80, Training loss = 0.23656007210040786
Iteration 90, Training loss = 0.2341159131314795
Model training time: 53.06645607948303
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0688469107589769
Iteration 10, Training loss = 0.9480168017723369
Iteration 20, Training loss = 0.6811134479669335
Iteration 30, Training loss = 0.38866924315907475
Iteration 40, Training loss = 0.35832117124489954
Iteration 50, Training loss = 0.34268313498387326
Iteration 60, Training loss = 0.3326944909610991
Iteration 70, Training loss = 0.32634446481310425
Iteration 80, Training loss = 0.32193266950349253
Iteration 90, Training loss = 0.31850329977764635
Model training time: 43.40419793128967
Device: cuda
Iteration 0, Training loss = 1.0131251090375333
Iteration 10, Training loss = 0.9818244663046867
Iteration 20, Training loss = 0.9290861057311512
Iteration 30, Training loss = 0.6944114744085954
Iteration 40, Training loss = 0.42074770875473577
Iteration 50, Training loss = 0.37139815752356165
Iteration 60, Training loss = 0.34915725684772103
Iteration 70, Training loss = 0.3354510719060321
Iteration 80, Training loss = 0.3269001276708111
Iteration 90, Training loss = 0.3211223581404432
Model training time: 43.027705669403076
Device: cuda
Iteration 0, Training loss = 0.9954174713707432
Iteration 10, Training loss = 0.9652560115149177
Iteration 20, Training loss = 0.7902597294304042
Iteration 30, Training loss = 0.4153558677896749
Iteration 40, Training loss = 0.3624213555462424
Iteration 50, Training loss = 0.3438188190105175
Iteration 60, Training loss = 0.3323257150033773
Iteration 70, Training loss = 0.32473903908856555
Iteration 80, Training loss = 0.3193651544557068
Iteration 90, Training loss = 0.31542873110162145
Model training time: 43.62731742858887
Device: cuda
Iteration 0, Training loss = 1.0440528193628529
Iteration 10, Training loss = 0.9846213876335154
Iteration 20, Training loss = 0.9394566855476786
Iteration 30, Training loss = 0.7078787654854772
Iteration 40, Training loss = 0.4002532191917336
Iteration 50, Training loss = 0.3596726134780244
Iteration 60, Training loss = 0.340457290843959
Iteration 70, Training loss = 0.3292468214366973
Iteration 80, Training loss = 0.3222898850645915
Iteration 90, Training loss = 0.3177161729076007
Model training time: 42.38716650009155
Device: cuda
Iteration 0, Training loss = 1.0094068610350675
Iteration 10, Training loss = 0.9974151716971225
Iteration 20, Training loss = 0.9862022169710072
Iteration 30, Training loss = 0.942658904461826
Iteration 40, Training loss = 0.6919130830152849
Iteration 50, Training loss = 0.39120381042253016
Iteration 60, Training loss = 0.3529784960839131
Iteration 70, Training loss = 0.33517448082746665
Iteration 80, Training loss = 0.32452261494810875
Iteration 90, Training loss = 0.3181376696607103
Model training time: 42.95832943916321
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6759318090235639
Iteration 10, Training loss = 0.28453703630562266
Iteration 20, Training loss = 0.26753934813758073
Iteration 30, Training loss = 0.2601657937535651
Iteration 40, Training loss = 0.25385529703075027
Iteration 50, Training loss = 0.24976478204960972
Iteration 60, Training loss = 0.24530077395514194
Iteration 70, Training loss = 0.2420695877428782
Iteration 80, Training loss = 0.239325168923662
Iteration 90, Training loss = 0.2366037143633383
Model training time: 51.303189754486084
Device: cuda
Iteration 0, Training loss = 0.7090929384381661
Iteration 10, Training loss = 0.27703530260206133
Iteration 20, Training loss = 0.2650869609404707
Iteration 30, Training loss = 0.25947702741002343
Iteration 40, Training loss = 0.25517181976366854
Iteration 50, Training loss = 0.25171744065769647
Iteration 60, Training loss = 0.24902214854053667
Iteration 70, Training loss = 0.24732493098653835
Iteration 80, Training loss = 0.24552116120915146
Iteration 90, Training loss = 0.24393353659939246
Model training time: 50.66491174697876
Device: cuda
Iteration 0, Training loss = 0.6858052947258545
Iteration 10, Training loss = 0.2787730977859249
Iteration 20, Training loss = 0.26196690073313494
Iteration 30, Training loss = 0.2551425318921449
Iteration 40, Training loss = 0.24966269771209929
Iteration 50, Training loss = 0.2455394087093217
Iteration 60, Training loss = 0.2422708242085309
Iteration 70, Training loss = 0.23856017210004404
Iteration 80, Training loss = 0.23508821670462088
Iteration 90, Training loss = 0.2324243188896566
Model training time: 51.92691111564636
Device: cuda
Iteration 0, Training loss = 0.7248729866440013
Iteration 10, Training loss = 0.2801626677158092
Iteration 20, Training loss = 0.26235487023436127
Iteration 30, Training loss = 0.2516510531522749
Iteration 40, Training loss = 0.246462884271116
Iteration 50, Training loss = 0.24249110719049236
Iteration 60, Training loss = 0.2394795032628512
Iteration 70, Training loss = 0.23690227493102556
Iteration 80, Training loss = 0.23412751549850364
Iteration 90, Training loss = 0.23151845208957755
Model training time: 51.94964122772217
Device: cuda
Iteration 0, Training loss = 0.6771681913452056
Iteration 10, Training loss = 0.2816026762091796
Iteration 20, Training loss = 0.26316586558957367
Iteration 30, Training loss = 0.2534196574520545
Iteration 40, Training loss = 0.24777306239267238
Iteration 50, Training loss = 0.2427071725936115
Iteration 60, Training loss = 0.2389919999761385
Iteration 70, Training loss = 0.23592379950798742
Iteration 80, Training loss = 0.23221760438011
Iteration 90, Training loss = 0.23037694013941373
Model training time: 51.603158950805664
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0371873110316279
Iteration 10, Training loss = 0.9548711746425952
Iteration 20, Training loss = 0.7204985506020793
Iteration 30, Training loss = 0.3968913608544098
Iteration 40, Training loss = 0.3604676958002132
Iteration 50, Training loss = 0.342970735080017
Iteration 60, Training loss = 0.3325364641827186
Iteration 70, Training loss = 0.3259742721987406
Iteration 80, Training loss = 0.32122164147457255
Iteration 90, Training loss = 0.3181050501168496
Model training time: 44.79051113128662
Device: cuda
Iteration 0, Training loss = 1.0572229770280548
Iteration 10, Training loss = 0.9831281676708065
Iteration 20, Training loss = 0.9190604701890783
Iteration 30, Training loss = 0.6007922064449828
Iteration 40, Training loss = 0.38537117528569037
Iteration 50, Training loss = 0.35791054710781894
Iteration 60, Training loss = 0.3430641448129753
Iteration 70, Training loss = 0.33404885488618663
Iteration 80, Training loss = 0.32811040847988454
Iteration 90, Training loss = 0.3238714010100676
Model training time: 46.33791732788086
Device: cuda
Iteration 0, Training loss = 0.9925048724358077
Iteration 10, Training loss = 0.8807815877635022
Iteration 20, Training loss = 0.4576619240042661
Iteration 30, Training loss = 0.360138513887477
Iteration 40, Training loss = 0.3412599539785639
Iteration 50, Training loss = 0.3301982347273942
Iteration 60, Training loss = 0.32340828917433506
Iteration 70, Training loss = 0.3185164417358923
Iteration 80, Training loss = 0.31527900197892733
Iteration 90, Training loss = 0.3125462486762689
Model training time: 47.81509590148926
Device: cuda
Iteration 0, Training loss = 1.0173779664691944
Iteration 10, Training loss = 0.973751160408625
Iteration 20, Training loss = 0.8213879852837569
Iteration 30, Training loss = 0.4280259129715312
Iteration 40, Training loss = 0.365170611884923
Iteration 50, Training loss = 0.3435049524968242
Iteration 60, Training loss = 0.33102696029960965
Iteration 70, Training loss = 0.32379017144297284
Iteration 80, Training loss = 0.3191105176223392
Iteration 90, Training loss = 0.315923644757444
Model training time: 46.83778429031372
Device: cuda
Iteration 0, Training loss = 1.0065719779698092
Iteration 10, Training loss = 0.9774056350492103
Iteration 20, Training loss = 0.8317065074426498
Iteration 30, Training loss = 0.4170250479158997
Iteration 40, Training loss = 0.36341045482972634
Iteration 50, Training loss = 0.3456676187620613
Iteration 60, Training loss = 0.3344126769654976
Iteration 70, Training loss = 0.3270564712854621
Iteration 80, Training loss = 0.3218777943177027
Iteration 90, Training loss = 0.31813704375781965
Model training time: 44.48409843444824
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7366081787223677
Iteration 10, Training loss = 0.2822068921914978
Iteration 20, Training loss = 0.2640993670350703
Iteration 30, Training loss = 0.2549748294524506
Iteration 40, Training loss = 0.24889320871154563
Iteration 50, Training loss = 0.24423980045405197
Iteration 60, Training loss = 0.24030141435906327
Iteration 70, Training loss = 0.23694221413813837
Iteration 80, Training loss = 0.2327838134624912
Iteration 90, Training loss = 0.22932377511739155
Iteration 100, Training loss = 0.22567268498873305
Iteration 110, Training loss = 0.22263430611886642
Iteration 120, Training loss = 0.22000985745042922
Iteration 130, Training loss = 0.2180339297320306
Iteration 140, Training loss = 0.21578836928124
Iteration 150, Training loss = 0.21383942085482885
Iteration 160, Training loss = 0.2127700257081241
Iteration 170, Training loss = 0.21173386234275943
Iteration 180, Training loss = 0.2098242637248074
Iteration 190, Training loss = 0.20853547012257517
Model training time: 103.2451024055481
Device: cuda
Iteration 0, Training loss = 0.6980495105427633
Iteration 10, Training loss = 0.27773892239060877
Iteration 20, Training loss = 0.25870686939062854
Iteration 30, Training loss = 0.2502466038987943
Iteration 40, Training loss = 0.24565326640738702
Iteration 50, Training loss = 0.24086954184868722
Iteration 60, Training loss = 0.23794883231634667
Iteration 70, Training loss = 0.23456930393492917
Iteration 80, Training loss = 0.23192921403533898
Iteration 90, Training loss = 0.22890503316684727
Iteration 100, Training loss = 0.22575584310163307
Iteration 110, Training loss = 0.22287784388726328
Iteration 120, Training loss = 0.22022364215505902
Iteration 130, Training loss = 0.2184794308126117
Iteration 140, Training loss = 0.21710906349140566
Iteration 150, Training loss = 0.2151102668382498
Iteration 160, Training loss = 0.21328011048691614
Iteration 170, Training loss = 0.21230955058415635
Iteration 180, Training loss = 0.21071328389622976
Iteration 190, Training loss = 0.2094650577853003
Model training time: 103.50796818733215
Device: cuda
Iteration 0, Training loss = 0.6681691556086552
Iteration 10, Training loss = 0.2763681719688469
Iteration 20, Training loss = 0.2622078199742348
Iteration 30, Training loss = 0.2509954520580122
Iteration 40, Training loss = 0.24266743853002715
Iteration 50, Training loss = 0.23818007221594273
Iteration 60, Training loss = 0.23455179778556848
Iteration 70, Training loss = 0.23163862585949263
Iteration 80, Training loss = 0.2287713971719615
Iteration 90, Training loss = 0.22560439439215324
Iteration 100, Training loss = 0.22273879723745166
Iteration 110, Training loss = 0.22040003458755067
Iteration 120, Training loss = 0.2185203094513451
Iteration 130, Training loss = 0.21612057576889565
Iteration 140, Training loss = 0.21512406295025607
Iteration 150, Training loss = 0.2128372511242406
Iteration 160, Training loss = 0.21073631284708838
Iteration 170, Training loss = 0.20930392104798598
Iteration 180, Training loss = 0.20816131717284136
Iteration 190, Training loss = 0.206438054824496
Model training time: 102.7468056678772
Device: cuda
Iteration 0, Training loss = 0.6423796453213287
Iteration 10, Training loss = 0.28180433409265976
Iteration 20, Training loss = 0.26497146781290415
Iteration 30, Training loss = 0.257029305124254
Iteration 40, Training loss = 0.25202467663272243
Iteration 50, Training loss = 0.24666502687529848
Iteration 60, Training loss = 0.24140388152356876
Iteration 70, Training loss = 0.2378085878007926
Iteration 80, Training loss = 0.234223344460238
Iteration 90, Training loss = 0.23140652117840313
Iteration 100, Training loss = 0.22858577302812663
Iteration 110, Training loss = 0.2267668085132182
Iteration 120, Training loss = 0.22485955656657208
Iteration 130, Training loss = 0.22347018360802973
Iteration 140, Training loss = 0.22184413926761606
Iteration 150, Training loss = 0.2196501316840943
Iteration 160, Training loss = 0.21856067249366504
Iteration 170, Training loss = 0.21620980282167546
Iteration 180, Training loss = 0.2147612826894184
Iteration 190, Training loss = 0.2131416063945172
Model training time: 104.04710006713867
Device: cuda
Iteration 0, Training loss = 0.7238646259319407
Iteration 10, Training loss = 0.2712226599346639
Iteration 20, Training loss = 0.25890499165791286
Iteration 30, Training loss = 0.2509233255579743
Iteration 40, Training loss = 0.24584763658573494
Iteration 50, Training loss = 0.24090963454136838
Iteration 60, Training loss = 0.23675837676259565
Iteration 70, Training loss = 0.2319810376942302
Iteration 80, Training loss = 0.22824130321855118
Iteration 90, Training loss = 0.2252630571728757
Iteration 100, Training loss = 0.22168519747495363
Iteration 110, Training loss = 0.21957940193339642
Iteration 120, Training loss = 0.21710360612151985
Iteration 130, Training loss = 0.2148288301943145
Iteration 140, Training loss = 0.21269446510379597
Iteration 150, Training loss = 0.21164328511127836
Iteration 160, Training loss = 0.20947752538482156
Iteration 170, Training loss = 0.20844491933794923
Iteration 180, Training loss = 0.2074106937910252
Iteration 190, Training loss = 0.20604051591841996
Model training time: 104.96783089637756
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0013832694537415
Iteration 10, Training loss = 0.9745104161359496
Iteration 20, Training loss = 0.8464740454861962
Iteration 30, Training loss = 0.45560391568387104
Iteration 40, Training loss = 0.37766475943041194
Iteration 50, Training loss = 0.3548873185967129
Iteration 60, Training loss = 0.34112658786427313
Iteration 70, Training loss = 0.33225901758627513
Iteration 80, Training loss = 0.32632304811015833
Iteration 90, Training loss = 0.32193907173508307
Iteration 100, Training loss = 0.31882807545670583
Iteration 110, Training loss = 0.3160844872058448
Iteration 120, Training loss = 0.3138350924481491
Iteration 130, Training loss = 0.31212602739761297
Iteration 140, Training loss = 0.31033503143319785
Iteration 150, Training loss = 0.30878117998056204
Iteration 160, Training loss = 0.30750945580279854
Iteration 170, Training loss = 0.3061213976281896
Iteration 180, Training loss = 0.3048759104805766
Iteration 190, Training loss = 0.30360178165949575
Model training time: 87.9062922000885
Device: cuda
Iteration 0, Training loss = 1.0154394972483125
Iteration 10, Training loss = 0.9826366333493887
Iteration 20, Training loss = 0.9118600310915608
Iteration 30, Training loss = 0.5481700651149195
Iteration 40, Training loss = 0.38243722843488825
Iteration 50, Training loss = 0.3602891156661886
Iteration 60, Training loss = 0.3467561499402829
Iteration 70, Training loss = 0.33752173677844516
Iteration 80, Training loss = 0.33130217666487427
Iteration 90, Training loss = 0.32714283644936565
Iteration 100, Training loss = 0.32361282016766274
Iteration 110, Training loss = 0.320707934802583
Iteration 120, Training loss = 0.3182889272167954
Iteration 130, Training loss = 0.31627717841479736
Iteration 140, Training loss = 0.3143549990314837
Iteration 150, Training loss = 0.312571674044138
Iteration 160, Training loss = 0.3109392894129776
Iteration 170, Training loss = 0.3095477387020432
Iteration 180, Training loss = 0.30804060753119195
Iteration 190, Training loss = 0.3067954108840617
Model training time: 90.2587263584137
Device: cuda
Iteration 0, Training loss = 1.009424033450734
Iteration 10, Training loss = 0.9875283552284102
Iteration 20, Training loss = 0.9328175928921734
Iteration 30, Training loss = 0.5824912089051693
Iteration 40, Training loss = 0.3709957304683494
Iteration 50, Training loss = 0.34901695465781785
Iteration 60, Training loss = 0.33615002036094666
Iteration 70, Training loss = 0.3282192275469297
Iteration 80, Training loss = 0.3231792423792959
Iteration 90, Training loss = 0.3195802087356623
Iteration 100, Training loss = 0.3168187389290073
Iteration 110, Training loss = 0.31461887009279493
Iteration 120, Training loss = 0.31273554720110813
Iteration 130, Training loss = 0.31109337003554327
Iteration 140, Training loss = 0.3096393237102407
Iteration 150, Training loss = 0.30788201988464986
Iteration 160, Training loss = 0.30666345071056567
Iteration 170, Training loss = 0.3052376119767205
Iteration 180, Training loss = 0.30405235205738657
Iteration 190, Training loss = 0.30294785134440183
Model training time: 89.64029026031494
Device: cuda
Iteration 0, Training loss = 1.0143943231994823
Iteration 10, Training loss = 0.9754398788435984
Iteration 20, Training loss = 0.8652536529172707
Iteration 30, Training loss = 0.4502072436396781
Iteration 40, Training loss = 0.3646688627092659
Iteration 50, Training loss = 0.3472422518683981
Iteration 60, Training loss = 0.3362678643361131
Iteration 70, Training loss = 0.32935370499444067
Iteration 80, Training loss = 0.3245059784510811
Iteration 90, Training loss = 0.32099283908744125
Iteration 100, Training loss = 0.31829196315700725
Iteration 110, Training loss = 0.31604377319247035
Iteration 120, Training loss = 0.3140519510314193
Iteration 130, Training loss = 0.31223431031149756
Iteration 140, Training loss = 0.31053672792259196
Iteration 150, Training loss = 0.3091096387359767
Iteration 160, Training loss = 0.30765368968031886
Iteration 170, Training loss = 0.30618775424888
Iteration 180, Training loss = 0.3047989033180634
Iteration 190, Training loss = 0.30348811525914626
Model training time: 88.3570008277893
Device: cuda
Iteration 0, Training loss = 1.0069866493596868
Iteration 10, Training loss = 0.9816469363958438
Iteration 20, Training loss = 0.8781729995049807
Iteration 30, Training loss = 0.45726447103242895
Iteration 40, Training loss = 0.35908606044100216
Iteration 50, Training loss = 0.3404265031867974
Iteration 60, Training loss = 0.3297636515255702
Iteration 70, Training loss = 0.32356535518862145
Iteration 80, Training loss = 0.31957501986630027
Iteration 90, Training loss = 0.31687520776331857
Iteration 100, Training loss = 0.3146759314737655
Iteration 110, Training loss = 0.31288875753091555
Iteration 120, Training loss = 0.3112060650641924
Iteration 130, Training loss = 0.30998493449595593
Iteration 140, Training loss = 0.30841173058416305
Iteration 150, Training loss = 0.3071203879813594
Iteration 160, Training loss = 0.30576579528917125
Iteration 170, Training loss = 0.3044696634062555
Iteration 180, Training loss = 0.3033600190505566
Iteration 190, Training loss = 0.30206578702961273
Model training time: 87.28836917877197
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6593297330649078
Iteration 10, Training loss = 0.28815562178811494
Iteration 20, Training loss = 0.2673373840406501
Iteration 30, Training loss = 0.25988368978656234
Iteration 40, Training loss = 0.25160455824604333
Iteration 50, Training loss = 0.24639818629759563
Iteration 60, Training loss = 0.24278440079302246
Iteration 70, Training loss = 0.23980053034374268
Iteration 80, Training loss = 0.23680762593018806
Iteration 90, Training loss = 0.23472398633168914
Iteration 100, Training loss = 0.2315942573994759
Iteration 110, Training loss = 0.22989714814949844
Iteration 120, Training loss = 0.22758744174465717
Iteration 130, Training loss = 0.22474671789325468
Iteration 140, Training loss = 0.22264240659574908
Iteration 150, Training loss = 0.22083935070304836
Iteration 160, Training loss = 0.2187427917256338
Iteration 170, Training loss = 0.2177141992325644
Iteration 180, Training loss = 0.21680977801650259
Iteration 190, Training loss = 0.21609342597948147
Model training time: 106.17475962638855
Device: cuda
Iteration 0, Training loss = 0.7750067319766084
Iteration 10, Training loss = 0.28252581858750403
Iteration 20, Training loss = 0.2654614875918439
Iteration 30, Training loss = 0.2600676645739027
Iteration 40, Training loss = 0.2561483688778796
Iteration 50, Training loss = 0.25271635103889584
Iteration 60, Training loss = 0.24944485631967572
Iteration 70, Training loss = 0.24780834282209452
Iteration 80, Training loss = 0.24574306459317197
Iteration 90, Training loss = 0.24402290469003937
Iteration 100, Training loss = 0.24224107422060887
Iteration 110, Training loss = 0.24114562008196158
Iteration 120, Training loss = 0.23947896300555718
Iteration 130, Training loss = 0.2381433211357195
Iteration 140, Training loss = 0.23692462277614465
Iteration 150, Training loss = 0.23628107195868908
Iteration 160, Training loss = 0.23494223185947963
Iteration 170, Training loss = 0.23450532721549489
Iteration 180, Training loss = 0.2335641967182298
Iteration 190, Training loss = 0.2327432326756003
Model training time: 103.85759735107422
Device: cuda
Iteration 0, Training loss = 0.7222430454183721
Iteration 10, Training loss = 0.279188603595729
Iteration 20, Training loss = 0.26127572902467
Iteration 30, Training loss = 0.25095911715495384
Iteration 40, Training loss = 0.24389413598086007
Iteration 50, Training loss = 0.23940197520914147
Iteration 60, Training loss = 0.23541473380806371
Iteration 70, Training loss = 0.2324260411193238
Iteration 80, Training loss = 0.228973112765871
Iteration 90, Training loss = 0.2261474931896743
Iteration 100, Training loss = 0.22386313376826467
Iteration 110, Training loss = 0.22162402517750823
Iteration 120, Training loss = 0.2195090847608541
Iteration 130, Training loss = 0.21740501061009726
Iteration 140, Training loss = 0.21630704365941283
Iteration 150, Training loss = 0.21494092084196809
Iteration 160, Training loss = 0.21362751856734907
Iteration 170, Training loss = 0.2123121722955392
Iteration 180, Training loss = 0.2107967709982655
Iteration 190, Training loss = 0.2101159092426733
Model training time: 104.47934889793396
Device: cuda
Iteration 0, Training loss = 0.6887206732865974
Iteration 10, Training loss = 0.2774430055883837
Iteration 20, Training loss = 0.26166236581005714
Iteration 30, Training loss = 0.25681951223505034
Iteration 40, Training loss = 0.25365629900454323
Iteration 50, Training loss = 0.2513533719695798
Iteration 60, Training loss = 0.24891867481406607
Iteration 70, Training loss = 0.2458545434453297
Iteration 80, Training loss = 0.24316265422300623
Iteration 90, Training loss = 0.24037757778391422
Iteration 100, Training loss = 0.23713526228943402
Iteration 110, Training loss = 0.23482382005265492
Iteration 120, Training loss = 0.23284513207454657
Iteration 130, Training loss = 0.23075414939668507
Iteration 140, Training loss = 0.22857397787507452
Iteration 150, Training loss = 0.22651942829278998
Iteration 160, Training loss = 0.22418213931541467
Iteration 170, Training loss = 0.22250010590937178
Iteration 180, Training loss = 0.22017522941886947
Iteration 190, Training loss = 0.21883340977294682
Model training time: 104.78622221946716
Device: cuda
Iteration 0, Training loss = 0.7195860726925709
Iteration 10, Training loss = 0.27805021830967497
Iteration 20, Training loss = 0.26261096003918616
Iteration 30, Training loss = 0.25687638340071384
Iteration 40, Training loss = 0.25077661076339625
Iteration 50, Training loss = 0.2449220847924743
Iteration 60, Training loss = 0.24062545184433892
Iteration 70, Training loss = 0.2370829094769591
Iteration 80, Training loss = 0.23448649611657815
Iteration 90, Training loss = 0.23165568983583704
Iteration 100, Training loss = 0.22946088228004896
Iteration 110, Training loss = 0.22720691361453285
Iteration 120, Training loss = 0.22519990837891513
Iteration 130, Training loss = 0.22376794921861146
Iteration 140, Training loss = 0.2219280866549754
Iteration 150, Training loss = 0.22013203672541545
Iteration 160, Training loss = 0.2186548870246289
Iteration 170, Training loss = 0.21762695572097712
Iteration 180, Training loss = 0.21679704145643383
Iteration 190, Training loss = 0.21539489668737602
Model training time: 104.58819627761841
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.010476137982731
Iteration 10, Training loss = 0.9824810492068745
Iteration 20, Training loss = 0.8837264739427959
Iteration 30, Training loss = 0.47804561975360205
Iteration 40, Training loss = 0.3643435228065775
Iteration 50, Training loss = 0.3434370797822031
Iteration 60, Training loss = 0.3313614358769202
Iteration 70, Training loss = 0.32401170291132847
Iteration 80, Training loss = 0.31911005631774736
Iteration 90, Training loss = 0.31589481039716893
Iteration 100, Training loss = 0.313450643860399
Iteration 110, Training loss = 0.3115407022812176
Iteration 120, Training loss = 0.30987028151606244
Iteration 130, Training loss = 0.30839435424267814
Iteration 140, Training loss = 0.3071470722447873
Iteration 150, Training loss = 0.3059444113591681
Iteration 160, Training loss = 0.3047350605196583
Iteration 170, Training loss = 0.3036794770175262
Iteration 180, Training loss = 0.30282097268335467
Iteration 190, Training loss = 0.30178225238877404
Model training time: 88.10934281349182
Device: cuda
Iteration 0, Training loss = 1.0554242699111633
Iteration 10, Training loss = 0.9929496075858792
Iteration 20, Training loss = 0.9768885578283675
Iteration 30, Training loss = 0.9139599217773927
Iteration 40, Training loss = 0.62175758247947
Iteration 50, Training loss = 0.3889779929820331
Iteration 60, Training loss = 0.3603845688426466
Iteration 70, Training loss = 0.3445614285893359
Iteration 80, Training loss = 0.3340283176403935
Iteration 90, Training loss = 0.3271726072916973
Iteration 100, Training loss = 0.3221731239672723
Iteration 110, Training loss = 0.31857722580938014
Iteration 120, Training loss = 0.3157249567150781
Iteration 130, Training loss = 0.3133557600670519
Iteration 140, Training loss = 0.3113439346305106
Iteration 150, Training loss = 0.3095589009887081
Iteration 160, Training loss = 0.30799841601176065
Iteration 170, Training loss = 0.30652376506071694
Iteration 180, Training loss = 0.3051750920622747
Iteration 190, Training loss = 0.30400056112332147
Model training time: 88.46282982826233
Device: cuda
Iteration 0, Training loss = 1.017827150175127
Iteration 10, Training loss = 0.9851310592298069
Iteration 20, Training loss = 0.9376926079356352
Iteration 30, Training loss = 0.7002518273727657
Iteration 40, Training loss = 0.41387312269384
Iteration 50, Training loss = 0.3723357865726688
Iteration 60, Training loss = 0.3522195743518649
Iteration 70, Training loss = 0.33915043422009694
Iteration 80, Training loss = 0.3302989568173452
Iteration 90, Training loss = 0.3240105583181393
Iteration 100, Training loss = 0.319590059341299
Iteration 110, Training loss = 0.31590086140514284
Iteration 120, Training loss = 0.31315335849656317
Iteration 130, Training loss = 0.31076679775631166
Iteration 140, Training loss = 0.3087096003405118
Iteration 150, Training loss = 0.3067547780513475
Iteration 160, Training loss = 0.3052499680159744
Iteration 170, Training loss = 0.3035149747645595
Iteration 180, Training loss = 0.3021089419830798
Iteration 190, Training loss = 0.3006098433769788
Model training time: 98.3134412765503
Device: cuda
Iteration 0, Training loss = 1.103278585770517
Iteration 10, Training loss = 0.9935272755980781
Iteration 20, Training loss = 0.9751291153794628
Iteration 30, Training loss = 0.8770304986120137
Iteration 40, Training loss = 0.4927465857951462
Iteration 50, Training loss = 0.3701073434609766
Iteration 60, Training loss = 0.3452761292854464
Iteration 70, Training loss = 0.33151081926020237
Iteration 80, Training loss = 0.3229427502713827
Iteration 90, Training loss = 0.31703147602427667
Iteration 100, Training loss = 0.312860003774016
Iteration 110, Training loss = 0.3095497135749452
Iteration 120, Training loss = 0.3068849896295019
Iteration 130, Training loss = 0.30461202917246205
Iteration 140, Training loss = 0.3026141367726407
Iteration 150, Training loss = 0.30083157215943923
Iteration 160, Training loss = 0.29902448348213917
Iteration 170, Training loss = 0.2974518289108542
Iteration 180, Training loss = 0.2960658252203147
Iteration 190, Training loss = 0.29448684777604467
Model training time: 93.81245255470276
Device: cuda
Iteration 0, Training loss = 1.0068716033030365
Iteration 10, Training loss = 0.9936860968645203
Iteration 20, Training loss = 0.9614977472919529
Iteration 30, Training loss = 0.7465540973095282
Iteration 40, Training loss = 0.39213757837511437
Iteration 50, Training loss = 0.3532286587083311
Iteration 60, Training loss = 0.33666851118964664
Iteration 70, Training loss = 0.3271745644491752
Iteration 80, Training loss = 0.32135799099616796
Iteration 90, Training loss = 0.31762085497812265
Iteration 100, Training loss = 0.3145312695035634
Iteration 110, Training loss = 0.3123401627485746
Iteration 120, Training loss = 0.3103387487317113
Iteration 130, Training loss = 0.3086872171006249
Iteration 140, Training loss = 0.3072429800870632
Iteration 150, Training loss = 0.3060990894713933
Iteration 160, Training loss = 0.3045941766793445
Iteration 170, Training loss = 0.30366818267698725
Iteration 180, Training loss = 0.3021823908348926
Iteration 190, Training loss = 0.3011345531223184
Model training time: 88.69606852531433
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.667265438138717
Iteration 10, Training loss = 0.28395786388950833
Iteration 20, Training loss = 0.26551878926756883
Iteration 30, Training loss = 0.2580439579688897
Iteration 40, Training loss = 0.25239253321946675
Iteration 50, Training loss = 0.24779134451188417
Iteration 60, Training loss = 0.24426602588438814
Iteration 70, Training loss = 0.24079459155154287
Iteration 80, Training loss = 0.23794096652373276
Iteration 90, Training loss = 0.2357755838634027
Iteration 100, Training loss = 0.2332770913550698
Iteration 110, Training loss = 0.23136438625993222
Iteration 120, Training loss = 0.2289633807651789
Iteration 130, Training loss = 0.22677733072645728
Iteration 140, Training loss = 0.22521221015340767
Iteration 150, Training loss = 0.22328864707280013
Iteration 160, Training loss = 0.2221166657748436
Iteration 170, Training loss = 0.2207821096151562
Iteration 180, Training loss = 0.21931477878738836
Iteration 190, Training loss = 0.21798903964117133
Model training time: 104.82741045951843
Device: cuda
Iteration 0, Training loss = 0.6867878968865762
Iteration 10, Training loss = 0.28289212053754426
Iteration 20, Training loss = 0.2721722066113793
Iteration 30, Training loss = 0.26537382153491995
Iteration 40, Training loss = 0.260859610057339
Iteration 50, Training loss = 0.2576872908282511
Iteration 60, Training loss = 0.2546560328462799
Iteration 70, Training loss = 0.25134584646825353
Iteration 80, Training loss = 0.24853046759782635
Iteration 90, Training loss = 0.24610054395533648
Iteration 100, Training loss = 0.24357664184622269
Iteration 110, Training loss = 0.24176999747969624
Iteration 120, Training loss = 0.23992322426803464
Iteration 130, Training loss = 0.23792118595385378
Iteration 140, Training loss = 0.2361431023507372
Iteration 150, Training loss = 0.23424107036564598
Iteration 160, Training loss = 0.23293322455543872
Iteration 170, Training loss = 0.23211480540022722
Iteration 180, Training loss = 0.23058679768450324
Iteration 190, Training loss = 0.22853154077512589
Model training time: 102.38233709335327
Device: cuda
Iteration 0, Training loss = 0.6469085934880859
Iteration 10, Training loss = 0.28106770110621004
Iteration 20, Training loss = 0.2688585938141652
Iteration 30, Training loss = 0.2617350408478164
Iteration 40, Training loss = 0.25657326661357
Iteration 50, Training loss = 0.25191985142866
Iteration 60, Training loss = 0.2482907986020349
Iteration 70, Training loss = 0.24603183605408263
Iteration 80, Training loss = 0.24474854328946974
Iteration 90, Training loss = 0.24276978987974923
Iteration 100, Training loss = 0.24107012582027307
Iteration 110, Training loss = 0.2377924987861377
Iteration 120, Training loss = 0.2342040108981346
Iteration 130, Training loss = 0.23208275330918177
Iteration 140, Training loss = 0.23039194970384927
Iteration 150, Training loss = 0.22863106727239293
Iteration 160, Training loss = 0.22709559748449856
Iteration 170, Training loss = 0.2250375330863074
Iteration 180, Training loss = 0.22485993216242686
Iteration 190, Training loss = 0.2229378369183575
Model training time: 104.27524089813232
Device: cuda
Iteration 0, Training loss = 0.6721034476096058
Iteration 10, Training loss = 0.2792631711892008
Iteration 20, Training loss = 0.26348418468208057
Iteration 30, Training loss = 0.2550831657651695
Iteration 40, Training loss = 0.2482901395954751
Iteration 50, Training loss = 0.24358961633778659
Iteration 60, Training loss = 0.2402129027875226
Iteration 70, Training loss = 0.23713281927472454
Iteration 80, Training loss = 0.2345873218832524
Iteration 90, Training loss = 0.23145715869764727
Iteration 100, Training loss = 0.22907691742981318
Iteration 110, Training loss = 0.2264492176743743
Iteration 120, Training loss = 0.2242868739308803
Iteration 130, Training loss = 0.22231442494809484
Iteration 140, Training loss = 0.22096892823318592
Iteration 150, Training loss = 0.21968245048066895
Iteration 160, Training loss = 0.21696365474932996
Iteration 170, Training loss = 0.21623271245500367
Iteration 180, Training loss = 0.21491787388704303
Iteration 190, Training loss = 0.21380826949928922
Model training time: 106.22129392623901
Device: cuda
Iteration 0, Training loss = 0.7076822880826908
Iteration 10, Training loss = 0.27848760193759536
Iteration 20, Training loss = 0.26204732042392287
Iteration 30, Training loss = 0.25466307129972493
Iteration 40, Training loss = 0.2484626103075307
Iteration 50, Training loss = 0.24530522559586795
Iteration 60, Training loss = 0.24225174420970982
Iteration 70, Training loss = 0.24017395989261298
Iteration 80, Training loss = 0.23767924081471006
Iteration 90, Training loss = 0.23580598381977105
Iteration 100, Training loss = 0.234899285982293
Iteration 110, Training loss = 0.23281181588335995
Iteration 120, Training loss = 0.23176590443451237
Iteration 130, Training loss = 0.2300644307137979
Iteration 140, Training loss = 0.22944408764309512
Iteration 150, Training loss = 0.22801326558931975
Iteration 160, Training loss = 0.22634741675658895
Iteration 170, Training loss = 0.22495291832176018
Iteration 180, Training loss = 0.2224772280627821
Iteration 190, Training loss = 0.22019719405519184
Model training time: 104.0065324306488
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0048187593715243
Iteration 10, Training loss = 0.9871159527117057
Iteration 20, Training loss = 0.9421854075450297
Iteration 30, Training loss = 0.7032575449268119
Iteration 40, Training loss = 0.39836582501777434
Iteration 50, Training loss = 0.35837669645325615
Iteration 60, Training loss = 0.33967058614942697
Iteration 70, Training loss = 0.32896501706241116
Iteration 80, Training loss = 0.3224768941865706
Iteration 90, Training loss = 0.31795582420600815
Iteration 100, Training loss = 0.3146050255429947
Iteration 110, Training loss = 0.3119808850724241
Iteration 120, Training loss = 0.30971017220118435
Iteration 130, Training loss = 0.30762604245189895
Iteration 140, Training loss = 0.3058201746956488
Iteration 150, Training loss = 0.3041284194184562
Iteration 160, Training loss = 0.3024976100348676
Iteration 170, Training loss = 0.30101061648447924
Iteration 180, Training loss = 0.2995162726747499
Iteration 190, Training loss = 0.2983345700017476
Model training time: 87.28649091720581
Device: cuda
Iteration 0, Training loss = 1.0063110879345032
Iteration 10, Training loss = 0.9891754699099727
Iteration 20, Training loss = 0.9500110118354492
Iteration 30, Training loss = 0.7234116167768151
Iteration 40, Training loss = 0.4029264692713216
Iteration 50, Training loss = 0.3677120348010167
Iteration 60, Training loss = 0.350077642799867
Iteration 70, Training loss = 0.3388372631721
Iteration 80, Training loss = 0.3308513800074633
Iteration 90, Training loss = 0.3252084003558459
Iteration 100, Training loss = 0.3213307510060202
Iteration 110, Training loss = 0.3180091227370948
Iteration 120, Training loss = 0.31539129963726453
Iteration 130, Training loss = 0.3129926405612551
Iteration 140, Training loss = 0.3108536795900174
Iteration 150, Training loss = 0.30880443906957245
Iteration 160, Training loss = 0.30691181417311075
Iteration 170, Training loss = 0.3051154067960836
Iteration 180, Training loss = 0.30335380530241907
Iteration 190, Training loss = 0.3017280092395247
Model training time: 87.92971849441528
Device: cuda
Iteration 0, Training loss = 0.9972388900942721
Iteration 10, Training loss = 0.9766250727540355
Iteration 20, Training loss = 0.8613236108501656
Iteration 30, Training loss = 0.43978529071692407
Iteration 40, Training loss = 0.3619937642362447
Iteration 50, Training loss = 0.34240680254689143
Iteration 60, Training loss = 0.33113971336341075
Iteration 70, Training loss = 0.3244283075194093
Iteration 80, Training loss = 0.32000763945227384
Iteration 90, Training loss = 0.3168221340305003
Iteration 100, Training loss = 0.31445142783711666
Iteration 110, Training loss = 0.3125191420388857
Iteration 120, Training loss = 0.31077052869317606
Iteration 130, Training loss = 0.30929414040384223
Iteration 140, Training loss = 0.307831506950659
Iteration 150, Training loss = 0.3066589583532285
Iteration 160, Training loss = 0.3055418711020352
Iteration 170, Training loss = 0.30408907800768537
Iteration 180, Training loss = 0.3030496697881897
Iteration 190, Training loss = 0.30190068400367986
Model training time: 87.59324359893799
Device: cuda
Iteration 0, Training loss = 1.0076386375086648
Iteration 10, Training loss = 0.9793645603893456
Iteration 20, Training loss = 0.8706309906506943
Iteration 30, Training loss = 0.4603448272040046
Iteration 40, Training loss = 0.3676474459162636
Iteration 50, Training loss = 0.34629024821389964
Iteration 60, Training loss = 0.33348764354683297
Iteration 70, Training loss = 0.32601639662686616
Iteration 80, Training loss = 0.32104894524264277
Iteration 90, Training loss = 0.3174350402942581
Iteration 100, Training loss = 0.31459457001515795
Iteration 110, Training loss = 0.31211374000111736
Iteration 120, Training loss = 0.30991322312675434
Iteration 130, Training loss = 0.3080140548165139
Iteration 140, Training loss = 0.30624113594504304
Iteration 150, Training loss = 0.30452784807500194
Iteration 160, Training loss = 0.3030971536372245
Iteration 170, Training loss = 0.30165177747859795
Iteration 180, Training loss = 0.3001975885459355
Iteration 190, Training loss = 0.29895876589756615
Model training time: 89.30747628211975
Device: cuda
Iteration 0, Training loss = 1.0079127881342216
Iteration 10, Training loss = 0.9802436112058653
Iteration 20, Training loss = 0.8923815084716021
Iteration 30, Training loss = 0.4928454496237037
Iteration 40, Training loss = 0.3634864628387133
Iteration 50, Training loss = 0.34403153052485885
Iteration 60, Training loss = 0.3319115193141286
Iteration 70, Training loss = 0.3242933849553796
Iteration 80, Training loss = 0.3191723867024117
Iteration 90, Training loss = 0.3155225040188136
Iteration 100, Training loss = 0.31279242980422467
Iteration 110, Training loss = 0.3102976966459872
Iteration 120, Training loss = 0.3082937271340996
Iteration 130, Training loss = 0.30644350203056314
Iteration 140, Training loss = 0.3047380266264622
Iteration 150, Training loss = 0.30306019551023733
Iteration 160, Training loss = 0.3016705057306382
Iteration 170, Training loss = 0.3001951459461494
Iteration 180, Training loss = 0.29857605479242727
Iteration 190, Training loss = 0.2972571462140245
Model training time: 88.7514238357544
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7305415959861896
Iteration 10, Training loss = 0.2784135923799822
Iteration 20, Training loss = 0.26345616582663817
Iteration 30, Training loss = 0.25687507130476234
Iteration 40, Training loss = 0.2519646430383583
Iteration 50, Training loss = 0.24776266392581978
Iteration 60, Training loss = 0.24187393513560007
Iteration 70, Training loss = 0.23870803765754145
Iteration 80, Training loss = 0.2356739970009783
Iteration 90, Training loss = 0.233056792227177
Iteration 100, Training loss = 0.23004500934128033
Iteration 110, Training loss = 0.2271739597506731
Iteration 120, Training loss = 0.2243691177511042
Iteration 130, Training loss = 0.22234053278373458
Iteration 140, Training loss = 0.22021124982805
Iteration 150, Training loss = 0.21768782921045224
Iteration 160, Training loss = 0.21596174170296936
Iteration 170, Training loss = 0.2149987422272599
Iteration 180, Training loss = 0.21286512804882868
Iteration 190, Training loss = 0.21172047245877706
Iteration 200, Training loss = 0.21032464531302164
Iteration 210, Training loss = 0.2088979478501523
Iteration 220, Training loss = 0.20744818477884622
Iteration 230, Training loss = 0.20628876243571392
Iteration 240, Training loss = 0.20625818233946044
Iteration 250, Training loss = 0.2049285320623735
Iteration 260, Training loss = 0.2039608797532017
Iteration 270, Training loss = 0.20334549600903404
Iteration 280, Training loss = 0.20390284464629163
Iteration 290, Training loss = 0.20246989686790737
Model training time: 157.37434577941895
Device: cuda
Iteration 0, Training loss = 0.8082736803457754
Iteration 10, Training loss = 0.2827206157671049
Iteration 20, Training loss = 0.2628486146000338
Iteration 30, Training loss = 0.2530827419268594
Iteration 40, Training loss = 0.24742094739441722
Iteration 50, Training loss = 0.24218738454306096
Iteration 60, Training loss = 0.2368550957079084
Iteration 70, Training loss = 0.2332558567854452
Iteration 80, Training loss = 0.2283487502783176
Iteration 90, Training loss = 0.2237839934756623
Iteration 100, Training loss = 0.2209940366717574
Iteration 110, Training loss = 0.21932158797762874
Iteration 120, Training loss = 0.21704234132322214
Iteration 130, Training loss = 0.21559308218537462
Iteration 140, Training loss = 0.2132877876246957
Iteration 150, Training loss = 0.21112664778588325
Iteration 160, Training loss = 0.21004139687911072
Iteration 170, Training loss = 0.20785657413863096
Iteration 180, Training loss = 0.20675680432784355
Iteration 190, Training loss = 0.2054598088631041
Iteration 200, Training loss = 0.20551653753866872
Iteration 210, Training loss = 0.20482127854632118
Iteration 220, Training loss = 0.20446605711598084
Iteration 230, Training loss = 0.20294420875735203
Iteration 240, Training loss = 0.20226366001634563
Iteration 250, Training loss = 0.20176321563038063
Iteration 260, Training loss = 0.20125390009246497
Iteration 270, Training loss = 0.20093817028417715
Iteration 280, Training loss = 0.20010747896737394
Iteration 290, Training loss = 0.19955321118127348
Model training time: 157.6225709915161
Device: cuda
Iteration 0, Training loss = 0.6359672465566861
Iteration 10, Training loss = 0.28114564415545495
Iteration 20, Training loss = 0.26261990456041356
Iteration 30, Training loss = 0.2549939428886836
Iteration 40, Training loss = 0.2510719354338854
Iteration 50, Training loss = 0.24723773352242556
Iteration 60, Training loss = 0.24278357426566016
Iteration 70, Training loss = 0.23910281897384952
Iteration 80, Training loss = 0.23365505874950718
Iteration 90, Training loss = 0.23078045196668864
Iteration 100, Training loss = 0.22789627874662455
Iteration 110, Training loss = 0.2261319465520307
Iteration 120, Training loss = 0.2230997847857689
Iteration 130, Training loss = 0.22175367470587137
Iteration 140, Training loss = 0.2191676597221423
Iteration 150, Training loss = 0.21721126588363623
Iteration 160, Training loss = 0.2148031621052913
Iteration 170, Training loss = 0.21293403528468371
Iteration 180, Training loss = 0.2107785579908558
Iteration 190, Training loss = 0.2091580610045798
Iteration 200, Training loss = 0.20771960678711066
Iteration 210, Training loss = 0.2061961965396387
Iteration 220, Training loss = 0.20520060163072465
Iteration 230, Training loss = 0.20387185174638076
Iteration 240, Training loss = 0.20281356106767065
Iteration 250, Training loss = 0.2025518949452526
Iteration 260, Training loss = 0.20124590856002839
Iteration 270, Training loss = 0.20082276888173659
Iteration 280, Training loss = 0.20039888546231585
Iteration 290, Training loss = 0.19984374896196996
Model training time: 155.63983178138733
Device: cuda
Iteration 0, Training loss = 0.7238496819795188
Iteration 10, Training loss = 0.27819491672097335
Iteration 20, Training loss = 0.2590379218500983
Iteration 30, Training loss = 0.250379029778678
Iteration 40, Training loss = 0.2432176526861387
Iteration 50, Training loss = 0.2380608104566252
Iteration 60, Training loss = 0.2331683003224126
Iteration 70, Training loss = 0.2286173383960135
Iteration 80, Training loss = 0.22567146224562828
Iteration 90, Training loss = 0.2225336858926039
Iteration 100, Training loss = 0.21997151560810807
Iteration 110, Training loss = 0.21815958703906427
Iteration 120, Training loss = 0.21576988536494696
Iteration 130, Training loss = 0.214324440671803
Iteration 140, Training loss = 0.2127601314904326
Iteration 150, Training loss = 0.21078783375623728
Iteration 160, Training loss = 0.2099124629402276
Iteration 170, Training loss = 0.2084210015851418
Iteration 180, Training loss = 0.20702553564822415
Iteration 190, Training loss = 0.20690072610180546
Iteration 200, Training loss = 0.2054805372081715
Iteration 210, Training loss = 0.20445322299479862
Iteration 220, Training loss = 0.2037253895048368
Iteration 230, Training loss = 0.20250445738723433
Iteration 240, Training loss = 0.20170018737166037
Iteration 250, Training loss = 0.20089572927853674
Iteration 260, Training loss = 0.20052687622624796
Iteration 270, Training loss = 0.20017139155408661
Iteration 280, Training loss = 0.19947618367849482
Iteration 290, Training loss = 0.19902192324075224
Model training time: 163.49059581756592
Device: cuda
Iteration 0, Training loss = 0.7565684197673498
Iteration 10, Training loss = 0.27467778792320674
Iteration 20, Training loss = 0.2635273442989256
Iteration 30, Training loss = 0.2579714264404976
Iteration 40, Training loss = 0.2541749904937952
Iteration 50, Training loss = 0.25120554372812587
Iteration 60, Training loss = 0.24788448627650306
Iteration 70, Training loss = 0.24516051397918212
Iteration 80, Training loss = 0.24369190910213218
Iteration 90, Training loss = 0.24077281451470628
Iteration 100, Training loss = 0.2389167419591914
Iteration 110, Training loss = 0.23677064696683145
Iteration 120, Training loss = 0.23511950510213508
Iteration 130, Training loss = 0.23378174476884755
Iteration 140, Training loss = 0.2326415691876527
Iteration 150, Training loss = 0.23034969581963075
Iteration 160, Training loss = 0.22936612434378548
Iteration 170, Training loss = 0.22757781847192934
Iteration 180, Training loss = 0.2253995019464458
Iteration 190, Training loss = 0.22417272192586132
Iteration 200, Training loss = 0.22334626176311087
Iteration 210, Training loss = 0.22169979406435034
Iteration 220, Training loss = 0.2210300205857211
Iteration 230, Training loss = 0.2198814499591078
Iteration 240, Training loss = 0.21794510740994252
Iteration 250, Training loss = 0.21612187773530478
Iteration 260, Training loss = 0.21533270494123927
Iteration 270, Training loss = 0.21429127747008067
Iteration 280, Training loss = 0.2129459768064663
Iteration 290, Training loss = 0.2115791498460146
Model training time: 160.4293932914734
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0149571299552917
Iteration 10, Training loss = 0.9915076131249166
Iteration 20, Training loss = 0.9574374177698362
Iteration 30, Training loss = 0.7328702297972709
Iteration 40, Training loss = 0.389959833235198
Iteration 50, Training loss = 0.36121673964992274
Iteration 60, Training loss = 0.34732989463549263
Iteration 70, Training loss = 0.338493876880797
Iteration 80, Training loss = 0.3324799220512912
Iteration 90, Training loss = 0.32804275728022503
Iteration 100, Training loss = 0.32464686470877463
Iteration 110, Training loss = 0.3217165175888498
Iteration 120, Training loss = 0.31920790957336564
Iteration 130, Training loss = 0.3168844354968094
Iteration 140, Training loss = 0.3149275101053801
Iteration 150, Training loss = 0.31299898374744534
Iteration 160, Training loss = 0.310999689934906
Iteration 170, Training loss = 0.3090406913878554
Iteration 180, Training loss = 0.30728440766040116
Iteration 190, Training loss = 0.30552947322768104
Iteration 200, Training loss = 0.30381818413373635
Iteration 210, Training loss = 0.30207156196922136
Iteration 220, Training loss = 0.3003404861717478
Iteration 230, Training loss = 0.2988209916193029
Iteration 240, Training loss = 0.29729889347680255
Iteration 250, Training loss = 0.29577340618605763
Iteration 260, Training loss = 0.2943157925119412
Iteration 270, Training loss = 0.2928755026283622
Iteration 280, Training loss = 0.291746042867261
Iteration 290, Training loss = 0.2905348540074023
Model training time: 130.74533414840698
Device: cuda
Iteration 0, Training loss = 1.002595109815459
Iteration 10, Training loss = 0.9836355268810909
Iteration 20, Training loss = 0.9110016429539742
Iteration 30, Training loss = 0.5496117909941777
Iteration 40, Training loss = 0.37888622704270963
Iteration 50, Training loss = 0.3552099783108829
Iteration 60, Training loss = 0.34110897236456306
Iteration 70, Training loss = 0.3321419796268241
Iteration 80, Training loss = 0.3259970565071695
Iteration 90, Training loss = 0.3214864042783765
Iteration 100, Training loss = 0.31800124496582344
Iteration 110, Training loss = 0.315271508931825
Iteration 120, Training loss = 0.3130416635414879
Iteration 130, Training loss = 0.31087644041017526
Iteration 140, Training loss = 0.3090666628129257
Iteration 150, Training loss = 0.3072081322964398
Iteration 160, Training loss = 0.30553204907199083
Iteration 170, Training loss = 0.3039449121054379
Iteration 180, Training loss = 0.3022597196712621
Iteration 190, Training loss = 0.3007381620837182
Iteration 200, Training loss = 0.29930808129957165
Iteration 210, Training loss = 0.29789996688360165
Iteration 220, Training loss = 0.296448435932037
Iteration 230, Training loss = 0.2951460272867512
Iteration 240, Training loss = 0.2938214382991087
Iteration 250, Training loss = 0.292659802637435
Iteration 260, Training loss = 0.2913857460671418
Iteration 270, Training loss = 0.2903668698437566
Iteration 280, Training loss = 0.28916282484592015
Iteration 290, Training loss = 0.2880528346425685
Model training time: 131.90739488601685
Device: cuda
Iteration 0, Training loss = 1.0155677355230577
Iteration 10, Training loss = 0.9796749791423576
Iteration 20, Training loss = 0.8792301970590402
Iteration 30, Training loss = 0.4913226271151919
Iteration 40, Training loss = 0.37625474170680195
Iteration 50, Training loss = 0.3565094684753522
Iteration 60, Training loss = 0.3439306173536737
Iteration 70, Training loss = 0.3353143652075717
Iteration 80, Training loss = 0.3296208834099712
Iteration 90, Training loss = 0.3248619057111821
Iteration 100, Training loss = 0.32183323620162346
Iteration 110, Training loss = 0.31896608077512817
Iteration 120, Training loss = 0.3167488150316636
Iteration 130, Training loss = 0.31462920949620715
Iteration 140, Training loss = 0.31287862093483276
Iteration 150, Training loss = 0.31105705899057895
Iteration 160, Training loss = 0.30936149421168296
Iteration 170, Training loss = 0.3076529512069127
Iteration 180, Training loss = 0.3062060872052253
Iteration 190, Training loss = 0.30463038843134127
Iteration 200, Training loss = 0.30311475423577333
Iteration 210, Training loss = 0.3017563937687412
Iteration 220, Training loss = 0.3006165506403902
Iteration 230, Training loss = 0.299049502207061
Iteration 240, Training loss = 0.2977134887733413
Iteration 250, Training loss = 0.29653137245781486
Iteration 260, Training loss = 0.29542500379587777
Iteration 270, Training loss = 0.2939778617774892
Iteration 280, Training loss = 0.2928905870413665
Iteration 290, Training loss = 0.2918031250485208
Model training time: 131.77688479423523
Device: cuda
Iteration 0, Training loss = 1.002899703910218
Iteration 10, Training loss = 0.9672531426385875
Iteration 20, Training loss = 0.7789371166621801
Iteration 30, Training loss = 0.39844470476555766
Iteration 40, Training loss = 0.3598695435261322
Iteration 50, Training loss = 0.3436014165529039
Iteration 60, Training loss = 0.3336938031144061
Iteration 70, Training loss = 0.32777765568246564
Iteration 80, Training loss = 0.3235201250951169
Iteration 90, Training loss = 0.32053728322284275
Iteration 100, Training loss = 0.3180733924194918
Iteration 110, Training loss = 0.31565676176735624
Iteration 120, Training loss = 0.31355051088347563
Iteration 130, Training loss = 0.3115533924059487
Iteration 140, Training loss = 0.3096809629847005
Iteration 150, Training loss = 0.3079190775932469
Iteration 160, Training loss = 0.3060623618072805
Iteration 170, Training loss = 0.3048209519853892
Iteration 180, Training loss = 0.3032521730149052
Iteration 190, Training loss = 0.3018190876984423
Iteration 200, Training loss = 0.30023583448683666
Iteration 210, Training loss = 0.2990094693625666
Iteration 220, Training loss = 0.2976898225001792
Iteration 230, Training loss = 0.2963577020940423
Iteration 240, Training loss = 0.2951147001390018
Iteration 250, Training loss = 0.2939358474122987
Iteration 260, Training loss = 0.2928446725768558
Iteration 270, Training loss = 0.29182307331960367
Iteration 280, Training loss = 0.2906382646204196
Iteration 290, Training loss = 0.28973481372972953
Model training time: 132.36824131011963
Device: cuda
Iteration 0, Training loss = 1.0141779182898218
Iteration 10, Training loss = 0.987319959999574
Iteration 20, Training loss = 0.926473686900035
Iteration 30, Training loss = 0.5704670342059748
Iteration 40, Training loss = 0.3715426003601014
Iteration 50, Training loss = 0.3498060857701244
Iteration 60, Training loss = 0.3374441522951565
Iteration 70, Training loss = 0.3295361421659553
Iteration 80, Training loss = 0.32387241136652506
Iteration 90, Training loss = 0.31988096621437745
Iteration 100, Training loss = 0.3167236430917756
Iteration 110, Training loss = 0.3139958663439924
Iteration 120, Training loss = 0.31166063781295505
Iteration 130, Training loss = 0.30958005688525286
Iteration 140, Training loss = 0.3077976894147748
Iteration 150, Training loss = 0.30604365745843465
Iteration 160, Training loss = 0.3042911504176569
Iteration 170, Training loss = 0.30271314254107257
Iteration 180, Training loss = 0.3011999088215482
Iteration 190, Training loss = 0.2996454242680032
Iteration 200, Training loss = 0.2981236108892189
Iteration 210, Training loss = 0.29659605767695146
Iteration 220, Training loss = 0.2952065268851654
Iteration 230, Training loss = 0.29386607979674606
Iteration 240, Training loss = 0.29240475780810915
Iteration 250, Training loss = 0.290954541279675
Iteration 260, Training loss = 0.2896467541811253
Iteration 270, Training loss = 0.28838847159040465
Iteration 280, Training loss = 0.2871959871695925
Iteration 290, Training loss = 0.2861768173969398
Model training time: 131.45652604103088
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7944763156317048
Iteration 10, Training loss = 0.28282334742116005
Iteration 20, Training loss = 0.26196828672152744
Iteration 30, Training loss = 0.25436656519806705
Iteration 40, Training loss = 0.24871465456435235
Iteration 50, Training loss = 0.24440434069956762
Iteration 60, Training loss = 0.24117941221731914
Iteration 70, Training loss = 0.23752720774879757
Iteration 80, Training loss = 0.23522876905723866
Iteration 90, Training loss = 0.2327395655377148
Iteration 100, Training loss = 0.22984635647720056
Iteration 110, Training loss = 0.22755238077361994
Iteration 120, Training loss = 0.2255181965325704
Iteration 130, Training loss = 0.22437800634787677
Iteration 140, Training loss = 0.22312054324886124
Iteration 150, Training loss = 0.22154000717199454
Iteration 160, Training loss = 0.2206489552741478
Iteration 170, Training loss = 0.21927672413518295
Iteration 180, Training loss = 0.21865032802698975
Iteration 190, Training loss = 0.21736205491005076
Iteration 200, Training loss = 0.21719859584660853
Iteration 210, Training loss = 0.2166803445098764
Iteration 220, Training loss = 0.21552504757847682
Iteration 230, Training loss = 0.21608198806643486
Iteration 240, Training loss = 0.21537512661686245
Iteration 250, Training loss = 0.2143946134329708
Iteration 260, Training loss = 0.2135954829379375
Iteration 270, Training loss = 0.21379813334519002
Iteration 280, Training loss = 0.21312540566625376
Iteration 290, Training loss = 0.21256869122133418
Model training time: 155.38368487358093
Device: cuda
Iteration 0, Training loss = 0.7419242007752597
Iteration 10, Training loss = 0.28774307455335346
Iteration 20, Training loss = 0.2677953375670292
Iteration 30, Training loss = 0.26078428652759905
Iteration 40, Training loss = 0.25581660067558865
Iteration 50, Training loss = 0.25196285448409167
Iteration 60, Training loss = 0.24707106075116567
Iteration 70, Training loss = 0.24292669079061283
Iteration 80, Training loss = 0.23969732672336894
Iteration 90, Training loss = 0.2354773205071327
Iteration 100, Training loss = 0.23318837772486573
Iteration 110, Training loss = 0.22977514482006323
Iteration 120, Training loss = 0.22792813043870014
Iteration 130, Training loss = 0.22511016679409052
Iteration 140, Training loss = 0.22328528258543615
Iteration 150, Training loss = 0.22188900988413693
Iteration 160, Training loss = 0.22031052692028857
Iteration 170, Training loss = 0.2191934640413335
Iteration 180, Training loss = 0.21823176529628313
Iteration 190, Training loss = 0.21724960411792807
Iteration 200, Training loss = 0.2163201421950256
Iteration 210, Training loss = 0.2154267017907727
Iteration 220, Training loss = 0.21473562672337376
Iteration 230, Training loss = 0.21412162242804544
Iteration 240, Training loss = 0.21336397028936313
Iteration 250, Training loss = 0.21265995769466095
Iteration 260, Training loss = 0.21270994569646245
Iteration 270, Training loss = 0.21213416571189936
Iteration 280, Training loss = 0.21172637633131433
Iteration 290, Training loss = 0.21048599344477525
Model training time: 155.22712111473083
Device: cuda
Iteration 0, Training loss = 0.7442449087741588
Iteration 10, Training loss = 0.28017641075370386
Iteration 20, Training loss = 0.2658635231217518
Iteration 30, Training loss = 0.25966316524703625
Iteration 40, Training loss = 0.256007089886192
Iteration 50, Training loss = 0.2521462020786709
Iteration 60, Training loss = 0.24923094516263747
Iteration 70, Training loss = 0.24586362876990228
Iteration 80, Training loss = 0.24324864675865912
Iteration 90, Training loss = 0.24056891849846299
Iteration 100, Training loss = 0.23832382903982305
Iteration 110, Training loss = 0.23635191665218183
Iteration 120, Training loss = 0.23458040463253316
Iteration 130, Training loss = 0.2326261819527455
Iteration 140, Training loss = 0.23094821552640013
Iteration 150, Training loss = 0.22900401499600445
Iteration 160, Training loss = 0.2277531721076723
Iteration 170, Training loss = 0.2266050743493849
Iteration 180, Training loss = 0.2248906582485677
Iteration 190, Training loss = 0.22432083589617913
Iteration 200, Training loss = 0.22284842825542062
Iteration 210, Training loss = 0.2223543208588555
Iteration 220, Training loss = 0.22103056182620312
Iteration 230, Training loss = 0.21996829859894354
Iteration 240, Training loss = 0.2188833781380486
Iteration 250, Training loss = 0.21787647775782512
Iteration 260, Training loss = 0.21679646188064003
Iteration 270, Training loss = 0.2158165258010421
Iteration 280, Training loss = 0.215037594686914
Iteration 290, Training loss = 0.2140099826036585
Model training time: 154.99207139015198
Device: cuda
Iteration 0, Training loss = 0.8183551497811555
Iteration 10, Training loss = 0.2748367001119884
Iteration 20, Training loss = 0.26018801587690166
Iteration 30, Training loss = 0.2525696245994031
Iteration 40, Training loss = 0.24757939124294978
Iteration 50, Training loss = 0.24250014388333943
Iteration 60, Training loss = 0.23939059696388013
Iteration 70, Training loss = 0.23626155130222404
Iteration 80, Training loss = 0.2340445028252665
Iteration 90, Training loss = 0.2310096488377084
Iteration 100, Training loss = 0.22938532204554388
Iteration 110, Training loss = 0.22747237305664267
Iteration 120, Training loss = 0.22615799076909304
Iteration 130, Training loss = 0.22484953796206894
Iteration 140, Training loss = 0.22350992938868935
Iteration 150, Training loss = 0.2223876829802557
Iteration 160, Training loss = 0.2215003385310023
Iteration 170, Training loss = 0.22055304412150498
Iteration 180, Training loss = 0.22022906065744868
Iteration 190, Training loss = 0.21852126540774006
Iteration 200, Training loss = 0.21790665295200545
Iteration 210, Training loss = 0.21677644256745932
Iteration 220, Training loss = 0.21566055103523102
Iteration 230, Training loss = 0.21519929967080997
Iteration 240, Training loss = 0.21397630737747175
Iteration 250, Training loss = 0.21367596419974622
Iteration 260, Training loss = 0.21292575516402
Iteration 270, Training loss = 0.2113299827851337
Iteration 280, Training loss = 0.21115723463366165
Iteration 290, Training loss = 0.21082819232460084
Model training time: 156.2564959526062
Device: cuda
Iteration 0, Training loss = 0.7249259696529218
Iteration 10, Training loss = 0.27798171473112293
Iteration 20, Training loss = 0.2608859752807721
Iteration 30, Training loss = 0.2507301255627637
Iteration 40, Training loss = 0.24551100089929584
Iteration 50, Training loss = 0.24205344359753495
Iteration 60, Training loss = 0.2389216828396765
Iteration 70, Training loss = 0.23688097801465388
Iteration 80, Training loss = 0.2349195971850477
Iteration 90, Training loss = 0.23262661459154424
Iteration 100, Training loss = 0.23091976160575922
Iteration 110, Training loss = 0.22958336997840365
Iteration 120, Training loss = 0.2276978615409814
Iteration 130, Training loss = 0.2264175984115491
Iteration 140, Training loss = 0.22497024477321934
Iteration 150, Training loss = 0.22335704601850118
Iteration 160, Training loss = 0.22217347191479825
Iteration 170, Training loss = 0.22104975803062932
Iteration 180, Training loss = 0.22007245090553315
Iteration 190, Training loss = 0.2190753374762887
Iteration 200, Training loss = 0.21852033505933335
Iteration 210, Training loss = 0.21734550824393373
Iteration 220, Training loss = 0.2169882361089346
Iteration 230, Training loss = 0.21619092755145947
Iteration 240, Training loss = 0.21559644950504164
Iteration 250, Training loss = 0.21484391045808504
Iteration 260, Training loss = 0.21497254819579908
Iteration 270, Training loss = 0.21385029340771727
Iteration 280, Training loss = 0.21269203814481707
Iteration 290, Training loss = 0.21265457377523256
Model training time: 155.54719257354736
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0081612458384932
Iteration 10, Training loss = 0.9909059105138802
Iteration 20, Training loss = 0.9507261264988066
Iteration 30, Training loss = 0.7502460064091347
Iteration 40, Training loss = 0.4175799839000148
Iteration 50, Training loss = 0.36475496695852744
Iteration 60, Training loss = 0.34454524280733406
Iteration 70, Training loss = 0.3314647675101751
Iteration 80, Training loss = 0.3233573644379149
Iteration 90, Training loss = 0.3180721996951911
Iteration 100, Training loss = 0.3143899521044034
Iteration 110, Training loss = 0.3114832397234642
Iteration 120, Training loss = 0.30933758511814596
Iteration 130, Training loss = 0.3074031177249717
Iteration 140, Training loss = 0.30598380990308366
Iteration 150, Training loss = 0.3045168949999186
Iteration 160, Training loss = 0.30324629116938706
Iteration 170, Training loss = 0.3020015618522577
Iteration 180, Training loss = 0.30079850888208964
Iteration 190, Training loss = 0.29987772415250036
Iteration 200, Training loss = 0.29867127889510214
Iteration 210, Training loss = 0.29800839464182427
Iteration 220, Training loss = 0.296917180424741
Iteration 230, Training loss = 0.29597047449384994
Iteration 240, Training loss = 0.2950205675734446
Iteration 250, Training loss = 0.29423706917222997
Iteration 260, Training loss = 0.29332159755449316
Iteration 270, Training loss = 0.29244562151862113
Iteration 280, Training loss = 0.29160148778492834
Iteration 290, Training loss = 0.2910459252845577
Model training time: 131.9292914867401
Device: cuda
Iteration 0, Training loss = 1.0045547561120178
Iteration 10, Training loss = 0.9981030175241373
Iteration 20, Training loss = 0.9923800121785364
Iteration 30, Training loss = 0.9756279986072106
Iteration 40, Training loss = 0.8851880810739919
Iteration 50, Training loss = 0.5095564052642979
Iteration 60, Training loss = 0.3685552036213817
Iteration 70, Training loss = 0.3465650123126859
Iteration 80, Training loss = 0.33387968427621134
Iteration 90, Training loss = 0.32654524949034247
Iteration 100, Training loss = 0.32128379335198504
Iteration 110, Training loss = 0.317269946089957
Iteration 120, Training loss = 0.3145515823299313
Iteration 130, Training loss = 0.3119098549352431
Iteration 140, Training loss = 0.3095784908381559
Iteration 150, Training loss = 0.3077739899657829
Iteration 160, Training loss = 0.3062458253007824
Iteration 170, Training loss = 0.30472794519714813
Iteration 180, Training loss = 0.3033308036823827
Iteration 190, Training loss = 0.3018180921709855
Iteration 200, Training loss = 0.30045902802999025
Iteration 210, Training loss = 0.2992174029891485
Iteration 220, Training loss = 0.29805307990485763
Iteration 230, Training loss = 0.296905688364194
Iteration 240, Training loss = 0.29570884966532773
Iteration 250, Training loss = 0.29465620252179753
Iteration 260, Training loss = 0.29340210641772635
Iteration 270, Training loss = 0.2922146379514699
Iteration 280, Training loss = 0.2910021503814485
Iteration 290, Training loss = 0.2900347378799471
Model training time: 131.7772297859192
Device: cuda
Iteration 0, Training loss = 0.9974423240662775
Iteration 10, Training loss = 0.9555666736049745
Iteration 20, Training loss = 0.7023549427709048
Iteration 30, Training loss = 0.3841163319407017
Iteration 40, Training loss = 0.35662862546651763
Iteration 50, Training loss = 0.3428527435003701
Iteration 60, Training loss = 0.3333401028021773
Iteration 70, Training loss = 0.32654803865036724
Iteration 80, Training loss = 0.32200364579083557
Iteration 90, Training loss = 0.3181898493418971
Iteration 100, Training loss = 0.3151300479872175
Iteration 110, Training loss = 0.31253237668740547
Iteration 120, Training loss = 0.3105097098262489
Iteration 130, Training loss = 0.30861500677704523
Iteration 140, Training loss = 0.3066776745544508
Iteration 150, Training loss = 0.30498282228918977
Iteration 160, Training loss = 0.3035598534720862
Iteration 170, Training loss = 0.30195371362833945
Iteration 180, Training loss = 0.3004154367936148
Iteration 190, Training loss = 0.2988359621268208
Iteration 200, Training loss = 0.29739758399367044
Iteration 210, Training loss = 0.2961880268109624
Iteration 220, Training loss = 0.29480452862620066
Iteration 230, Training loss = 0.2937335286641236
Iteration 240, Training loss = 0.2923091110428367
Iteration 250, Training loss = 0.29129959518410103
Iteration 260, Training loss = 0.2900935334602222
Iteration 270, Training loss = 0.28912262100789504
Iteration 280, Training loss = 0.2881523034474463
Iteration 290, Training loss = 0.2872832968794982
Model training time: 143.4161195755005
Device: cuda
Iteration 0, Training loss = 1.0018891647421997
Iteration 10, Training loss = 0.9725801118493946
Iteration 20, Training loss = 0.8396998278742552
Iteration 30, Training loss = 0.43731373682004776
Iteration 40, Training loss = 0.3662134897117176
Iteration 50, Training loss = 0.3459253573821763
Iteration 60, Training loss = 0.333499946115092
Iteration 70, Training loss = 0.32561179138629837
Iteration 80, Training loss = 0.32050572792135773
Iteration 90, Training loss = 0.3163069822886377
Iteration 100, Training loss = 0.31317463612801805
Iteration 110, Training loss = 0.31046812344549934
Iteration 120, Training loss = 0.30836814450726957
Iteration 130, Training loss = 0.30633992728829096
Iteration 140, Training loss = 0.30469461629667816
Iteration 150, Training loss = 0.30301991785193183
Iteration 160, Training loss = 0.3015364833518899
Iteration 170, Training loss = 0.30017055011834703
Iteration 180, Training loss = 0.2989149036440665
Iteration 190, Training loss = 0.2978267391498672
Iteration 200, Training loss = 0.29665450253873415
Iteration 210, Training loss = 0.2954666215845228
Iteration 220, Training loss = 0.29456856502242584
Iteration 230, Training loss = 0.29334487086815925
Iteration 240, Training loss = 0.2924650009658377
Iteration 250, Training loss = 0.2915842859818629
Iteration 260, Training loss = 0.29063290987190843
Iteration 270, Training loss = 0.2895836053150041
Iteration 280, Training loss = 0.28890419994947697
Iteration 290, Training loss = 0.2880713249089931
Model training time: 130.7915816307068
Device: cuda
Iteration 0, Training loss = 0.9995962575474894
Iteration 10, Training loss = 0.9861940856995941
Iteration 20, Training loss = 0.9293806055700519
Iteration 30, Training loss = 0.6129751171168057
Iteration 40, Training loss = 0.3825126540357784
Iteration 50, Training loss = 0.35012754841231836
Iteration 60, Training loss = 0.334014361295804
Iteration 70, Training loss = 0.32429193784697
Iteration 80, Training loss = 0.31834095875229734
Iteration 90, Training loss = 0.3140275531220667
Iteration 100, Training loss = 0.3107731006408142
Iteration 110, Training loss = 0.3082403111977381
Iteration 120, Training loss = 0.3062516787871899
Iteration 130, Training loss = 0.30418508160706004
Iteration 140, Training loss = 0.30248207546608213
Iteration 150, Training loss = 0.3009931004761784
Iteration 160, Training loss = 0.2994813850154022
Iteration 170, Training loss = 0.29813523056576385
Iteration 180, Training loss = 0.2965981436049967
Iteration 190, Training loss = 0.2950458468522056
Iteration 200, Training loss = 0.293680943133756
Iteration 210, Training loss = 0.2923086627451906
Iteration 220, Training loss = 0.2911347963150419
Iteration 230, Training loss = 0.28954231592919
Iteration 240, Training loss = 0.2882408597314906
Iteration 250, Training loss = 0.2869017012687919
Iteration 260, Training loss = 0.28604353098401725
Iteration 270, Training loss = 0.2845031238244463
Iteration 280, Training loss = 0.28327332476726746
Iteration 290, Training loss = 0.28217398526809984
Model training time: 132.4057092666626
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7517210635882025
Iteration 10, Training loss = 0.28306293038349173
Iteration 20, Training loss = 0.2644176945523257
Iteration 30, Training loss = 0.2573607286695418
Iteration 40, Training loss = 0.25173221057682293
Iteration 50, Training loss = 0.24784046718196776
Iteration 60, Training loss = 0.24401566110208595
Iteration 70, Training loss = 0.24087607218335963
Iteration 80, Training loss = 0.23788445246277362
Iteration 90, Training loss = 0.23520831573420806
Iteration 100, Training loss = 0.2330056213149147
Iteration 110, Training loss = 0.23073164651454506
Iteration 120, Training loss = 0.22811550143700246
Iteration 130, Training loss = 0.2271777058502808
Iteration 140, Training loss = 0.2241526519559486
Iteration 150, Training loss = 0.22299497134388216
Iteration 160, Training loss = 0.2210472885510246
Iteration 170, Training loss = 0.2189305799744897
Iteration 180, Training loss = 0.21762277875050506
Iteration 190, Training loss = 0.2161267214876399
Iteration 200, Training loss = 0.21422935445898958
Iteration 210, Training loss = 0.2133021827056535
Iteration 220, Training loss = 0.21196033397901434
Iteration 230, Training loss = 0.21118145446872597
Iteration 240, Training loss = 0.21039010073818248
Iteration 250, Training loss = 0.2087862586679239
Iteration 260, Training loss = 0.20826575500083316
Iteration 270, Training loss = 0.2075058964259255
Iteration 280, Training loss = 0.20658811608433147
Iteration 290, Training loss = 0.20559712644603292
Model training time: 156.07030510902405
Device: cuda
Iteration 0, Training loss = 0.7563719210050297
Iteration 10, Training loss = 0.2814636734690851
Iteration 20, Training loss = 0.27113017248616667
Iteration 30, Training loss = 0.26471863405612134
Iteration 40, Training loss = 0.26013446155457465
Iteration 50, Training loss = 0.25680761392843926
Iteration 60, Training loss = 0.2528408930499386
Iteration 70, Training loss = 0.2504097149858463
Iteration 80, Training loss = 0.24836780287812465
Iteration 90, Training loss = 0.2455464959613636
Iteration 100, Training loss = 0.24382088142142747
Iteration 110, Training loss = 0.24122543369598018
Iteration 120, Training loss = 0.2386641311425418
Iteration 130, Training loss = 0.23503894464408226
Iteration 140, Training loss = 0.23142093472345113
Iteration 150, Training loss = 0.22847122038752923
Iteration 160, Training loss = 0.22701646412833262
Iteration 170, Training loss = 0.22552398128601886
Iteration 180, Training loss = 0.22509835050700652
Iteration 190, Training loss = 0.22284686127925613
Iteration 200, Training loss = 0.2217218543585507
Iteration 210, Training loss = 0.22155458931123373
Iteration 220, Training loss = 0.22064824598969907
Iteration 230, Training loss = 0.21895992591724558
Iteration 240, Training loss = 0.2173951978256281
Iteration 250, Training loss = 0.21719660036283892
Iteration 260, Training loss = 0.21637123355926094
Iteration 270, Training loss = 0.21530535323748287
Iteration 280, Training loss = 0.21466753438800645
Iteration 290, Training loss = 0.21392733662346372
Model training time: 155.55508661270142
Device: cuda
Iteration 0, Training loss = 0.6869804638530382
Iteration 10, Training loss = 0.2787652706443253
Iteration 20, Training loss = 0.26267380453195177
Iteration 30, Training loss = 0.2545109243102859
Iteration 40, Training loss = 0.24719167450221918
Iteration 50, Training loss = 0.24203045906151754
Iteration 60, Training loss = 0.23797334045099577
Iteration 70, Training loss = 0.2349728084379045
Iteration 80, Training loss = 0.23038996920242147
Iteration 90, Training loss = 0.22841873134488633
Iteration 100, Training loss = 0.22497356297858692
Iteration 110, Training loss = 0.22240257469743274
Iteration 120, Training loss = 0.22046800809247152
Iteration 130, Training loss = 0.2183627698110466
Iteration 140, Training loss = 0.21693913469014386
Iteration 150, Training loss = 0.21542420221118316
Iteration 160, Training loss = 0.2140213915712897
Iteration 170, Training loss = 0.21245402102212063
Iteration 180, Training loss = 0.21184592046402845
Iteration 190, Training loss = 0.21095915883779526
Iteration 200, Training loss = 0.21000751819614924
Iteration 210, Training loss = 0.20951696536695408
Iteration 220, Training loss = 0.20865783997367138
Iteration 230, Training loss = 0.20709127748777445
Iteration 240, Training loss = 0.20661588114196971
Iteration 250, Training loss = 0.20573139477512162
Iteration 260, Training loss = 0.20578549206076466
Iteration 270, Training loss = 0.204584702587806
Iteration 280, Training loss = 0.20389655251371658
Iteration 290, Training loss = 0.20374726614132344
Model training time: 154.97652673721313
Device: cuda
Iteration 0, Training loss = 0.681453177556576
Iteration 10, Training loss = 0.2844102586152767
Iteration 20, Training loss = 0.26615921485510635
Iteration 30, Training loss = 0.25753709877735187
Iteration 40, Training loss = 0.25377457507730394
Iteration 50, Training loss = 0.2505473379794391
Iteration 60, Training loss = 0.24782079636834148
Iteration 70, Training loss = 0.24575246912010068
Iteration 80, Training loss = 0.2429007808644027
Iteration 90, Training loss = 0.24115760200465275
Iteration 100, Training loss = 0.2391336061980476
Iteration 110, Training loss = 0.23803758337985517
Iteration 120, Training loss = 0.23579555921274584
Iteration 130, Training loss = 0.23472473894857032
Iteration 140, Training loss = 0.2330483159379578
Iteration 150, Training loss = 0.23136664135043328
Iteration 160, Training loss = 0.22964045910511988
Iteration 170, Training loss = 0.22742886182831795
Iteration 180, Training loss = 0.22630285643961182
Iteration 190, Training loss = 0.22466135694313857
Iteration 200, Training loss = 0.22276017626355116
Iteration 210, Training loss = 0.22123248402489298
Iteration 220, Training loss = 0.22033727240295445
Iteration 230, Training loss = 0.21844020556234564
Iteration 240, Training loss = 0.21743344948525578
Iteration 250, Training loss = 0.2166685058620305
Iteration 260, Training loss = 0.21588871744404983
Iteration 270, Training loss = 0.2153086331255788
Iteration 280, Training loss = 0.21461306443803246
Iteration 290, Training loss = 0.21316307968038334
Model training time: 155.96353793144226
Device: cuda
Iteration 0, Training loss = 0.8874234466951061
Iteration 10, Training loss = 0.279528809084442
Iteration 20, Training loss = 0.2693733701586146
Iteration 30, Training loss = 0.2623363140672806
Iteration 40, Training loss = 0.2566929730895645
Iteration 50, Training loss = 0.2532239870571917
Iteration 60, Training loss = 0.24919695450448529
Iteration 70, Training loss = 0.24618310244983968
Iteration 80, Training loss = 0.2431986994373885
Iteration 90, Training loss = 0.24055949124094939
Iteration 100, Training loss = 0.237893802111576
Iteration 110, Training loss = 0.23588813347944912
Iteration 120, Training loss = 0.23385530890621803
Iteration 130, Training loss = 0.2316949585718768
Iteration 140, Training loss = 0.23053213423591548
Iteration 150, Training loss = 0.22894471737648614
Iteration 160, Training loss = 0.22747585650073415
Iteration 170, Training loss = 0.22645604496790192
Iteration 180, Training loss = 0.22529548933373236
Iteration 190, Training loss = 0.22386258766596312
Iteration 200, Training loss = 0.22329513738540702
Iteration 210, Training loss = 0.22264430696284512
Iteration 220, Training loss = 0.2212167034696869
Iteration 230, Training loss = 0.22152545372560872
Iteration 240, Training loss = 0.21944130652248137
Iteration 250, Training loss = 0.21961204041039106
Iteration 260, Training loss = 0.2189052697568771
Iteration 270, Training loss = 0.2172366744471087
Iteration 280, Training loss = 0.2170837672729469
Iteration 290, Training loss = 0.21668502009196375
Model training time: 155.54547095298767
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0054033529960502
Iteration 10, Training loss = 0.9657315341237094
Iteration 20, Training loss = 0.8279243978259061
Iteration 30, Training loss = 0.46810048771683876
Iteration 40, Training loss = 0.38515269841105826
Iteration 50, Training loss = 0.3626348821865733
Iteration 60, Training loss = 0.34827680277838835
Iteration 70, Training loss = 0.33826856538833777
Iteration 80, Training loss = 0.33156266970871145
Iteration 90, Training loss = 0.3268354818874352
Iteration 100, Training loss = 0.32312743796924126
Iteration 110, Training loss = 0.32030625540321156
Iteration 120, Training loss = 0.317663324009131
Iteration 130, Training loss = 0.3159102078788794
Iteration 140, Training loss = 0.3137945758300601
Iteration 150, Training loss = 0.3123935986192983
Iteration 160, Training loss = 0.31086287847731364
Iteration 170, Training loss = 0.30931382716495825
Iteration 180, Training loss = 0.3079922113197768
Iteration 190, Training loss = 0.30648686033256406
Iteration 200, Training loss = 0.30545526589593935
Iteration 210, Training loss = 0.30393346979961555
Iteration 220, Training loss = 0.30269879073842676
Iteration 230, Training loss = 0.3014684765448582
Iteration 240, Training loss = 0.30031514824446987
Iteration 250, Training loss = 0.2992294699350512
Iteration 260, Training loss = 0.2981893330920695
Iteration 270, Training loss = 0.29699064331178804
Iteration 280, Training loss = 0.2961154153466802
Iteration 290, Training loss = 0.29514905424297
Model training time: 130.9644114971161
Device: cuda
Iteration 0, Training loss = 1.028579144659689
Iteration 10, Training loss = 0.9934381740866792
Iteration 20, Training loss = 0.9693000434963236
Iteration 30, Training loss = 0.844428483207347
Iteration 40, Training loss = 0.4661300282645745
Iteration 50, Training loss = 0.37301673590415324
Iteration 60, Training loss = 0.35010974571000575
Iteration 70, Training loss = 0.3372865477824904
Iteration 80, Training loss = 0.3295111562019399
Iteration 90, Training loss = 0.32475806144911784
Iteration 100, Training loss = 0.32143360859833964
Iteration 110, Training loss = 0.3189129347734821
Iteration 120, Training loss = 0.31661025526737185
Iteration 130, Training loss = 0.31478793056767446
Iteration 140, Training loss = 0.313236070512715
Iteration 150, Training loss = 0.3113286035184999
Iteration 160, Training loss = 0.309819001406792
Iteration 170, Training loss = 0.30846774386868925
Iteration 180, Training loss = 0.3071528035813613
Iteration 190, Training loss = 0.3060456855136892
Iteration 200, Training loss = 0.3049015436348557
Iteration 210, Training loss = 0.30363946617299076
Iteration 220, Training loss = 0.30253978622017413
Iteration 230, Training loss = 0.3014521945547538
Iteration 240, Training loss = 0.30061108503517747
Iteration 250, Training loss = 0.2994403675126394
Iteration 260, Training loss = 0.2982967633744995
Iteration 270, Training loss = 0.2975609982093079
Iteration 280, Training loss = 0.29633515485118145
Iteration 290, Training loss = 0.2953780072472863
Model training time: 131.6874222755432
Device: cuda
Iteration 0, Training loss = 0.9977517996804189
Iteration 10, Training loss = 0.9691968008911928
Iteration 20, Training loss = 0.8415379764525711
Iteration 30, Training loss = 0.465717894149173
Iteration 40, Training loss = 0.38191516401548364
Iteration 50, Training loss = 0.3565624574555612
Iteration 60, Training loss = 0.3406496770301108
Iteration 70, Training loss = 0.3304839357192522
Iteration 80, Training loss = 0.32396842416637456
Iteration 90, Training loss = 0.3193719032926363
Iteration 100, Training loss = 0.3161785020414045
Iteration 110, Training loss = 0.31358245491404224
Iteration 120, Training loss = 0.31146976165924467
Iteration 130, Training loss = 0.30961627471677905
Iteration 140, Training loss = 0.30812091315413215
Iteration 150, Training loss = 0.30658111122344367
Iteration 160, Training loss = 0.3051796143842956
Iteration 170, Training loss = 0.3039277102482521
Iteration 180, Training loss = 0.30246538710363263
Iteration 190, Training loss = 0.30133740544391313
Iteration 200, Training loss = 0.3000427469719409
Iteration 210, Training loss = 0.29886996249959197
Iteration 220, Training loss = 0.2974810551391964
Iteration 230, Training loss = 0.2964339107996615
Iteration 240, Training loss = 0.29522868994506163
Iteration 250, Training loss = 0.2939590428195912
Iteration 260, Training loss = 0.29292521573440794
Iteration 270, Training loss = 0.2917196428263447
Iteration 280, Training loss = 0.29081974212828904
Iteration 290, Training loss = 0.2900861011542939
Model training time: 134.99149250984192
Device: cuda
Iteration 0, Training loss = 1.0251268666535256
Iteration 10, Training loss = 0.9358148685956406
Iteration 20, Training loss = 0.6231174677538237
Iteration 30, Training loss = 0.3746882932707126
Iteration 40, Training loss = 0.3508995939903051
Iteration 50, Training loss = 0.33832438037557117
Iteration 60, Training loss = 0.3305190427648242
Iteration 70, Training loss = 0.32551529828558246
Iteration 80, Training loss = 0.32185606214671103
Iteration 90, Training loss = 0.3189545540955396
Iteration 100, Training loss = 0.31651032313740574
Iteration 110, Training loss = 0.3145964685782393
Iteration 120, Training loss = 0.3127919885783161
Iteration 130, Training loss = 0.31097405053873617
Iteration 140, Training loss = 0.3093139814082127
Iteration 150, Training loss = 0.3078877003119298
Iteration 160, Training loss = 0.30651807481959714
Iteration 170, Training loss = 0.30522313183647093
Iteration 180, Training loss = 0.3039879351673923
Iteration 190, Training loss = 0.30273059508413724
Iteration 200, Training loss = 0.30155771062427517
Iteration 210, Training loss = 0.30044777032898934
Iteration 220, Training loss = 0.2993371923956975
Iteration 230, Training loss = 0.2982077179318767
Iteration 240, Training loss = 0.2970599691061073
Iteration 250, Training loss = 0.2961145993908439
Iteration 260, Training loss = 0.29515789616988297
Iteration 270, Training loss = 0.2941422835714015
Iteration 280, Training loss = 0.2930860806247513
Iteration 290, Training loss = 0.29215815562312886
Model training time: 133.50746512413025
Device: cuda
Iteration 0, Training loss = 1.008960920081589
Iteration 10, Training loss = 0.9905918435380765
Iteration 20, Training loss = 0.9531884604014149
Iteration 30, Training loss = 0.7294638953543748
Iteration 40, Training loss = 0.3987713393374159
Iteration 50, Training loss = 0.35877738672942283
Iteration 60, Training loss = 0.3409378086430975
Iteration 70, Training loss = 0.3295217247332557
Iteration 80, Training loss = 0.3227007874168149
Iteration 90, Training loss = 0.31822167459613765
Iteration 100, Training loss = 0.314728313872081
Iteration 110, Training loss = 0.3121753341132734
Iteration 120, Training loss = 0.3095227131940262
Iteration 130, Training loss = 0.3075315041273616
Iteration 140, Training loss = 0.30558687466541734
Iteration 150, Training loss = 0.3036940839980474
Iteration 160, Training loss = 0.3019709122743791
Iteration 170, Training loss = 0.30031750883374897
Iteration 180, Training loss = 0.29883635455413243
Iteration 190, Training loss = 0.2970992449229046
Iteration 200, Training loss = 0.29572984105377453
Iteration 210, Training loss = 0.29426591033364036
Iteration 220, Training loss = 0.29261283125484827
Iteration 230, Training loss = 0.2914439338748738
Iteration 240, Training loss = 0.2903737630740205
Iteration 250, Training loss = 0.28925094890608916
Iteration 260, Training loss = 0.28809940603902207
Iteration 270, Training loss = 0.2872059381145253
Iteration 280, Training loss = 0.2863462879784748
Iteration 290, Training loss = 0.28574772622914346
Model training time: 133.82765579223633
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.960937641956956
Iteration 10, Training loss = 0.3034064594818198
Iteration 20, Training loss = 0.28749061638606344
Iteration 30, Training loss = 0.276301898278188
Iteration 40, Training loss = 0.27136282669605266
Iteration 50, Training loss = 0.2679486669564017
Iteration 60, Training loss = 0.2652080588510647
Iteration 70, Training loss = 0.2619615281405656
Iteration 80, Training loss = 0.260618067054069
Iteration 90, Training loss = 0.25897179234430984
Model training time: 31.069065809249878
Device: cuda
Iteration 0, Training loss = 0.9479304482971412
Iteration 10, Training loss = 0.30321256247695516
Iteration 20, Training loss = 0.28970172000679995
Iteration 30, Training loss = 0.2791971193563535
Iteration 40, Training loss = 0.2699047121160848
Iteration 50, Training loss = 0.26401500724220045
Iteration 60, Training loss = 0.260357237164525
Iteration 70, Training loss = 0.2565457043800377
Iteration 80, Training loss = 0.2539830318323656
Iteration 90, Training loss = 0.2520415729658615
Model training time: 34.037946939468384
Device: cuda
Iteration 0, Training loss = 0.9514146484614571
Iteration 10, Training loss = 0.30064208409636495
Iteration 20, Training loss = 0.28713376247796457
Iteration 30, Training loss = 0.27788397734579834
Iteration 40, Training loss = 0.27044063748944785
Iteration 50, Training loss = 0.2643287457561723
Iteration 60, Training loss = 0.26039301042539487
Iteration 70, Training loss = 0.256831341571566
Iteration 80, Training loss = 0.2537328950376902
Iteration 90, Training loss = 0.25115770573057417
Model training time: 33.71392607688904
Device: cuda
Iteration 0, Training loss = 0.9885204635956437
Iteration 10, Training loss = 0.3011251652010397
Iteration 20, Training loss = 0.2873510256913549
Iteration 30, Training loss = 0.2787273626252649
Iteration 40, Training loss = 0.2713791868392972
Iteration 50, Training loss = 0.26548799652840205
Iteration 60, Training loss = 0.2604980623160583
Iteration 70, Training loss = 0.256762712330058
Iteration 80, Training loss = 0.25336103817978917
Iteration 90, Training loss = 0.2502253534281311
Model training time: 33.58834147453308
Device: cuda
Iteration 0, Training loss = 0.9983648948623363
Iteration 10, Training loss = 0.3026441456615061
Iteration 20, Training loss = 0.2896122401895154
Iteration 30, Training loss = 0.2761496670436168
Iteration 40, Training loss = 0.2673901977818369
Iteration 50, Training loss = 0.26208404491201115
Iteration 60, Training loss = 0.2578739706012938
Iteration 70, Training loss = 0.2534500391083063
Iteration 80, Training loss = 0.25102137518677736
Iteration 90, Training loss = 0.24821909203909445
Model training time: 31.95640206336975
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.023207454289791
Iteration 10, Training loss = 1.001743848916989
Iteration 20, Training loss = 0.998191021202843
Iteration 30, Training loss = 0.9939145183793588
Iteration 40, Training loss = 0.9914341678365993
Iteration 50, Training loss = 0.9821468637761287
Iteration 60, Training loss = 0.9753496531703046
Iteration 70, Training loss = 0.9612242236805423
Iteration 80, Training loss = 0.9421152282452238
Iteration 90, Training loss = 0.9086797790250917
Model training time: 26.61401629447937
Device: cuda
Iteration 0, Training loss = 1.0621491241570256
Iteration 10, Training loss = 0.9968414652174797
Iteration 20, Training loss = 0.9916996117951213
Iteration 30, Training loss = 0.9853480619508863
Iteration 40, Training loss = 0.9781274676034992
Iteration 50, Training loss = 0.9648596281590669
Iteration 60, Training loss = 0.9418083976432321
Iteration 70, Training loss = 0.9069522547261151
Iteration 80, Training loss = 0.8485582957521153
Iteration 90, Training loss = 0.7550048460994941
Model training time: 26.175675868988037
Device: cuda
Iteration 0, Training loss = 1.00387297016411
Iteration 10, Training loss = 0.9912284676003571
Iteration 20, Training loss = 0.9861077329386836
Iteration 30, Training loss = 0.9781090107516967
Iteration 40, Training loss = 0.9684407417325006
Iteration 50, Training loss = 0.950534352933727
Iteration 60, Training loss = 0.9223070314540955
Iteration 70, Training loss = 0.8744608264614419
Iteration 80, Training loss = 0.7974907598921642
Iteration 90, Training loss = 0.6836564674757529
Model training time: 26.341123342514038
Device: cuda
Iteration 0, Training loss = 1.0028559997461843
Iteration 10, Training loss = 0.993649637353593
Iteration 20, Training loss = 0.987329664149722
Iteration 30, Training loss = 0.9796521868106823
Iteration 40, Training loss = 0.965364476620863
Iteration 50, Training loss = 0.947178998719091
Iteration 60, Training loss = 0.9180461433198717
Iteration 70, Training loss = 0.8701775730520055
Iteration 80, Training loss = 0.7937026595435857
Iteration 90, Training loss = 0.6826291118842968
Model training time: 26.193735122680664
Device: cuda
Iteration 0, Training loss = 1.000687909874939
Iteration 10, Training loss = 0.990120818361568
Iteration 20, Training loss = 0.9788961672552542
Iteration 30, Training loss = 0.9641324278813054
Iteration 40, Training loss = 0.940498051436051
Iteration 50, Training loss = 0.9038102953618276
Iteration 60, Training loss = 0.8398205283759297
Iteration 70, Training loss = 0.7348631135219537
Iteration 80, Training loss = 0.5995762315061357
Iteration 90, Training loss = 0.47635872971608445
Model training time: 26.251712322235107
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9996620802487728
Iteration 10, Training loss = 0.31279602833992043
Iteration 20, Training loss = 0.2916814453791881
Iteration 30, Training loss = 0.28112596582963273
Iteration 40, Training loss = 0.2728551084868574
Iteration 50, Training loss = 0.2668268398386269
Iteration 60, Training loss = 0.2638030355967185
Iteration 70, Training loss = 0.2616102984130095
Iteration 80, Training loss = 0.2587212074709975
Iteration 90, Training loss = 0.25769079451399723
Model training time: 30.22616958618164
Device: cuda
Iteration 0, Training loss = 1.0530077753147642
Iteration 10, Training loss = 0.3309653114149536
Iteration 20, Training loss = 0.2925731742367652
Iteration 30, Training loss = 0.2806890795484257
Iteration 40, Training loss = 0.2753139878960623
Iteration 50, Training loss = 0.27177768197036595
Iteration 60, Training loss = 0.2694166002858088
Iteration 70, Training loss = 0.2673006614913111
Iteration 80, Training loss = 0.26567639975588103
Iteration 90, Training loss = 0.2633834711955365
Model training time: 30.018575429916382
Device: cuda
Iteration 0, Training loss = 1.1244356204634127
Iteration 10, Training loss = 0.33828948913277057
Iteration 20, Training loss = 0.2894595806725359
Iteration 30, Training loss = 0.27481491885323456
Iteration 40, Training loss = 0.26901652174439405
Iteration 50, Training loss = 0.2629926391125877
Iteration 60, Training loss = 0.26067286063939477
Iteration 70, Training loss = 0.2583479677925363
Iteration 80, Training loss = 0.2562961332535974
Iteration 90, Training loss = 0.25500538405300915
Model training time: 30.140065670013428
Device: cuda
Iteration 0, Training loss = 0.9965031469501735
Iteration 10, Training loss = 0.3138333515844483
Iteration 20, Training loss = 0.28945410489172174
Iteration 30, Training loss = 0.2773583933783038
Iteration 40, Training loss = 0.2682201494485284
Iteration 50, Training loss = 0.26141563147882335
Iteration 60, Training loss = 0.2564321386713337
Iteration 70, Training loss = 0.2527919502361961
Iteration 80, Training loss = 0.24993948886791864
Iteration 90, Training loss = 0.2477029991754587
Model training time: 30.04946255683899
Device: cuda
Iteration 0, Training loss = 0.9654283172266495
Iteration 10, Training loss = 0.30867736192717066
Iteration 20, Training loss = 0.28560715157916583
Iteration 30, Training loss = 0.27632791858508393
Iteration 40, Training loss = 0.27072879866413446
Iteration 50, Training loss = 0.2673212515487187
Iteration 60, Training loss = 0.2640240008488369
Iteration 70, Training loss = 0.2621488858392273
Iteration 80, Training loss = 0.2599645673436819
Iteration 90, Training loss = 0.2585324255596612
Model training time: 30.273972511291504
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0338599111722864
Iteration 10, Training loss = 0.9947229842632865
Iteration 20, Training loss = 0.9907082765574616
Iteration 30, Training loss = 0.983391655240082
Iteration 40, Training loss = 0.9750985701878866
Iteration 50, Training loss = 0.9609320684907517
Iteration 60, Training loss = 0.9403274773399611
Iteration 70, Training loss = 0.9076370536417201
Iteration 80, Training loss = 0.8548210234457744
Iteration 90, Training loss = 0.7764542867308077
Model training time: 25.817373752593994
Device: cuda
Iteration 0, Training loss = 1.0115603905369117
Iteration 10, Training loss = 0.996284506459167
Iteration 20, Training loss = 0.9914230338617224
Iteration 30, Training loss = 0.9834385082341622
Iteration 40, Training loss = 0.9769590770564793
Iteration 50, Training loss = 0.9654088262198628
Iteration 60, Training loss = 0.9470168239828469
Iteration 70, Training loss = 0.9179299994943223
Iteration 80, Training loss = 0.8721011327084712
Iteration 90, Training loss = 0.7992528275303219
Model training time: 26.580235719680786
Device: cuda
Iteration 0, Training loss = 1.0654429283694944
Iteration 10, Training loss = 0.9941343564342185
Iteration 20, Training loss = 0.9914117130104471
Iteration 30, Training loss = 0.9840761115009659
Iteration 40, Training loss = 0.974799540883677
Iteration 50, Training loss = 0.9587187614417882
Iteration 60, Training loss = 0.9378356026566547
Iteration 70, Training loss = 0.9019756368968798
Iteration 80, Training loss = 0.8404225463740491
Iteration 90, Training loss = 0.7444570575647308
Model training time: 27.744356155395508
Device: cuda
Iteration 0, Training loss = 1.0114050470112603
Iteration 10, Training loss = 0.9993398111799489
Iteration 20, Training loss = 0.9916800112252074
Iteration 30, Training loss = 0.9814134004035434
Iteration 40, Training loss = 0.9715457722760629
Iteration 50, Training loss = 0.9546439434595154
Iteration 60, Training loss = 0.9324382252739247
Iteration 70, Training loss = 0.8938909247877517
Iteration 80, Training loss = 0.8346855623134668
Iteration 90, Training loss = 0.7486106547468526
Model training time: 27.277055978775024
Device: cuda
Iteration 0, Training loss = 1.0008677791282174
Iteration 10, Training loss = 0.9964845209305989
Iteration 20, Training loss = 0.9931005115670283
Iteration 30, Training loss = 0.9890699544966509
Iteration 40, Training loss = 0.9846270087260555
Iteration 50, Training loss = 0.9738389319843717
Iteration 60, Training loss = 0.9620977971288893
Iteration 70, Training loss = 0.942785618385831
Iteration 80, Training loss = 0.9120927625808163
Iteration 90, Training loss = 0.8631233561442094
Model training time: 26.785333156585693
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9818755526473557
Iteration 10, Training loss = 0.3064184456631757
Iteration 20, Training loss = 0.29367855997477177
Iteration 30, Training loss = 0.2839260560590864
Iteration 40, Training loss = 0.27536865633323
Iteration 50, Training loss = 0.26933805601320404
Iteration 60, Training loss = 0.2643276399028474
Iteration 70, Training loss = 0.26112131925596704
Iteration 80, Training loss = 0.2584300591173955
Iteration 90, Training loss = 0.25717668498051915
Model training time: 30.771859645843506
Device: cuda
Iteration 0, Training loss = 0.962007064462284
Iteration 10, Training loss = 0.30500874542383755
Iteration 20, Training loss = 0.29151671754133296
Iteration 30, Training loss = 0.2819132235819015
Iteration 40, Training loss = 0.27449961917267907
Iteration 50, Training loss = 0.2691254923525064
Iteration 60, Training loss = 0.265843794433679
Iteration 70, Training loss = 0.261574422283737
Iteration 80, Training loss = 0.2592174309319344
Iteration 90, Training loss = 0.2570736230427516
Model training time: 31.35049343109131
Device: cuda
Iteration 0, Training loss = 0.9720265478039709
Iteration 10, Training loss = 0.3035134287272099
Iteration 20, Training loss = 0.29049509288608166
Iteration 30, Training loss = 0.28134588047790066
Iteration 40, Training loss = 0.2749969090744493
Iteration 50, Training loss = 0.2696727441560819
Iteration 60, Training loss = 0.26560790192533806
Iteration 70, Training loss = 0.2623908851961583
Iteration 80, Training loss = 0.25968357197184494
Iteration 90, Training loss = 0.2569982600600823
Model training time: 30.269505500793457
Device: cuda
Iteration 0, Training loss = 0.9832424607829772
Iteration 10, Training loss = 0.30031919205822233
Iteration 20, Training loss = 0.2883948748526366
Iteration 30, Training loss = 0.27957220785859704
Iteration 40, Training loss = 0.2713067751503797
Iteration 50, Training loss = 0.2641795810294036
Iteration 60, Training loss = 0.26056619650787777
Iteration 70, Training loss = 0.25837564112051675
Iteration 80, Training loss = 0.25606399375027505
Iteration 90, Training loss = 0.25362206344011323
Model training time: 29.68601703643799
Device: cuda
Iteration 0, Training loss = 1.0113570128662
Iteration 10, Training loss = 0.30392407042824704
Iteration 20, Training loss = 0.28859401904586435
Iteration 30, Training loss = 0.28005098735076794
Iteration 40, Training loss = 0.27367166638518303
Iteration 50, Training loss = 0.2689136763413747
Iteration 60, Training loss = 0.2640532661895245
Iteration 70, Training loss = 0.26060769300241976
Iteration 80, Training loss = 0.2574340108656077
Iteration 90, Training loss = 0.2555421734057763
Model training time: 30.038196802139282
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0242548420809316
Iteration 10, Training loss = 1.0064796030809338
Iteration 20, Training loss = 0.9995696432348611
Iteration 30, Training loss = 0.9950582272764565
Iteration 40, Training loss = 0.9858076632886693
Iteration 50, Training loss = 0.9761641549603375
Iteration 60, Training loss = 0.9601468933377288
Iteration 70, Training loss = 0.936664120586598
Iteration 80, Training loss = 0.9007452242616294
Iteration 90, Training loss = 0.8395706608099638
Model training time: 26.337078332901
Device: cuda
Iteration 0, Training loss = 1.118686598280202
Iteration 10, Training loss = 0.9966681533389621
Iteration 20, Training loss = 0.9964729181810278
Iteration 30, Training loss = 0.9889515681543212
Iteration 40, Training loss = 0.984050236462395
Iteration 50, Training loss = 0.9774008151413738
Iteration 60, Training loss = 0.9651630221357668
Iteration 70, Training loss = 0.943078026679403
Iteration 80, Training loss = 0.9115343020446058
Iteration 90, Training loss = 0.8605393496688437
Model training time: 25.88160490989685
Device: cuda
Iteration 0, Training loss = 1.0235456475889049
Iteration 10, Training loss = 1.0010526361672774
Iteration 20, Training loss = 0.9980080674811838
Iteration 30, Training loss = 0.9954378028998628
Iteration 40, Training loss = 0.9914015042033173
Iteration 50, Training loss = 0.9858338665847041
Iteration 60, Training loss = 0.9768838174101235
Iteration 70, Training loss = 0.9646470019782799
Iteration 80, Training loss = 0.9467274045598679
Iteration 90, Training loss = 0.9125152444781889
Model training time: 26.31618046760559
Device: cuda
Iteration 0, Training loss = 1.0573352713515793
Iteration 10, Training loss = 0.9954164724994973
Iteration 20, Training loss = 0.9884737191568826
Iteration 30, Training loss = 0.9825864920869541
Iteration 40, Training loss = 0.9729908546387861
Iteration 50, Training loss = 0.9586154625611605
Iteration 60, Training loss = 0.9343187636223392
Iteration 70, Training loss = 0.8995677123323155
Iteration 80, Training loss = 0.840460827673115
Iteration 90, Training loss = 0.7555391298975922
Model training time: 26.96889305114746
Device: cuda
Iteration 0, Training loss = 1.0097002608764574
Iteration 10, Training loss = 0.9879370072613591
Iteration 20, Training loss = 0.9828021840196877
Iteration 30, Training loss = 0.9735019541017099
Iteration 40, Training loss = 0.9624846839674429
Iteration 50, Training loss = 0.9398063856046557
Iteration 60, Training loss = 0.9057502134698601
Iteration 70, Training loss = 0.852428042082395
Iteration 80, Training loss = 0.7666996671381779
Iteration 90, Training loss = 0.6485465122881718
Model training time: 25.836525678634644
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9491907064465509
Iteration 10, Training loss = 0.30271592419504545
Iteration 20, Training loss = 0.29064437336679816
Iteration 30, Training loss = 0.2796814833286304
Iteration 40, Training loss = 0.27097775034858407
Iteration 50, Training loss = 0.2657543367521774
Iteration 60, Training loss = 0.2616380006434837
Iteration 70, Training loss = 0.2586295035294289
Iteration 80, Training loss = 0.25538565369619837
Iteration 90, Training loss = 0.25270031662522885
Iteration 100, Training loss = 0.25202502623416373
Iteration 110, Training loss = 0.2481351179489191
Iteration 120, Training loss = 0.2455377638483969
Iteration 130, Training loss = 0.24437801485908203
Iteration 140, Training loss = 0.24309977111609085
Iteration 150, Training loss = 0.2409573303544579
Iteration 160, Training loss = 0.23962983478238617
Iteration 170, Training loss = 0.23790646027683635
Iteration 180, Training loss = 0.2364704509936093
Iteration 190, Training loss = 0.23555043145366336
Model training time: 60.51307964324951
Device: cuda
Iteration 0, Training loss = 0.9101482615378744
Iteration 10, Training loss = 0.30446110622606415
Iteration 20, Training loss = 0.28809427469968796
Iteration 30, Training loss = 0.27824179169492447
Iteration 40, Training loss = 0.27123293550550076
Iteration 50, Training loss = 0.26604938906603964
Iteration 60, Training loss = 0.2627839834793754
Iteration 70, Training loss = 0.2601074702353869
Iteration 80, Training loss = 0.2581015056098141
Iteration 90, Training loss = 0.25575532008340396
Iteration 100, Training loss = 0.2546949999559905
Iteration 110, Training loss = 0.2521318473317773
Iteration 120, Training loss = 0.2514743478113902
Iteration 130, Training loss = 0.25059848995024453
Iteration 140, Training loss = 0.24923192461331686
Iteration 150, Training loss = 0.24804010612953112
Iteration 160, Training loss = 0.2469959572173547
Iteration 170, Training loss = 0.24606266031086732
Iteration 180, Training loss = 0.24538758731838586
Iteration 190, Training loss = 0.2443494025059944
Model training time: 60.11998629570007
Device: cuda
Iteration 0, Training loss = 0.9683673016115087
Iteration 10, Training loss = 0.2999040355572954
Iteration 20, Training loss = 0.28662584758035226
Iteration 30, Training loss = 0.2785021854195618
Iteration 40, Training loss = 0.2712717693712976
Iteration 50, Training loss = 0.26631692850935285
Iteration 60, Training loss = 0.2630734737728529
Iteration 70, Training loss = 0.2610118450317982
Iteration 80, Training loss = 0.25693785147246534
Iteration 90, Training loss = 0.2542754021531718
Iteration 100, Training loss = 0.25186452888636196
Iteration 110, Training loss = 0.25039779787190297
Iteration 120, Training loss = 0.24794380744730216
Iteration 130, Training loss = 0.24547373863378008
Iteration 140, Training loss = 0.24352513091719669
Iteration 150, Training loss = 0.24145228329344073
Iteration 160, Training loss = 0.23953304079851667
Iteration 170, Training loss = 0.23688064756744726
Iteration 180, Training loss = 0.2355422142551141
Iteration 190, Training loss = 0.23480709681764317
Model training time: 60.22695851325989
Device: cuda
Iteration 0, Training loss = 0.9357999197526831
Iteration 10, Training loss = 0.29783707014892413
Iteration 20, Training loss = 0.2828922213419624
Iteration 30, Training loss = 0.2720419954562533
Iteration 40, Training loss = 0.2655326085174141
Iteration 50, Training loss = 0.2622361664034894
Iteration 60, Training loss = 0.25840506929418317
Iteration 70, Training loss = 0.25524337596939384
Iteration 80, Training loss = 0.25249674289986707
Iteration 90, Training loss = 0.25128792679828144
Iteration 100, Training loss = 0.24951970688818734
Iteration 110, Training loss = 0.2477301216283858
Iteration 120, Training loss = 0.2462481846627982
Iteration 130, Training loss = 0.2449414158500911
Iteration 140, Training loss = 0.24471007467468003
Iteration 150, Training loss = 0.24254983422404902
Iteration 160, Training loss = 0.24137344074566008
Iteration 170, Training loss = 0.240337862988601
Iteration 180, Training loss = 0.2384053786523676
Iteration 190, Training loss = 0.23689588400476796
Model training time: 60.41032934188843
Device: cuda
Iteration 0, Training loss = 0.9874798350288096
Iteration 10, Training loss = 0.30231108124129435
Iteration 20, Training loss = 0.28614291969848715
Iteration 30, Training loss = 0.277076480721218
Iteration 40, Training loss = 0.2698364383428569
Iteration 50, Training loss = 0.2624095952237286
Iteration 60, Training loss = 0.2572784345003142
Iteration 70, Training loss = 0.25394768631400694
Iteration 80, Training loss = 0.25093452657622417
Iteration 90, Training loss = 0.24775108162331697
Iteration 100, Training loss = 0.24610540803503875
Iteration 110, Training loss = 0.24424704272246014
Iteration 120, Training loss = 0.24182091977285303
Iteration 130, Training loss = 0.24044745552223085
Iteration 140, Training loss = 0.23896150636500207
Iteration 150, Training loss = 0.23819956057457534
Iteration 160, Training loss = 0.23643792701372202
Iteration 170, Training loss = 0.23496064011025544
Iteration 180, Training loss = 0.23379603186667253
Iteration 190, Training loss = 0.23328450613695642
Model training time: 59.73948788642883
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.185697128807289
Iteration 10, Training loss = 1.0005002833794856
Iteration 20, Training loss = 0.9965958863064863
Iteration 30, Training loss = 0.9914920368920201
Iteration 40, Training loss = 0.9849032022526875
Iteration 50, Training loss = 0.977042928122092
Iteration 60, Training loss = 0.9632236182977612
Iteration 70, Training loss = 0.9418733318646749
Iteration 80, Training loss = 0.9044017149630376
Iteration 90, Training loss = 0.8446898119173188
Iteration 100, Training loss = 0.7463875119236932
Iteration 110, Training loss = 0.6156476360300313
Iteration 120, Training loss = 0.4899679785909284
Iteration 130, Training loss = 0.4139043928488441
Iteration 140, Training loss = 0.38270818575280874
Iteration 150, Training loss = 0.37006632278218937
Iteration 160, Training loss = 0.362700415622209
Iteration 170, Training loss = 0.35778788819117247
Iteration 180, Training loss = 0.3531630124878768
Iteration 190, Training loss = 0.3494653521096649
Model training time: 51.86681079864502
Device: cuda
Iteration 0, Training loss = 1.0908380876416746
Iteration 10, Training loss = 0.9938486935435862
Iteration 20, Training loss = 0.9870601342496089
Iteration 30, Training loss = 0.9754642654156339
Iteration 40, Training loss = 0.9595596837824669
Iteration 50, Training loss = 0.936259747127404
Iteration 60, Training loss = 0.8963613685778373
Iteration 70, Training loss = 0.8301587136471329
Iteration 80, Training loss = 0.726172894095453
Iteration 90, Training loss = 0.5951458918299652
Iteration 100, Training loss = 0.4802030962014544
Iteration 110, Training loss = 0.41200602461318464
Iteration 120, Training loss = 0.3859673532598836
Iteration 130, Training loss = 0.3730003971984421
Iteration 140, Training loss = 0.3662183541750562
Iteration 150, Training loss = 0.36110169120168917
Iteration 160, Training loss = 0.3567005494510494
Iteration 170, Training loss = 0.3518030319524848
Iteration 180, Training loss = 0.3484597720817667
Iteration 190, Training loss = 0.34548039036096584
Model training time: 51.896504163742065
Device: cuda
Iteration 0, Training loss = 1.0021764820324626
Iteration 10, Training loss = 0.997307562885653
Iteration 20, Training loss = 0.9956334578818169
Iteration 30, Training loss = 0.9889945540451197
Iteration 40, Training loss = 0.985462812985775
Iteration 50, Training loss = 0.9754758835702703
Iteration 60, Training loss = 0.9617171286096895
Iteration 70, Training loss = 0.944231200909269
Iteration 80, Training loss = 0.9100727366940411
Iteration 90, Training loss = 0.8563424931920093
Iteration 100, Training loss = 0.7726181795919574
Iteration 110, Training loss = 0.6563554237718168
Iteration 120, Training loss = 0.5322742342660969
Iteration 130, Training loss = 0.4432427430354455
Iteration 140, Training loss = 0.40126616007463944
Iteration 150, Training loss = 0.38447916270166205
Iteration 160, Training loss = 0.37385222906075816
Iteration 170, Training loss = 0.3665015637730631
Iteration 180, Training loss = 0.3607435830405369
Iteration 190, Training loss = 0.3550704490879308
Model training time: 51.44315218925476
Device: cuda
Iteration 0, Training loss = 1.0670403724419322
Iteration 10, Training loss = 0.985672067184955
Iteration 20, Training loss = 0.9764205418923051
Iteration 30, Training loss = 0.9631026807614571
Iteration 40, Training loss = 0.9429967116906447
Iteration 50, Training loss = 0.9071301591857044
Iteration 60, Training loss = 0.849083231555091
Iteration 70, Training loss = 0.7565420236852434
Iteration 80, Training loss = 0.6299148089643838
Iteration 90, Training loss = 0.5036325147330473
Iteration 100, Training loss = 0.4199164492496546
Iteration 110, Training loss = 0.3799831265412667
Iteration 120, Training loss = 0.3633414817892987
Iteration 130, Training loss = 0.3547729536675025
Iteration 140, Training loss = 0.35012537694495655
Iteration 150, Training loss = 0.34443457777373454
Iteration 160, Training loss = 0.34029489863609924
Iteration 170, Training loss = 0.3374240363853565
Iteration 180, Training loss = 0.3345504537008811
Iteration 190, Training loss = 0.3317776609590088
Model training time: 51.6332733631134
Device: cuda
Iteration 0, Training loss = 1.011003779904278
Iteration 10, Training loss = 1.003170225067415
Iteration 20, Training loss = 0.9976034789269673
Iteration 30, Training loss = 0.9929195452427518
Iteration 40, Training loss = 0.9875451216951084
Iteration 50, Training loss = 0.9813876186591991
Iteration 60, Training loss = 0.9695364876069884
Iteration 70, Training loss = 0.9539665994436844
Iteration 80, Training loss = 0.9292393310922356
Iteration 90, Training loss = 0.8872771795821075
Iteration 100, Training loss = 0.821362831166401
Iteration 110, Training loss = 0.7263531224163258
Iteration 120, Training loss = 0.6110210651936738
Iteration 130, Training loss = 0.5076937305466565
Iteration 140, Training loss = 0.43681498008649705
Iteration 150, Training loss = 0.40171327344749286
Iteration 160, Training loss = 0.3841262658437093
Iteration 170, Training loss = 0.37239154215884096
Iteration 180, Training loss = 0.363635686788582
Iteration 190, Training loss = 0.35696092629490267
Model training time: 52.272822856903076
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9692278193390887
Iteration 10, Training loss = 0.3073194247225057
Iteration 20, Training loss = 0.29381260923717334
Iteration 30, Training loss = 0.282513243564661
Iteration 40, Training loss = 0.2743219460410196
Iteration 50, Training loss = 0.26841443086016004
Iteration 60, Training loss = 0.26307932035502607
Iteration 70, Training loss = 0.2603868904753008
Iteration 80, Training loss = 0.2567713267849263
Iteration 90, Training loss = 0.2545573062078964
Iteration 100, Training loss = 0.2515545574147344
Iteration 110, Training loss = 0.24982643203027005
Iteration 120, Training loss = 0.24783000839505218
Iteration 130, Training loss = 0.24583412703252644
Iteration 140, Training loss = 0.24375527114539908
Iteration 150, Training loss = 0.24309859021706282
Iteration 160, Training loss = 0.24164454512981978
Iteration 170, Training loss = 0.2407700862861486
Iteration 180, Training loss = 0.23977570134948417
Iteration 190, Training loss = 0.23968066275119781
Model training time: 60.030553340911865
Device: cuda
Iteration 0, Training loss = 0.9822591402104511
Iteration 10, Training loss = 0.30888186495949105
Iteration 20, Training loss = 0.2903226806201797
Iteration 30, Training loss = 0.27836788510498794
Iteration 40, Training loss = 0.2708198030429762
Iteration 50, Training loss = 0.2667679161121304
Iteration 60, Training loss = 0.26282182688586375
Iteration 70, Training loss = 0.2595084944496984
Iteration 80, Training loss = 0.2566492278938708
Iteration 90, Training loss = 0.2544038223039701
Iteration 100, Training loss = 0.2519910271472977
Iteration 110, Training loss = 0.25010028762229974
Iteration 120, Training loss = 0.24769832679758902
Iteration 130, Training loss = 0.24694148353908374
Iteration 140, Training loss = 0.24470230404305574
Iteration 150, Training loss = 0.24325173017051485
Iteration 160, Training loss = 0.24209333182389034
Iteration 170, Training loss = 0.24083132609940958
Iteration 180, Training loss = 0.24060767025187396
Iteration 190, Training loss = 0.23924556410543008
Model training time: 59.4972620010376
Device: cuda
Iteration 0, Training loss = 0.9601757080082732
Iteration 10, Training loss = 0.3035978625937936
Iteration 20, Training loss = 0.2873600013374131
Iteration 30, Training loss = 0.27929031769722557
Iteration 40, Training loss = 0.27205621328762764
Iteration 50, Training loss = 0.2667888789145267
Iteration 60, Training loss = 0.2627009416116033
Iteration 70, Training loss = 0.26008738411797416
Iteration 80, Training loss = 0.2568589617016811
Iteration 90, Training loss = 0.25532797252498385
Iteration 100, Training loss = 0.25217376048294243
Iteration 110, Training loss = 0.25060232955476514
Iteration 120, Training loss = 0.2491917554019154
Iteration 130, Training loss = 0.24709979304368945
Iteration 140, Training loss = 0.24496851443956438
Iteration 150, Training loss = 0.24292688348874952
Iteration 160, Training loss = 0.2410827066011475
Iteration 170, Training loss = 0.24018159652677712
Iteration 180, Training loss = 0.2381811454028323
Iteration 190, Training loss = 0.23822689024002655
Model training time: 60.498632192611694
Device: cuda
Iteration 0, Training loss = 0.9878081221511399
Iteration 10, Training loss = 0.3093520753481538
Iteration 20, Training loss = 0.28851474468834737
Iteration 30, Training loss = 0.2794948963151462
Iteration 40, Training loss = 0.2737678717011991
Iteration 50, Training loss = 0.27002359951895794
Iteration 60, Training loss = 0.26637278662787545
Iteration 70, Training loss = 0.2636646852617103
Iteration 80, Training loss = 0.2617962730031658
Iteration 90, Training loss = 0.25897532089608877
Iteration 100, Training loss = 0.2571366606415182
Iteration 110, Training loss = 0.25577809139726243
Iteration 120, Training loss = 0.2544290316493615
Iteration 130, Training loss = 0.2531949754282472
Iteration 140, Training loss = 0.2508494988947675
Iteration 150, Training loss = 0.24844514243844626
Iteration 160, Training loss = 0.24671321203887175
Iteration 170, Training loss = 0.24490146094186294
Iteration 180, Training loss = 0.24309422863998276
Iteration 190, Training loss = 0.24139988994684772
Model training time: 61.14156770706177
Device: cuda
Iteration 0, Training loss = 0.9694931184612035
Iteration 10, Training loss = 0.3130477078007039
Iteration 20, Training loss = 0.28901368872698957
Iteration 30, Training loss = 0.2771950988593885
Iteration 40, Training loss = 0.26916368370470795
Iteration 50, Training loss = 0.2635826677587873
Iteration 60, Training loss = 0.2594584841011227
Iteration 70, Training loss = 0.2560180555579167
Iteration 80, Training loss = 0.2534771378417522
Iteration 90, Training loss = 0.251004403879965
Iteration 100, Training loss = 0.24998335021993387
Iteration 110, Training loss = 0.24694848305361283
Iteration 120, Training loss = 0.24508659689178788
Iteration 130, Training loss = 0.2441081903550936
Iteration 140, Training loss = 0.24286869009911727
Iteration 150, Training loss = 0.24046035686840758
Iteration 160, Training loss = 0.2393213465521877
Iteration 170, Training loss = 0.23773305413227727
Iteration 180, Training loss = 0.2362434685158269
Iteration 190, Training loss = 0.23602107692743846
Model training time: 59.843637228012085
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1043009539157296
Iteration 10, Training loss = 0.9872354942819347
Iteration 20, Training loss = 0.9800362604251807
Iteration 30, Training loss = 0.9680098436304913
Iteration 40, Training loss = 0.9536529574993152
Iteration 50, Training loss = 0.929140575842005
Iteration 60, Training loss = 0.8934212119682975
Iteration 70, Training loss = 0.8401854854563008
Iteration 80, Training loss = 0.7644956715153035
Iteration 90, Training loss = 0.66725159180913
Iteration 100, Training loss = 0.5674614830005572
Iteration 110, Training loss = 0.48645082051339356
Iteration 120, Training loss = 0.4384425304794081
Iteration 130, Training loss = 0.4133608023037657
Iteration 140, Training loss = 0.4001351547413978
Iteration 150, Training loss = 0.3908635948015296
Iteration 160, Training loss = 0.38368790276384585
Iteration 170, Training loss = 0.37756775355569405
Iteration 180, Training loss = 0.37126913468762873
Iteration 190, Training loss = 0.3669951297522743
Model training time: 51.32722234725952
Device: cuda
Iteration 0, Training loss = 1.0075022753885978
Iteration 10, Training loss = 0.9980538723549405
Iteration 20, Training loss = 0.9967105037348282
Iteration 30, Training loss = 0.9943819642066956
Iteration 40, Training loss = 0.9909311723018038
Iteration 50, Training loss = 0.985315942245981
Iteration 60, Training loss = 0.9789593481787161
Iteration 70, Training loss = 0.9705332511864998
Iteration 80, Training loss = 0.9553785367288451
Iteration 90, Training loss = 0.9298261905062026
Iteration 100, Training loss = 0.890911586906599
Iteration 110, Training loss = 0.8281106198755439
Iteration 120, Training loss = 0.7330115532529526
Iteration 130, Training loss = 0.621018915504649
Iteration 140, Training loss = 0.5179511874482252
Iteration 150, Training loss = 0.4499125956481206
Iteration 160, Training loss = 0.41369164832260297
Iteration 170, Training loss = 0.39811669714784853
Iteration 180, Training loss = 0.38428810872317515
Iteration 190, Training loss = 0.3757515359615934
Model training time: 53.55090641975403
Device: cuda
Iteration 0, Training loss = 1.002168105132338
Iteration 10, Training loss = 0.9982046911106017
Iteration 20, Training loss = 0.9951630954581182
Iteration 30, Training loss = 0.9917963970686503
Iteration 40, Training loss = 0.9874303602941946
Iteration 50, Training loss = 0.9823984685151473
Iteration 60, Training loss = 0.9765502744250827
Iteration 70, Training loss = 0.9651007905674441
Iteration 80, Training loss = 0.9522162180591897
Iteration 90, Training loss = 0.9312002094471512
Iteration 100, Training loss = 0.8969468788248329
Iteration 110, Training loss = 0.8430675237938978
Iteration 120, Training loss = 0.766789479388131
Iteration 130, Training loss = 0.6659753784753274
Iteration 140, Training loss = 0.5599063712617626
Iteration 150, Training loss = 0.47588409864960085
Iteration 160, Training loss = 0.42560019981170044
Iteration 170, Training loss = 0.39846789944862976
Iteration 180, Training loss = 0.38458075788285995
Iteration 190, Training loss = 0.3748042655451862
Model training time: 58.78709626197815
Device: cuda
Iteration 0, Training loss = 1.07571635355696
Iteration 10, Training loss = 0.9913829648552309
Iteration 20, Training loss = 0.9853892211176923
Iteration 30, Training loss = 0.9771557110519224
Iteration 40, Training loss = 0.9632400241163042
Iteration 50, Training loss = 0.9437925127393382
Iteration 60, Training loss = 0.9119775050504196
Iteration 70, Training loss = 0.8612396486715418
Iteration 80, Training loss = 0.7744225597611948
Iteration 90, Training loss = 0.6561365291692208
Iteration 100, Training loss = 0.5348522797010947
Iteration 110, Training loss = 0.44934748645853884
Iteration 120, Training loss = 0.4065435370385359
Iteration 130, Training loss = 0.38492377800642
Iteration 140, Training loss = 0.37364770975953715
Iteration 150, Training loss = 0.36569771141821633
Iteration 160, Training loss = 0.3587357250676639
Iteration 170, Training loss = 0.3536016934591791
Iteration 180, Training loss = 0.34984001531693093
Iteration 190, Training loss = 0.34276216027241396
Model training time: 58.93268609046936
Device: cuda
Iteration 0, Training loss = 1.0491443279284787
Iteration 10, Training loss = 0.9987178937248562
Iteration 20, Training loss = 0.9936677154135589
Iteration 30, Training loss = 0.9878291268855477
Iteration 40, Training loss = 0.979257111100183
Iteration 50, Training loss = 0.9704269098774823
Iteration 60, Training loss = 0.9540688421415247
Iteration 70, Training loss = 0.9305319829263549
Iteration 80, Training loss = 0.891465020928406
Iteration 90, Training loss = 0.8305470822513967
Iteration 100, Training loss = 0.7360156492334633
Iteration 110, Training loss = 0.6180228467148859
Iteration 120, Training loss = 0.5041749188001605
Iteration 130, Training loss = 0.4308022735775381
Iteration 140, Training loss = 0.3939015578100647
Iteration 150, Training loss = 0.3768260112563193
Iteration 160, Training loss = 0.36453430548958154
Iteration 170, Training loss = 0.3579957238286014
Iteration 180, Training loss = 0.350154090186824
Iteration 190, Training loss = 0.34430486273362443
Model training time: 52.881879568099976
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9943089182826056
Iteration 10, Training loss = 0.308211002588848
Iteration 20, Training loss = 0.29370738637908067
Iteration 30, Training loss = 0.28053485825297914
Iteration 40, Training loss = 0.2695314912620374
Iteration 50, Training loss = 0.2626853394839499
Iteration 60, Training loss = 0.25919105800021675
Iteration 70, Training loss = 0.2554378431776296
Iteration 80, Training loss = 0.2530929381792672
Iteration 90, Training loss = 0.25066546073570345
Iteration 100, Training loss = 0.24772724757591882
Iteration 110, Training loss = 0.24638722293474824
Iteration 120, Training loss = 0.24563882625909242
Iteration 130, Training loss = 0.24438789292522098
Iteration 140, Training loss = 0.24382036071324695
Iteration 150, Training loss = 0.24232661695296062
Iteration 160, Training loss = 0.24073206276997275
Iteration 170, Training loss = 0.24080085948757504
Iteration 180, Training loss = 0.2394466624383765
Iteration 190, Training loss = 0.23889927411280967
Model training time: 59.50192093849182
Device: cuda
Iteration 0, Training loss = 0.9505209673717978
Iteration 10, Training loss = 0.3046207562952802
Iteration 20, Training loss = 0.2935819111152548
Iteration 30, Training loss = 0.28286872908545
Iteration 40, Training loss = 0.2751792286329223
Iteration 50, Training loss = 0.2685975011996025
Iteration 60, Training loss = 0.26352287915305816
Iteration 70, Training loss = 0.2589625418546119
Iteration 80, Training loss = 0.256616549719359
Iteration 90, Training loss = 0.25408379996312414
Iteration 100, Training loss = 0.2530437589843492
Iteration 110, Training loss = 0.2512735279933842
Iteration 120, Training loss = 0.24864121065767492
Iteration 130, Training loss = 0.24719745226672307
Iteration 140, Training loss = 0.24579394788701753
Iteration 150, Training loss = 0.24437143311696352
Iteration 160, Training loss = 0.24271779622576664
Iteration 170, Training loss = 0.24064185225589263
Iteration 180, Training loss = 0.23912664398479
Iteration 190, Training loss = 0.2376537454373019
Model training time: 59.40808701515198
Device: cuda
Iteration 0, Training loss = 0.9557468816278062
Iteration 10, Training loss = 0.3030892885537539
Iteration 20, Training loss = 0.2893100141903053
Iteration 30, Training loss = 0.28010168844375055
Iteration 40, Training loss = 0.2729673258204391
Iteration 50, Training loss = 0.26715773282851574
Iteration 60, Training loss = 0.2630315477721357
Iteration 70, Training loss = 0.2597440709742371
Iteration 80, Training loss = 0.2585309112561498
Iteration 90, Training loss = 0.2559910770775615
Iteration 100, Training loss = 0.25461068180736135
Iteration 110, Training loss = 0.25314632266903847
Iteration 120, Training loss = 0.25088047275796604
Iteration 130, Training loss = 0.248811360610568
Iteration 140, Training loss = 0.24780037704009364
Iteration 150, Training loss = 0.24542430298771836
Iteration 160, Training loss = 0.24393036605223364
Iteration 170, Training loss = 0.24267201895875054
Iteration 180, Training loss = 0.24097007700210607
Iteration 190, Training loss = 0.23964695499283103
Model training time: 59.169509172439575
Device: cuda
Iteration 0, Training loss = 0.9802831604284941
Iteration 10, Training loss = 0.2998032623155106
Iteration 20, Training loss = 0.2846724684687628
Iteration 30, Training loss = 0.2750947486951155
Iteration 40, Training loss = 0.26944163319281333
Iteration 50, Training loss = 0.2656617713867178
Iteration 60, Training loss = 0.26365151929394637
Iteration 70, Training loss = 0.2582972220680564
Iteration 80, Training loss = 0.25602812255206314
Iteration 90, Training loss = 0.2536048265039057
Iteration 100, Training loss = 0.2513407493631045
Iteration 110, Training loss = 0.25062122192359776
Iteration 120, Training loss = 0.24723418215334703
Iteration 130, Training loss = 0.24613128591706787
Iteration 140, Training loss = 0.2454903638665227
Iteration 150, Training loss = 0.24397170388900138
Iteration 160, Training loss = 0.2426935883535855
Iteration 170, Training loss = 0.24267953324721056
Iteration 180, Training loss = 0.2422634021959443
Iteration 190, Training loss = 0.24029331794683484
Model training time: 59.907233476638794
Device: cuda
Iteration 0, Training loss = 0.9606233377387559
Iteration 10, Training loss = 0.30288132334101026
Iteration 20, Training loss = 0.29001207952050195
Iteration 30, Training loss = 0.2799336333637652
Iteration 40, Training loss = 0.2724066209317981
Iteration 50, Training loss = 0.2666490803234243
Iteration 60, Training loss = 0.26343747267976475
Iteration 70, Training loss = 0.26097646017293424
Iteration 80, Training loss = 0.25903843480031846
Iteration 90, Training loss = 0.2578338813306629
Iteration 100, Training loss = 0.25580441563025763
Iteration 110, Training loss = 0.25494306955648505
Iteration 120, Training loss = 0.2531936345756918
Iteration 130, Training loss = 0.2518394106756086
Iteration 140, Training loss = 0.2511373076965843
Iteration 150, Training loss = 0.24988764690028298
Iteration 160, Training loss = 0.24928944806257883
Iteration 170, Training loss = 0.24883982049670197
Iteration 180, Training loss = 0.24769079274889352
Iteration 190, Training loss = 0.24750194259455816
Model training time: 59.59565258026123
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0113182874117497
Iteration 10, Training loss = 0.9923184476612846
Iteration 20, Training loss = 0.9859432304538966
Iteration 30, Training loss = 0.9762213497922041
Iteration 40, Training loss = 0.9619820899721505
Iteration 50, Training loss = 0.9421688217471763
Iteration 60, Training loss = 0.908451750802533
Iteration 70, Training loss = 0.8521848215861021
Iteration 80, Training loss = 0.7639540364776832
Iteration 90, Training loss = 0.6457193780348497
Iteration 100, Training loss = 0.5258483762902338
Iteration 110, Training loss = 0.4438655372835012
Iteration 120, Training loss = 0.40683469658600535
Iteration 130, Training loss = 0.3875659494008419
Iteration 140, Training loss = 0.3780956875154938
Iteration 150, Training loss = 0.3706855405355997
Iteration 160, Training loss = 0.36470361514655864
Iteration 170, Training loss = 0.3592685963508587
Iteration 180, Training loss = 0.3548771776438911
Iteration 190, Training loss = 0.3505842038542752
Model training time: 50.94994139671326
Device: cuda
Iteration 0, Training loss = 1.0246775341782592
Iteration 10, Training loss = 0.9968921493216989
Iteration 20, Training loss = 0.9940050702164138
Iteration 30, Training loss = 0.9916891740716022
Iteration 40, Training loss = 0.9874230130859043
Iteration 50, Training loss = 0.981308290635906
Iteration 60, Training loss = 0.9728283971404108
Iteration 70, Training loss = 0.9600682293159374
Iteration 80, Training loss = 0.9384777174479719
Iteration 90, Training loss = 0.9038209674726938
Iteration 100, Training loss = 0.8479179902641094
Iteration 110, Training loss = 0.7605671528456868
Iteration 120, Training loss = 0.6418944803125041
Iteration 130, Training loss = 0.5232475029241636
Iteration 140, Training loss = 0.43979255552741064
Iteration 150, Training loss = 0.39983701734727134
Iteration 160, Training loss = 0.3816246547272816
Iteration 170, Training loss = 0.37162317306811105
Iteration 180, Training loss = 0.3651953402924653
Iteration 190, Training loss = 0.35925626078089656
Model training time: 51.83849000930786
Device: cuda
Iteration 0, Training loss = 1.0276218730470408
Iteration 10, Training loss = 1.0017774767345853
Iteration 20, Training loss = 0.9952943480533102
Iteration 30, Training loss = 0.9878366203699711
Iteration 40, Training loss = 0.9782753457193789
Iteration 50, Training loss = 0.9669249714284703
Iteration 60, Training loss = 0.9449394345283508
Iteration 70, Training loss = 0.9086591494832061
Iteration 80, Training loss = 0.8491165393216599
Iteration 90, Training loss = 0.7483242714174704
Iteration 100, Training loss = 0.6151762958885967
Iteration 110, Training loss = 0.48762352814996873
Iteration 120, Training loss = 0.4124494722643912
Iteration 130, Training loss = 0.38293870779627187
Iteration 140, Training loss = 0.37187612474252635
Iteration 150, Training loss = 0.36466833632349394
Iteration 160, Training loss = 0.3600697216492344
Iteration 170, Training loss = 0.35561295084043404
Iteration 180, Training loss = 0.3516176221307349
Iteration 190, Training loss = 0.34842934189499286
Model training time: 52.92485070228577
Device: cuda
Iteration 0, Training loss = 1.0505290444634388
Iteration 10, Training loss = 1.0026506939947892
Iteration 20, Training loss = 1.000499019012359
Iteration 30, Training loss = 0.9979975517245306
Iteration 40, Training loss = 0.9937294712965039
Iteration 50, Training loss = 0.9921327492465144
Iteration 60, Training loss = 0.984896975726897
Iteration 70, Training loss = 0.9787444581732082
Iteration 80, Training loss = 0.9654624755831732
Iteration 90, Training loss = 0.9485418854128336
Iteration 100, Training loss = 0.923546836859938
Iteration 110, Training loss = 0.8785512369611989
Iteration 120, Training loss = 0.8077467196229575
Iteration 130, Training loss = 0.7051597503936233
Iteration 140, Training loss = 0.5835022785237446
Iteration 150, Training loss = 0.4781650180258037
Iteration 160, Training loss = 0.4171657601128454
Iteration 170, Training loss = 0.38913898803473673
Iteration 180, Training loss = 0.3759277408969575
Iteration 190, Training loss = 0.3673077856284985
Model training time: 54.099348306655884
Device: cuda
Iteration 0, Training loss = 1.0102966211844182
Iteration 10, Training loss = 1.0030766905505877
Iteration 20, Training loss = 0.998655400990288
Iteration 30, Training loss = 0.9912119484754001
Iteration 40, Training loss = 0.9852800984025577
Iteration 50, Training loss = 0.9769617854685023
Iteration 60, Training loss = 0.963072401313966
Iteration 70, Training loss = 0.9434257253356602
Iteration 80, Training loss = 0.9079168834836011
Iteration 90, Training loss = 0.8527609412220941
Iteration 100, Training loss = 0.7636652680986745
Iteration 110, Training loss = 0.6383689823357955
Iteration 120, Training loss = 0.5094812014252668
Iteration 130, Training loss = 0.42266839829044067
Iteration 140, Training loss = 0.3848102911946854
Iteration 150, Training loss = 0.36784465467008415
Iteration 160, Training loss = 0.3602276345094045
Iteration 170, Training loss = 0.35501733026354787
Iteration 180, Training loss = 0.3507104990418982
Iteration 190, Training loss = 0.34662948217656875
Model training time: 50.646645069122314
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9550493198316454
Iteration 10, Training loss = 0.3035563360809704
Iteration 20, Training loss = 0.28914548021151826
Iteration 30, Training loss = 0.2769049997059044
Iteration 40, Training loss = 0.27022398615948817
Iteration 50, Training loss = 0.26638472022641685
Iteration 60, Training loss = 0.2627306043525825
Iteration 70, Training loss = 0.2597903727693258
Iteration 80, Training loss = 0.25830302232705454
Iteration 90, Training loss = 0.2570256578605532
Iteration 100, Training loss = 0.25439935717461765
Iteration 110, Training loss = 0.2525843571925509
Iteration 120, Training loss = 0.25120613961548044
Iteration 130, Training loss = 0.25040648586076236
Iteration 140, Training loss = 0.24877902012372363
Iteration 150, Training loss = 0.24683636322545544
Iteration 160, Training loss = 0.2451349795224586
Iteration 170, Training loss = 0.24352967551940882
Iteration 180, Training loss = 0.24189988548916894
Iteration 190, Training loss = 0.24034695899572925
Iteration 200, Training loss = 0.2392783626050189
Iteration 210, Training loss = 0.2372951479493708
Iteration 220, Training loss = 0.2361768033121519
Iteration 230, Training loss = 0.2346094828368961
Iteration 240, Training loss = 0.23298753236514935
Iteration 250, Training loss = 0.2329970366784916
Iteration 260, Training loss = 0.23169453838021284
Iteration 270, Training loss = 0.2303345693266334
Iteration 280, Training loss = 0.22866087518452446
Iteration 290, Training loss = 0.22819018947041553
Model training time: 87.47245860099792
Device: cuda
Iteration 0, Training loss = 0.9442439777551642
Iteration 10, Training loss = 0.30570573593683287
Iteration 20, Training loss = 0.28792908928100613
Iteration 30, Training loss = 0.2750839238293505
Iteration 40, Training loss = 0.26846092367085855
Iteration 50, Training loss = 0.26402489689813147
Iteration 60, Training loss = 0.26023179436651406
Iteration 70, Training loss = 0.2579831719254526
Iteration 80, Training loss = 0.25631717164159395
Iteration 90, Training loss = 0.2544464820968932
Iteration 100, Training loss = 0.2523416346973843
Iteration 110, Training loss = 0.25097318911466043
Iteration 120, Training loss = 0.24955472659661573
Iteration 130, Training loss = 0.24747606975156905
Iteration 140, Training loss = 0.24599060826543448
Iteration 150, Training loss = 0.24459430503384502
Iteration 160, Training loss = 0.24286272369577114
Iteration 170, Training loss = 0.24155544615598115
Iteration 180, Training loss = 0.2397449825696899
Iteration 190, Training loss = 0.23909793236261404
Iteration 200, Training loss = 0.2374999661376511
Iteration 210, Training loss = 0.2364226100957336
Iteration 220, Training loss = 0.23388892642541784
Iteration 230, Training loss = 0.2320177334446262
Iteration 240, Training loss = 0.23123173422859486
Iteration 250, Training loss = 0.2293056774398555
Iteration 260, Training loss = 0.22844880362207762
Iteration 270, Training loss = 0.22731161945396
Iteration 280, Training loss = 0.22613624922463282
Iteration 290, Training loss = 0.22525638291513286
Model training time: 88.65890645980835
Device: cuda
Iteration 0, Training loss = 0.9527655605244751
Iteration 10, Training loss = 0.29944770880367444
Iteration 20, Training loss = 0.2874785766437434
Iteration 30, Training loss = 0.28000643381462004
Iteration 40, Training loss = 0.273524217415547
Iteration 50, Training loss = 0.2686790222131112
Iteration 60, Training loss = 0.2658269187606475
Iteration 70, Training loss = 0.26232994941697607
Iteration 80, Training loss = 0.25941007929867593
Iteration 90, Training loss = 0.2585100662233173
Iteration 100, Training loss = 0.2553783488518374
Iteration 110, Training loss = 0.2536423528108044
Iteration 120, Training loss = 0.2514444696514503
Iteration 130, Training loss = 0.25025021975886996
Iteration 140, Training loss = 0.24876373495168733
Iteration 150, Training loss = 0.24771155809290743
Iteration 160, Training loss = 0.246056511098348
Iteration 170, Training loss = 0.24504826023526813
Iteration 180, Training loss = 0.24357375588969907
Iteration 190, Training loss = 0.24226872155055906
Iteration 200, Training loss = 0.2418829950229557
Iteration 210, Training loss = 0.2404903066906952
Iteration 220, Training loss = 0.24021891209382365
Iteration 230, Training loss = 0.23946258184558528
Iteration 240, Training loss = 0.238476459890748
Iteration 250, Training loss = 0.2387753166438301
Iteration 260, Training loss = 0.23773485519315884
Iteration 270, Training loss = 0.23656530833042763
Iteration 280, Training loss = 0.23621050300805466
Iteration 290, Training loss = 0.23522144876384504
Model training time: 89.01136112213135
Device: cuda
Iteration 0, Training loss = 0.9670095904437817
Iteration 10, Training loss = 0.3020193466313795
Iteration 20, Training loss = 0.28945906813017985
Iteration 30, Training loss = 0.27979708804888426
Iteration 40, Training loss = 0.27053087978547324
Iteration 50, Training loss = 0.26312162874689426
Iteration 60, Training loss = 0.2586521439362263
Iteration 70, Training loss = 0.2560629478255332
Iteration 80, Training loss = 0.25296899179617566
Iteration 90, Training loss = 0.2510018820204021
Iteration 100, Training loss = 0.24900298936355517
Iteration 110, Training loss = 0.2464087584240425
Iteration 120, Training loss = 0.2457270903719796
Iteration 130, Training loss = 0.24391390271664817
Iteration 140, Training loss = 0.2418178489818665
Iteration 150, Training loss = 0.24039318160590342
Iteration 160, Training loss = 0.2380893622043628
Iteration 170, Training loss = 0.23700136255814833
Iteration 180, Training loss = 0.23526667936700554
Iteration 190, Training loss = 0.2351299192810404
Iteration 200, Training loss = 0.23298997410397598
Iteration 210, Training loss = 0.23214717954397202
Iteration 220, Training loss = 0.23149120440517645
Iteration 230, Training loss = 0.23052315496736103
Iteration 240, Training loss = 0.2282834415850432
Iteration 250, Training loss = 0.22758497322959023
Iteration 260, Training loss = 0.22554102752375718
Iteration 270, Training loss = 0.2249143867964906
Iteration 280, Training loss = 0.22276192701957076
Iteration 290, Training loss = 0.22225885510732588
Model training time: 88.3996160030365
Device: cuda
Iteration 0, Training loss = 0.9678391945246912
Iteration 10, Training loss = 0.298286564540172
Iteration 20, Training loss = 0.28188249619974604
Iteration 30, Training loss = 0.2705944944623012
Iteration 40, Training loss = 0.2643960548771752
Iteration 50, Training loss = 0.26078916603816304
Iteration 60, Training loss = 0.2572848119813463
Iteration 70, Training loss = 0.2549789708809576
Iteration 80, Training loss = 0.25280466661361106
Iteration 90, Training loss = 0.2504614135205458
Iteration 100, Training loss = 0.24947402147567216
Iteration 110, Training loss = 0.24821140563142471
Iteration 120, Training loss = 0.24554161540264094
Iteration 130, Training loss = 0.24413872664965294
Iteration 140, Training loss = 0.24249990939518104
Iteration 150, Training loss = 0.24077010151124806
Iteration 160, Training loss = 0.23918722555545216
Iteration 170, Training loss = 0.23803594971192632
Iteration 180, Training loss = 0.2357483166427428
Iteration 190, Training loss = 0.23480427689886324
Iteration 200, Training loss = 0.2336551886825746
Iteration 210, Training loss = 0.2319879436838454
Iteration 220, Training loss = 0.23008394939599983
Iteration 230, Training loss = 0.2292083956769123
Iteration 240, Training loss = 0.22763007965640744
Iteration 250, Training loss = 0.22638676783456896
Iteration 260, Training loss = 0.22527349704273658
Iteration 270, Training loss = 0.22397836049397787
Iteration 280, Training loss = 0.22299963354632474
Iteration 290, Training loss = 0.22200605510801508
Model training time: 89.85819959640503
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0008893943063304
Iteration 10, Training loss = 0.9969192034956338
Iteration 20, Training loss = 0.9953899665731163
Iteration 30, Training loss = 0.9892063230132135
Iteration 40, Training loss = 0.9867163766985354
Iteration 50, Training loss = 0.981508995167875
Iteration 60, Training loss = 0.9709196948774771
Iteration 70, Training loss = 0.9559186213258384
Iteration 80, Training loss = 0.9318543303991862
Iteration 90, Training loss = 0.8925310171744674
Iteration 100, Training loss = 0.8304887746555217
Iteration 110, Training loss = 0.7347764639174881
Iteration 120, Training loss = 0.6155473726671099
Iteration 130, Training loss = 0.5097378832130617
Iteration 140, Training loss = 0.4467959160678053
Iteration 150, Training loss = 0.41481538109733285
Iteration 160, Training loss = 0.39947214895400446
Iteration 170, Training loss = 0.38895229563332984
Iteration 180, Training loss = 0.3810398295017832
Iteration 190, Training loss = 0.37442407199150124
Iteration 200, Training loss = 0.3677136141464906
Iteration 210, Training loss = 0.3622645243354466
Iteration 220, Training loss = 0.3569320902444314
Iteration 230, Training loss = 0.35339182550492493
Iteration 240, Training loss = 0.3489175343859023
Iteration 250, Training loss = 0.34572023102914656
Iteration 260, Training loss = 0.34221002855450633
Iteration 270, Training loss = 0.3392564524774966
Iteration 280, Training loss = 0.33660034914523507
Iteration 290, Training loss = 0.33503090017947595
Model training time: 75.5231499671936
Device: cuda
Iteration 0, Training loss = 1.1942703231521274
Iteration 10, Training loss = 0.990181851502202
Iteration 20, Training loss = 0.9806925755192116
Iteration 30, Training loss = 0.9652447375122476
Iteration 40, Training loss = 0.9431931796857125
Iteration 50, Training loss = 0.904168765038108
Iteration 60, Training loss = 0.8407694330250007
Iteration 70, Training loss = 0.7402500615315737
Iteration 80, Training loss = 0.6136192708199727
Iteration 90, Training loss = 0.49707845659647587
Iteration 100, Training loss = 0.42745524954392716
Iteration 110, Training loss = 0.39591826404926284
Iteration 120, Training loss = 0.38221725267200657
Iteration 130, Training loss = 0.37363196326338727
Iteration 140, Training loss = 0.3680318746589808
Iteration 150, Training loss = 0.360853659214029
Iteration 160, Training loss = 0.35634675953123307
Iteration 170, Training loss = 0.351825432141046
Iteration 180, Training loss = 0.3482257029140629
Iteration 190, Training loss = 0.34522660177398995
Iteration 200, Training loss = 0.3432446975926846
Iteration 210, Training loss = 0.3401020697637457
Iteration 220, Training loss = 0.33717818061510724
Iteration 230, Training loss = 0.33532238524893054
Iteration 240, Training loss = 0.3336398455399822
Iteration 250, Training loss = 0.33323893148973943
Iteration 260, Training loss = 0.33053050186611027
Iteration 270, Training loss = 0.32955705284496434
Iteration 280, Training loss = 0.329952461923954
Iteration 290, Training loss = 0.3271427626051189
Model training time: 77.58095049858093
Device: cuda
Iteration 0, Training loss = 1.0066712475629245
Iteration 10, Training loss = 0.9970807441766711
Iteration 20, Training loss = 0.9915943068006764
Iteration 30, Training loss = 0.9819029529313534
Iteration 40, Training loss = 0.9741908841662936
Iteration 50, Training loss = 0.9548885113951089
Iteration 60, Training loss = 0.9261311589231813
Iteration 70, Training loss = 0.8805466222302349
Iteration 80, Training loss = 0.8021514237214977
Iteration 90, Training loss = 0.6846941281919894
Iteration 100, Training loss = 0.5495354803863931
Iteration 110, Training loss = 0.44793977865562346
Iteration 120, Training loss = 0.39893429722762913
Iteration 130, Training loss = 0.37947615066876156
Iteration 140, Training loss = 0.3694879982062584
Iteration 150, Training loss = 0.36381992308989813
Iteration 160, Training loss = 0.35704299490808866
Iteration 170, Training loss = 0.3521552622894158
Iteration 180, Training loss = 0.3479297682283005
Iteration 190, Training loss = 0.3449434398309044
Iteration 200, Training loss = 0.3405997905034374
Iteration 210, Training loss = 0.338000495724632
Iteration 220, Training loss = 0.3365281185378199
Iteration 230, Training loss = 0.3326582816487925
Iteration 240, Training loss = 0.3306607160591273
Iteration 250, Training loss = 0.32961394151915674
Iteration 260, Training loss = 0.327027491181369
Iteration 270, Training loss = 0.3256663275513672
Iteration 280, Training loss = 0.3239634314884886
Iteration 290, Training loss = 0.3245143390796035
Model training time: 76.87826895713806
Device: cuda
Iteration 0, Training loss = 1.0133816254311714
Iteration 10, Training loss = 0.9987719418921909
Iteration 20, Training loss = 0.9919766829785518
Iteration 30, Training loss = 0.9851620315353652
Iteration 40, Training loss = 0.9746988196879769
Iteration 50, Training loss = 0.9595324132177565
Iteration 60, Training loss = 0.936953578306281
Iteration 70, Training loss = 0.8962707070336826
Iteration 80, Training loss = 0.8309111230903201
Iteration 90, Training loss = 0.7317250823053185
Iteration 100, Training loss = 0.6073899824838132
Iteration 110, Training loss = 0.4950694272627577
Iteration 120, Training loss = 0.42623847607829146
Iteration 130, Training loss = 0.39368917969401906
Iteration 140, Training loss = 0.3788715135936
Iteration 150, Training loss = 0.36879509053944387
Iteration 160, Training loss = 0.36146585081798444
Iteration 170, Training loss = 0.3549060365860013
Iteration 180, Training loss = 0.3502267127595662
Iteration 190, Training loss = 0.34517691727133765
Iteration 200, Training loss = 0.34185173351695575
Iteration 210, Training loss = 0.33855692164045603
Iteration 220, Training loss = 0.3352862444620777
Iteration 230, Training loss = 0.3338651390755234
Iteration 240, Training loss = 0.33002072702283447
Iteration 250, Training loss = 0.3291924040098697
Iteration 260, Training loss = 0.3268697411541778
Iteration 270, Training loss = 0.32534297523291217
Iteration 280, Training loss = 0.32358112528128324
Iteration 290, Training loss = 0.3227025591520872
Model training time: 76.07345581054688
Device: cuda
Iteration 0, Training loss = 1.056020620652443
Iteration 10, Training loss = 0.9873470635229838
Iteration 20, Training loss = 0.9800669404043667
Iteration 30, Training loss = 0.9718364526108267
Iteration 40, Training loss = 0.955287076856779
Iteration 50, Training loss = 0.9289783723112466
Iteration 60, Training loss = 0.8849729701228763
Iteration 70, Training loss = 0.8160068602953556
Iteration 80, Training loss = 0.7109327912330627
Iteration 90, Training loss = 0.5790833845230692
Iteration 100, Training loss = 0.4668443505314813
Iteration 110, Training loss = 0.40353604575286167
Iteration 120, Training loss = 0.37632077334871616
Iteration 130, Training loss = 0.3657790408042318
Iteration 140, Training loss = 0.3590721125188081
Iteration 150, Training loss = 0.3539341737826665
Iteration 160, Training loss = 0.3487637299270446
Iteration 170, Training loss = 0.34569948666913497
Iteration 180, Training loss = 0.3415212421745494
Iteration 190, Training loss = 0.33954903346616866
Iteration 200, Training loss = 0.3358711752914576
Iteration 210, Training loss = 0.3330019854980966
Iteration 220, Training loss = 0.3323116401831309
Iteration 230, Training loss = 0.33068685575527845
Iteration 240, Training loss = 0.32927278396875964
Iteration 250, Training loss = 0.32620678438944517
Iteration 260, Training loss = 0.3242860514904566
Iteration 270, Training loss = 0.3225024814766962
Iteration 280, Training loss = 0.3223769457443901
Iteration 290, Training loss = 0.3207741913156233
Model training time: 75.14447546005249
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0212441665538843
Iteration 10, Training loss = 0.31498837240652183
Iteration 20, Training loss = 0.291467417517434
Iteration 30, Training loss = 0.2791857747496038
Iteration 40, Training loss = 0.27287188481881425
Iteration 50, Training loss = 0.2684308422216471
Iteration 60, Training loss = 0.2651511011060309
Iteration 70, Training loss = 0.26304107450920605
Iteration 80, Training loss = 0.2618321077403239
Iteration 90, Training loss = 0.2596572513741571
Iteration 100, Training loss = 0.2587119675992768
Iteration 110, Training loss = 0.2569573684303081
Iteration 120, Training loss = 0.25615345762259717
Iteration 130, Training loss = 0.25477139565391815
Iteration 140, Training loss = 0.2546931061047863
Iteration 150, Training loss = 0.2534772354191628
Iteration 160, Training loss = 0.252261094533015
Iteration 170, Training loss = 0.25090610037967204
Iteration 180, Training loss = 0.2504188491454447
Iteration 190, Training loss = 0.2492216838442761
Iteration 200, Training loss = 0.24841883397044767
Iteration 210, Training loss = 0.2477787625818437
Iteration 220, Training loss = 0.24671655130271175
Iteration 230, Training loss = 0.24696294347877087
Iteration 240, Training loss = 0.2462169332564741
Iteration 250, Training loss = 0.24632481867132555
Iteration 260, Training loss = 0.24648064104975134
Iteration 270, Training loss = 0.2445782770353239
Iteration 280, Training loss = 0.243590933509207
Iteration 290, Training loss = 0.24366773308619208
Model training time: 89.30086088180542
Device: cuda
Iteration 0, Training loss = 0.9690548040440693
Iteration 10, Training loss = 0.3112073387356772
Iteration 20, Training loss = 0.2922457173682641
Iteration 30, Training loss = 0.2793860411370434
Iteration 40, Training loss = 0.2725270563277645
Iteration 50, Training loss = 0.26810948637084686
Iteration 60, Training loss = 0.2655724379175527
Iteration 70, Training loss = 0.2629737356938602
Iteration 80, Training loss = 0.26314728177975916
Iteration 90, Training loss = 0.25961999411600223
Iteration 100, Training loss = 0.25786832266527676
Iteration 110, Training loss = 0.25697417013743074
Iteration 120, Training loss = 0.2557871833731587
Iteration 130, Training loss = 0.25448366844855647
Iteration 140, Training loss = 0.2556755839338625
Iteration 150, Training loss = 0.25352425808491913
Iteration 160, Training loss = 0.25274370117607897
Iteration 170, Training loss = 0.2523268341370251
Iteration 180, Training loss = 0.2513621290380828
Iteration 190, Training loss = 0.2516806497450036
Iteration 200, Training loss = 0.2505813783421609
Iteration 210, Training loss = 0.2497545537237384
Iteration 220, Training loss = 0.24992145864283982
Iteration 230, Training loss = 0.24910794605235548
Iteration 240, Training loss = 0.24934657645110347
Iteration 250, Training loss = 0.2491055551358467
Iteration 260, Training loss = 0.2478419419935936
Iteration 270, Training loss = 0.24736319478727192
Iteration 280, Training loss = 0.24734736781045435
Iteration 290, Training loss = 0.24714230879205437
Model training time: 88.21958374977112
Device: cuda
Iteration 0, Training loss = 0.9674737001386817
Iteration 10, Training loss = 0.3038160327983939
Iteration 20, Training loss = 0.2878410551283095
Iteration 30, Training loss = 0.2801263128645754
Iteration 40, Training loss = 0.2730537699400515
Iteration 50, Training loss = 0.26731570518535114
Iteration 60, Training loss = 0.2627619652860407
Iteration 70, Training loss = 0.2591786316915411
Iteration 80, Training loss = 0.2569621444684296
Iteration 90, Training loss = 0.25447388900362927
Iteration 100, Training loss = 0.25237328253218516
Iteration 110, Training loss = 0.2499568433577312
Iteration 120, Training loss = 0.24819705556124305
Iteration 130, Training loss = 0.2468020605004352
Iteration 140, Training loss = 0.24490158983330795
Iteration 150, Training loss = 0.2442065146594232
Iteration 160, Training loss = 0.24191193559751417
Iteration 170, Training loss = 0.241475116479512
Iteration 180, Training loss = 0.2392540992314113
Iteration 190, Training loss = 0.23854147801220704
Iteration 200, Training loss = 0.23748619332549653
Iteration 210, Training loss = 0.2374041793499016
Iteration 220, Training loss = 0.23534024226060812
Iteration 230, Training loss = 0.23436359186535297
Iteration 240, Training loss = 0.23404308124152934
Iteration 250, Training loss = 0.23333076502821873
Iteration 260, Training loss = 0.23250380727116038
Iteration 270, Training loss = 0.23068974804187167
Iteration 280, Training loss = 0.23044681955794782
Iteration 290, Training loss = 0.22891467888862038
Model training time: 91.20925426483154
Device: cuda
Iteration 0, Training loss = 0.9611763502088722
Iteration 10, Training loss = 0.303084600014963
Iteration 20, Training loss = 0.28735970831723606
Iteration 30, Training loss = 0.27968376219848506
Iteration 40, Training loss = 0.2731204153474978
Iteration 50, Training loss = 0.2671708330008143
Iteration 60, Training loss = 0.263464475347512
Iteration 70, Training loss = 0.260178258129652
Iteration 80, Training loss = 0.25722430711207184
Iteration 90, Training loss = 0.2556060343476885
Iteration 100, Training loss = 0.2548564249550663
Iteration 110, Training loss = 0.2538029688999849
Iteration 120, Training loss = 0.2517573771197439
Iteration 130, Training loss = 0.25034961086396434
Iteration 140, Training loss = 0.24969946776611218
Iteration 150, Training loss = 0.24912116827740186
Iteration 160, Training loss = 0.2491447655259123
Iteration 170, Training loss = 0.24724911786795814
Iteration 180, Training loss = 0.24725176660335008
Iteration 190, Training loss = 0.24614768093766798
Iteration 200, Training loss = 0.24534889342992203
Iteration 210, Training loss = 0.24407465345617654
Iteration 220, Training loss = 0.24308608529936288
Iteration 230, Training loss = 0.24212931845211177
Iteration 240, Training loss = 0.2417070278942873
Iteration 250, Training loss = 0.23973781727074425
Iteration 260, Training loss = 0.23903851840951015
Iteration 270, Training loss = 0.23828291108354854
Iteration 280, Training loss = 0.2372301081960328
Iteration 290, Training loss = 0.23602014585681583
Model training time: 98.03740096092224
Device: cuda
Iteration 0, Training loss = 1.0997699430023415
Iteration 10, Training loss = 0.31644705197085504
Iteration 20, Training loss = 0.29161274609502386
Iteration 30, Training loss = 0.2775002103568851
Iteration 40, Training loss = 0.26918476600433894
Iteration 50, Training loss = 0.26459188280618134
Iteration 60, Training loss = 0.2617175712245674
Iteration 70, Training loss = 0.2594956660616225
Iteration 80, Training loss = 0.2579978429104971
Iteration 90, Training loss = 0.25584529610215756
Iteration 100, Training loss = 0.2533266203486977
Iteration 110, Training loss = 0.25246330951722923
Iteration 120, Training loss = 0.24993236632882684
Iteration 130, Training loss = 0.248507432927544
Iteration 140, Training loss = 0.24678033075614828
Iteration 150, Training loss = 0.2450921842513453
Iteration 160, Training loss = 0.24323830552003234
Iteration 170, Training loss = 0.2426408467013479
Iteration 180, Training loss = 0.24219626839754085
Iteration 190, Training loss = 0.23948461058491094
Iteration 200, Training loss = 0.23850564878199987
Iteration 210, Training loss = 0.23746207273668712
Iteration 220, Training loss = 0.2364027571419011
Iteration 230, Training loss = 0.23507417061766564
Iteration 240, Training loss = 0.23484717215892773
Iteration 250, Training loss = 0.23368915335999596
Iteration 260, Training loss = 0.2322875285062237
Iteration 270, Training loss = 0.23186512424606057
Iteration 280, Training loss = 0.23076917508230116
Iteration 290, Training loss = 0.2297050343187535
Model training time: 89.46087646484375
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0606594785400059
Iteration 10, Training loss = 0.9998896530860863
Iteration 20, Training loss = 0.996999019873891
Iteration 30, Training loss = 0.9954485412381121
Iteration 40, Training loss = 0.9913435057165542
Iteration 50, Training loss = 0.9901444626891095
Iteration 60, Training loss = 0.9825783993886865
Iteration 70, Training loss = 0.9775575279901569
Iteration 80, Training loss = 0.9654674881322373
Iteration 90, Training loss = 0.9475428459149052
Iteration 100, Training loss = 0.920881702416185
Iteration 110, Training loss = 0.8766346441374885
Iteration 120, Training loss = 0.8068101907121963
Iteration 130, Training loss = 0.7042489725610485
Iteration 140, Training loss = 0.5882263534886826
Iteration 150, Training loss = 0.4912839887510751
Iteration 160, Training loss = 0.43011637931860586
Iteration 170, Training loss = 0.3993903106249473
Iteration 180, Training loss = 0.3833437960792855
Iteration 190, Training loss = 0.37264419494619694
Iteration 200, Training loss = 0.36505502599161027
Iteration 210, Training loss = 0.35875527719080735
Iteration 220, Training loss = 0.35378363195824736
Iteration 230, Training loss = 0.34900699267928725
Iteration 240, Training loss = 0.3452710920341925
Iteration 250, Training loss = 0.34092567076429653
Iteration 260, Training loss = 0.3379761859558631
Iteration 270, Training loss = 0.33539329325231376
Iteration 280, Training loss = 0.33259012366550555
Iteration 290, Training loss = 0.33088802549862056
Model training time: 76.68016028404236
Device: cuda
Iteration 0, Training loss = 1.1341669507648633
Iteration 10, Training loss = 0.9934447586248463
Iteration 20, Training loss = 0.9913369898058942
Iteration 30, Training loss = 0.9894212426770712
Iteration 40, Training loss = 0.9840654558029728
Iteration 50, Training loss = 0.9782687987106434
Iteration 60, Training loss = 0.9712002815831686
Iteration 70, Training loss = 0.9601584891766166
Iteration 80, Training loss = 0.9454098882306601
Iteration 90, Training loss = 0.9186087869791593
Iteration 100, Training loss = 0.8833584114549241
Iteration 110, Training loss = 0.8321834151871539
Iteration 120, Training loss = 0.7568440949859251
Iteration 130, Training loss = 0.6625114980959086
Iteration 140, Training loss = 0.5658754804284101
Iteration 150, Training loss = 0.48537682752678357
Iteration 160, Training loss = 0.4344102820912421
Iteration 170, Training loss = 0.40677943638557396
Iteration 180, Training loss = 0.3900564986726512
Iteration 190, Training loss = 0.37926592321499536
Iteration 200, Training loss = 0.37111562476066
Iteration 210, Training loss = 0.36442270291888196
Iteration 220, Training loss = 0.35864653585901585
Iteration 230, Training loss = 0.35435950244970366
Iteration 240, Training loss = 0.3491958560431061
Iteration 250, Training loss = 0.34617151694309306
Iteration 260, Training loss = 0.34220780237861304
Iteration 270, Training loss = 0.3401605931025196
Iteration 280, Training loss = 0.3363286306460698
Iteration 290, Training loss = 0.3340752072236388
Model training time: 76.30774307250977
Device: cuda
Iteration 0, Training loss = 1.116760213881875
Iteration 10, Training loss = 0.9994674062959238
Iteration 20, Training loss = 0.9958258328806375
Iteration 30, Training loss = 0.9951015885037501
Iteration 40, Training loss = 0.9929975844235812
Iteration 50, Training loss = 0.9924440694891888
Iteration 60, Training loss = 0.987689056834161
Iteration 70, Training loss = 0.9843670495466333
Iteration 80, Training loss = 0.9764358952713473
Iteration 90, Training loss = 0.9648126510606296
Iteration 100, Training loss = 0.949061513810918
Iteration 110, Training loss = 0.9215162785156913
Iteration 120, Training loss = 0.8788536843182384
Iteration 130, Training loss = 0.8123046865209865
Iteration 140, Training loss = 0.7162735976170802
Iteration 150, Training loss = 0.603877623156073
Iteration 160, Training loss = 0.5038576162091776
Iteration 170, Training loss = 0.4412164215160453
Iteration 180, Training loss = 0.40925899241569536
Iteration 190, Training loss = 0.3929387922016319
Iteration 200, Training loss = 0.3829682624858359
Iteration 210, Training loss = 0.37469455369428734
Iteration 220, Training loss = 0.3680579158418996
Iteration 230, Training loss = 0.36355147600749843
Iteration 240, Training loss = 0.3564218743411815
Iteration 250, Training loss = 0.35215032842136235
Iteration 260, Training loss = 0.34779902867936857
Iteration 270, Training loss = 0.3438899990009225
Iteration 280, Training loss = 0.3408724325146652
Iteration 290, Training loss = 0.3376726886068565
Model training time: 76.82137417793274
Device: cuda
Iteration 0, Training loss = 1.1422755199929941
Iteration 10, Training loss = 0.9996350899986599
Iteration 20, Training loss = 1.000211947494083
Iteration 30, Training loss = 0.9977917815176185
Iteration 40, Training loss = 0.9970278996200377
Iteration 50, Training loss = 0.9968168922668494
Iteration 60, Training loss = 0.9952508167944093
Iteration 70, Training loss = 0.9937413778282018
Iteration 80, Training loss = 0.9931699455072338
Iteration 90, Training loss = 0.9910468315156762
Iteration 100, Training loss = 0.9878766738274247
Iteration 110, Training loss = 0.9852609087303641
Iteration 120, Training loss = 0.9785403088095107
Iteration 130, Training loss = 0.9701760137714626
Iteration 140, Training loss = 0.9528750209992635
Iteration 150, Training loss = 0.9296111337804563
Iteration 160, Training loss = 0.8895095123185052
Iteration 170, Training loss = 0.8236158959531553
Iteration 180, Training loss = 0.7262657810236521
Iteration 190, Training loss = 0.6085029244422913
Iteration 200, Training loss = 0.49913295304429706
Iteration 210, Training loss = 0.4314401468649003
Iteration 220, Training loss = 0.3968254630116449
Iteration 230, Training loss = 0.3808060428082655
Iteration 240, Training loss = 0.3706144750262228
Iteration 250, Training loss = 0.36325223002456813
Iteration 260, Training loss = 0.3572764604707847
Iteration 270, Training loss = 0.35197730362415314
Iteration 280, Training loss = 0.3474717960121551
Iteration 290, Training loss = 0.3425720799084447
Model training time: 75.55455160140991
Device: cuda
Iteration 0, Training loss = 1.0057325780679638
Iteration 10, Training loss = 0.9974399177348556
Iteration 20, Training loss = 0.9941270898505685
Iteration 30, Training loss = 0.9924767624928755
Iteration 40, Training loss = 0.9886389377612422
Iteration 50, Training loss = 0.98553265609603
Iteration 60, Training loss = 0.9781130483185035
Iteration 70, Training loss = 0.9673026972922726
Iteration 80, Training loss = 0.9535831008556385
Iteration 90, Training loss = 0.9271096293477045
Iteration 100, Training loss = 0.8870240167719154
Iteration 110, Training loss = 0.8191972334603757
Iteration 120, Training loss = 0.720886813989584
Iteration 130, Training loss = 0.6008393864556787
Iteration 140, Training loss = 0.49476934249562343
Iteration 150, Training loss = 0.42667330254391195
Iteration 160, Training loss = 0.39402598624932017
Iteration 170, Training loss = 0.377031568383825
Iteration 180, Training loss = 0.3667792307869824
Iteration 190, Training loss = 0.3591104684676525
Iteration 200, Training loss = 0.3522907289617879
Iteration 210, Training loss = 0.34671989932728275
Iteration 220, Training loss = 0.34236592407099864
Iteration 230, Training loss = 0.33721225849096326
Iteration 240, Training loss = 0.3350900348257903
Iteration 250, Training loss = 0.3317425705240544
Iteration 260, Training loss = 0.32886069386765576
Iteration 270, Training loss = 0.32646061796784975
Iteration 280, Training loss = 0.3242928776620091
Iteration 290, Training loss = 0.3224757199270138
Model training time: 76.60815215110779
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0082542023796965
Iteration 10, Training loss = 0.3080079271885508
Iteration 20, Training loss = 0.2951033275196518
Iteration 30, Training loss = 0.28657818142918573
Iteration 40, Training loss = 0.2785433679819107
Iteration 50, Training loss = 0.27309802800848865
Iteration 60, Training loss = 0.26838370207427203
Iteration 70, Training loss = 0.26541071194381527
Iteration 80, Training loss = 0.26289915163879807
Iteration 90, Training loss = 0.2602400200594451
Iteration 100, Training loss = 0.25808011250939344
Iteration 110, Training loss = 0.2578809602537017
Iteration 120, Training loss = 0.2551069246686023
Iteration 130, Training loss = 0.25406245027043395
Iteration 140, Training loss = 0.2533885734740663
Iteration 150, Training loss = 0.25092656608077063
Iteration 160, Training loss = 0.24973992632639005
Iteration 170, Training loss = 0.24901269495055295
Iteration 180, Training loss = 0.2472487727297101
Iteration 190, Training loss = 0.24666301639759597
Iteration 200, Training loss = 0.24477461030805744
Iteration 210, Training loss = 0.24459530283575473
Iteration 220, Training loss = 0.24256310910706358
Iteration 230, Training loss = 0.2415012272371762
Iteration 240, Training loss = 0.24000257699961824
Iteration 250, Training loss = 0.23940653231552833
Iteration 260, Training loss = 0.2375651472720547
Iteration 270, Training loss = 0.23679300789959765
Iteration 280, Training loss = 0.23585922468975545
Iteration 290, Training loss = 0.23454867058618056
Model training time: 87.80865025520325
Device: cuda
Iteration 0, Training loss = 1.0101101081728359
Iteration 10, Training loss = 0.3039558384441523
Iteration 20, Training loss = 0.2872554861764977
Iteration 30, Training loss = 0.2760950262131898
Iteration 40, Training loss = 0.2688200522736075
Iteration 50, Training loss = 0.26396665668142016
Iteration 60, Training loss = 0.2602979760023131
Iteration 70, Training loss = 0.25707289162609315
Iteration 80, Training loss = 0.2548218068941204
Iteration 90, Training loss = 0.25264693379114217
Iteration 100, Training loss = 0.25043935202746
Iteration 110, Training loss = 0.24841735526415462
Iteration 120, Training loss = 0.2477118986600263
Iteration 130, Training loss = 0.2441893881501783
Iteration 140, Training loss = 0.24296935374178172
Iteration 150, Training loss = 0.24173187439280433
Iteration 160, Training loss = 0.24092678995668024
Iteration 170, Training loss = 0.23900040473051118
Iteration 180, Training loss = 0.23756371582475838
Iteration 190, Training loss = 0.23768492917651715
Iteration 200, Training loss = 0.23604993439382976
Iteration 210, Training loss = 0.23659224714633922
Iteration 220, Training loss = 0.23441764075255048
Iteration 230, Training loss = 0.23313061054346065
Iteration 240, Training loss = 0.2336966886324583
Iteration 250, Training loss = 0.23162724887547287
Iteration 260, Training loss = 0.23087168006217423
Iteration 270, Training loss = 0.2305166987693252
Iteration 280, Training loss = 0.22946026572138792
Iteration 290, Training loss = 0.2291223353881767
Model training time: 88.27811288833618
Device: cuda
Iteration 0, Training loss = 0.9517778152428963
Iteration 10, Training loss = 0.30190210749849605
Iteration 20, Training loss = 0.28842665013915675
Iteration 30, Training loss = 0.278976457418451
Iteration 40, Training loss = 0.2710519045015464
Iteration 50, Training loss = 0.2662208526606721
Iteration 60, Training loss = 0.2623231708643517
Iteration 70, Training loss = 0.2588183051290143
Iteration 80, Training loss = 0.2566286082860928
Iteration 90, Training loss = 0.2538879233713887
Iteration 100, Training loss = 0.2519011198034609
Iteration 110, Training loss = 0.2508618606677378
Iteration 120, Training loss = 0.24862756950843737
Iteration 130, Training loss = 0.24781385906364606
Iteration 140, Training loss = 0.24619191499867876
Iteration 150, Training loss = 0.2448858070200768
Iteration 160, Training loss = 0.24316219557166677
Iteration 170, Training loss = 0.24179087132935362
Iteration 180, Training loss = 0.24029114496880682
Iteration 190, Training loss = 0.2390775774124164
Iteration 200, Training loss = 0.23817769758367308
Iteration 210, Training loss = 0.23673652144877808
Iteration 220, Training loss = 0.23643429760915646
Iteration 230, Training loss = 0.2351670741027104
Iteration 240, Training loss = 0.23557454513178933
Iteration 250, Training loss = 0.2339239381433685
Iteration 260, Training loss = 0.2327610546911972
Iteration 270, Training loss = 0.23205836700356525
Iteration 280, Training loss = 0.23129024558165223
Iteration 290, Training loss = 0.23096918642232958
Model training time: 88.50449514389038
Device: cuda
Iteration 0, Training loss = 1.014059022716854
Iteration 10, Training loss = 0.304259709905887
Iteration 20, Training loss = 0.29000800063356685
Iteration 30, Training loss = 0.2793907048045725
Iteration 40, Training loss = 0.2717318124889176
Iteration 50, Training loss = 0.26661458589892456
Iteration 60, Training loss = 0.2625968985367512
Iteration 70, Training loss = 0.2593879131735235
Iteration 80, Training loss = 0.2561017019665184
Iteration 90, Training loss = 0.25502730747639846
Iteration 100, Training loss = 0.25226449386941063
Iteration 110, Training loss = 0.2502306657280899
Iteration 120, Training loss = 0.24781514351494646
Iteration 130, Training loss = 0.24547951225785242
Iteration 140, Training loss = 0.24328295371814626
Iteration 150, Training loss = 0.24140524662635177
Iteration 160, Training loss = 0.24035123048197243
Iteration 170, Training loss = 0.23916632188978978
Iteration 180, Training loss = 0.23794173168531363
Iteration 190, Training loss = 0.23678833253429707
Iteration 200, Training loss = 0.23663272373918173
Iteration 210, Training loss = 0.23506631417838847
Iteration 220, Training loss = 0.23478661509959595
Iteration 230, Training loss = 0.23279367937989856
Iteration 240, Training loss = 0.23247159398408326
Iteration 250, Training loss = 0.23199112454186316
Iteration 260, Training loss = 0.23073976497719254
Iteration 270, Training loss = 0.23030704033115637
Iteration 280, Training loss = 0.22924553890879026
Iteration 290, Training loss = 0.22831446666648422
Model training time: 90.20668768882751
Device: cuda
Iteration 0, Training loss = 0.9432711638690193
Iteration 10, Training loss = 0.3008208462725515
Iteration 20, Training loss = 0.28685150068739185
Iteration 30, Training loss = 0.277323511775565
Iteration 40, Training loss = 0.26862298736825657
Iteration 50, Training loss = 0.26283681569036077
Iteration 60, Training loss = 0.25907305350914095
Iteration 70, Training loss = 0.2554042338605088
Iteration 80, Training loss = 0.2524396903488947
Iteration 90, Training loss = 0.25046237820876394
Iteration 100, Training loss = 0.24754082526705692
Iteration 110, Training loss = 0.24672059243283986
Iteration 120, Training loss = 0.24470189368523262
Iteration 130, Training loss = 0.2432043412745287
Iteration 140, Training loss = 0.240973435378305
Iteration 150, Training loss = 0.2401232597404632
Iteration 160, Training loss = 0.2397704842513886
Iteration 170, Training loss = 0.2375484829507588
Iteration 180, Training loss = 0.2361594702742526
Iteration 190, Training loss = 0.23496730398872626
Iteration 200, Training loss = 0.23423316498885408
Iteration 210, Training loss = 0.23489741466327566
Iteration 220, Training loss = 0.23205005003202364
Iteration 230, Training loss = 0.23116032293309335
Iteration 240, Training loss = 0.23046312365986873
Iteration 250, Training loss = 0.2288878164321616
Iteration 260, Training loss = 0.22819086423386697
Iteration 270, Training loss = 0.22729763529006985
Iteration 280, Training loss = 0.226208229355766
Iteration 290, Training loss = 0.2255792761410492
Model training time: 88.70447373390198
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0217892392246044
Iteration 10, Training loss = 1.0012119570792009
Iteration 20, Training loss = 0.9975516433301179
Iteration 30, Training loss = 0.9927846164519084
Iteration 40, Training loss = 0.9901753568994827
Iteration 50, Training loss = 0.9801382917712852
Iteration 60, Training loss = 0.9714819521143816
Iteration 70, Training loss = 0.955351580168314
Iteration 80, Training loss = 0.9321047827817391
Iteration 90, Training loss = 0.8916125617165497
Iteration 100, Training loss = 0.8269629773598363
Iteration 110, Training loss = 0.7324230208777
Iteration 120, Training loss = 0.6122976332758936
Iteration 130, Training loss = 0.5020405164231425
Iteration 140, Training loss = 0.4332916758918532
Iteration 150, Training loss = 0.40006623507121913
Iteration 160, Training loss = 0.384950730247774
Iteration 170, Training loss = 0.37609207579767073
Iteration 180, Training loss = 0.3691663698873658
Iteration 190, Training loss = 0.36458721872113176
Iteration 200, Training loss = 0.35812343606626357
Iteration 210, Training loss = 0.35368232947328815
Iteration 220, Training loss = 0.34984930097192957
Iteration 230, Training loss = 0.3466996532563426
Iteration 240, Training loss = 0.3426061502257407
Iteration 250, Training loss = 0.3395993198461579
Iteration 260, Training loss = 0.3372174359606084
Iteration 270, Training loss = 0.33580799163251684
Iteration 280, Training loss = 0.33332937797486495
Iteration 290, Training loss = 0.3302419552048623
Model training time: 76.33682537078857
Device: cuda
Iteration 0, Training loss = 1.0085480676756964
Iteration 10, Training loss = 0.9920888587760465
Iteration 20, Training loss = 0.985783534925341
Iteration 30, Training loss = 0.9786277732987335
Iteration 40, Training loss = 0.9670012524162513
Iteration 50, Training loss = 0.9487124585011155
Iteration 60, Training loss = 0.9203330975223855
Iteration 70, Training loss = 0.8746792924576912
Iteration 80, Training loss = 0.800040254558342
Iteration 90, Training loss = 0.6909114093596233
Iteration 100, Training loss = 0.5643492572837405
Iteration 110, Training loss = 0.46251130284030656
Iteration 120, Training loss = 0.40872054529074886
Iteration 130, Training loss = 0.38649169042490533
Iteration 140, Training loss = 0.37579897557191805
Iteration 150, Training loss = 0.3690532877825309
Iteration 160, Training loss = 0.3630071229836791
Iteration 170, Training loss = 0.3583628963589092
Iteration 180, Training loss = 0.35382619514557473
Iteration 190, Training loss = 0.34990142321816964
Iteration 200, Training loss = 0.3473081588025254
Iteration 210, Training loss = 0.34412867233949007
Iteration 220, Training loss = 0.3407278664158162
Iteration 230, Training loss = 0.33855757426812455
Iteration 240, Training loss = 0.3364087919826093
Iteration 250, Training loss = 0.3347674585914842
Iteration 260, Training loss = 0.3330732915856412
Iteration 270, Training loss = 0.33200560934877626
Iteration 280, Training loss = 0.3296039077826744
Iteration 290, Training loss = 0.32914095454745823
Model training time: 77.32919001579285
Device: cuda
Iteration 0, Training loss = 1.1880323296583792
Iteration 10, Training loss = 0.9957975650179214
Iteration 20, Training loss = 0.9916623753050099
Iteration 30, Training loss = 0.9848706396881509
Iteration 40, Training loss = 0.976307127210829
Iteration 50, Training loss = 0.9631342743905846
Iteration 60, Training loss = 0.9458841068157251
Iteration 70, Training loss = 0.914552724879721
Iteration 80, Training loss = 0.8595109161547416
Iteration 90, Training loss = 0.7789337694069037
Iteration 100, Training loss = 0.6704502023648524
Iteration 110, Training loss = 0.557776684490379
Iteration 120, Training loss = 0.47249289267305017
Iteration 130, Training loss = 0.42657943796996334
Iteration 140, Training loss = 0.40409300163172296
Iteration 150, Training loss = 0.39146735882701506
Iteration 160, Training loss = 0.3821674146226063
Iteration 170, Training loss = 0.37439278509593815
Iteration 180, Training loss = 0.3679862321862852
Iteration 190, Training loss = 0.36145254336117544
Iteration 200, Training loss = 0.3571775636235297
Iteration 210, Training loss = 0.3511606593062912
Iteration 220, Training loss = 0.3470506292322408
Iteration 230, Training loss = 0.344519282668685
Iteration 240, Training loss = 0.3400486559395629
Iteration 250, Training loss = 0.3371154730014755
Iteration 260, Training loss = 0.33470794105011487
Iteration 270, Training loss = 0.3326767083383413
Iteration 280, Training loss = 0.3302337243649119
Iteration 290, Training loss = 0.32800455463393297
Model training time: 76.76755094528198
Device: cuda
Iteration 0, Training loss = 1.0626013509317296
Iteration 10, Training loss = 0.9973918887728078
Iteration 20, Training loss = 0.9928279029574372
Iteration 30, Training loss = 0.9874651095141536
Iteration 40, Training loss = 0.9800843360343418
Iteration 50, Training loss = 0.9699014479987287
Iteration 60, Training loss = 0.9522624750068223
Iteration 70, Training loss = 0.9242958340091981
Iteration 80, Training loss = 0.8748033648527763
Iteration 90, Training loss = 0.7961989048022579
Iteration 100, Training loss = 0.677640214058512
Iteration 110, Training loss = 0.5455440547731187
Iteration 120, Training loss = 0.4476365179975252
Iteration 130, Training loss = 0.4012187474591721
Iteration 140, Training loss = 0.3846081027374175
Iteration 150, Training loss = 0.37583384263342706
Iteration 160, Training loss = 0.36862976467551817
Iteration 170, Training loss = 0.3637290848913976
Iteration 180, Training loss = 0.3592970104177217
Iteration 190, Training loss = 0.3561617029461884
Iteration 200, Training loss = 0.35213274555505764
Iteration 210, Training loss = 0.34911978806274524
Iteration 220, Training loss = 0.3471067840062478
Iteration 230, Training loss = 0.3431882296783337
Iteration 240, Training loss = 0.34082296936984224
Iteration 250, Training loss = 0.3387146774985364
Iteration 260, Training loss = 0.33677679127541144
Iteration 270, Training loss = 0.3346171921146089
Iteration 280, Training loss = 0.3335619432770688
Iteration 290, Training loss = 0.3314405756008222
Model training time: 76.29549169540405
Device: cuda
Iteration 0, Training loss = 1.029544218438835
Iteration 10, Training loss = 0.9986732786404338
Iteration 20, Training loss = 0.9953359048723599
Iteration 30, Training loss = 0.9912441597468611
Iteration 40, Training loss = 0.987249587757
Iteration 50, Training loss = 0.9816430821510904
Iteration 60, Training loss = 0.9701330651695602
Iteration 70, Training loss = 0.9539061537111438
Iteration 80, Training loss = 0.9308775798710072
Iteration 90, Training loss = 0.8915119194178189
Iteration 100, Training loss = 0.8316653989939298
Iteration 110, Training loss = 0.7364379576438866
Iteration 120, Training loss = 0.6197428347695852
Iteration 130, Training loss = 0.5073622178628249
Iteration 140, Training loss = 0.43521558900098295
Iteration 150, Training loss = 0.40124540282908266
Iteration 160, Training loss = 0.38582178072077067
Iteration 170, Training loss = 0.37572020760192965
Iteration 180, Training loss = 0.36921988960337526
Iteration 190, Training loss = 0.3636547658898404
Iteration 200, Training loss = 0.35715798234594043
Iteration 210, Training loss = 0.35271943781686865
Iteration 220, Training loss = 0.3485096280557522
Iteration 230, Training loss = 0.3447183579350439
Iteration 240, Training loss = 0.34163104731967486
Iteration 250, Training loss = 0.3389151585850739
Iteration 260, Training loss = 0.3351798324625273
Iteration 270, Training loss = 0.332904448111852
Iteration 280, Training loss = 0.3306610238724861
Iteration 290, Training loss = 0.32904055232299123
Model training time: 75.84620308876038
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9234822785508805
Iteration 10, Training loss = 0.29158067472890953
Iteration 20, Training loss = 0.2701245650721057
Iteration 30, Training loss = 0.2590915991920204
Iteration 40, Training loss = 0.25355994179484925
Iteration 50, Training loss = 0.2502374812456721
Iteration 60, Training loss = 0.24543989334561397
Iteration 70, Training loss = 0.24409557396662984
Iteration 80, Training loss = 0.23984937110672827
Iteration 90, Training loss = 0.2360415439242902
Model training time: 29.74098563194275
Device: cuda
Iteration 0, Training loss = 0.8099119167972878
Iteration 10, Training loss = 0.2885229350000188
Iteration 20, Training loss = 0.26856827656715965
Iteration 30, Training loss = 0.25935131818919943
Iteration 40, Training loss = 0.2539748951864703
Iteration 50, Training loss = 0.25203554213478946
Iteration 60, Training loss = 0.24877791387447412
Iteration 70, Training loss = 0.2457273570091828
Iteration 80, Training loss = 0.2436024282793492
Iteration 90, Training loss = 0.23980577045304763
Model training time: 29.925272703170776
Device: cuda
Iteration 0, Training loss = 0.7645657338957855
Iteration 10, Training loss = 0.2873637039880246
Iteration 20, Training loss = 0.26923061032658036
Iteration 30, Training loss = 0.26121677649049946
Iteration 40, Training loss = 0.25362265901865017
Iteration 50, Training loss = 0.24835818228514298
Iteration 60, Training loss = 0.24403896103590583
Iteration 70, Training loss = 0.24060948175076702
Iteration 80, Training loss = 0.23735846694252918
Iteration 90, Training loss = 0.2341724227807948
Model training time: 29.6919207572937
Device: cuda
Iteration 0, Training loss = 0.7656355045987788
Iteration 10, Training loss = 0.2916221658964664
Iteration 20, Training loss = 0.27173631778661755
Iteration 30, Training loss = 0.2611057569007367
Iteration 40, Training loss = 0.25387790943113503
Iteration 50, Training loss = 0.24838147938683414
Iteration 60, Training loss = 0.24560914786109603
Iteration 70, Training loss = 0.24073100014441257
Iteration 80, Training loss = 0.2384020282306533
Iteration 90, Training loss = 0.2361127710428791
Model training time: 29.297980546951294
Device: cuda
Iteration 0, Training loss = 1.0470400181945396
Iteration 10, Training loss = 0.2787781439109701
Iteration 20, Training loss = 0.2646345500136919
Iteration 30, Training loss = 0.25696353164416
Iteration 40, Training loss = 0.25268944421252193
Iteration 50, Training loss = 0.24894347279400064
Iteration 60, Training loss = 0.24372818353383438
Iteration 70, Training loss = 0.23875071493467848
Iteration 80, Training loss = 0.23616422601224143
Iteration 90, Training loss = 0.23150819316866317
Model training time: 29.54188632965088
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0009646057218746
Iteration 10, Training loss = 0.9915925840248808
Iteration 20, Training loss = 0.9767760473173022
Iteration 30, Training loss = 0.9481946319197687
Iteration 40, Training loss = 0.8595656222767301
Iteration 50, Training loss = 0.6405092867964132
Iteration 60, Training loss = 0.42711913628854614
Iteration 70, Training loss = 0.37960482003608187
Iteration 80, Training loss = 0.36607899362050395
Iteration 90, Training loss = 0.3578027565986062
Model training time: 26.198458433151245
Device: cuda
Iteration 0, Training loss = 1.0756190367366956
Iteration 10, Training loss = 0.9912174318147742
Iteration 20, Training loss = 0.980782521519684
Iteration 30, Training loss = 0.9535938078078671
Iteration 40, Training loss = 0.8859610632421889
Iteration 50, Training loss = 0.7174124566541202
Iteration 60, Training loss = 0.4854610074257505
Iteration 70, Training loss = 0.39379573919346944
Iteration 80, Training loss = 0.37467633490113245
Iteration 90, Training loss = 0.3641301947803313
Model training time: 26.384002447128296
Device: cuda
Iteration 0, Training loss = 1.0270117892735247
Iteration 10, Training loss = 0.9894262175053214
Iteration 20, Training loss = 0.9663121655655368
Iteration 30, Training loss = 0.9154831047219355
Iteration 40, Training loss = 0.7661080091183888
Iteration 50, Training loss = 0.5016484079153641
Iteration 60, Training loss = 0.38910170165812913
Iteration 70, Training loss = 0.3684552560969827
Iteration 80, Training loss = 0.35817831711492676
Iteration 90, Training loss = 0.3492665298751011
Model training time: 25.17902183532715
Device: cuda
Iteration 0, Training loss = 1.052257582185349
Iteration 10, Training loss = 0.986608656420224
Iteration 20, Training loss = 0.969490110874176
Iteration 30, Training loss = 0.9274716198732311
Iteration 40, Training loss = 0.8121968520724255
Iteration 50, Training loss = 0.5751734244938634
Iteration 60, Training loss = 0.41451849446492495
Iteration 70, Training loss = 0.3798790910249747
Iteration 80, Training loss = 0.36610949420986544
Iteration 90, Training loss = 0.35579150152091243
Model training time: 25.303855895996094
Device: cuda
Iteration 0, Training loss = 0.9922832840882637
Iteration 10, Training loss = 0.9705028677908119
Iteration 20, Training loss = 0.9281857681735126
Iteration 30, Training loss = 0.8051642270191856
Iteration 40, Training loss = 0.5526434496980934
Iteration 50, Training loss = 0.4011811763048172
Iteration 60, Training loss = 0.37174249559208966
Iteration 70, Training loss = 0.3595743430553427
Iteration 80, Training loss = 0.3512162196175488
Iteration 90, Training loss = 0.3441839318920449
Model training time: 25.156874179840088
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0804899334907532
Iteration 10, Training loss = 0.2955859453925764
Iteration 20, Training loss = 0.270120221707556
Iteration 30, Training loss = 0.26159835059717657
Iteration 40, Training loss = 0.2566486143907487
Iteration 50, Training loss = 0.25169422617857007
Iteration 60, Training loss = 0.24799031202775845
Iteration 70, Training loss = 0.24533538045226663
Iteration 80, Training loss = 0.24303801876047385
Iteration 90, Training loss = 0.24050949114388315
Model training time: 29.373785734176636
Device: cuda
Iteration 0, Training loss = 1.0037928016289421
Iteration 10, Training loss = 0.2920092900018185
Iteration 20, Training loss = 0.2717705011943688
Iteration 30, Training loss = 0.2617791802554891
Iteration 40, Training loss = 0.25790211857085066
Iteration 50, Training loss = 0.2544470896611467
Iteration 60, Training loss = 0.252023310477031
Iteration 70, Training loss = 0.24750150376615893
Iteration 80, Training loss = 0.24500702728251905
Iteration 90, Training loss = 0.24354691105188378
Model training time: 29.605849981307983
Device: cuda
Iteration 0, Training loss = 0.9957011114571981
Iteration 10, Training loss = 0.29002782673651467
Iteration 20, Training loss = 0.272555702622386
Iteration 30, Training loss = 0.2657492471777875
Iteration 40, Training loss = 0.2592319175241074
Iteration 50, Training loss = 0.25584821334639607
Iteration 60, Training loss = 0.25236770478279696
Iteration 70, Training loss = 0.2504761315846213
Iteration 80, Training loss = 0.2478948727178113
Iteration 90, Training loss = 0.24357932989580044
Model training time: 29.921976566314697
Device: cuda
Iteration 0, Training loss = 0.8869006288224373
Iteration 10, Training loss = 0.28684389828771784
Iteration 20, Training loss = 0.27108489876783987
Iteration 30, Training loss = 0.2620233169932296
Iteration 40, Training loss = 0.2564302342021523
Iteration 50, Training loss = 0.25308076612615354
Iteration 60, Training loss = 0.25068997167014845
Iteration 70, Training loss = 0.2490612011456835
Iteration 80, Training loss = 0.2474871757597739
Iteration 90, Training loss = 0.246169305977902
Model training time: 29.93420672416687
Device: cuda
Iteration 0, Training loss = 0.9313409770456489
Iteration 10, Training loss = 0.28823024664380126
Iteration 20, Training loss = 0.2710443608066886
Iteration 30, Training loss = 0.26363255461488944
Iteration 40, Training loss = 0.25784308952841783
Iteration 50, Training loss = 0.2547531494339883
Iteration 60, Training loss = 0.25100990131065465
Iteration 70, Training loss = 0.24958059072926425
Iteration 80, Training loss = 0.24755136056798668
Iteration 90, Training loss = 0.24556854558019822
Model training time: 32.18356466293335
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0045247023232318
Iteration 10, Training loss = 0.9493971037979864
Iteration 20, Training loss = 0.8560750015115969
Iteration 30, Training loss = 0.6480793246036567
Iteration 40, Training loss = 0.44715440107716453
Iteration 50, Training loss = 0.39125051360199414
Iteration 60, Training loss = 0.37471159781522795
Iteration 70, Training loss = 0.3617948711206372
Iteration 80, Training loss = 0.35276008062604547
Iteration 90, Training loss = 0.3452335256309325
Model training time: 28.686389923095703
Device: cuda
Iteration 0, Training loss = 1.0161525396909115
Iteration 10, Training loss = 0.9962163892921042
Iteration 20, Training loss = 0.9918720031130142
Iteration 30, Training loss = 0.9823292240428464
Iteration 40, Training loss = 0.9561688617807655
Iteration 50, Training loss = 0.8882666138059275
Iteration 60, Training loss = 0.7249478527889159
Iteration 70, Training loss = 0.49669754879486155
Iteration 80, Training loss = 0.3986537515541206
Iteration 90, Training loss = 0.3721324950456619
Model training time: 28.563613176345825
Device: cuda
Iteration 0, Training loss = 1.0051242257085975
Iteration 10, Training loss = 0.9921170657383647
Iteration 20, Training loss = 0.9868226154990818
Iteration 30, Training loss = 0.9758499091950016
Iteration 40, Training loss = 0.9481898785213342
Iteration 50, Training loss = 0.8897008003244078
Iteration 60, Training loss = 0.7449887488487262
Iteration 70, Training loss = 0.5307523707548777
Iteration 80, Training loss = 0.4067309890392322
Iteration 90, Training loss = 0.37348297648671747
Model training time: 28.46716856956482
Device: cuda
Iteration 0, Training loss = 0.996107806618087
Iteration 10, Training loss = 0.9903556819699236
Iteration 20, Training loss = 0.9789356943489849
Iteration 30, Training loss = 0.949990889876361
Iteration 40, Training loss = 0.8841696251993594
Iteration 50, Training loss = 0.7268808046400835
Iteration 60, Training loss = 0.5163384003627703
Iteration 70, Training loss = 0.4137098242119315
Iteration 80, Training loss = 0.38299853185524685
Iteration 90, Training loss = 0.36658054640615617
Model training time: 27.007102012634277
Device: cuda
Iteration 0, Training loss = 1.0027935098910676
Iteration 10, Training loss = 0.9898643503730423
Iteration 20, Training loss = 0.9788959489928352
Iteration 30, Training loss = 0.9531339717947919
Iteration 40, Training loss = 0.8833731254517744
Iteration 50, Training loss = 0.7088384193498731
Iteration 60, Training loss = 0.48471660876043754
Iteration 70, Training loss = 0.39014655689974337
Iteration 80, Training loss = 0.3648582432725003
Iteration 90, Training loss = 0.351560662334092
Model training time: 26.25334072113037
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.917116738171969
Iteration 10, Training loss = 0.29031747673156755
Iteration 20, Training loss = 0.27822540196531637
Iteration 30, Training loss = 0.27059930735740106
Iteration 40, Training loss = 0.26559594518320573
Iteration 50, Training loss = 0.26074290336762074
Iteration 60, Training loss = 0.25673900073133227
Iteration 70, Training loss = 0.2528819891349705
Iteration 80, Training loss = 0.25078087800366866
Iteration 90, Training loss = 0.24902640657436445
Model training time: 29.471299409866333
Device: cuda
Iteration 0, Training loss = 0.9334166604251677
Iteration 10, Training loss = 0.28994626613054875
Iteration 20, Training loss = 0.27613833928165804
Iteration 30, Training loss = 0.2694410129589735
Iteration 40, Training loss = 0.26540473315882795
Iteration 50, Training loss = 0.26399896679005186
Iteration 60, Training loss = 0.260110049036102
Iteration 70, Training loss = 0.25747417460605143
Iteration 80, Training loss = 0.25628834977242104
Iteration 90, Training loss = 0.2534476376817998
Model training time: 30.27877426147461
Device: cuda
Iteration 0, Training loss = 0.86776520707757
Iteration 10, Training loss = 0.28706789822970036
Iteration 20, Training loss = 0.268509942876256
Iteration 30, Training loss = 0.260246110279203
Iteration 40, Training loss = 0.2553032961300606
Iteration 50, Training loss = 0.24936009403588116
Iteration 60, Training loss = 0.2468580723096783
Iteration 70, Training loss = 0.24396368483270425
Iteration 80, Training loss = 0.2399149592515927
Iteration 90, Training loss = 0.2375180507843621
Model training time: 30.437377452850342
Device: cuda
Iteration 0, Training loss = 0.8312996623020817
Iteration 10, Training loss = 0.2902784374888968
Iteration 20, Training loss = 0.2749638712204597
Iteration 30, Training loss = 0.26498634181016884
Iteration 40, Training loss = 0.25992446986661444
Iteration 50, Training loss = 0.2558240731773169
Iteration 60, Training loss = 0.25291279832953995
Iteration 70, Training loss = 0.24739901084398877
Iteration 80, Training loss = 0.24402580867354998
Iteration 90, Training loss = 0.24115692885313633
Model training time: 29.26902413368225
Device: cuda
Iteration 0, Training loss = 0.8309331091417782
Iteration 10, Training loss = 0.2879623431658399
Iteration 20, Training loss = 0.2696803598012325
Iteration 30, Training loss = 0.25959839682648145
Iteration 40, Training loss = 0.25585603350458513
Iteration 50, Training loss = 0.25157935687021354
Iteration 60, Training loss = 0.24986388789858796
Iteration 70, Training loss = 0.24640725560234364
Iteration 80, Training loss = 0.24348379498806552
Iteration 90, Training loss = 0.24111120881521758
Model training time: 30.152254343032837
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0217717399343775
Iteration 10, Training loss = 0.9857037222903707
Iteration 20, Training loss = 0.9582542702195725
Iteration 30, Training loss = 0.8936334335861574
Iteration 40, Training loss = 0.724845105899129
Iteration 50, Training loss = 0.49342768750904836
Iteration 60, Training loss = 0.4027096215221617
Iteration 70, Training loss = 0.3795073952363885
Iteration 80, Training loss = 0.36547591022535225
Iteration 90, Training loss = 0.3554031607753413
Model training time: 25.44915008544922
Device: cuda
Iteration 0, Training loss = 1.0296489789290129
Iteration 10, Training loss = 0.973569762447606
Iteration 20, Training loss = 0.9322499468706656
Iteration 30, Training loss = 0.8178485842142704
Iteration 40, Training loss = 0.5800218848502579
Iteration 50, Training loss = 0.4157701318246731
Iteration 60, Training loss = 0.3802811243539847
Iteration 70, Training loss = 0.3668100091714214
Iteration 80, Training loss = 0.35711080894090125
Iteration 90, Training loss = 0.3499033798054221
Model training time: 25.804299116134644
Device: cuda
Iteration 0, Training loss = 1.0726990201622968
Iteration 10, Training loss = 0.9929502594874101
Iteration 20, Training loss = 0.9781304235043733
Iteration 30, Training loss = 0.9494267454469837
Iteration 40, Training loss = 0.867338043048186
Iteration 50, Training loss = 0.6628763155948713
Iteration 60, Training loss = 0.43150949010238554
Iteration 70, Training loss = 0.36679973024964907
Iteration 80, Training loss = 0.35293799451583824
Iteration 90, Training loss = 0.3454831291512015
Model training time: 25.193464040756226
Device: cuda
Iteration 0, Training loss = 1.0039111217438887
Iteration 10, Training loss = 0.9843099238216013
Iteration 20, Training loss = 0.9542365877524667
Iteration 30, Training loss = 0.8729361407998679
Iteration 40, Training loss = 0.665091606873821
Iteration 50, Training loss = 0.4294510359925348
Iteration 60, Training loss = 0.3689730101161533
Iteration 70, Training loss = 0.3564567036098904
Iteration 80, Training loss = 0.3489253335240958
Iteration 90, Training loss = 0.34421762392140814
Model training time: 25.202793836593628
Device: cuda
Iteration 0, Training loss = 1.00149010690514
Iteration 10, Training loss = 0.9910307460361056
Iteration 20, Training loss = 0.977002541512107
Iteration 30, Training loss = 0.944069270638452
Iteration 40, Training loss = 0.8756710858160747
Iteration 50, Training loss = 0.735283872211613
Iteration 60, Training loss = 0.5353010996528293
Iteration 70, Training loss = 0.41619510264788273
Iteration 80, Training loss = 0.38218349297553444
Iteration 90, Training loss = 0.36662975774295087
Model training time: 25.51171565055847
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8945070948577734
Iteration 10, Training loss = 0.28815977934046066
Iteration 20, Training loss = 0.2737860617718259
Iteration 30, Training loss = 0.2660919324931315
Iteration 40, Training loss = 0.26040754801985144
Iteration 50, Training loss = 0.2570719803876923
Iteration 60, Training loss = 0.25304719636549694
Iteration 70, Training loss = 0.25139092027709103
Iteration 80, Training loss = 0.24822936091877987
Iteration 90, Training loss = 0.2443343806957853
Iteration 100, Training loss = 0.2423923191960883
Iteration 110, Training loss = 0.24058646096843453
Iteration 120, Training loss = 0.23843121910152804
Iteration 130, Training loss = 0.23531562367067244
Iteration 140, Training loss = 0.23381330252845506
Iteration 150, Training loss = 0.23143444433448396
Iteration 160, Training loss = 0.2289365477104118
Iteration 170, Training loss = 0.22760229075444494
Iteration 180, Training loss = 0.22556127791387448
Iteration 190, Training loss = 0.22368303992322103
Model training time: 59.12084722518921
Device: cuda
Iteration 0, Training loss = 0.8389060734838679
Iteration 10, Training loss = 0.2899680034333957
Iteration 20, Training loss = 0.26940446566556386
Iteration 30, Training loss = 0.26026682638459736
Iteration 40, Training loss = 0.2561042251938207
Iteration 50, Training loss = 0.2517960968512844
Iteration 60, Training loss = 0.24911543705325195
Iteration 70, Training loss = 0.24646764180222572
Iteration 80, Training loss = 0.24295491509247516
Iteration 90, Training loss = 0.24059317448144948
Iteration 100, Training loss = 0.23790152543696805
Iteration 110, Training loss = 0.23549826896708945
Iteration 120, Training loss = 0.2341596900264998
Iteration 130, Training loss = 0.23130900074462385
Iteration 140, Training loss = 0.22945997082956748
Iteration 150, Training loss = 0.22848941590906918
Iteration 160, Training loss = 0.2268250627434196
Iteration 170, Training loss = 0.22506623830340336
Iteration 180, Training loss = 0.22342071192707993
Iteration 190, Training loss = 0.22241918155968476
Model training time: 59.48554039001465
Device: cuda
Iteration 0, Training loss = 0.8628697601205485
Iteration 10, Training loss = 0.2830668166279793
Iteration 20, Training loss = 0.26562701241261716
Iteration 30, Training loss = 0.25870622266173937
Iteration 40, Training loss = 0.25255402767859797
Iteration 50, Training loss = 0.24967570851246515
Iteration 60, Training loss = 0.2446359647932836
Iteration 70, Training loss = 0.24106272090460368
Iteration 80, Training loss = 0.23754521171395906
Iteration 90, Training loss = 0.23434282285003846
Iteration 100, Training loss = 0.23207905496232176
Iteration 110, Training loss = 0.22899176929020076
Iteration 120, Training loss = 0.22658905062986456
Iteration 130, Training loss = 0.2250634059669891
Iteration 140, Training loss = 0.22383152106821824
Iteration 150, Training loss = 0.22161579855542252
Iteration 160, Training loss = 0.2213332716393586
Iteration 170, Training loss = 0.22053415353459435
Iteration 180, Training loss = 0.21786577427300854
Iteration 190, Training loss = 0.21624133869069787
Model training time: 61.15962600708008
Device: cuda
Iteration 0, Training loss = 0.7700396027541967
Iteration 10, Training loss = 0.28564271550391607
Iteration 20, Training loss = 0.2679054798279408
Iteration 30, Training loss = 0.25828658033540286
Iteration 40, Training loss = 0.2539562928143907
Iteration 50, Training loss = 0.25077675113787395
Iteration 60, Training loss = 0.24515667849260828
Iteration 70, Training loss = 0.24251039992064094
Iteration 80, Training loss = 0.24019685574775732
Iteration 90, Training loss = 0.23722860595022421
Iteration 100, Training loss = 0.235208656080967
Iteration 110, Training loss = 0.23273266362395265
Iteration 120, Training loss = 0.23149111359447672
Iteration 130, Training loss = 0.22986779196394813
Iteration 140, Training loss = 0.22677843693805777
Iteration 150, Training loss = 0.22616999200432772
Iteration 160, Training loss = 0.22339235555722517
Iteration 170, Training loss = 0.22182146229461772
Iteration 180, Training loss = 0.22065641252314988
Iteration 190, Training loss = 0.21923053253819977
Model training time: 60.590739250183105
Device: cuda
Iteration 0, Training loss = 0.8485495694305586
Iteration 10, Training loss = 0.28260332187592696
Iteration 20, Training loss = 0.2641853396803285
Iteration 30, Training loss = 0.25708971435321126
Iteration 40, Training loss = 0.2516397847501552
Iteration 50, Training loss = 0.24759344724209412
Iteration 60, Training loss = 0.2440176129989002
Iteration 70, Training loss = 0.2415231136236214
Iteration 80, Training loss = 0.2388193091188652
Iteration 90, Training loss = 0.23703602629439266
Iteration 100, Training loss = 0.23521998560658974
Iteration 110, Training loss = 0.23186607511291182
Iteration 120, Training loss = 0.22991963443548782
Iteration 130, Training loss = 0.2289133751809885
Iteration 140, Training loss = 0.2271345114506385
Iteration 150, Training loss = 0.22631128988980095
Iteration 160, Training loss = 0.2241140695081817
Iteration 170, Training loss = 0.22392190326959038
Iteration 180, Training loss = 0.2222615045265875
Iteration 190, Training loss = 0.2208443355301152
Model training time: 59.434624671936035
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.119416831916081
Iteration 10, Training loss = 0.9952835573090447
Iteration 20, Training loss = 0.9876672884692317
Iteration 30, Training loss = 0.9737846154521629
Iteration 40, Training loss = 0.9412253242759889
Iteration 50, Training loss = 0.8575716894029995
Iteration 60, Training loss = 0.6662560295367587
Iteration 70, Training loss = 0.45573512324388477
Iteration 80, Training loss = 0.39344950718580235
Iteration 90, Training loss = 0.37742000091191075
Iteration 100, Training loss = 0.3677282587485613
Iteration 110, Training loss = 0.3582924725352854
Iteration 120, Training loss = 0.3509483829068677
Iteration 130, Training loss = 0.3447745907421849
Iteration 140, Training loss = 0.3396577696869339
Iteration 150, Training loss = 0.3355674244787382
Iteration 160, Training loss = 0.33157609705475793
Iteration 170, Training loss = 0.32889886718729267
Iteration 180, Training loss = 0.32621522564530947
Iteration 190, Training loss = 0.32424646320838285
Model training time: 50.73349189758301
Device: cuda
Iteration 0, Training loss = 1.0093947634604818
Iteration 10, Training loss = 0.9805955532668293
Iteration 20, Training loss = 0.9486947508825772
Iteration 30, Training loss = 0.8753644718928038
Iteration 40, Training loss = 0.7000175001828567
Iteration 50, Training loss = 0.47997746505023203
Iteration 60, Training loss = 0.40201898307903955
Iteration 70, Training loss = 0.3808593134517255
Iteration 80, Training loss = 0.3672525768982615
Iteration 90, Training loss = 0.3572276896180738
Iteration 100, Training loss = 0.34885466300347
Iteration 110, Training loss = 0.34288588821312077
Iteration 120, Training loss = 0.3373421355722031
Iteration 130, Training loss = 0.33454600350868297
Iteration 140, Training loss = 0.3301067315581916
Iteration 150, Training loss = 0.3272145160874307
Iteration 160, Training loss = 0.3249027610257052
Iteration 170, Training loss = 0.3233850902837256
Iteration 180, Training loss = 0.32103922200087764
Iteration 190, Training loss = 0.32072562447204683
Model training time: 50.5896315574646
Device: cuda
Iteration 0, Training loss = 1.0019023343560776
Iteration 10, Training loss = 0.993799304616624
Iteration 20, Training loss = 0.9843400106337912
Iteration 30, Training loss = 0.9666848345367229
Iteration 40, Training loss = 0.9221564491013974
Iteration 50, Training loss = 0.8045098021410514
Iteration 60, Training loss = 0.5632900745396453
Iteration 70, Training loss = 0.4081135595910215
Iteration 80, Training loss = 0.3786595915826622
Iteration 90, Training loss = 0.3660305295733438
Iteration 100, Training loss = 0.35742359410449503
Iteration 110, Training loss = 0.35071093391105174
Iteration 120, Training loss = 0.34537452063410756
Iteration 130, Training loss = 0.3409389638238483
Iteration 140, Training loss = 0.33712859530955697
Iteration 150, Training loss = 0.33485814173152484
Iteration 160, Training loss = 0.33234180114119527
Iteration 170, Training loss = 0.3284160843577938
Iteration 180, Training loss = 0.32615501048484286
Iteration 190, Training loss = 0.32575899117810714
Model training time: 51.080649852752686
Device: cuda
Iteration 0, Training loss = 1.0265274484088456
Iteration 10, Training loss = 0.9974651489280848
Iteration 20, Training loss = 0.9868452246638312
Iteration 30, Training loss = 0.9696139912098503
Iteration 40, Training loss = 0.9242722043668591
Iteration 50, Training loss = 0.8170642145877876
Iteration 60, Training loss = 0.6055850547868848
Iteration 70, Training loss = 0.4373326824195143
Iteration 80, Training loss = 0.39200215819089307
Iteration 90, Training loss = 0.37504217813268376
Iteration 100, Training loss = 0.3637869209770995
Iteration 110, Training loss = 0.3543321483809015
Iteration 120, Training loss = 0.347544209320764
Iteration 130, Training loss = 0.34150330967085374
Iteration 140, Training loss = 0.3381168550051353
Iteration 150, Training loss = 0.33347069050954736
Iteration 160, Training loss = 0.3300766857781848
Iteration 170, Training loss = 0.32745170614857605
Iteration 180, Training loss = 0.3248839193352179
Iteration 190, Training loss = 0.3238591836486462
Model training time: 53.414639711380005
Device: cuda
Iteration 0, Training loss = 1.0258073988168135
Iteration 10, Training loss = 0.9982088156368422
Iteration 20, Training loss = 0.9901731264763984
Iteration 30, Training loss = 0.9776084178311814
Iteration 40, Training loss = 0.9464424425277157
Iteration 50, Training loss = 0.8646489502727122
Iteration 60, Training loss = 0.6703155232224487
Iteration 70, Training loss = 0.44865156011880886
Iteration 80, Training loss = 0.37911050640730465
Iteration 90, Training loss = 0.3599206721725095
Iteration 100, Training loss = 0.34796453422106405
Iteration 110, Training loss = 0.33880814463620024
Iteration 120, Training loss = 0.3327675998786797
Iteration 130, Training loss = 0.3278594030706203
Iteration 140, Training loss = 0.32381957754996665
Iteration 150, Training loss = 0.32144157725255845
Iteration 160, Training loss = 0.3188897115020936
Iteration 170, Training loss = 0.3166269836362433
Iteration 180, Training loss = 0.31541581761433884
Iteration 190, Training loss = 0.3132672725956221
Model training time: 51.37117409706116
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9007439289403998
Iteration 10, Training loss = 0.29373112227317794
Iteration 20, Training loss = 0.2776803644958902
Iteration 30, Training loss = 0.2673397695528712
Iteration 40, Training loss = 0.25935762116442557
Iteration 50, Training loss = 0.25265224670298436
Iteration 60, Training loss = 0.24781275011491086
Iteration 70, Training loss = 0.2456758979725953
Iteration 80, Training loss = 0.24247346537700598
Iteration 90, Training loss = 0.2398355373941758
Iteration 100, Training loss = 0.23990118442382213
Iteration 110, Training loss = 0.23787846401837712
Iteration 120, Training loss = 0.2358043871568021
Iteration 130, Training loss = 0.2345071166465824
Iteration 140, Training loss = 0.2324229353579922
Iteration 150, Training loss = 0.23117240187194613
Iteration 160, Training loss = 0.23087968428929648
Iteration 170, Training loss = 0.22849430824103562
Iteration 180, Training loss = 0.22799277237216056
Iteration 190, Training loss = 0.2259083468341021
Model training time: 58.72934937477112
Device: cuda
Iteration 0, Training loss = 0.8881978493381814
Iteration 10, Training loss = 0.28436036379153024
Iteration 20, Training loss = 0.26402511832794706
Iteration 30, Training loss = 0.25677814528993936
Iteration 40, Training loss = 0.250904515961518
Iteration 50, Training loss = 0.24591460050160183
Iteration 60, Training loss = 0.24269037445386252
Iteration 70, Training loss = 0.23998883866889464
Iteration 80, Training loss = 0.23804975405407414
Iteration 90, Training loss = 0.23783960186196987
Iteration 100, Training loss = 0.23512739414610148
Iteration 110, Training loss = 0.23405003461284915
Iteration 120, Training loss = 0.2329001625738858
Iteration 130, Training loss = 0.23125709589696736
Iteration 140, Training loss = 0.23001925442097843
Iteration 150, Training loss = 0.22881742314871958
Iteration 160, Training loss = 0.2273826292819447
Iteration 170, Training loss = 0.2270818771659464
Iteration 180, Training loss = 0.2261237523334038
Iteration 190, Training loss = 0.22546373268112468
Model training time: 59.59750819206238
Device: cuda
Iteration 0, Training loss = 0.9785885596332918
Iteration 10, Training loss = 0.28821907631584986
Iteration 20, Training loss = 0.27488520071990247
Iteration 30, Training loss = 0.2662527789384271
Iteration 40, Training loss = 0.26086046586290074
Iteration 50, Training loss = 0.25596654775062044
Iteration 60, Training loss = 0.2538599659135376
Iteration 70, Training loss = 0.2531751781918
Iteration 80, Training loss = 0.2493948779532299
Iteration 90, Training loss = 0.24807883961045224
Iteration 100, Training loss = 0.24657090378988192
Iteration 110, Training loss = 0.24550787917369807
Iteration 120, Training loss = 0.24404465461122818
Iteration 130, Training loss = 0.24336263559002808
Iteration 140, Training loss = 0.24182528502123368
Iteration 150, Training loss = 0.24085536300416155
Iteration 160, Training loss = 0.23925828627774104
Iteration 170, Training loss = 0.237844691220401
Iteration 180, Training loss = 0.2358005913199434
Iteration 190, Training loss = 0.23369233587369825
Model training time: 59.479158878326416
Device: cuda
Iteration 0, Training loss = 0.9684090749653065
Iteration 10, Training loss = 0.28902292528734114
Iteration 20, Training loss = 0.2680281973691378
Iteration 30, Training loss = 0.2614829049666147
Iteration 40, Training loss = 0.2588824994034237
Iteration 50, Training loss = 0.25232094436308034
Iteration 60, Training loss = 0.2484927297646297
Iteration 70, Training loss = 0.2451318502786079
Iteration 80, Training loss = 0.24095448948766873
Iteration 90, Training loss = 0.23802247553487907
Iteration 100, Training loss = 0.23459102666896323
Iteration 110, Training loss = 0.23253315075296135
Iteration 120, Training loss = 0.2314270123983351
Iteration 130, Training loss = 0.22858978555974177
Iteration 140, Training loss = 0.22622218170171773
Iteration 150, Training loss = 0.22434267110150793
Iteration 160, Training loss = 0.22325473984226513
Iteration 170, Training loss = 0.22132457425628882
Iteration 180, Training loss = 0.22019636818176305
Iteration 190, Training loss = 0.21918289345387676
Model training time: 59.91490864753723
Device: cuda
Iteration 0, Training loss = 0.9294985076367567
Iteration 10, Training loss = 0.286242774611222
Iteration 20, Training loss = 0.2641267411518788
Iteration 30, Training loss = 0.25717951105412656
Iteration 40, Training loss = 0.25207155259047154
Iteration 50, Training loss = 0.2489718092236542
Iteration 60, Training loss = 0.24696667449197907
Iteration 70, Training loss = 0.24412247700535733
Iteration 80, Training loss = 0.24156746847762
Iteration 90, Training loss = 0.23953552194983488
Iteration 100, Training loss = 0.2377368179352387
Iteration 110, Training loss = 0.2360158414008537
Iteration 120, Training loss = 0.23567280626815298
Iteration 130, Training loss = 0.23373252085009635
Iteration 140, Training loss = 0.2323597517710377
Iteration 150, Training loss = 0.23100357473904384
Iteration 160, Training loss = 0.22992225117297563
Iteration 170, Training loss = 0.22822504479815994
Iteration 180, Training loss = 0.2270669492618473
Iteration 190, Training loss = 0.22548310523447784
Model training time: 59.107264041900635
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0319928846497466
Iteration 10, Training loss = 0.9968737055714004
Iteration 20, Training loss = 0.9886553297584183
Iteration 30, Training loss = 0.9766967558630423
Iteration 40, Training loss = 0.9501119636107183
Iteration 50, Training loss = 0.8868510913157809
Iteration 60, Training loss = 0.7334697027713204
Iteration 70, Training loss = 0.514859981223005
Iteration 80, Training loss = 0.41250429946731254
Iteration 90, Training loss = 0.3844770591184137
Iteration 100, Training loss = 0.36866546065911004
Iteration 110, Training loss = 0.35516791628754657
Iteration 120, Training loss = 0.34541688614709365
Iteration 130, Training loss = 0.3387669407659107
Iteration 140, Training loss = 0.3323222561446941
Iteration 150, Training loss = 0.32797105912713037
Iteration 160, Training loss = 0.3245997016772556
Iteration 170, Training loss = 0.3225125835281639
Iteration 180, Training loss = 0.3193262174797519
Iteration 190, Training loss = 0.3169918775846417
Model training time: 51.28710150718689
Device: cuda
Iteration 0, Training loss = 1.0087520382254596
Iteration 10, Training loss = 0.989183259183082
Iteration 20, Training loss = 0.9777508867535614
Iteration 30, Training loss = 0.9494713366319592
Iteration 40, Training loss = 0.8829084646298689
Iteration 50, Training loss = 0.7273742617040441
Iteration 60, Training loss = 0.518769212535038
Iteration 70, Training loss = 0.41414194348929584
Iteration 80, Training loss = 0.38024270476926353
Iteration 90, Training loss = 0.3642414652063075
Iteration 100, Training loss = 0.352059640314268
Iteration 110, Training loss = 0.3437579327447403
Iteration 120, Training loss = 0.33638596786681
Iteration 130, Training loss = 0.33085665430711664
Iteration 140, Training loss = 0.32680641359465135
Iteration 150, Training loss = 0.32361584383508435
Iteration 160, Training loss = 0.3206450515899105
Iteration 170, Training loss = 0.3191410071320004
Iteration 180, Training loss = 0.31707702325162107
Iteration 190, Training loss = 0.3146513728559881
Model training time: 51.320576906204224
Device: cuda
Iteration 0, Training loss = 1.0044716792406092
Iteration 10, Training loss = 0.99564781209121
Iteration 20, Training loss = 0.9885978266812753
Iteration 30, Training loss = 0.971384652858771
Iteration 40, Training loss = 0.9343217452943037
Iteration 50, Training loss = 0.8330685872098674
Iteration 60, Training loss = 0.6092929788257765
Iteration 70, Training loss = 0.4265063095495897
Iteration 80, Training loss = 0.3788799585927512
Iteration 90, Training loss = 0.3615315602597407
Iteration 100, Training loss = 0.35016587815710887
Iteration 110, Training loss = 0.3415243097981393
Iteration 120, Training loss = 0.33516816031817653
Iteration 130, Training loss = 0.33064509805849784
Iteration 140, Training loss = 0.32547504282084067
Iteration 150, Training loss = 0.3226936129124268
Iteration 160, Training loss = 0.31972200981372795
Iteration 170, Training loss = 0.3179070314635401
Iteration 180, Training loss = 0.3163103030787574
Iteration 190, Training loss = 0.3157839492899208
Model training time: 53.216163873672485
Device: cuda
Iteration 0, Training loss = 1.0049707166814574
Iteration 10, Training loss = 0.9980969408859953
Iteration 20, Training loss = 0.9910093902965674
Iteration 30, Training loss = 0.9798154318390261
Iteration 40, Training loss = 0.9599892646794158
Iteration 50, Training loss = 0.9129963272435654
Iteration 60, Training loss = 0.7937798888786979
Iteration 70, Training loss = 0.5997875905843173
Iteration 80, Training loss = 0.45737645613110584
Iteration 90, Training loss = 0.40349213584609656
Iteration 100, Training loss = 0.3807961683054477
Iteration 110, Training loss = 0.36554039712401404
Iteration 120, Training loss = 0.3537941263349736
Iteration 130, Training loss = 0.3443336979922465
Iteration 140, Training loss = 0.33720664759189034
Iteration 150, Training loss = 0.3328933404839557
Iteration 160, Training loss = 0.32746316125427466
Iteration 170, Training loss = 0.3245837891447371
Iteration 180, Training loss = 0.3208559511796288
Iteration 190, Training loss = 0.3175286254013218
Model training time: 52.383397340774536
Device: cuda
Iteration 0, Training loss = 1.020793578762939
Iteration 10, Training loss = 0.9940025717164007
Iteration 20, Training loss = 0.9799581023230068
Iteration 30, Training loss = 0.9504708233086959
Iteration 40, Training loss = 0.8787257576334304
Iteration 50, Training loss = 0.6992418252039647
Iteration 60, Training loss = 0.4747103857245422
Iteration 70, Training loss = 0.3919208683253486
Iteration 80, Training loss = 0.3697492426288301
Iteration 90, Training loss = 0.35601597556457426
Iteration 100, Training loss = 0.3465034742142267
Iteration 110, Training loss = 0.33820259743842523
Iteration 120, Training loss = 0.3318034332731496
Iteration 130, Training loss = 0.32726677957076383
Iteration 140, Training loss = 0.32444631023971354
Iteration 150, Training loss = 0.3210729143901724
Iteration 160, Training loss = 0.3182727616046362
Iteration 170, Training loss = 0.3165691318575311
Iteration 180, Training loss = 0.3148737824049549
Iteration 190, Training loss = 0.31425140092626286
Model training time: 52.52647852897644
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9281414974714823
Iteration 10, Training loss = 0.2870463708028701
Iteration 20, Training loss = 0.27180362204854613
Iteration 30, Training loss = 0.26320117578846247
Iteration 40, Training loss = 0.25794483170561167
Iteration 50, Training loss = 0.2542566710336197
Iteration 60, Training loss = 0.25164582123215073
Iteration 70, Training loss = 0.2505017341191066
Iteration 80, Training loss = 0.24792405775779688
Iteration 90, Training loss = 0.24668343041254126
Iteration 100, Training loss = 0.2440417866056092
Iteration 110, Training loss = 0.24338227370079013
Iteration 120, Training loss = 0.2402527897398253
Iteration 130, Training loss = 0.23860104130086116
Iteration 140, Training loss = 0.23662975315310528
Iteration 150, Training loss = 0.23521877825260162
Iteration 160, Training loss = 0.2344309969080819
Iteration 170, Training loss = 0.23260815459605
Iteration 180, Training loss = 0.23139820697802851
Iteration 190, Training loss = 0.23011098262192548
Model training time: 60.04136085510254
Device: cuda
Iteration 0, Training loss = 0.8127760177361216
Iteration 10, Training loss = 0.2907636152661365
Iteration 20, Training loss = 0.2739967250305673
Iteration 30, Training loss = 0.26620841541008095
Iteration 40, Training loss = 0.2615179891387622
Iteration 50, Training loss = 0.25700590370789816
Iteration 60, Training loss = 0.25390144919859614
Iteration 70, Training loss = 0.2508074753526328
Iteration 80, Training loss = 0.2488237729539042
Iteration 90, Training loss = 0.2461507428743413
Iteration 100, Training loss = 0.24408348149435533
Iteration 110, Training loss = 0.24362723025434835
Iteration 120, Training loss = 0.24170539074617883
Iteration 130, Training loss = 0.24010009714514738
Iteration 140, Training loss = 0.23874441703448548
Iteration 150, Training loss = 0.23707444562716184
Iteration 160, Training loss = 0.23424186766291585
Iteration 170, Training loss = 0.2327897644777229
Iteration 180, Training loss = 0.22892004402651303
Iteration 190, Training loss = 0.22656593550950432
Model training time: 60.6335232257843
Device: cuda
Iteration 0, Training loss = 0.9001189559554132
Iteration 10, Training loss = 0.28963134563775456
Iteration 20, Training loss = 0.26935748104455964
Iteration 30, Training loss = 0.2574904092771996
Iteration 40, Training loss = 0.251353332205959
Iteration 50, Training loss = 0.24614763144709637
Iteration 60, Training loss = 0.2426657929368641
Iteration 70, Training loss = 0.23882500129045495
Iteration 80, Training loss = 0.23579874757119423
Iteration 90, Training loss = 0.23408854558416034
Iteration 100, Training loss = 0.23113142742194992
Iteration 110, Training loss = 0.22844337650399277
Iteration 120, Training loss = 0.22588523930829504
Iteration 130, Training loss = 0.22457736406637274
Iteration 140, Training loss = 0.22253369064866632
Iteration 150, Training loss = 0.22105507599414834
Iteration 160, Training loss = 0.21925054515761452
Iteration 170, Training loss = 0.21770174826544839
Iteration 180, Training loss = 0.21687307043208015
Iteration 190, Training loss = 0.21698713032663733
Model training time: 61.072919607162476
Device: cuda
Iteration 0, Training loss = 0.9016348853491355
Iteration 10, Training loss = 0.2901659209083244
Iteration 20, Training loss = 0.27165142299184475
Iteration 30, Training loss = 0.26121645791519094
Iteration 40, Training loss = 0.25629814530628314
Iteration 50, Training loss = 0.25362591866565787
Iteration 60, Training loss = 0.25049907062652604
Iteration 70, Training loss = 0.24856576388296875
Iteration 80, Training loss = 0.24481020911016327
Iteration 90, Training loss = 0.24269031873647717
Iteration 100, Training loss = 0.24096757220329293
Iteration 110, Training loss = 0.23946143154072877
Iteration 120, Training loss = 0.23753303476577795
Iteration 130, Training loss = 0.2351287835534068
Iteration 140, Training loss = 0.23358819702109276
Iteration 150, Training loss = 0.23165780097102198
Iteration 160, Training loss = 0.23007872257975565
Iteration 170, Training loss = 0.22910192760004514
Iteration 180, Training loss = 0.22659695666769278
Iteration 190, Training loss = 0.2244640540889496
Model training time: 61.873268604278564
Device: cuda
Iteration 0, Training loss = 1.0174871449885161
Iteration 10, Training loss = 0.288025329798316
Iteration 20, Training loss = 0.27005199137805164
Iteration 30, Training loss = 0.2624235821950839
Iteration 40, Training loss = 0.2582990970228605
Iteration 50, Training loss = 0.2550316372715333
Iteration 60, Training loss = 0.25147075843119965
Iteration 70, Training loss = 0.2501032483822482
Iteration 80, Training loss = 0.2475430758463012
Iteration 90, Training loss = 0.24347721288601556
Iteration 100, Training loss = 0.24113392138826675
Iteration 110, Training loss = 0.2398637907156622
Iteration 120, Training loss = 0.23787035818261223
Iteration 130, Training loss = 0.23822736948872533
Iteration 140, Training loss = 0.23500193572706646
Iteration 150, Training loss = 0.23337773039289142
Iteration 160, Training loss = 0.23218298498271167
Iteration 170, Training loss = 0.23089520716004902
Iteration 180, Training loss = 0.22898909453176647
Iteration 190, Training loss = 0.22825647055526863
Model training time: 62.63577127456665
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0945211283250706
Iteration 10, Training loss = 0.984684515690458
Iteration 20, Training loss = 0.9639891573196449
Iteration 30, Training loss = 0.9179750784583713
Iteration 40, Training loss = 0.7942705871402354
Iteration 50, Training loss = 0.5612688827629827
Iteration 60, Training loss = 0.41218237571670235
Iteration 70, Training loss = 0.3774547740313166
Iteration 80, Training loss = 0.36150454647011226
Iteration 90, Training loss = 0.35109599409759906
Iteration 100, Training loss = 0.34317336602199483
Iteration 110, Training loss = 0.3367121360008267
Iteration 120, Training loss = 0.33255467970590086
Iteration 130, Training loss = 0.32836565173766463
Iteration 140, Training loss = 0.32518439016480377
Iteration 150, Training loss = 0.3230293222527573
Iteration 160, Training loss = 0.3208294316478398
Iteration 170, Training loss = 0.3186945648153047
Iteration 180, Training loss = 0.31816341900739115
Iteration 190, Training loss = 0.31564174138981366
Model training time: 58.23604345321655
Device: cuda
Iteration 0, Training loss = 1.0150246185381055
Iteration 10, Training loss = 0.9999815084508076
Iteration 20, Training loss = 0.9975098397420801
Iteration 30, Training loss = 0.9896255371651211
Iteration 40, Training loss = 0.9763814948031292
Iteration 50, Training loss = 0.9450603855980767
Iteration 60, Training loss = 0.8588578690941208
Iteration 70, Training loss = 0.6529328385412981
Iteration 80, Training loss = 0.44636408351181783
Iteration 90, Training loss = 0.3846176481909222
Iteration 100, Training loss = 0.36799037528498735
Iteration 110, Training loss = 0.3568114777981947
Iteration 120, Training loss = 0.34894228550259043
Iteration 130, Training loss = 0.3417489722299115
Iteration 140, Training loss = 0.33643850626576927
Iteration 150, Training loss = 0.33296251570544955
Iteration 160, Training loss = 0.32910773784354114
Iteration 170, Training loss = 0.3260652895854867
Iteration 180, Training loss = 0.32375313769935987
Iteration 190, Training loss = 0.321432631228857
Model training time: 56.75489091873169
Device: cuda
Iteration 0, Training loss = 1.017300139590738
Iteration 10, Training loss = 0.9922415665958239
Iteration 20, Training loss = 0.9841659204395496
Iteration 30, Training loss = 0.962136741709594
Iteration 40, Training loss = 0.9151591091340291
Iteration 50, Training loss = 0.8077084125528013
Iteration 60, Training loss = 0.6048360720348819
Iteration 70, Training loss = 0.44032745545613017
Iteration 80, Training loss = 0.3911042901628835
Iteration 90, Training loss = 0.3744613675103671
Iteration 100, Training loss = 0.36017485754789363
Iteration 110, Training loss = 0.34964914670313035
Iteration 120, Training loss = 0.3425611747779708
Iteration 130, Training loss = 0.33646355490177726
Iteration 140, Training loss = 0.33248205180617346
Iteration 150, Training loss = 0.32755222535075773
Iteration 160, Training loss = 0.3250713423974272
Iteration 170, Training loss = 0.3221467572853761
Iteration 180, Training loss = 0.319723775997254
Iteration 190, Training loss = 0.31909845107131535
Model training time: 53.260573863983154
Device: cuda
Iteration 0, Training loss = 1.0107658033209723
Iteration 10, Training loss = 0.9967581330290163
Iteration 20, Training loss = 0.9898192444861224
Iteration 30, Training loss = 0.971128387439654
Iteration 40, Training loss = 0.9390967738801155
Iteration 50, Training loss = 0.8621300755491579
Iteration 60, Training loss = 0.6954273935677349
Iteration 70, Training loss = 0.4979626365041963
Iteration 80, Training loss = 0.4131275407502041
Iteration 90, Training loss = 0.3873556380830525
Iteration 100, Training loss = 0.37105470780589156
Iteration 110, Training loss = 0.35923343443352246
Iteration 120, Training loss = 0.3501995453512035
Iteration 130, Training loss = 0.34370271084101306
Iteration 140, Training loss = 0.33720299680739785
Iteration 150, Training loss = 0.333166883331566
Iteration 160, Training loss = 0.33063487348637144
Iteration 170, Training loss = 0.3263604145695046
Iteration 180, Training loss = 0.3245830712255072
Iteration 190, Training loss = 0.3226666888177107
Model training time: 52.82799744606018
Device: cuda
Iteration 0, Training loss = 1.0167933143855292
Iteration 10, Training loss = 0.9898535920226056
Iteration 20, Training loss = 0.9744730303828842
Iteration 30, Training loss = 0.9379283025644828
Iteration 40, Training loss = 0.8467037070200639
Iteration 50, Training loss = 0.6374927531693868
Iteration 60, Training loss = 0.4296432360646805
Iteration 70, Training loss = 0.37181055178676825
Iteration 80, Training loss = 0.35529214093362654
Iteration 90, Training loss = 0.34539750542329706
Iteration 100, Training loss = 0.3373044744349908
Iteration 110, Training loss = 0.3316222370966621
Iteration 120, Training loss = 0.3275581812800992
Iteration 130, Training loss = 0.324004131598749
Iteration 140, Training loss = 0.3214782030110198
Iteration 150, Training loss = 0.320297423840145
Iteration 160, Training loss = 0.31768966325815173
Iteration 170, Training loss = 0.3160584389011641
Iteration 180, Training loss = 0.3149718031791097
Iteration 190, Training loss = 0.31362651612447656
Model training time: 51.15043640136719
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8650932119088472
Iteration 10, Training loss = 0.2856708362626569
Iteration 20, Training loss = 0.26696788177686037
Iteration 30, Training loss = 0.2595281124043004
Iteration 40, Training loss = 0.2551610101248331
Iteration 50, Training loss = 0.25118335509213846
Iteration 60, Training loss = 0.24905379976771305
Iteration 70, Training loss = 0.2466167710757486
Iteration 80, Training loss = 0.24379575443296617
Iteration 90, Training loss = 0.23997084407702737
Iteration 100, Training loss = 0.23844757193816457
Iteration 110, Training loss = 0.23500227521439104
Iteration 120, Training loss = 0.23285629989012427
Iteration 130, Training loss = 0.22973292979641238
Iteration 140, Training loss = 0.22761293892987108
Iteration 150, Training loss = 0.2249489646890889
Iteration 160, Training loss = 0.2225211675544292
Iteration 170, Training loss = 0.22187381504094544
Iteration 180, Training loss = 0.21975894419899308
Iteration 190, Training loss = 0.21759483628514883
Iteration 200, Training loss = 0.21616883925049776
Iteration 210, Training loss = 0.2149169549058025
Iteration 220, Training loss = 0.21384636737442247
Iteration 230, Training loss = 0.21293590851308067
Iteration 240, Training loss = 0.21087523524599952
Iteration 250, Training loss = 0.21083303011845853
Iteration 260, Training loss = 0.20844477109143122
Iteration 270, Training loss = 0.20877318339791276
Iteration 280, Training loss = 0.2065337574928279
Iteration 290, Training loss = 0.20555169957775424
Model training time: 88.86322903633118
Device: cuda
Iteration 0, Training loss = 0.7892094959959316
Iteration 10, Training loss = 0.2900838333627452
Iteration 20, Training loss = 0.2734743775664896
Iteration 30, Training loss = 0.26435015066234385
Iteration 40, Training loss = 0.2576851239382933
Iteration 50, Training loss = 0.2526891127516682
Iteration 60, Training loss = 0.24970067994318146
Iteration 70, Training loss = 0.24521046088657517
Iteration 80, Training loss = 0.24227169372033383
Iteration 90, Training loss = 0.2393688903266681
Iteration 100, Training loss = 0.23682449844436368
Iteration 110, Training loss = 0.2348029860407834
Iteration 120, Training loss = 0.23181484564059024
Iteration 130, Training loss = 0.22940991808106934
Iteration 140, Training loss = 0.22763929850813272
Iteration 150, Training loss = 0.22607852568948902
Iteration 160, Training loss = 0.2247870271116639
Iteration 170, Training loss = 0.22316706500911482
Iteration 180, Training loss = 0.221546887235653
Iteration 190, Training loss = 0.22029230722050736
Iteration 200, Training loss = 0.21917180756583882
Iteration 210, Training loss = 0.2181842816264733
Iteration 220, Training loss = 0.21682310053979717
Iteration 230, Training loss = 0.21502936055550828
Iteration 240, Training loss = 0.21346248153615113
Iteration 250, Training loss = 0.21455887006820687
Iteration 260, Training loss = 0.21088002902874048
Iteration 270, Training loss = 0.21056071650866726
Iteration 280, Training loss = 0.20915629833504773
Iteration 290, Training loss = 0.2085287894891656
Model training time: 91.96206378936768
Device: cuda
Iteration 0, Training loss = 0.8199008016482644
Iteration 10, Training loss = 0.2893225886467574
Iteration 20, Training loss = 0.2668900508883495
Iteration 30, Training loss = 0.25838274559105073
Iteration 40, Training loss = 0.25131793806086417
Iteration 50, Training loss = 0.24665245852896556
Iteration 60, Training loss = 0.24257409554604747
Iteration 70, Training loss = 0.24092060907019508
Iteration 80, Training loss = 0.23800195814762715
Iteration 90, Training loss = 0.2363816117102973
Iteration 100, Training loss = 0.23327598745984154
Iteration 110, Training loss = 0.23024487430634705
Iteration 120, Training loss = 0.228443605888293
Iteration 130, Training loss = 0.224677720543555
Iteration 140, Training loss = 0.22194468953471252
Iteration 150, Training loss = 0.22030189136664072
Iteration 160, Training loss = 0.218004974010198
Iteration 170, Training loss = 0.21719800569297035
Iteration 180, Training loss = 0.21509835591926668
Iteration 190, Training loss = 0.21408045075941776
Iteration 200, Training loss = 0.21241018467623254
Iteration 210, Training loss = 0.2116509190792047
Iteration 220, Training loss = 0.210787326241461
Iteration 230, Training loss = 0.20903265260267948
Iteration 240, Training loss = 0.2083108614464313
Iteration 250, Training loss = 0.20612609415238606
Iteration 260, Training loss = 0.20582824207590397
Iteration 270, Training loss = 0.2052655554767968
Iteration 280, Training loss = 0.2041606896597406
Iteration 290, Training loss = 0.20303263900360624
Model training time: 89.20146298408508
Device: cuda
Iteration 0, Training loss = 0.9402416832205178
Iteration 10, Training loss = 0.286289695740322
Iteration 20, Training loss = 0.27018782633226274
Iteration 30, Training loss = 0.2611979188550498
Iteration 40, Training loss = 0.25580383016579394
Iteration 50, Training loss = 0.25137604413113157
Iteration 60, Training loss = 0.24888386955295783
Iteration 70, Training loss = 0.24723560193886504
Iteration 80, Training loss = 0.24494469561726576
Iteration 90, Training loss = 0.2429634721742736
Iteration 100, Training loss = 0.24181595081148516
Iteration 110, Training loss = 0.23943112941755765
Iteration 120, Training loss = 0.2372980912166517
Iteration 130, Training loss = 0.23649806921608782
Iteration 140, Training loss = 0.23473916062410327
Iteration 150, Training loss = 0.23211247279592181
Iteration 160, Training loss = 0.2304438865127195
Iteration 170, Training loss = 0.2274064468084902
Iteration 180, Training loss = 0.22604462957900504
Iteration 190, Training loss = 0.22395708714274393
Iteration 200, Training loss = 0.22179647491893906
Iteration 210, Training loss = 0.22046223556361913
Iteration 220, Training loss = 0.21963520243691936
Iteration 230, Training loss = 0.2179501046952994
Iteration 240, Training loss = 0.2170353462730629
Iteration 250, Training loss = 0.21534557014271832
Iteration 260, Training loss = 0.2143372626264314
Iteration 270, Training loss = 0.21402027087223127
Iteration 280, Training loss = 0.21260589688728396
Iteration 290, Training loss = 0.21057147784653493
Model training time: 89.4825439453125
Device: cuda
Iteration 0, Training loss = 0.8464490846159377
Iteration 10, Training loss = 0.28282079597314197
Iteration 20, Training loss = 0.2674842761550549
Iteration 30, Training loss = 0.26033408601502867
Iteration 40, Training loss = 0.2549628927439883
Iteration 50, Training loss = 0.2515254730835629
Iteration 60, Training loss = 0.24825700354460933
Iteration 70, Training loss = 0.24487263712906032
Iteration 80, Training loss = 0.24174870755793393
Iteration 90, Training loss = 0.23926169522430585
Iteration 100, Training loss = 0.23741823380839996
Iteration 110, Training loss = 0.23539124080092436
Iteration 120, Training loss = 0.2331429430540057
Iteration 130, Training loss = 0.23161674895580264
Iteration 140, Training loss = 0.23054976408608294
Iteration 150, Training loss = 0.2288517217345284
Iteration 160, Training loss = 0.22748073705152613
Iteration 170, Training loss = 0.22548609982798065
Iteration 180, Training loss = 0.22442962114073803
Iteration 190, Training loss = 0.22273992038003487
Iteration 200, Training loss = 0.22183138589208254
Iteration 210, Training loss = 0.2196264849336827
Iteration 220, Training loss = 0.21869660158088242
Iteration 230, Training loss = 0.21716010394159724
Iteration 240, Training loss = 0.21669492787785
Iteration 250, Training loss = 0.2145742937778505
Iteration 260, Training loss = 0.2139450284305978
Iteration 270, Training loss = 0.21217716215313345
Iteration 280, Training loss = 0.21101079193722222
Iteration 290, Training loss = 0.21033871710156474
Model training time: 91.91637420654297
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.006638483148842
Iteration 10, Training loss = 0.9946167463265755
Iteration 20, Training loss = 0.9822396104750426
Iteration 30, Training loss = 0.9556544544040293
Iteration 40, Training loss = 0.8858929210238986
Iteration 50, Training loss = 0.7099431875535256
Iteration 60, Training loss = 0.47879691321204826
Iteration 70, Training loss = 0.39445610848313944
Iteration 80, Training loss = 0.3734253498811077
Iteration 90, Training loss = 0.36228670849316363
Iteration 100, Training loss = 0.35310841578504315
Iteration 110, Training loss = 0.3451150888262164
Iteration 120, Training loss = 0.34011251670150944
Iteration 130, Training loss = 0.3358084703413185
Iteration 140, Training loss = 0.33182760259667454
Iteration 150, Training loss = 0.3288479341544967
Iteration 160, Training loss = 0.32701934960441315
Iteration 170, Training loss = 0.3254824026627241
Iteration 180, Training loss = 0.32362852666689
Iteration 190, Training loss = 0.32166521438365975
Iteration 200, Training loss = 0.31989947198957636
Iteration 210, Training loss = 0.3193282243134319
Iteration 220, Training loss = 0.3177223474075253
Iteration 230, Training loss = 0.3166937360872969
Iteration 240, Training loss = 0.31584369927500755
Iteration 250, Training loss = 0.31515920378159784
Iteration 260, Training loss = 0.31379077961479407
Iteration 270, Training loss = 0.3134380433582453
Iteration 280, Training loss = 0.31281853938736204
Iteration 290, Training loss = 0.3113921990861063
Model training time: 77.4105019569397
Device: cuda
Iteration 0, Training loss = 1.0001192225350275
Iteration 10, Training loss = 0.9910885325952429
Iteration 20, Training loss = 0.9804401158710608
Iteration 30, Training loss = 0.9562851340298492
Iteration 40, Training loss = 0.8911178889481918
Iteration 50, Training loss = 0.726201997143059
Iteration 60, Training loss = 0.487307777295366
Iteration 70, Training loss = 0.39015930790256187
Iteration 80, Training loss = 0.3678534045311564
Iteration 90, Training loss = 0.3558925061266203
Iteration 100, Training loss = 0.34789818762869074
Iteration 110, Training loss = 0.3417885606559578
Iteration 120, Training loss = 0.3367792057674288
Iteration 130, Training loss = 0.33261559176560185
Iteration 140, Training loss = 0.3295105810902545
Iteration 150, Training loss = 0.32839088645822184
Iteration 160, Training loss = 0.3253234214540841
Iteration 170, Training loss = 0.32361637660558673
Iteration 180, Training loss = 0.3227662976813201
Iteration 190, Training loss = 0.32062891521603587
Iteration 200, Training loss = 0.31905217627108384
Iteration 210, Training loss = 0.31840000103637217
Iteration 220, Training loss = 0.3172691439948796
Iteration 230, Training loss = 0.31580205484864793
Iteration 240, Training loss = 0.3150632925655531
Iteration 250, Training loss = 0.3146738438070684
Iteration 260, Training loss = 0.31436556210552435
Iteration 270, Training loss = 0.31239007427784554
Iteration 280, Training loss = 0.31211347597232764
Iteration 290, Training loss = 0.31090921041182273
Model training time: 76.82061791419983
Device: cuda
Iteration 0, Training loss = 1.0368261687133624
Iteration 10, Training loss = 0.990852195283641
Iteration 20, Training loss = 0.9786519367913693
Iteration 30, Training loss = 0.9477725259347815
Iteration 40, Training loss = 0.8658638091191001
Iteration 50, Training loss = 0.6616586726068875
Iteration 60, Training loss = 0.44727983538079374
Iteration 70, Training loss = 0.38848630860807815
Iteration 80, Training loss = 0.3698877872908173
Iteration 90, Training loss = 0.35878823999909387
Iteration 100, Training loss = 0.35004516622582493
Iteration 110, Training loss = 0.34308363137325804
Iteration 120, Training loss = 0.3382664915300222
Iteration 130, Training loss = 0.33437616295284694
Iteration 140, Training loss = 0.330360555274475
Iteration 150, Training loss = 0.32763176803715566
Iteration 160, Training loss = 0.3253062494135133
Iteration 170, Training loss = 0.32329917508334927
Iteration 180, Training loss = 0.32170098170566097
Iteration 190, Training loss = 0.3203118202335017
Iteration 200, Training loss = 0.319074090336256
Iteration 210, Training loss = 0.3178507640309956
Iteration 220, Training loss = 0.31815728645969704
Iteration 230, Training loss = 0.316390999129429
Iteration 240, Training loss = 0.3150966429480032
Iteration 250, Training loss = 0.31495169596971523
Iteration 260, Training loss = 0.31342674482271865
Iteration 270, Training loss = 0.3137940064000623
Iteration 280, Training loss = 0.3114681531121765
Iteration 290, Training loss = 0.311270444361484
Model training time: 76.52263855934143
Device: cuda
Iteration 0, Training loss = 1.0219568182592806
Iteration 10, Training loss = 0.9805599079615828
Iteration 20, Training loss = 0.9472702229080568
Iteration 30, Training loss = 0.8570730401122052
Iteration 40, Training loss = 0.6413332295878498
Iteration 50, Training loss = 0.42871707433087813
Iteration 60, Training loss = 0.37233017672951096
Iteration 70, Training loss = 0.35729369002839795
Iteration 80, Training loss = 0.348081999282906
Iteration 90, Training loss = 0.34005305027040306
Iteration 100, Training loss = 0.33412898666616797
Iteration 110, Training loss = 0.33081142949885217
Iteration 120, Training loss = 0.3269562524729881
Iteration 130, Training loss = 0.32339296194810224
Iteration 140, Training loss = 0.32178437436260465
Iteration 150, Training loss = 0.32150794515287245
Iteration 160, Training loss = 0.3178477378715064
Iteration 170, Training loss = 0.3163658091843416
Iteration 180, Training loss = 0.3153656860768507
Iteration 190, Training loss = 0.31448612466526493
Iteration 200, Training loss = 0.312742849478975
Iteration 210, Training loss = 0.3120293074471939
Iteration 220, Training loss = 0.31089335147309416
Iteration 230, Training loss = 0.309972351371954
Iteration 240, Training loss = 0.30981713461415206
Iteration 250, Training loss = 0.3078090772032738
Iteration 260, Training loss = 0.30751219132672186
Iteration 270, Training loss = 0.30642067608625995
Iteration 280, Training loss = 0.3066000983406956
Iteration 290, Training loss = 0.3053110836496676
Model training time: 76.29022645950317
Device: cuda
Iteration 0, Training loss = 1.0902256939722144
Iteration 10, Training loss = 0.9993595544266816
Iteration 20, Training loss = 0.9941780512459613
Iteration 30, Training loss = 0.9907205435964797
Iteration 40, Training loss = 0.9795646100228536
Iteration 50, Training loss = 0.95757420051501
Iteration 60, Training loss = 0.8959847772179018
Iteration 70, Training loss = 0.736301219837677
Iteration 80, Training loss = 0.49659347562974204
Iteration 90, Training loss = 0.3931081933531784
Iteration 100, Training loss = 0.37173972476795675
Iteration 110, Training loss = 0.35990473642441384
Iteration 120, Training loss = 0.35043087479284996
Iteration 130, Training loss = 0.344667612714468
Iteration 140, Training loss = 0.33872130160458425
Iteration 150, Training loss = 0.3353556344906489
Iteration 160, Training loss = 0.33110062756400177
Iteration 170, Training loss = 0.32759527905263763
Iteration 180, Training loss = 0.32610761068293437
Iteration 190, Training loss = 0.32360946102706706
Iteration 200, Training loss = 0.322401671881837
Iteration 210, Training loss = 0.32165361760895034
Iteration 220, Training loss = 0.3193747143526584
Iteration 230, Training loss = 0.3177095595477284
Iteration 240, Training loss = 0.3170082443866177
Iteration 250, Training loss = 0.3152612142516795
Iteration 260, Training loss = 0.31470292713043196
Iteration 270, Training loss = 0.3143097681987689
Iteration 280, Training loss = 0.31351065182167553
Iteration 290, Training loss = 0.31208137383207607
Model training time: 79.11592698097229
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8818947409374126
Iteration 10, Training loss = 0.29344115294696055
Iteration 20, Training loss = 0.2735121802647333
Iteration 30, Training loss = 0.2629632186414539
Iteration 40, Training loss = 0.25789309462199467
Iteration 50, Training loss = 0.25490872817482924
Iteration 60, Training loss = 0.25066570660486315
Iteration 70, Training loss = 0.24782085465492257
Iteration 80, Training loss = 0.24502216239914226
Iteration 90, Training loss = 0.243069674477773
Iteration 100, Training loss = 0.24137860360208918
Iteration 110, Training loss = 0.23895919927652332
Iteration 120, Training loss = 0.2375013596481747
Iteration 130, Training loss = 0.23589073395095586
Iteration 140, Training loss = 0.23349482296169669
Iteration 150, Training loss = 0.23174286047041703
Iteration 160, Training loss = 0.22965374596164997
Iteration 170, Training loss = 0.22836115484796285
Iteration 180, Training loss = 0.2268257300922836
Iteration 190, Training loss = 0.22569080892104457
Iteration 200, Training loss = 0.22383079559042834
Iteration 210, Training loss = 0.22312422251499794
Iteration 220, Training loss = 0.22250774988661642
Iteration 230, Training loss = 0.22161763262201623
Iteration 240, Training loss = 0.22042557467585025
Iteration 250, Training loss = 0.21970060025004373
Iteration 260, Training loss = 0.21826661853254706
Iteration 270, Training loss = 0.21724300079299633
Iteration 280, Training loss = 0.21699648105723846
Iteration 290, Training loss = 0.21618275931060027
Model training time: 89.69709610939026
Device: cuda
Iteration 0, Training loss = 0.9649464430440451
Iteration 10, Training loss = 0.28983735995016235
Iteration 20, Training loss = 0.26948933576904055
Iteration 30, Training loss = 0.2607905908987142
Iteration 40, Training loss = 0.25408356910310503
Iteration 50, Training loss = 0.25044775045148415
Iteration 60, Training loss = 0.24721917931152426
Iteration 70, Training loss = 0.24352334493744202
Iteration 80, Training loss = 0.24233471184681002
Iteration 90, Training loss = 0.2406073517125586
Iteration 100, Training loss = 0.239096546050719
Iteration 110, Training loss = 0.23777374938346338
Iteration 120, Training loss = 0.2369570539193453
Iteration 130, Training loss = 0.23511134041248313
Iteration 140, Training loss = 0.23436070665501166
Iteration 150, Training loss = 0.23160320441216087
Iteration 160, Training loss = 0.23140549796502946
Iteration 170, Training loss = 0.22992773141693953
Iteration 180, Training loss = 0.22863677694745685
Iteration 190, Training loss = 0.22793579861017818
Iteration 200, Training loss = 0.22643617407423286
Iteration 210, Training loss = 0.22521927600033617
Iteration 220, Training loss = 0.22421271911853755
Iteration 230, Training loss = 0.22267026227453482
Iteration 240, Training loss = 0.22234874981756947
Iteration 250, Training loss = 0.22038370226892295
Iteration 260, Training loss = 0.22023425373622185
Iteration 270, Training loss = 0.21887323573447656
Iteration 280, Training loss = 0.21727566205073093
Iteration 290, Training loss = 0.2160920618812819
Model training time: 90.44882535934448
Device: cuda
Iteration 0, Training loss = 0.8708902211580876
Iteration 10, Training loss = 0.2890414531176217
Iteration 20, Training loss = 0.2710568546385005
Iteration 30, Training loss = 0.2619290132669435
Iteration 40, Training loss = 0.25742960346925664
Iteration 50, Training loss = 0.2529378816989309
Iteration 60, Training loss = 0.2488232876583574
Iteration 70, Training loss = 0.24527665807141197
Iteration 80, Training loss = 0.2421046425970856
Iteration 90, Training loss = 0.23948100396400487
Iteration 100, Training loss = 0.2370829008123725
Iteration 110, Training loss = 0.23534044025888765
Iteration 120, Training loss = 0.23350783819017779
Iteration 130, Training loss = 0.23119176006403522
Iteration 140, Training loss = 0.22985037198461197
Iteration 150, Training loss = 0.2278190373942472
Iteration 160, Training loss = 0.22558723098557928
Iteration 170, Training loss = 0.22472260978774747
Iteration 180, Training loss = 0.22280414438478036
Iteration 190, Training loss = 0.2210567313907803
Iteration 200, Training loss = 0.22003359840687922
Iteration 210, Training loss = 0.2191130058201039
Iteration 220, Training loss = 0.21762833780280633
Iteration 230, Training loss = 0.21719555545976196
Iteration 240, Training loss = 0.21577823417630174
Iteration 250, Training loss = 0.21576983254888785
Iteration 260, Training loss = 0.21457761623289273
Iteration 270, Training loss = 0.2131065776310681
Iteration 280, Training loss = 0.21387533044469528
Iteration 290, Training loss = 0.21200867180807004
Model training time: 89.45450615882874
Device: cuda
Iteration 0, Training loss = 1.090684407575119
Iteration 10, Training loss = 0.2871569182705764
Iteration 20, Training loss = 0.2703625003352833
Iteration 30, Training loss = 0.2619475140663737
Iteration 40, Training loss = 0.25620694656446935
Iteration 50, Training loss = 0.2525697142101716
Iteration 60, Training loss = 0.2511324963491896
Iteration 70, Training loss = 0.24869387042551225
Iteration 80, Training loss = 0.24785007126089456
Iteration 90, Training loss = 0.2455789819719711
Iteration 100, Training loss = 0.24380677888070904
Iteration 110, Training loss = 0.24271637320086575
Iteration 120, Training loss = 0.24081259271228947
Iteration 130, Training loss = 0.23979607778758819
Iteration 140, Training loss = 0.23848041744479812
Iteration 150, Training loss = 0.23542835322267192
Iteration 160, Training loss = 0.2329830001445784
Iteration 170, Training loss = 0.2318508155679933
Iteration 180, Training loss = 0.22915473306812525
Iteration 190, Training loss = 0.22733071565196133
Iteration 200, Training loss = 0.22602510794205366
Iteration 210, Training loss = 0.22450309941446148
Iteration 220, Training loss = 0.2225927523008867
Iteration 230, Training loss = 0.221414710768467
Iteration 240, Training loss = 0.2207706856770792
Iteration 250, Training loss = 0.21945228974744319
Iteration 260, Training loss = 0.21951428358105646
Iteration 270, Training loss = 0.21784014064044768
Iteration 280, Training loss = 0.2168025097625267
Iteration 290, Training loss = 0.2168029068028869
Model training time: 88.90538001060486
Device: cuda
Iteration 0, Training loss = 0.9542986907532826
Iteration 10, Training loss = 0.28799345686239897
Iteration 20, Training loss = 0.2713724843041908
Iteration 30, Training loss = 0.26075436466414
Iteration 40, Training loss = 0.25515883505920284
Iteration 50, Training loss = 0.25116288150854155
Iteration 60, Training loss = 0.2475437262064017
Iteration 70, Training loss = 0.24438352015427345
Iteration 80, Training loss = 0.24182552718309966
Iteration 90, Training loss = 0.23972517788266215
Iteration 100, Training loss = 0.23617007547386604
Iteration 110, Training loss = 0.23469306956886668
Iteration 120, Training loss = 0.2317325230597874
Iteration 130, Training loss = 0.23018930186540032
Iteration 140, Training loss = 0.2288800680263031
Iteration 150, Training loss = 0.22686390870291254
Iteration 160, Training loss = 0.2252931547337684
Iteration 170, Training loss = 0.22370502032375567
Iteration 180, Training loss = 0.22258053475244033
Iteration 190, Training loss = 0.22152582206875807
Iteration 200, Training loss = 0.2213000515880792
Iteration 210, Training loss = 0.218086588497899
Iteration 220, Training loss = 0.21825604284731087
Iteration 230, Training loss = 0.21577567488387012
Iteration 240, Training loss = 0.21558503605029433
Iteration 250, Training loss = 0.2137013622312154
Iteration 260, Training loss = 0.21352981570838153
Iteration 270, Training loss = 0.2117552801894681
Iteration 280, Training loss = 0.21153115938251146
Iteration 290, Training loss = 0.21110469667951842
Model training time: 89.15434622764587
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.01016976983075
Iteration 10, Training loss = 0.9848106431500347
Iteration 20, Training loss = 0.9686146562802043
Iteration 30, Training loss = 0.9259643338728643
Iteration 40, Training loss = 0.8173658518111648
Iteration 50, Training loss = 0.6090383621805532
Iteration 60, Training loss = 0.44097373105477594
Iteration 70, Training loss = 0.3921412075487312
Iteration 80, Training loss = 0.37493672815785895
Iteration 90, Training loss = 0.36167470297375737
Iteration 100, Training loss = 0.351361477288647
Iteration 110, Training loss = 0.3436936939828062
Iteration 120, Training loss = 0.3378892674826194
Iteration 130, Training loss = 0.3345328028075361
Iteration 140, Training loss = 0.3290525126284447
Iteration 150, Training loss = 0.3259194971282701
Iteration 160, Training loss = 0.323462399955533
Iteration 170, Training loss = 0.3212994011703897
Iteration 180, Training loss = 0.31997339708217676
Iteration 190, Training loss = 0.3180455553214907
Iteration 200, Training loss = 0.31610295680410044
Iteration 210, Training loss = 0.3147210144046424
Iteration 220, Training loss = 0.3141216167073319
Iteration 230, Training loss = 0.31299671386751
Iteration 240, Training loss = 0.31217953037236623
Iteration 250, Training loss = 0.3115615484795133
Iteration 260, Training loss = 0.3099805254867111
Iteration 270, Training loss = 0.30926289837717436
Iteration 280, Training loss = 0.30854541056110085
Iteration 290, Training loss = 0.3075895428225614
Model training time: 76.1183500289917
Device: cuda
Iteration 0, Training loss = 1.0833019499041607
Iteration 10, Training loss = 0.9995257238259062
Iteration 20, Training loss = 0.992955305726056
Iteration 30, Training loss = 0.9855279165189623
Iteration 40, Training loss = 0.9697272308206789
Iteration 50, Training loss = 0.9375649356611685
Iteration 60, Training loss = 0.853968448109097
Iteration 70, Training loss = 0.673446927525571
Iteration 80, Training loss = 0.47290705511535425
Iteration 90, Training loss = 0.3972624120648932
Iteration 100, Training loss = 0.3737746138071668
Iteration 110, Training loss = 0.3616060248895544
Iteration 120, Training loss = 0.3511185235735299
Iteration 130, Training loss = 0.3435485718618844
Iteration 140, Training loss = 0.33771311833662687
Iteration 150, Training loss = 0.3333034480827442
Iteration 160, Training loss = 0.33014044755898814
Iteration 170, Training loss = 0.3276161858856966
Iteration 180, Training loss = 0.32459596620089765
Iteration 190, Training loss = 0.32273362504975234
Iteration 200, Training loss = 0.3212641989407332
Iteration 210, Training loss = 0.3187954597139128
Iteration 220, Training loss = 0.3169670025795554
Iteration 230, Training loss = 0.31729665988885264
Iteration 240, Training loss = 0.3141194082400649
Iteration 250, Training loss = 0.31357799340849335
Iteration 260, Training loss = 0.3116553806596332
Iteration 270, Training loss = 0.31075509432433307
Iteration 280, Training loss = 0.30991601332086294
Iteration 290, Training loss = 0.3084493897459357
Model training time: 76.11751317977905
Device: cuda
Iteration 0, Training loss = 1.0068977357108813
Iteration 10, Training loss = 0.9968284158315059
Iteration 20, Training loss = 0.9910754958788554
Iteration 30, Training loss = 0.9822480420847446
Iteration 40, Training loss = 0.9612505672058621
Iteration 50, Training loss = 0.9087669043149349
Iteration 60, Training loss = 0.7707769078908911
Iteration 70, Training loss = 0.5401595730136558
Iteration 80, Training loss = 0.4119987911792193
Iteration 90, Training loss = 0.3812618691852127
Iteration 100, Training loss = 0.36709088418218827
Iteration 110, Training loss = 0.3542503902445669
Iteration 120, Training loss = 0.3445038588150688
Iteration 130, Training loss = 0.3373989051810785
Iteration 140, Training loss = 0.3315567861576587
Iteration 150, Training loss = 0.32705053629506614
Iteration 160, Training loss = 0.3239142545611386
Iteration 170, Training loss = 0.32018763582775556
Iteration 180, Training loss = 0.31839067402525223
Iteration 190, Training loss = 0.3173890053362086
Iteration 200, Training loss = 0.3139192421655148
Iteration 210, Training loss = 0.3131636348755463
Iteration 220, Training loss = 0.3112478527613884
Iteration 230, Training loss = 0.31200308658650533
Iteration 240, Training loss = 0.3092941697956859
Iteration 250, Training loss = 0.30782780076426586
Iteration 260, Training loss = 0.3073122235024033
Iteration 270, Training loss = 0.3072453180085058
Iteration 280, Training loss = 0.30566832383617687
Iteration 290, Training loss = 0.30465043238971545
Model training time: 77.18707370758057
Device: cuda
Iteration 0, Training loss = 1.0088594290945265
Iteration 10, Training loss = 0.9970294902290123
Iteration 20, Training loss = 0.985062953642601
Iteration 30, Training loss = 0.9704408245386132
Iteration 40, Training loss = 0.9267883510981205
Iteration 50, Training loss = 0.8093008662479512
Iteration 60, Training loss = 0.5668944264379676
Iteration 70, Training loss = 0.40036364209248826
Iteration 80, Training loss = 0.3654102119414703
Iteration 90, Training loss = 0.3531170788306545
Iteration 100, Training loss = 0.3435900280441063
Iteration 110, Training loss = 0.33684248725573224
Iteration 120, Training loss = 0.3317405286330532
Iteration 130, Training loss = 0.32776137963297286
Iteration 140, Training loss = 0.3241923830071509
Iteration 150, Training loss = 0.3219244011502335
Iteration 160, Training loss = 0.31968801267049163
Iteration 170, Training loss = 0.3182705658069555
Iteration 180, Training loss = 0.31645436628141266
Iteration 190, Training loss = 0.3142887362535449
Iteration 200, Training loss = 0.313188305079649
Iteration 210, Training loss = 0.31171790564405744
Iteration 220, Training loss = 0.3104249283599393
Iteration 230, Training loss = 0.30953488192984446
Iteration 240, Training loss = 0.3104934884586196
Iteration 250, Training loss = 0.3077810495228007
Iteration 260, Training loss = 0.30697558791453133
Iteration 270, Training loss = 0.3063459473387631
Iteration 280, Training loss = 0.3049016516421728
Iteration 290, Training loss = 0.3040973641302275
Model training time: 84.07616090774536
Device: cuda
Iteration 0, Training loss = 1.0145217603531436
Iteration 10, Training loss = 0.9964781527357977
Iteration 20, Training loss = 0.9857926541480465
Iteration 30, Training loss = 0.9673543342069727
Iteration 40, Training loss = 0.9149447927152478
Iteration 50, Training loss = 0.7806914825946236
Iteration 60, Training loss = 0.5430949504536707
Iteration 70, Training loss = 0.40477189745592035
Iteration 80, Training loss = 0.369600599658662
Iteration 90, Training loss = 0.35579607776109723
Iteration 100, Training loss = 0.3437878713371673
Iteration 110, Training loss = 0.3361641478855253
Iteration 120, Training loss = 0.330433275964525
Iteration 130, Training loss = 0.32603169430569173
Iteration 140, Training loss = 0.3230991822078032
Iteration 150, Training loss = 0.3199658671007064
Iteration 160, Training loss = 0.31764376055503235
Iteration 170, Training loss = 0.3160188665640527
Iteration 180, Training loss = 0.31433611424360874
Iteration 190, Training loss = 0.3132058745924977
Iteration 200, Training loss = 0.31131569778429713
Iteration 210, Training loss = 0.3104808208879065
Iteration 220, Training loss = 0.3091443453865927
Iteration 230, Training loss = 0.3089960927260671
Iteration 240, Training loss = 0.30704095678916876
Iteration 250, Training loss = 0.30596199394136236
Iteration 260, Training loss = 0.3059195930471167
Iteration 270, Training loss = 0.30463393774009556
Iteration 280, Training loss = 0.30401755948573495
Iteration 290, Training loss = 0.30348482634422286
Model training time: 83.7531406879425
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9580889629281085
Iteration 10, Training loss = 0.2955923202821022
Iteration 20, Training loss = 0.27804093466000857
Iteration 30, Training loss = 0.26581940810749494
Iteration 40, Training loss = 0.2569914181595263
Iteration 50, Training loss = 0.25335398791492847
Iteration 60, Training loss = 0.2495557994802217
Iteration 70, Training loss = 0.24631000590929086
Iteration 80, Training loss = 0.24359741860973663
Iteration 90, Training loss = 0.24220916938378614
Iteration 100, Training loss = 0.24015459627057043
Iteration 110, Training loss = 0.237824196724788
Iteration 120, Training loss = 0.23623959385398505
Iteration 130, Training loss = 0.23505506578131
Iteration 140, Training loss = 0.2332811344432946
Iteration 150, Training loss = 0.23171779686126157
Iteration 160, Training loss = 0.2307971001463236
Iteration 170, Training loss = 0.2300430688161205
Iteration 180, Training loss = 0.229112902449237
Iteration 190, Training loss = 0.22875687677935125
Iteration 200, Training loss = 0.22775991930477862
Iteration 210, Training loss = 0.22647051981105898
Iteration 220, Training loss = 0.22647372892369394
Iteration 230, Training loss = 0.22509443558356612
Iteration 240, Training loss = 0.22453143192086242
Iteration 250, Training loss = 0.2233436029530378
Iteration 260, Training loss = 0.22249566324523107
Iteration 270, Training loss = 0.22202865322286955
Iteration 280, Training loss = 0.22250706446919463
Iteration 290, Training loss = 0.2223404454436279
Model training time: 89.5259141921997
Device: cuda
Iteration 0, Training loss = 0.8546658499517302
Iteration 10, Training loss = 0.28900590330218345
Iteration 20, Training loss = 0.27103167232395947
Iteration 30, Training loss = 0.26283893629836574
Iteration 40, Training loss = 0.25732993284141387
Iteration 50, Training loss = 0.25354223482418753
Iteration 60, Training loss = 0.25082023754931876
Iteration 70, Training loss = 0.24785477045365578
Iteration 80, Training loss = 0.24537649574343134
Iteration 90, Training loss = 0.24292361559931208
Iteration 100, Training loss = 0.2408429625025694
Iteration 110, Training loss = 0.23847135026817737
Iteration 120, Training loss = 0.23735252778599228
Iteration 130, Training loss = 0.2356702842646175
Iteration 140, Training loss = 0.2341085565838837
Iteration 150, Training loss = 0.23248075153948605
Iteration 160, Training loss = 0.23151123620893643
Iteration 170, Training loss = 0.23020186502000559
Iteration 180, Training loss = 0.22981127315529304
Iteration 190, Training loss = 0.2275274529117317
Iteration 200, Training loss = 0.22627374529838562
Iteration 210, Training loss = 0.22556930100140365
Iteration 220, Training loss = 0.22440435316251672
Iteration 230, Training loss = 0.22334343536896406
Iteration 240, Training loss = 0.22108999542568042
Iteration 250, Training loss = 0.22032464202043514
Iteration 260, Training loss = 0.21909697620189134
Iteration 270, Training loss = 0.217529248284257
Iteration 280, Training loss = 0.21727603555157565
Iteration 290, Training loss = 0.21535058614712407
Model training time: 88.15382957458496
Device: cuda
Iteration 0, Training loss = 0.8994140774731475
Iteration 10, Training loss = 0.2856028447116631
Iteration 20, Training loss = 0.26703353946047703
Iteration 30, Training loss = 0.26014939544425497
Iteration 40, Training loss = 0.2564031058103566
Iteration 50, Training loss = 0.25277075902131446
Iteration 60, Training loss = 0.25064305036108275
Iteration 70, Training loss = 0.2483772353273659
Iteration 80, Training loss = 0.24626784904855462
Iteration 90, Training loss = 0.24533429835441609
Iteration 100, Training loss = 0.2414010357165682
Iteration 110, Training loss = 0.23845267914919463
Iteration 120, Training loss = 0.2352670389385039
Iteration 130, Training loss = 0.23327624686242301
Iteration 140, Training loss = 0.231812197085164
Iteration 150, Training loss = 0.229880479870787
Iteration 160, Training loss = 0.22791504035681343
Iteration 170, Training loss = 0.22669086299368724
Iteration 180, Training loss = 0.22453889941823654
Iteration 190, Training loss = 0.2239862663158472
Iteration 200, Training loss = 0.2229513873656591
Iteration 210, Training loss = 0.22093693584491667
Iteration 220, Training loss = 0.22012261661210497
Iteration 230, Training loss = 0.2179808932657979
Iteration 240, Training loss = 0.2173383345350551
Iteration 250, Training loss = 0.21524814138377923
Iteration 260, Training loss = 0.21409409262851817
Iteration 270, Training loss = 0.21309095451941237
Iteration 280, Training loss = 0.21203833566483668
Iteration 290, Training loss = 0.21083048291972295
Model training time: 89.25766730308533
Device: cuda
Iteration 0, Training loss = 0.9581332465876704
Iteration 10, Training loss = 0.28865294230444993
Iteration 20, Training loss = 0.2700440881476886
Iteration 30, Training loss = 0.2620098427441961
Iteration 40, Training loss = 0.2563515879249803
Iteration 50, Training loss = 0.2523665014600408
Iteration 60, Training loss = 0.24896700406708003
Iteration 70, Training loss = 0.24505106028583315
Iteration 80, Training loss = 0.2422176133391362
Iteration 90, Training loss = 0.24011243627843074
Iteration 100, Training loss = 0.23816856432364183
Iteration 110, Training loss = 0.23706641364932637
Iteration 120, Training loss = 0.23430905192370577
Iteration 130, Training loss = 0.23289058497850446
Iteration 140, Training loss = 0.2315053558651952
Iteration 150, Training loss = 0.2309337579037832
Iteration 160, Training loss = 0.22949266588486336
Iteration 170, Training loss = 0.22835128570812335
Iteration 180, Training loss = 0.22635547467619901
Iteration 190, Training loss = 0.2255090039205436
Iteration 200, Training loss = 0.22449334829613782
Iteration 210, Training loss = 0.2231420421441972
Iteration 220, Training loss = 0.22174373906159747
Iteration 230, Training loss = 0.22006645546731166
Iteration 240, Training loss = 0.2195978783035048
Iteration 250, Training loss = 0.21898945564953026
Iteration 260, Training loss = 0.2174682748562472
Iteration 270, Training loss = 0.21610535342912168
Iteration 280, Training loss = 0.21493634095658426
Iteration 290, Training loss = 0.2143126321227654
Model training time: 90.19124722480774
Device: cuda
Iteration 0, Training loss = 0.8829713237746326
Iteration 10, Training loss = 0.29223113258679706
Iteration 20, Training loss = 0.27377862292499355
Iteration 30, Training loss = 0.26455636851597525
Iteration 40, Training loss = 0.2570003913220576
Iteration 50, Training loss = 0.2523981147774176
Iteration 60, Training loss = 0.24957835785864632
Iteration 70, Training loss = 0.24615361102393285
Iteration 80, Training loss = 0.24368410273162638
Iteration 90, Training loss = 0.24181325524901423
Iteration 100, Training loss = 0.2396206508440096
Iteration 110, Training loss = 0.2373079493785826
Iteration 120, Training loss = 0.23636620801284117
Iteration 130, Training loss = 0.23495190997342558
Iteration 140, Training loss = 0.23300438773804816
Iteration 150, Training loss = 0.23199338494723545
Iteration 160, Training loss = 0.23085251828466635
Iteration 170, Training loss = 0.2301107030271908
Iteration 180, Training loss = 0.22865654826884108
Iteration 190, Training loss = 0.22768606313904702
Iteration 200, Training loss = 0.22860186557838882
Iteration 210, Training loss = 0.22447668865395054
Iteration 220, Training loss = 0.22342850030332373
Iteration 230, Training loss = 0.22109466585992038
Iteration 240, Training loss = 0.22059129570849276
Iteration 250, Training loss = 0.21905649219446136
Iteration 260, Training loss = 0.21760631827772528
Iteration 270, Training loss = 0.21739990462139608
Iteration 280, Training loss = 0.21678608026049564
Iteration 290, Training loss = 0.21522644766862842
Model training time: 87.81230759620667
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9954958739488021
Iteration 10, Training loss = 0.9870144123040535
Iteration 20, Training loss = 0.971705667062658
Iteration 30, Training loss = 0.9292050834727172
Iteration 40, Training loss = 0.8196417608698785
Iteration 50, Training loss = 0.5975074212332279
Iteration 60, Training loss = 0.4337071176744314
Iteration 70, Training loss = 0.389951317206673
Iteration 80, Training loss = 0.37416174331149044
Iteration 90, Training loss = 0.3624545391199093
Iteration 100, Training loss = 0.35387588533514364
Iteration 110, Training loss = 0.3459788611977573
Iteration 120, Training loss = 0.34014132366952116
Iteration 130, Training loss = 0.3358595971611963
Iteration 140, Training loss = 0.3308923061631152
Iteration 150, Training loss = 0.32712143098098645
Iteration 160, Training loss = 0.32443951998931775
Iteration 170, Training loss = 0.32209569950034656
Iteration 180, Training loss = 0.32022230939012797
Iteration 190, Training loss = 0.31820336128202614
Iteration 200, Training loss = 0.3176261380962703
Iteration 210, Training loss = 0.31661519703369784
Iteration 220, Training loss = 0.31526691398182927
Iteration 230, Training loss = 0.31383648234001105
Iteration 240, Training loss = 0.31234534876646053
Iteration 250, Training loss = 0.31135122841539015
Iteration 260, Training loss = 0.31080314041911694
Iteration 270, Training loss = 0.31007438451771574
Iteration 280, Training loss = 0.309035070348477
Iteration 290, Training loss = 0.3084504317114319
Model training time: 76.63621211051941
Device: cuda
Iteration 0, Training loss = 0.9959817416426064
Iteration 10, Training loss = 0.9852517690635534
Iteration 20, Training loss = 0.9663503573136629
Iteration 30, Training loss = 0.9188300388446753
Iteration 40, Training loss = 0.7925760440780345
Iteration 50, Training loss = 0.5550946515539418
Iteration 60, Training loss = 0.4110742576312328
Iteration 70, Training loss = 0.38008610897018136
Iteration 80, Training loss = 0.3673330286994648
Iteration 90, Training loss = 0.35713424148478945
Iteration 100, Training loss = 0.35019743068206716
Iteration 110, Training loss = 0.3438381675792777
Iteration 120, Training loss = 0.3408592769489196
Iteration 130, Training loss = 0.33593748949000224
Iteration 140, Training loss = 0.3325828898356157
Iteration 150, Training loss = 0.33071689358080064
Iteration 160, Training loss = 0.32710356114135275
Iteration 170, Training loss = 0.326197043709133
Iteration 180, Training loss = 0.3240161158756357
Iteration 190, Training loss = 0.3222901204933867
Iteration 200, Training loss = 0.32070005724683476
Iteration 210, Training loss = 0.3198492652984057
Iteration 220, Training loss = 0.318638662857134
Iteration 230, Training loss = 0.3177189382090085
Iteration 240, Training loss = 0.31603331100825527
Iteration 250, Training loss = 0.31518073631945437
Iteration 260, Training loss = 0.31470885703673107
Iteration 270, Training loss = 0.3140274413685868
Iteration 280, Training loss = 0.3126004744986981
Iteration 290, Training loss = 0.3118656870103689
Model training time: 77.65622520446777
Device: cuda
Iteration 0, Training loss = 1.0070649719468638
Iteration 10, Training loss = 0.9999110551847927
Iteration 20, Training loss = 0.9957909134850986
Iteration 30, Training loss = 0.9893409926534275
Iteration 40, Training loss = 0.9791305010445452
Iteration 50, Training loss = 0.952277522156204
Iteration 60, Training loss = 0.8812431932071557
Iteration 70, Training loss = 0.7074364626753158
Iteration 80, Training loss = 0.4790867109229599
Iteration 90, Training loss = 0.39673256946070756
Iteration 100, Training loss = 0.373769055026165
Iteration 110, Training loss = 0.36052770574311704
Iteration 120, Training loss = 0.3502778057458896
Iteration 130, Training loss = 0.34349184170149377
Iteration 140, Training loss = 0.3377269682820868
Iteration 150, Training loss = 0.3328895156366238
Iteration 160, Training loss = 0.329902497322663
Iteration 170, Training loss = 0.3272257487555057
Iteration 180, Training loss = 0.3252225105025342
Iteration 190, Training loss = 0.32267203048807414
Iteration 200, Training loss = 0.32040741355810765
Iteration 210, Training loss = 0.3186284735582877
Iteration 220, Training loss = 0.31767993531941213
Iteration 230, Training loss = 0.31613438284915424
Iteration 240, Training loss = 0.31596865954894376
Iteration 250, Training loss = 0.31460441418603996
Iteration 260, Training loss = 0.31267751040666
Iteration 270, Training loss = 0.31108527159057375
Iteration 280, Training loss = 0.31055251750105245
Iteration 290, Training loss = 0.3096322519479742
Model training time: 76.92774415016174
Device: cuda
Iteration 0, Training loss = 0.9981958742302973
Iteration 10, Training loss = 0.9848256206166917
Iteration 20, Training loss = 0.9592157870099165
Iteration 30, Training loss = 0.8957225186525336
Iteration 40, Training loss = 0.7348254426665928
Iteration 50, Training loss = 0.5021535367637441
Iteration 60, Training loss = 0.39984716964516664
Iteration 70, Training loss = 0.37278280432385524
Iteration 80, Training loss = 0.3589560967568614
Iteration 90, Training loss = 0.34924274675800027
Iteration 100, Training loss = 0.3395728020420397
Iteration 110, Training loss = 0.334070709016588
Iteration 120, Training loss = 0.32898538865616933
Iteration 130, Training loss = 0.32590770779024575
Iteration 140, Training loss = 0.3220367391328305
Iteration 150, Training loss = 0.3195661022755259
Iteration 160, Training loss = 0.318912615954588
Iteration 170, Training loss = 0.31599511054978857
Iteration 180, Training loss = 0.3142892947951377
Iteration 190, Training loss = 0.31363272047849095
Iteration 200, Training loss = 0.3121035025603529
Iteration 210, Training loss = 0.312532600933227
Iteration 220, Training loss = 0.3104147106984963
Iteration 230, Training loss = 0.3089939853275456
Iteration 240, Training loss = 0.3088783657781168
Iteration 250, Training loss = 0.3079579193954882
Iteration 260, Training loss = 0.30644553496641813
Iteration 270, Training loss = 0.30618638167346734
Iteration 280, Training loss = 0.30510247304387716
Iteration 290, Training loss = 0.30453245680113344
Model training time: 77.53187680244446
Device: cuda
Iteration 0, Training loss = 1.009829520603309
Iteration 10, Training loss = 0.9865041542168401
Iteration 20, Training loss = 0.9630098357292765
Iteration 30, Training loss = 0.9106543162018781
Iteration 40, Training loss = 0.764769989223296
Iteration 50, Training loss = 0.5158298719620359
Iteration 60, Training loss = 0.390873184742559
Iteration 70, Training loss = 0.3675921264190029
Iteration 80, Training loss = 0.3565206023805959
Iteration 90, Training loss = 0.34883231795640385
Iteration 100, Training loss = 0.3422735770399444
Iteration 110, Training loss = 0.33684013056870243
Iteration 120, Training loss = 0.33306207900174
Iteration 130, Training loss = 0.32965499537002635
Iteration 140, Training loss = 0.32601527272215214
Iteration 150, Training loss = 0.3236473178230046
Iteration 160, Training loss = 0.32138261918860356
Iteration 170, Training loss = 0.31949713567028876
Iteration 180, Training loss = 0.3172277937764707
Iteration 190, Training loss = 0.3160857905224326
Iteration 200, Training loss = 0.3152633204264341
Iteration 210, Training loss = 0.31340203423430957
Iteration 220, Training loss = 0.3118940124621138
Iteration 230, Training loss = 0.3113349763955471
Iteration 240, Training loss = 0.3104296176905793
Iteration 250, Training loss = 0.3089648629156288
Iteration 260, Training loss = 0.30791566207788995
Iteration 270, Training loss = 0.3068708292959969
Iteration 280, Training loss = 0.3066794988901719
Iteration 290, Training loss = 0.3064180001040588
Model training time: 76.98749876022339
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9941791508060235
Iteration 10, Training loss = 0.3234992422736608
Iteration 20, Training loss = 0.29675982233423454
Iteration 30, Training loss = 0.28799121697934776
Iteration 40, Training loss = 0.27982131162515056
Iteration 50, Training loss = 0.27376270494782007
Iteration 60, Training loss = 0.2681562624012048
Iteration 70, Training loss = 0.2646798881200644
Iteration 80, Training loss = 0.25865685696212143
Iteration 90, Training loss = 0.2557621379024707
Model training time: 18.79396080970764
Device: cuda
Iteration 0, Training loss = 0.9779055359271857
Iteration 10, Training loss = 0.31500736781610894
Iteration 20, Training loss = 0.2973107678385881
Iteration 30, Training loss = 0.28805616383369154
Iteration 40, Training loss = 0.2798246073608215
Iteration 50, Training loss = 0.27163376917059606
Iteration 60, Training loss = 0.2672328163798039
Iteration 70, Training loss = 0.26717841954758537
Iteration 80, Training loss = 0.2624728920368048
Iteration 90, Training loss = 0.25905178845501864
Model training time: 19.042505502700806
Device: cuda
Iteration 0, Training loss = 0.9965831975524242
Iteration 10, Training loss = 0.31999333016574383
Iteration 20, Training loss = 0.29363133314137274
Iteration 30, Training loss = 0.2858262626597515
Iteration 40, Training loss = 0.27907097067397374
Iteration 50, Training loss = 0.27315210020886016
Iteration 60, Training loss = 0.27010533285255617
Iteration 70, Training loss = 0.26478542000628436
Iteration 80, Training loss = 0.2631290235485022
Iteration 90, Training loss = 0.26068941245858485
Model training time: 18.966773509979248
Device: cuda
Iteration 0, Training loss = 0.9731529349317918
Iteration 10, Training loss = 0.30924760779509175
Iteration 20, Training loss = 0.2936221702167621
Iteration 30, Training loss = 0.2860769716879496
Iteration 40, Training loss = 0.27928278108055776
Iteration 50, Training loss = 0.2749101730206838
Iteration 60, Training loss = 0.2706877061953911
Iteration 70, Training loss = 0.26351815619720864
Iteration 80, Training loss = 0.25969776045531034
Iteration 90, Training loss = 0.2582802538974927
Model training time: 19.0100359916687
Device: cuda
Iteration 0, Training loss = 0.9836096717761114
Iteration 10, Training loss = 0.31043915192668253
Iteration 20, Training loss = 0.2938697203420676
Iteration 30, Training loss = 0.28581868656552756
Iteration 40, Training loss = 0.2780534650843877
Iteration 50, Training loss = 0.2725230847986845
Iteration 60, Training loss = 0.2644574818416284
Iteration 70, Training loss = 0.2596778356685088
Iteration 80, Training loss = 0.25846380109970385
Iteration 90, Training loss = 0.25522190511513215
Model training time: 19.098076581954956
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0072407229588582
Iteration 10, Training loss = 0.9979968432050484
Iteration 20, Training loss = 0.9950184850738599
Iteration 30, Training loss = 0.9948652168879142
Iteration 40, Training loss = 0.9914567631024581
Iteration 50, Training loss = 0.99148096373448
Iteration 60, Training loss = 0.9893713834194037
Iteration 70, Training loss = 0.9887609865802985
Iteration 80, Training loss = 0.982148714363575
Iteration 90, Training loss = 0.9777704356954648
Model training time: 16.79651713371277
Device: cuda
Iteration 0, Training loss = 1.2124662777552238
Iteration 10, Training loss = 0.993517249249495
Iteration 20, Training loss = 0.9929688240473087
Iteration 30, Training loss = 0.9935406498037852
Iteration 40, Training loss = 0.9920292754585927
Iteration 50, Training loss = 0.9916849468763058
Iteration 60, Training loss = 0.9849384329639949
Iteration 70, Training loss = 0.9810557250793164
Iteration 80, Training loss = 0.9750558292636504
Iteration 90, Training loss = 0.9679423238222415
Model training time: 16.687705755233765
Device: cuda
Iteration 0, Training loss = 1.3039511459378095
Iteration 10, Training loss = 0.9963261399131554
Iteration 20, Training loss = 0.9926876276731491
Iteration 30, Training loss = 0.99190776852461
Iteration 40, Training loss = 0.9919369048797168
Iteration 50, Training loss = 0.9846127291138356
Iteration 60, Training loss = 0.9794660760806158
Iteration 70, Training loss = 0.9728029639675067
Iteration 80, Training loss = 0.9654991116661292
Iteration 90, Training loss = 0.9547905245652566
Model training time: 16.93907928466797
Device: cuda
Iteration 0, Training loss = 1.1046283038762899
Iteration 10, Training loss = 1.0019264312890859
Iteration 20, Training loss = 0.9994956409701934
Iteration 30, Training loss = 0.996694109760798
Iteration 40, Training loss = 0.9991045903701049
Iteration 50, Training loss = 0.9940722005871626
Iteration 60, Training loss = 0.9902596032390227
Iteration 70, Training loss = 0.9881484342309145
Iteration 80, Training loss = 0.984598868741439
Iteration 90, Training loss = 0.9770834531921607
Model training time: 16.672653198242188
Device: cuda
Iteration 0, Training loss = 1.299092978812181
Iteration 10, Training loss = 1.0048393687376609
Iteration 20, Training loss = 1.0056699990079954
Iteration 30, Training loss = 1.0060500078476393
Iteration 40, Training loss = 1.0041382662378824
Iteration 50, Training loss = 0.9984448540669221
Iteration 60, Training loss = 0.999527187874684
Iteration 70, Training loss = 0.9982868117781786
Iteration 80, Training loss = 0.9983369564780822
Iteration 90, Training loss = 0.9971945045086054
Model training time: 16.90928077697754
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1671313339701066
Iteration 10, Training loss = 0.36908716479173076
Iteration 20, Training loss = 0.310440390299146
Iteration 30, Training loss = 0.29372374956997543
Iteration 40, Training loss = 0.2833663299679756
Iteration 50, Training loss = 0.275328408067043
Iteration 60, Training loss = 0.27047199569642544
Iteration 70, Training loss = 0.26706937256340796
Iteration 80, Training loss = 0.26664991662479365
Iteration 90, Training loss = 0.2609686908813623
Model training time: 18.94963049888611
Device: cuda
Iteration 0, Training loss = 1.2429080737324862
Iteration 10, Training loss = 0.37762178366000837
Iteration 20, Training loss = 0.3133986291403954
Iteration 30, Training loss = 0.29371973342047286
Iteration 40, Training loss = 0.2834943104535341
Iteration 50, Training loss = 0.2775353924013101
Iteration 60, Training loss = 0.27343717575646365
Iteration 70, Training loss = 0.2670948011084245
Iteration 80, Training loss = 0.2633482761537799
Iteration 90, Training loss = 0.2612888029275032
Model training time: 18.741010189056396
Device: cuda
Iteration 0, Training loss = 1.0661377992767553
Iteration 10, Training loss = 0.3586656843813566
Iteration 20, Training loss = 0.3082147464156151
Iteration 30, Training loss = 0.2936295159161091
Iteration 40, Training loss = 0.2867808717374618
Iteration 50, Training loss = 0.2785152977762314
Iteration 60, Training loss = 0.27051818986924797
Iteration 70, Training loss = 0.2656056577196488
Iteration 80, Training loss = 0.2635520722430486
Iteration 90, Training loss = 0.26131413361200917
Model training time: 18.9209942817688
Device: cuda
Iteration 0, Training loss = 1.4273847135213704
Iteration 10, Training loss = 0.465808166620823
Iteration 20, Training loss = 0.33111742038566333
Iteration 30, Training loss = 0.2955221331749971
Iteration 40, Training loss = 0.2818674095548116
Iteration 50, Training loss = 0.276557271153881
Iteration 60, Training loss = 0.27014114593084043
Iteration 70, Training loss = 0.26500055852990884
Iteration 80, Training loss = 0.2624192414088891
Iteration 90, Training loss = 0.25983242618923
Model training time: 18.694879055023193
Device: cuda
Iteration 0, Training loss = 0.9956736163451121
Iteration 10, Training loss = 0.33230953462995017
Iteration 20, Training loss = 0.2995809786594831
Iteration 30, Training loss = 0.2851203956569617
Iteration 40, Training loss = 0.2775766310783533
Iteration 50, Training loss = 0.2731753308326006
Iteration 60, Training loss = 0.26983133660486114
Iteration 70, Training loss = 0.26650405359955937
Iteration 80, Training loss = 0.26346437002603823
Iteration 90, Training loss = 0.26157769474845666
Model training time: 19.357221841812134
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.01785053140842
Iteration 10, Training loss = 0.9958056641312746
Iteration 20, Training loss = 0.9933872887721429
Iteration 30, Training loss = 0.9916995007258195
Iteration 40, Training loss = 0.9973043266397256
Iteration 50, Training loss = 0.9901386401974238
Iteration 60, Training loss = 0.9927383036567614
Iteration 70, Training loss = 0.9840978630460225
Iteration 80, Training loss = 0.9797499483594527
Iteration 90, Training loss = 0.9770082819920319
Model training time: 16.91372585296631
Device: cuda
Iteration 0, Training loss = 1.0208623248797197
Iteration 10, Training loss = 1.0040137836566339
Iteration 20, Training loss = 1.0074327037884638
Iteration 30, Training loss = 1.003396254319411
Iteration 40, Training loss = 0.9980920467239159
Iteration 50, Training loss = 0.9976913980566539
Iteration 60, Training loss = 0.9964411608301676
Iteration 70, Training loss = 0.9923367849909343
Iteration 80, Training loss = 0.9944540078823383
Iteration 90, Training loss = 0.992738541502219
Model training time: 16.988991498947144
Device: cuda
Iteration 0, Training loss = 1.0812328933523252
Iteration 10, Training loss = 0.99348328434504
Iteration 20, Training loss = 0.9893469311870061
Iteration 30, Training loss = 0.9836838388672242
Iteration 40, Training loss = 0.9839444584571398
Iteration 50, Training loss = 0.974354122693722
Iteration 60, Training loss = 0.9630643335672525
Iteration 70, Training loss = 0.9572238532396463
Iteration 80, Training loss = 0.9461342354233448
Iteration 90, Training loss = 0.9250305458330191
Model training time: 16.52410387992859
Device: cuda
Iteration 0, Training loss = 1.0435890337595572
Iteration 10, Training loss = 0.9954074357564633
Iteration 20, Training loss = 0.9951371103525162
Iteration 30, Training loss = 0.9944028172355431
Iteration 40, Training loss = 0.9912888161264933
Iteration 50, Training loss = 0.9934872130935009
Iteration 60, Training loss = 0.9876125890475053
Iteration 70, Training loss = 0.9865398710736861
Iteration 80, Training loss = 0.9835498728431188
Iteration 90, Training loss = 0.983223404448766
Model training time: 16.875563144683838
Device: cuda
Iteration 0, Training loss = 1.7543868915392802
Iteration 10, Training loss = 1.0019571781158447
Iteration 20, Training loss = 1.000683327133839
Iteration 30, Training loss = 1.003192421908562
Iteration 40, Training loss = 1.0017297709217439
Iteration 50, Training loss = 0.9966052805001919
Iteration 60, Training loss = 0.9975585272678962
Iteration 70, Training loss = 0.9979169552142804
Iteration 80, Training loss = 0.9995124311401293
Iteration 90, Training loss = 0.9962318505232151
Model training time: 17.222330331802368
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1582014658130133
Iteration 10, Training loss = 0.3479439404148322
Iteration 20, Training loss = 0.30062228068709373
Iteration 30, Training loss = 0.29066901095211506
Iteration 40, Training loss = 0.28234441635700375
Iteration 50, Training loss = 0.27366932624807727
Iteration 60, Training loss = 0.2699016210838006
Iteration 70, Training loss = 0.2669323548101462
Iteration 80, Training loss = 0.2644165363162756
Iteration 90, Training loss = 0.26236554091939557
Model training time: 18.84869122505188
Device: cuda
Iteration 0, Training loss = 1.0023226875525255
Iteration 10, Training loss = 0.32739962365191716
Iteration 20, Training loss = 0.2983047434916863
Iteration 30, Training loss = 0.2872086665951289
Iteration 40, Training loss = 0.2810310325943507
Iteration 50, Training loss = 0.2732099025295331
Iteration 60, Training loss = 0.268814874956241
Iteration 70, Training loss = 0.26665630291860837
Iteration 80, Training loss = 0.26628694138847864
Iteration 90, Training loss = 0.2619724472841391
Model training time: 18.970479249954224
Device: cuda
Iteration 0, Training loss = 1.0516078191307874
Iteration 10, Training loss = 0.327265129209711
Iteration 20, Training loss = 0.29366787293782604
Iteration 30, Training loss = 0.28541928486755264
Iteration 40, Training loss = 0.2772130828637343
Iteration 50, Training loss = 0.27141973777459216
Iteration 60, Training loss = 0.2681257090029808
Iteration 70, Training loss = 0.26542237415336645
Iteration 80, Training loss = 0.2613693540199445
Iteration 90, Training loss = 0.2595702644724112
Model training time: 19.33886408805847
Device: cuda
Iteration 0, Training loss = 1.0339370117737696
Iteration 10, Training loss = 0.3228222279307934
Iteration 20, Training loss = 0.29749527401649034
Iteration 30, Training loss = 0.28527718137663144
Iteration 40, Training loss = 0.2761059966511451
Iteration 50, Training loss = 0.2723808427556203
Iteration 60, Training loss = 0.2670398990695293
Iteration 70, Training loss = 0.2631797776199304
Iteration 80, Training loss = 0.2602206518730292
Iteration 90, Training loss = 0.2584629458590196
Model training time: 19.243562698364258
Device: cuda
Iteration 0, Training loss = 0.9947528833380113
Iteration 10, Training loss = 0.32548997536874735
Iteration 20, Training loss = 0.29826901824428487
Iteration 30, Training loss = 0.2851754699188929
Iteration 40, Training loss = 0.27908554314993894
Iteration 50, Training loss = 0.27359607729774255
Iteration 60, Training loss = 0.27030539455322117
Iteration 70, Training loss = 0.2665218250969282
Iteration 80, Training loss = 0.26210288660457504
Iteration 90, Training loss = 0.2587590438242142
Model training time: 18.78731870651245
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.5206890495923848
Iteration 10, Training loss = 0.9973339770848935
Iteration 20, Training loss = 0.9947870866610453
Iteration 30, Training loss = 0.9924170999572828
Iteration 40, Training loss = 0.9974526596757082
Iteration 50, Training loss = 0.9893442249068847
Iteration 60, Training loss = 0.9912571769494277
Iteration 70, Training loss = 0.9881119355559349
Iteration 80, Training loss = 0.9880545259668276
Iteration 90, Training loss = 0.9821929364250257
Model training time: 16.761988162994385
Device: cuda
Iteration 0, Training loss = 1.0626501544163778
Iteration 10, Training loss = 0.9918356555012556
Iteration 20, Training loss = 0.9902139741640824
Iteration 30, Training loss = 0.9875030809870133
Iteration 40, Training loss = 0.9860779511240813
Iteration 50, Training loss = 0.9777892948343203
Iteration 60, Training loss = 0.974997365130828
Iteration 70, Training loss = 0.9700691774487495
Iteration 80, Training loss = 0.9592021480202675
Iteration 90, Training loss = 0.9531563577743677
Model training time: 16.661659479141235
Device: cuda
Iteration 0, Training loss = 1.0653067162403693
Iteration 10, Training loss = 1.0008355573965952
Iteration 20, Training loss = 1.0018083355747736
Iteration 30, Training loss = 1.0007303689534848
Iteration 40, Training loss = 1.0015720501542091
Iteration 50, Training loss = 0.9966595774659743
Iteration 60, Training loss = 0.9950688378168986
Iteration 70, Training loss = 0.9890042732541378
Iteration 80, Training loss = 0.9901219285451449
Iteration 90, Training loss = 0.9890910089015961
Model training time: 16.95079803466797
Device: cuda
Iteration 0, Training loss = 1.0084717898414686
Iteration 10, Training loss = 1.0009651596729572
Iteration 20, Training loss = 0.999856527035053
Iteration 30, Training loss = 1.003945995408755
Iteration 40, Training loss = 0.9961764932825015
Iteration 50, Training loss = 0.9928315837795918
Iteration 60, Training loss = 0.9940710801344651
Iteration 70, Training loss = 0.9888908559313188
Iteration 80, Training loss = 0.9863074330183176
Iteration 90, Training loss = 0.984378189421617
Model training time: 16.66818356513977
Device: cuda
Iteration 0, Training loss = 1.1243500675146396
Iteration 10, Training loss = 1.0124775985112557
Iteration 20, Training loss = 1.0043428952877338
Iteration 30, Training loss = 0.9981767913469901
Iteration 40, Training loss = 1.0003976604113212
Iteration 50, Training loss = 0.9908870642001812
Iteration 60, Training loss = 0.9896006188713588
Iteration 70, Training loss = 0.9843581060950573
Iteration 80, Training loss = 0.9773767601985198
Iteration 90, Training loss = 0.9747351281918012
Model training time: 16.97490382194519
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9883341697546152
Iteration 10, Training loss = 0.3175299187692312
Iteration 20, Training loss = 0.2983178434750208
Iteration 30, Training loss = 0.2915632122984299
Iteration 40, Training loss = 0.2840044217614027
Iteration 50, Training loss = 0.2774935092490453
Iteration 60, Training loss = 0.2730907562833566
Iteration 70, Training loss = 0.2712853022206288
Iteration 80, Training loss = 0.26599687782044595
Iteration 90, Training loss = 0.2628430613818077
Iteration 100, Training loss = 0.25920099213432807
Iteration 110, Training loss = 0.2563982189704592
Iteration 120, Training loss = 0.25509057394587076
Iteration 130, Training loss = 0.25312400337022084
Iteration 140, Training loss = 0.2504857752758723
Iteration 150, Training loss = 0.24851662422028872
Iteration 160, Training loss = 0.2469427638615553
Iteration 170, Training loss = 0.24475778983189508
Iteration 180, Training loss = 0.2442912317525882
Iteration 190, Training loss = 0.2440580353140831
Model training time: 38.17379903793335
Device: cuda
Iteration 0, Training loss = 0.9979871712051905
Iteration 10, Training loss = 0.3152777629976089
Iteration 20, Training loss = 0.2953084349059142
Iteration 30, Training loss = 0.2865774410848434
Iteration 40, Training loss = 0.27882994754383195
Iteration 50, Training loss = 0.2724992912262678
Iteration 60, Training loss = 0.2692556641995907
Iteration 70, Training loss = 0.2659744330896781
Iteration 80, Training loss = 0.26414848348269093
Iteration 90, Training loss = 0.26082177761082465
Iteration 100, Training loss = 0.25830499398020595
Iteration 110, Training loss = 0.257116766503224
Iteration 120, Training loss = 0.2549754447088792
Iteration 130, Training loss = 0.25308024345968777
Iteration 140, Training loss = 0.25406969324327433
Iteration 150, Training loss = 0.25306327655338323
Iteration 160, Training loss = 0.2507639177716695
Iteration 170, Training loss = 0.24979045714896458
Iteration 180, Training loss = 0.2482184352209935
Iteration 190, Training loss = 0.2480556475571715
Model training time: 38.091464042663574
Device: cuda
Iteration 0, Training loss = 0.9909275804574673
Iteration 10, Training loss = 0.31528057458882147
Iteration 20, Training loss = 0.2932634118657846
Iteration 30, Training loss = 0.283475486561656
Iteration 40, Training loss = 0.2759929959877179
Iteration 50, Training loss = 0.2701444926743324
Iteration 60, Training loss = 0.2653551587405113
Iteration 70, Training loss = 0.2627692320025884
Iteration 80, Training loss = 0.25835668582182664
Iteration 90, Training loss = 0.2549725037354689
Iteration 100, Training loss = 0.25346244871616364
Iteration 110, Training loss = 0.251382331435497
Iteration 120, Training loss = 0.24965348130521867
Iteration 130, Training loss = 0.24965767992230561
Iteration 140, Training loss = 0.25097933268317807
Iteration 150, Training loss = 0.24552559250822434
Iteration 160, Training loss = 0.24606983994062132
Iteration 170, Training loss = 0.24187663584374464
Iteration 180, Training loss = 0.24040871801284644
Iteration 190, Training loss = 0.23998805565329698
Model training time: 37.50998306274414
Device: cuda
Iteration 0, Training loss = 0.9976370598261173
Iteration 10, Training loss = 0.308596905034322
Iteration 20, Training loss = 0.29396075282532436
Iteration 30, Training loss = 0.28411467015170133
Iteration 40, Training loss = 0.2766403266443656
Iteration 50, Training loss = 0.27154789879344976
Iteration 60, Training loss = 0.2670272931170005
Iteration 70, Training loss = 0.2635242019135218
Iteration 80, Training loss = 0.2602506804351623
Iteration 90, Training loss = 0.2591897601691576
Iteration 100, Training loss = 0.2569043735185495
Iteration 110, Training loss = 0.2548400262513986
Iteration 120, Training loss = 0.25191965541587424
Iteration 130, Training loss = 0.252331743733241
Iteration 140, Training loss = 0.24736733448046905
Iteration 150, Training loss = 0.24556915003519791
Iteration 160, Training loss = 0.24784086372416753
Iteration 170, Training loss = 0.24419436489160246
Iteration 180, Training loss = 0.2435522833122657
Iteration 190, Training loss = 0.24125194535232508
Model training time: 37.128113985061646
Device: cuda
Iteration 0, Training loss = 0.9728814415060557
Iteration 10, Training loss = 0.30818997452465385
Iteration 20, Training loss = 0.29637124561346495
Iteration 30, Training loss = 0.2862826014080873
Iteration 40, Training loss = 0.27823691428280795
Iteration 50, Training loss = 0.272224178394446
Iteration 60, Training loss = 0.26622109616605133
Iteration 70, Training loss = 0.26395927656155366
Iteration 80, Training loss = 0.2589285561385063
Iteration 90, Training loss = 0.25751609068650466
Iteration 100, Training loss = 0.2540928336003652
Iteration 110, Training loss = 0.2516940634411115
Iteration 120, Training loss = 0.25007256492972374
Iteration 130, Training loss = 0.24802814782238924
Iteration 140, Training loss = 0.24662497482047632
Iteration 150, Training loss = 0.24500720482319593
Iteration 160, Training loss = 0.24481554658940205
Iteration 170, Training loss = 0.2431899099968947
Iteration 180, Training loss = 0.24106697680858466
Iteration 190, Training loss = 0.2407852656279619
Model training time: 37.90782976150513
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1914642424537585
Iteration 10, Training loss = 0.9938391349636592
Iteration 20, Training loss = 0.9971458178300124
Iteration 30, Training loss = 0.9902740373061254
Iteration 40, Training loss = 0.9905666422385436
Iteration 50, Training loss = 0.9858950330660894
Iteration 60, Training loss = 0.9751718256335992
Iteration 70, Training loss = 0.9747618075746757
Iteration 80, Training loss = 0.9631809620903089
Iteration 90, Training loss = 0.9543590356524174
Iteration 100, Training loss = 0.941999906530747
Iteration 110, Training loss = 0.9284727281102767
Iteration 120, Training loss = 0.9015522335584347
Iteration 130, Training loss = 0.8765006850545223
Iteration 140, Training loss = 0.8393598273396492
Iteration 150, Training loss = 0.7880579892259377
Iteration 160, Training loss = 0.7329738117181338
Iteration 170, Training loss = 0.6662277080691777
Iteration 180, Training loss = 0.6009610630571842
Iteration 190, Training loss = 0.5296408534049988
Model training time: 33.86801862716675
Device: cuda
Iteration 0, Training loss = 1.176510238303588
Iteration 10, Training loss = 0.9986252503899428
Iteration 20, Training loss = 0.9949615282508043
Iteration 30, Training loss = 0.9893221150224025
Iteration 40, Training loss = 0.9879092270365128
Iteration 50, Training loss = 0.9840103233089814
Iteration 60, Training loss = 0.9782301330795655
Iteration 70, Training loss = 0.9747596451869378
Iteration 80, Training loss = 0.9682216460888202
Iteration 90, Training loss = 0.9598539758187074
Iteration 100, Training loss = 0.9531732247425959
Iteration 110, Training loss = 0.9360364921964132
Iteration 120, Training loss = 0.9287498237995001
Iteration 130, Training loss = 0.8996990810220058
Iteration 140, Training loss = 0.8768963779394443
Iteration 150, Training loss = 0.8470814663630265
Iteration 160, Training loss = 0.8003963381052017
Iteration 170, Training loss = 0.7491710271972877
Iteration 180, Training loss = 0.6897864702802438
Iteration 190, Training loss = 0.6246811690238806
Model training time: 33.85791254043579
Device: cuda
Iteration 0, Training loss = 1.1105390340089798
Iteration 10, Training loss = 0.996153457233539
Iteration 20, Training loss = 0.9971754740063961
Iteration 30, Training loss = 0.9934887416087664
Iteration 40, Training loss = 0.9872218410556133
Iteration 50, Training loss = 0.9822162699240905
Iteration 60, Training loss = 0.9770145267248154
Iteration 70, Training loss = 0.9753410369157791
Iteration 80, Training loss = 0.9647832730641732
Iteration 90, Training loss = 0.9632878716175373
Iteration 100, Training loss = 0.947451053903653
Iteration 110, Training loss = 0.9307227937074808
Iteration 120, Training loss = 0.9190229406723609
Iteration 130, Training loss = 0.8981293325240796
Iteration 140, Training loss = 0.8684861671466094
Iteration 150, Training loss = 0.8372965896358857
Iteration 160, Training loss = 0.7913732488568013
Iteration 170, Training loss = 0.7402737438678741
Iteration 180, Training loss = 0.681909149655929
Iteration 190, Training loss = 0.6205258527054236
Model training time: 34.74772763252258
Device: cuda
Iteration 0, Training loss = 1.0047534486422172
Iteration 10, Training loss = 0.9964287206530571
Iteration 20, Training loss = 0.9948002753349451
Iteration 30, Training loss = 0.9942858551557248
Iteration 40, Training loss = 0.9890116286965517
Iteration 50, Training loss = 0.9902113085755935
Iteration 60, Training loss = 0.9886206611990929
Iteration 70, Training loss = 0.9836427431840163
Iteration 80, Training loss = 0.9863845258951187
Iteration 90, Training loss = 0.9774258893269759
Iteration 100, Training loss = 0.9716711686207697
Iteration 110, Training loss = 0.9637815599831251
Iteration 120, Training loss = 0.9576084808661387
Iteration 130, Training loss = 0.9534037995796937
Iteration 140, Training loss = 0.9439355518955451
Iteration 150, Training loss = 0.9252406135201454
Iteration 160, Training loss = 0.906007625735723
Iteration 170, Training loss = 0.8775851044517297
Iteration 180, Training loss = 0.8472124676291759
Iteration 190, Training loss = 0.8076050344568032
Model training time: 38.11268973350525
Device: cuda
Iteration 0, Training loss = 1.021437766460272
Iteration 10, Training loss = 0.9997683327931625
Iteration 20, Training loss = 0.991033136844635
Iteration 30, Training loss = 0.9902409418271139
Iteration 40, Training loss = 0.9855272168150315
Iteration 50, Training loss = 0.9829714659314889
Iteration 60, Training loss = 0.976264588534832
Iteration 70, Training loss = 0.9778217386740905
Iteration 80, Training loss = 0.9670932069420815
Iteration 90, Training loss = 0.958309969649865
Iteration 100, Training loss = 0.9449452385306358
Iteration 110, Training loss = 0.9307408229662821
Iteration 120, Training loss = 0.9169258933800918
Iteration 130, Training loss = 0.8917382135987282
Iteration 140, Training loss = 0.8669185168468035
Iteration 150, Training loss = 0.8319488104719382
Iteration 160, Training loss = 0.7823750124527857
Iteration 170, Training loss = 0.7322621322595156
Iteration 180, Training loss = 0.6721527438897353
Iteration 190, Training loss = 0.6034731472340914
Model training time: 37.510881185531616
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9955042262489979
Iteration 10, Training loss = 0.3603117857128382
Iteration 20, Training loss = 0.3085211595663658
Iteration 30, Training loss = 0.2906363150821282
Iteration 40, Training loss = 0.2823952850527488
Iteration 50, Training loss = 0.27628983127383083
Iteration 60, Training loss = 0.2719660908556901
Iteration 70, Training loss = 0.27037627866061836
Iteration 80, Training loss = 0.26820814108046204
Iteration 90, Training loss = 0.26708033895836425
Iteration 100, Training loss = 0.2641616598344766
Iteration 110, Training loss = 0.2618354647778548
Iteration 120, Training loss = 0.2591328653865136
Iteration 130, Training loss = 0.2570608840682186
Iteration 140, Training loss = 0.2564411894060098
Iteration 150, Training loss = 0.25719470674028766
Iteration 160, Training loss = 0.2547883569048001
Iteration 170, Training loss = 0.2544129831859699
Iteration 180, Training loss = 0.2545453467621253
Iteration 190, Training loss = 0.25329119482865703
Model training time: 40.64233469963074
Device: cuda
Iteration 0, Training loss = 1.443559515934724
Iteration 10, Training loss = 0.41584753359739596
Iteration 20, Training loss = 0.31539296831649083
Iteration 30, Training loss = 0.2914930349932267
Iteration 40, Training loss = 0.282171059686404
Iteration 50, Training loss = 0.27261259578741515
Iteration 60, Training loss = 0.26794038268809134
Iteration 70, Training loss = 0.26479907133258307
Iteration 80, Training loss = 0.265185697720601
Iteration 90, Training loss = 0.2601101817563176
Iteration 100, Training loss = 0.2585455004412394
Iteration 110, Training loss = 0.25822011496012026
Iteration 120, Training loss = 0.2563017409008283
Iteration 130, Training loss = 0.25501566767119443
Iteration 140, Training loss = 0.25373930691813046
Iteration 150, Training loss = 0.2520212890723577
Iteration 160, Training loss = 0.25318798580421853
Iteration 170, Training loss = 0.25139913536035097
Iteration 180, Training loss = 0.24950376009711853
Iteration 190, Training loss = 0.2487515603693632
Model training time: 39.0291314125061
Device: cuda
Iteration 0, Training loss = 0.9901912567707208
Iteration 10, Training loss = 0.32563363445492893
Iteration 20, Training loss = 0.29513945215596604
Iteration 30, Training loss = 0.2839278391060921
Iteration 40, Training loss = 0.2771840589837386
Iteration 50, Training loss = 0.26891264591652614
Iteration 60, Training loss = 0.26639517668921214
Iteration 70, Training loss = 0.26474901747245055
Iteration 80, Training loss = 0.2590552687358398
Iteration 90, Training loss = 0.2564664457279902
Iteration 100, Training loss = 0.25426521711051464
Iteration 110, Training loss = 0.2526806060654613
Iteration 120, Training loss = 0.25277444089834505
Iteration 130, Training loss = 0.2517611642296498
Iteration 140, Training loss = 0.24811919012035316
Iteration 150, Training loss = 0.24797332580559528
Iteration 160, Training loss = 0.24715055869175837
Iteration 170, Training loss = 0.24463073221536782
Iteration 180, Training loss = 0.2425472172550284
Iteration 190, Training loss = 0.24215052405802104
Model training time: 37.293612480163574
Device: cuda
Iteration 0, Training loss = 0.9931387167710525
Iteration 10, Training loss = 0.3301475907747562
Iteration 20, Training loss = 0.29604472320240277
Iteration 30, Training loss = 0.28670630045235157
Iteration 40, Training loss = 0.2774001324119476
Iteration 50, Training loss = 0.27112782646257144
Iteration 60, Training loss = 0.2706020177843479
Iteration 70, Training loss = 0.265446883554642
Iteration 80, Training loss = 0.2626241848159295
Iteration 90, Training loss = 0.2608876791424476
Iteration 100, Training loss = 0.25814276575469053
Iteration 110, Training loss = 0.25582754468688595
Iteration 120, Training loss = 0.25583929024063623
Iteration 130, Training loss = 0.25239638103028905
Iteration 140, Training loss = 0.24896699548340762
Iteration 150, Training loss = 0.24931708336449587
Iteration 160, Training loss = 0.24574887774025017
Iteration 170, Training loss = 0.2460512425034092
Iteration 180, Training loss = 0.24394118814514235
Iteration 190, Training loss = 0.24265985190868378
Model training time: 37.327751874923706
Device: cuda
Iteration 0, Training loss = 1.0914749594835134
Iteration 10, Training loss = 0.3444658418974051
Iteration 20, Training loss = 0.305121739466603
Iteration 30, Training loss = 0.2936419506485646
Iteration 40, Training loss = 0.2838318388049419
Iteration 50, Training loss = 0.2775487992912531
Iteration 60, Training loss = 0.2719993950942388
Iteration 70, Training loss = 0.2682226165555991
Iteration 80, Training loss = 0.26427932857320857
Iteration 90, Training loss = 0.2613530716357323
Iteration 100, Training loss = 0.259760511179383
Iteration 110, Training loss = 0.2563269445672631
Iteration 120, Training loss = 0.25476266579845774
Iteration 130, Training loss = 0.25386821005779964
Iteration 140, Training loss = 0.2516516422709593
Iteration 150, Training loss = 0.2520164247029103
Iteration 160, Training loss = 0.25038404017686844
Iteration 170, Training loss = 0.24825386925098988
Iteration 180, Training loss = 0.24731297260866716
Iteration 190, Training loss = 0.24576153785276872
Model training time: 37.47519063949585
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.009908710534756
Iteration 10, Training loss = 1.001422756566451
Iteration 20, Training loss = 0.9978268031890576
Iteration 30, Training loss = 0.9914901554584503
Iteration 40, Training loss = 0.9930919838639406
Iteration 50, Training loss = 0.9923706031762637
Iteration 60, Training loss = 0.9863607419224886
Iteration 70, Training loss = 0.9819164362091285
Iteration 80, Training loss = 0.9780070197123748
Iteration 90, Training loss = 0.9735880574354758
Iteration 100, Training loss = 0.966979246873122
Iteration 110, Training loss = 0.9606364071369171
Iteration 120, Training loss = 0.9539139769398249
Iteration 130, Training loss = 0.936938980450997
Iteration 140, Training loss = 0.9250347740375079
Iteration 150, Training loss = 0.9035113792006786
Iteration 160, Training loss = 0.8824371752830652
Iteration 170, Training loss = 0.8549889005147494
Iteration 180, Training loss = 0.816787029688175
Iteration 190, Training loss = 0.7780387080632724
Model training time: 33.030953884124756
Device: cuda
Iteration 0, Training loss = 1.3834075652636015
Iteration 10, Training loss = 0.9971091391948553
Iteration 20, Training loss = 0.9944806672059573
Iteration 30, Training loss = 0.9961780068966058
Iteration 40, Training loss = 0.9923271485246145
Iteration 50, Training loss = 0.9896917503613692
Iteration 60, Training loss = 0.9905160241402112
Iteration 70, Training loss = 0.9846489905164792
Iteration 80, Training loss = 0.9813603042409971
Iteration 90, Training loss = 0.9738790286848178
Iteration 100, Training loss = 0.9749271330925134
Iteration 110, Training loss = 0.966945228095238
Iteration 120, Training loss = 0.9592713432816359
Iteration 130, Training loss = 0.946582052570123
Iteration 140, Training loss = 0.9311160651537088
Iteration 150, Training loss = 0.9187628666941936
Iteration 160, Training loss = 0.894530482017077
Iteration 170, Training loss = 0.8611214350049312
Iteration 180, Training loss = 0.8271731075185996
Iteration 190, Training loss = 0.7757664953286831
Model training time: 32.86913824081421
Device: cuda
Iteration 0, Training loss = 1.1988449732844646
Iteration 10, Training loss = 1.0055374680803373
Iteration 20, Training loss = 1.0054330040629094
Iteration 30, Training loss = 1.002994848558536
Iteration 40, Training loss = 1.0021540224552155
Iteration 50, Training loss = 1.002172707938231
Iteration 60, Training loss = 0.9979104147507594
Iteration 70, Training loss = 0.9918523190113214
Iteration 80, Training loss = 0.9899591872325311
Iteration 90, Training loss = 0.9935357834284122
Iteration 100, Training loss = 0.9831508524142779
Iteration 110, Training loss = 0.9796537785575941
Iteration 120, Training loss = 0.9773448447768505
Iteration 130, Training loss = 0.9713734537363052
Iteration 140, Training loss = 0.9662708072708204
Iteration 150, Training loss = 0.9611255830297103
Iteration 160, Training loss = 0.9483329888719779
Iteration 170, Training loss = 0.9378389733342024
Iteration 180, Training loss = 0.9227050221883334
Iteration 190, Training loss = 0.9050880303749671
Model training time: 32.715411901474
Device: cuda
Iteration 0, Training loss = 0.9997005548614722
Iteration 10, Training loss = 0.9921464438621814
Iteration 20, Training loss = 0.9927966726514009
Iteration 30, Training loss = 0.9924213181321437
Iteration 40, Training loss = 0.9896497629009761
Iteration 50, Training loss = 0.985415773322949
Iteration 60, Training loss = 0.9791446345356795
Iteration 70, Training loss = 0.976422246832114
Iteration 80, Training loss = 0.9773420147024668
Iteration 90, Training loss = 0.9676261882369335
Iteration 100, Training loss = 0.9626318130355614
Iteration 110, Training loss = 0.953109259215685
Iteration 120, Training loss = 0.9484575327772361
Iteration 130, Training loss = 0.9383746672135133
Iteration 140, Training loss = 0.9238329008221626
Iteration 150, Training loss = 0.9059503118579204
Iteration 160, Training loss = 0.886236456151192
Iteration 170, Training loss = 0.8579220227324046
Iteration 180, Training loss = 0.8274643684809024
Iteration 190, Training loss = 0.7904694733711389
Model training time: 32.68769669532776
Device: cuda
Iteration 0, Training loss = 1.1072406963660166
Iteration 10, Training loss = 0.9952055147060981
Iteration 20, Training loss = 0.998572011406605
Iteration 30, Training loss = 0.9923383129330782
Iteration 40, Training loss = 0.9911088083799069
Iteration 50, Training loss = 0.9896719684967628
Iteration 60, Training loss = 0.9862377139238211
Iteration 70, Training loss = 0.9857870838963069
Iteration 80, Training loss = 0.9786435124965814
Iteration 90, Training loss = 0.9766395458808312
Iteration 100, Training loss = 0.9708576655158629
Iteration 110, Training loss = 0.9642784423553027
Iteration 120, Training loss = 0.9593997259552662
Iteration 130, Training loss = 0.9510576106034793
Iteration 140, Training loss = 0.93928445990269
Iteration 150, Training loss = 0.9264325029574908
Iteration 160, Training loss = 0.9061901494860649
Iteration 170, Training loss = 0.8829079677279179
Iteration 180, Training loss = 0.8517768239745727
Iteration 190, Training loss = 0.8171340903410544
Model training time: 33.086963415145874
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.2422427592369227
Iteration 10, Training loss = 0.36778726586355615
Iteration 20, Training loss = 0.3047705828570403
Iteration 30, Training loss = 0.292447183567744
Iteration 40, Training loss = 0.28599843096274596
Iteration 50, Training loss = 0.2796749092925053
Iteration 60, Training loss = 0.2761738539601748
Iteration 70, Training loss = 0.27295203492618525
Iteration 80, Training loss = 0.2695122796755571
Iteration 90, Training loss = 0.2675535170218119
Iteration 100, Training loss = 0.26694878620597035
Iteration 110, Training loss = 0.26528474788826245
Iteration 120, Training loss = 0.26473765146846956
Iteration 130, Training loss = 0.2624368659005715
Iteration 140, Training loss = 0.2611512773885177
Iteration 150, Training loss = 0.2616074373229192
Iteration 160, Training loss = 0.26106547908141065
Iteration 170, Training loss = 0.25957093473810416
Iteration 180, Training loss = 0.26006620133725494
Iteration 190, Training loss = 0.25896898628427434
Model training time: 36.92487621307373
Device: cuda
Iteration 0, Training loss = 0.9872634041194732
Iteration 10, Training loss = 0.3220534443568725
Iteration 20, Training loss = 0.2955298482512052
Iteration 30, Training loss = 0.2866448575200943
Iteration 40, Training loss = 0.2788654787895771
Iteration 50, Training loss = 0.2716103923292114
Iteration 60, Training loss = 0.2676184731893815
Iteration 70, Training loss = 0.2663616384737767
Iteration 80, Training loss = 0.2608407022288212
Iteration 90, Training loss = 0.25988781967988384
Iteration 100, Training loss = 0.25714457350281567
Iteration 110, Training loss = 0.2560286180904278
Iteration 120, Training loss = 0.25409355329779476
Iteration 130, Training loss = 0.2534409610984417
Iteration 140, Training loss = 0.25308312475681305
Iteration 150, Training loss = 0.25193544950049657
Iteration 160, Training loss = 0.25050925391797835
Iteration 170, Training loss = 0.24888619049810445
Iteration 180, Training loss = 0.2493114548806961
Iteration 190, Training loss = 0.24706306179555562
Model training time: 37.70415425300598
Device: cuda
Iteration 0, Training loss = 0.9871614386255925
Iteration 10, Training loss = 0.3154470291561805
Iteration 20, Training loss = 0.2950500463350461
Iteration 30, Training loss = 0.28657643339381766
Iteration 40, Training loss = 0.2820403491361783
Iteration 50, Training loss = 0.27412375736121947
Iteration 60, Training loss = 0.26924392516509843
Iteration 70, Training loss = 0.2674032012717082
Iteration 80, Training loss = 0.2640167751277869
Iteration 90, Training loss = 0.26083785481750965
Iteration 100, Training loss = 0.2586907476473313
Iteration 110, Training loss = 0.25803949497640133
Iteration 120, Training loss = 0.2554617331434901
Iteration 130, Training loss = 0.25468801964934057
Iteration 140, Training loss = 0.25412864768161225
Iteration 150, Training loss = 0.25298409894681895
Iteration 160, Training loss = 0.25173265687548196
Iteration 170, Training loss = 0.25093520676287323
Iteration 180, Training loss = 0.2501894671183366
Iteration 190, Training loss = 0.24930200883402273
Model training time: 37.76177263259888
Device: cuda
Iteration 0, Training loss = 0.9855026717369373
Iteration 10, Training loss = 0.31269554879802924
Iteration 20, Training loss = 0.29712294027782404
Iteration 30, Training loss = 0.2904323499936324
Iteration 40, Training loss = 0.28360146613648307
Iteration 50, Training loss = 0.2782093161860338
Iteration 60, Training loss = 0.2732325843893565
Iteration 70, Training loss = 0.26891437574074817
Iteration 80, Training loss = 0.26653862543977225
Iteration 90, Training loss = 0.2620769812224003
Iteration 100, Training loss = 0.25860549237292546
Iteration 110, Training loss = 0.25614962884439874
Iteration 120, Training loss = 0.2536993665763965
Iteration 130, Training loss = 0.2529080157669691
Iteration 140, Training loss = 0.2502488638632573
Iteration 150, Training loss = 0.25026143729113615
Iteration 160, Training loss = 0.24642411805689335
Iteration 170, Training loss = 0.24516896846202704
Iteration 180, Training loss = 0.24442180231786692
Iteration 190, Training loss = 0.2448117700047218
Model training time: 37.0851035118103
Device: cuda
Iteration 0, Training loss = 0.9891084765012448
Iteration 10, Training loss = 0.3180649471111022
Iteration 20, Training loss = 0.2982685494308288
Iteration 30, Training loss = 0.291868474191198
Iteration 40, Training loss = 0.2824443547198406
Iteration 50, Training loss = 0.2781588032555122
Iteration 60, Training loss = 0.2730312652599353
Iteration 70, Training loss = 0.2695463804098276
Iteration 80, Training loss = 0.2665137340529607
Iteration 90, Training loss = 0.26528895947222525
Iteration 100, Training loss = 0.26170648061312163
Iteration 110, Training loss = 0.25745873898267746
Iteration 120, Training loss = 0.25518658929146254
Iteration 130, Training loss = 0.2554633872440228
Iteration 140, Training loss = 0.25127482399917567
Iteration 150, Training loss = 0.24989076252453601
Iteration 160, Training loss = 0.24657905259384558
Iteration 170, Training loss = 0.24501856925109258
Iteration 180, Training loss = 0.24387281946837902
Iteration 190, Training loss = 0.24394183792173862
Model training time: 37.42961049079895
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0023501750368338
Iteration 10, Training loss = 0.9967899981599587
Iteration 20, Training loss = 0.998155570947207
Iteration 30, Training loss = 0.9955666689918592
Iteration 40, Training loss = 0.9948370846418234
Iteration 50, Training loss = 0.9929868521598669
Iteration 60, Training loss = 0.991608239710331
Iteration 70, Training loss = 0.9885718381175628
Iteration 80, Training loss = 0.9885751616496307
Iteration 90, Training loss = 0.9841384469316556
Iteration 100, Training loss = 0.9814034224702761
Iteration 110, Training loss = 0.9803310391994623
Iteration 120, Training loss = 0.9773693331159078
Iteration 130, Training loss = 0.968929634644435
Iteration 140, Training loss = 0.9683946353884844
Iteration 150, Training loss = 0.9606297440253772
Iteration 160, Training loss = 0.9486369089438365
Iteration 170, Training loss = 0.9393177032470703
Iteration 180, Training loss = 0.9303399175405502
Iteration 190, Training loss = 0.908890850841999
Model training time: 33.76956868171692
Device: cuda
Iteration 0, Training loss = 1.005421788073503
Iteration 10, Training loss = 0.9912047936366155
Iteration 20, Training loss = 0.9888510251274476
Iteration 30, Training loss = 0.989493959225141
Iteration 40, Training loss = 0.9832493702952678
Iteration 50, Training loss = 0.9766151114152029
Iteration 60, Training loss = 0.9753544370715435
Iteration 70, Training loss = 0.9647112053174239
Iteration 80, Training loss = 0.9590774671389506
Iteration 90, Training loss = 0.944229030265258
Iteration 100, Training loss = 0.9338829855506237
Iteration 110, Training loss = 0.9104621886060789
Iteration 120, Training loss = 0.8918090262092077
Iteration 130, Training loss = 0.8610138188187892
Iteration 140, Training loss = 0.8224380979171166
Iteration 150, Training loss = 0.7710662830907565
Iteration 160, Training loss = 0.7155617423928701
Iteration 170, Training loss = 0.6506087547884538
Iteration 180, Training loss = 0.587366528235949
Iteration 190, Training loss = 0.5279679441681275
Model training time: 33.55061602592468
Device: cuda
Iteration 0, Training loss = 1.1405716492579534
Iteration 10, Training loss = 1.0011220316474254
Iteration 20, Training loss = 0.999500973866536
Iteration 30, Training loss = 0.9947809531138494
Iteration 40, Training loss = 0.9915902047203138
Iteration 50, Training loss = 0.9919708480055516
Iteration 60, Training loss = 0.9890241072728083
Iteration 70, Training loss = 0.9857703286867875
Iteration 80, Training loss = 0.981155052781105
Iteration 90, Training loss = 0.9744609651657251
Iteration 100, Training loss = 0.9699551600676316
Iteration 110, Training loss = 0.9656426556981527
Iteration 120, Training loss = 0.9569838872322669
Iteration 130, Training loss = 0.9486725926399231
Iteration 140, Training loss = 0.9329008546013099
Iteration 150, Training loss = 0.915289870248391
Iteration 160, Training loss = 0.8987662998529581
Iteration 170, Training loss = 0.870592878988156
Iteration 180, Training loss = 0.8342587162668889
Iteration 190, Training loss = 0.7958245225823842
Model training time: 33.192699670791626
Device: cuda
Iteration 0, Training loss = 1.0018547704586616
Iteration 10, Training loss = 1.0024350950351129
Iteration 20, Training loss = 0.9986573572342212
Iteration 30, Training loss = 0.9967082744607558
Iteration 40, Training loss = 0.9926655435791383
Iteration 50, Training loss = 0.9978151716865026
Iteration 60, Training loss = 0.9901201919867442
Iteration 70, Training loss = 0.9905238403723791
Iteration 80, Training loss = 0.9931617998159848
Iteration 90, Training loss = 0.9850249920900052
Iteration 100, Training loss = 0.9872758961640872
Iteration 110, Training loss = 0.9788402003737596
Iteration 120, Training loss = 0.9784083320544317
Iteration 130, Training loss = 0.9717815558497722
Iteration 140, Training loss = 0.9649766838321319
Iteration 150, Training loss = 0.9600165062225782
Iteration 160, Training loss = 0.9559027031064034
Iteration 170, Training loss = 0.9418888034728857
Iteration 180, Training loss = 0.9237790560493102
Iteration 190, Training loss = 0.9088738715419402
Model training time: 33.46298003196716
Device: cuda
Iteration 0, Training loss = 1.0797958213549395
Iteration 10, Training loss = 0.9916578210317172
Iteration 20, Training loss = 0.9857576191425323
Iteration 30, Training loss = 0.9834317593620374
Iteration 40, Training loss = 0.9832108771571746
Iteration 50, Training loss = 0.9747046484397008
Iteration 60, Training loss = 0.9699040496578584
Iteration 70, Training loss = 0.9630087688565254
Iteration 80, Training loss = 0.950198925458468
Iteration 90, Training loss = 0.9408313104739556
Iteration 100, Training loss = 0.9210815934034494
Iteration 110, Training loss = 0.8971947411505076
Iteration 120, Training loss = 0.8739703194453166
Iteration 130, Training loss = 0.8420135201169894
Iteration 140, Training loss = 0.7910852220195991
Iteration 150, Training loss = 0.7374596309203368
Iteration 160, Training loss = 0.6741781644523144
Iteration 170, Training loss = 0.606591695203231
Iteration 180, Training loss = 0.5429389989719942
Iteration 190, Training loss = 0.4906103258522657
Model training time: 33.699766397476196
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.040817072758308
Iteration 10, Training loss = 0.3183237940359574
Iteration 20, Training loss = 0.29738702725332516
Iteration 30, Training loss = 0.2891370887653186
Iteration 40, Training loss = 0.2823106898711278
Iteration 50, Training loss = 0.2739714220022926
Iteration 60, Training loss = 0.2707549859411441
Iteration 70, Training loss = 0.2659491045543781
Iteration 80, Training loss = 0.2618841613427951
Iteration 90, Training loss = 0.2603352488233493
Iteration 100, Training loss = 0.25647339382423806
Iteration 110, Training loss = 0.2551011245411176
Iteration 120, Training loss = 0.25419003115250516
Iteration 130, Training loss = 0.25324305275884956
Iteration 140, Training loss = 0.2525033367654452
Iteration 150, Training loss = 0.2514140954097876
Iteration 160, Training loss = 0.2505110518003886
Iteration 170, Training loss = 0.2511606846864407
Iteration 180, Training loss = 0.2498630481557204
Iteration 190, Training loss = 0.24927449477120087
Iteration 200, Training loss = 0.24777209665626287
Iteration 210, Training loss = 0.2525975698461899
Iteration 220, Training loss = 0.24717757249107727
Iteration 230, Training loss = 0.2462269404473213
Iteration 240, Training loss = 0.24591916240751743
Iteration 250, Training loss = 0.24505702391839945
Iteration 260, Training loss = 0.24545257882430002
Iteration 270, Training loss = 0.24514937716034743
Iteration 280, Training loss = 0.24405033907924706
Iteration 290, Training loss = 0.24525508872018412
Model training time: 57.545817613601685
Device: cuda
Iteration 0, Training loss = 0.9874753751433812
Iteration 10, Training loss = 0.3158584155428868
Iteration 20, Training loss = 0.29690554852669054
Iteration 30, Training loss = 0.28664009645581245
Iteration 40, Training loss = 0.27989598721838915
Iteration 50, Training loss = 0.268793815961824
Iteration 60, Training loss = 0.2638731071582207
Iteration 70, Training loss = 0.2598978915753273
Iteration 80, Training loss = 0.2562432075635745
Iteration 90, Training loss = 0.25390985493476576
Iteration 100, Training loss = 0.25094982164983565
Iteration 110, Training loss = 0.2486621827746813
Iteration 120, Training loss = 0.24625876474265868
Iteration 130, Training loss = 0.2449308168143034
Iteration 140, Training loss = 0.243111847470013
Iteration 150, Training loss = 0.24286670646128747
Iteration 160, Training loss = 0.24100837340721717
Iteration 170, Training loss = 0.2398083206409445
Iteration 180, Training loss = 0.2378945700967541
Iteration 190, Training loss = 0.23974078334867954
Iteration 200, Training loss = 0.23718628659844398
Iteration 210, Training loss = 0.23625829357367295
Iteration 220, Training loss = 0.23647967912256718
Iteration 230, Training loss = 0.23716740403324366
Iteration 240, Training loss = 0.23305279503648096
Iteration 250, Training loss = 0.23186104830641013
Iteration 260, Training loss = 0.23004677017720845
Iteration 270, Training loss = 0.2296281658972685
Iteration 280, Training loss = 0.2284307276400236
Iteration 290, Training loss = 0.23006518966016862
Model training time: 56.69344186782837
Device: cuda
Iteration 0, Training loss = 0.9913459746883466
Iteration 10, Training loss = 0.3128003982397226
Iteration 20, Training loss = 0.2931256794299071
Iteration 30, Training loss = 0.2846955583932308
Iteration 40, Training loss = 0.27708644792437553
Iteration 50, Training loss = 0.2693425767983382
Iteration 60, Training loss = 0.26326389675243544
Iteration 70, Training loss = 0.2609220423664038
Iteration 80, Training loss = 0.25834853316728884
Iteration 90, Training loss = 0.25585089072298545
Iteration 100, Training loss = 0.25331515346009
Iteration 110, Training loss = 0.25257476634131026
Iteration 120, Training loss = 0.25034007086203647
Iteration 130, Training loss = 0.25005699866093123
Iteration 140, Training loss = 0.24889416605807269
Iteration 150, Training loss = 0.24619789550510737
Iteration 160, Training loss = 0.2456663601960127
Iteration 170, Training loss = 0.2462892601123223
Iteration 180, Training loss = 0.24220633406478626
Iteration 190, Training loss = 0.24056603379834157
Iteration 200, Training loss = 0.2393818831501099
Iteration 210, Training loss = 0.2380610044615773
Iteration 220, Training loss = 0.23656443499315244
Iteration 230, Training loss = 0.23515657840583187
Iteration 240, Training loss = 0.23464198804532105
Iteration 250, Training loss = 0.23443693968539053
Iteration 260, Training loss = 0.23425693430293065
Iteration 270, Training loss = 0.23213925628134838
Iteration 280, Training loss = 0.2295912947290792
Iteration 290, Training loss = 0.2291177733299824
Model training time: 55.721372842788696
Device: cuda
Iteration 0, Training loss = 0.9755095272110059
Iteration 10, Training loss = 0.3111180846507733
Iteration 20, Training loss = 0.29418563183683616
Iteration 30, Training loss = 0.2865880186168047
Iteration 40, Training loss = 0.27891735116449684
Iteration 50, Training loss = 0.27147096094603723
Iteration 60, Training loss = 0.27008765677993113
Iteration 70, Training loss = 0.26161816558585715
Iteration 80, Training loss = 0.2581917541817977
Iteration 90, Training loss = 0.2563247229330815
Iteration 100, Training loss = 0.25689817091020256
Iteration 110, Training loss = 0.2525822799652815
Iteration 120, Training loss = 0.25239913967939526
Iteration 130, Training loss = 0.2520895159015289
Iteration 140, Training loss = 0.24776164757517669
Iteration 150, Training loss = 0.24801842696391618
Iteration 160, Training loss = 0.24462707670262226
Iteration 170, Training loss = 0.2443654379592492
Iteration 180, Training loss = 0.2416916213069971
Iteration 190, Training loss = 0.2400984585715028
Iteration 200, Training loss = 0.23970711288543847
Iteration 210, Training loss = 0.23927482325010574
Iteration 220, Training loss = 0.2375870018194501
Iteration 230, Training loss = 0.23650901234493807
Iteration 240, Training loss = 0.2379059654015761
Iteration 250, Training loss = 0.2353808435683067
Iteration 260, Training loss = 0.2347351053299812
Iteration 270, Training loss = 0.23314750151565442
Iteration 280, Training loss = 0.23336067480536607
Iteration 290, Training loss = 0.23324704077094793
Model training time: 56.955647230148315
Device: cuda
Iteration 0, Training loss = 0.97963970211836
Iteration 10, Training loss = 0.3117479171890479
Iteration 20, Training loss = 0.29318110243632245
Iteration 30, Training loss = 0.2861565026239707
Iteration 40, Training loss = 0.2783142024507889
Iteration 50, Training loss = 0.2716259476370536
Iteration 60, Training loss = 0.26689290498884827
Iteration 70, Training loss = 0.2646179429900188
Iteration 80, Training loss = 0.2611651079585919
Iteration 90, Training loss = 0.25715501635120463
Iteration 100, Training loss = 0.25541087483557373
Iteration 110, Training loss = 0.25199875249885595
Iteration 120, Training loss = 0.25096329072347057
Iteration 130, Training loss = 0.24789191940082952
Iteration 140, Training loss = 0.24654590453092867
Iteration 150, Training loss = 0.24501418164716318
Iteration 160, Training loss = 0.24373448482499674
Iteration 170, Training loss = 0.24307739734649658
Iteration 180, Training loss = 0.24028768655485833
Iteration 190, Training loss = 0.2406194553925441
Iteration 200, Training loss = 0.23839507008401248
Iteration 210, Training loss = 0.2398312144124737
Iteration 220, Training loss = 0.23687570484784934
Iteration 230, Training loss = 0.23574720050853032
Iteration 240, Training loss = 0.23580835100549918
Iteration 250, Training loss = 0.2347110710464991
Iteration 260, Training loss = 0.23327622662943143
Iteration 270, Training loss = 0.23301776613180453
Iteration 280, Training loss = 0.23224431176025134
Iteration 290, Training loss = 0.23105482155313858
Model training time: 57.64441895484924
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0236398881444564
Iteration 10, Training loss = 0.9850558758928225
Iteration 20, Training loss = 0.977313237121472
Iteration 30, Training loss = 0.9813327668951108
Iteration 40, Training loss = 0.9678649747600923
Iteration 50, Training loss = 0.9622988855609527
Iteration 60, Training loss = 0.952652589059793
Iteration 70, Training loss = 0.9397863419010088
Iteration 80, Training loss = 0.9223450508255225
Iteration 90, Training loss = 0.9035353064537048
Iteration 100, Training loss = 0.8770163448957297
Iteration 110, Training loss = 0.8459189700392576
Iteration 120, Training loss = 0.8015589003379529
Iteration 130, Training loss = 0.7522556214378431
Iteration 140, Training loss = 0.6925038145138667
Iteration 150, Training loss = 0.6298888291303928
Iteration 160, Training loss = 0.5697344154692613
Iteration 170, Training loss = 0.5156989023089409
Iteration 180, Training loss = 0.47174603835894513
Iteration 190, Training loss = 0.43993867790469754
Iteration 200, Training loss = 0.41737833246588707
Iteration 210, Training loss = 0.40582748516820943
Iteration 220, Training loss = 0.3970292992889881
Iteration 230, Training loss = 0.38936903642920345
Iteration 240, Training loss = 0.38588199105400306
Iteration 250, Training loss = 0.3806521575897932
Iteration 260, Training loss = 0.3751659748645929
Iteration 270, Training loss = 0.37618048661030257
Iteration 280, Training loss = 0.3724780102761892
Iteration 290, Training loss = 0.36716329492628574
Model training time: 50.601303815841675
Device: cuda
Iteration 0, Training loss = 1.4910473628686025
Iteration 10, Training loss = 0.9969901855175312
Iteration 20, Training loss = 1.0002008516054888
Iteration 30, Training loss = 0.9982163762817016
Iteration 40, Training loss = 0.9949142812536313
Iteration 50, Training loss = 0.994880942771068
Iteration 60, Training loss = 0.9935264300841552
Iteration 70, Training loss = 0.9900333216557136
Iteration 80, Training loss = 0.9902057705017236
Iteration 90, Training loss = 0.9859318251793201
Iteration 100, Training loss = 0.9819561753135461
Iteration 110, Training loss = 0.9803916296133628
Iteration 120, Training loss = 0.9779200525237963
Iteration 130, Training loss = 0.9649885348402537
Iteration 140, Training loss = 0.9614331504473319
Iteration 150, Training loss = 0.9482687740371778
Iteration 160, Training loss = 0.934987353017697
Iteration 170, Training loss = 0.9170301419038039
Iteration 180, Training loss = 0.8905051482411531
Iteration 190, Training loss = 0.85901479021861
Iteration 200, Training loss = 0.8204758407977911
Iteration 210, Training loss = 0.7754184317130309
Iteration 220, Training loss = 0.7146522990212991
Iteration 230, Training loss = 0.6491490742908075
Iteration 240, Training loss = 0.5799464763930211
Iteration 250, Training loss = 0.5228466236820588
Iteration 260, Training loss = 0.4686208146695907
Iteration 270, Training loss = 0.43323021124188715
Iteration 280, Training loss = 0.4061782950392136
Iteration 290, Training loss = 0.39093367440196186
Model training time: 50.6314001083374
Device: cuda
Iteration 0, Training loss = 1.0207136703225284
Iteration 10, Training loss = 0.9939382064800996
Iteration 20, Training loss = 0.9930601859321961
Iteration 30, Training loss = 0.9881209748295637
Iteration 40, Training loss = 0.9916353523731232
Iteration 50, Training loss = 0.9855850969369595
Iteration 60, Training loss = 0.979175794009979
Iteration 70, Training loss = 0.9749703487524619
Iteration 80, Training loss = 0.9686432480812073
Iteration 90, Training loss = 0.9607001181978446
Iteration 100, Training loss = 0.9523088083817408
Iteration 110, Training loss = 0.9390968021291953
Iteration 120, Training loss = 0.9271158874034882
Iteration 130, Training loss = 0.9008485468534323
Iteration 140, Training loss = 0.873230445270355
Iteration 150, Training loss = 0.8377039954066277
Iteration 160, Training loss = 0.7934373422310903
Iteration 170, Training loss = 0.7381590868418033
Iteration 180, Training loss = 0.6788285950628611
Iteration 190, Training loss = 0.6078536541989217
Iteration 200, Training loss = 0.5437692753397502
Iteration 210, Training loss = 0.4910642120700616
Iteration 220, Training loss = 0.44288118900014806
Iteration 230, Training loss = 0.41509432130708146
Iteration 240, Training loss = 0.3957175085177788
Iteration 250, Training loss = 0.38309590676083016
Iteration 260, Training loss = 0.37811967300680965
Iteration 270, Training loss = 0.3687664703107797
Iteration 280, Training loss = 0.3654623149106136
Iteration 290, Training loss = 0.36145302610328567
Model training time: 50.72975254058838
Device: cuda
Iteration 0, Training loss = 1.045133613049984
Iteration 10, Training loss = 0.9868160698276299
Iteration 20, Training loss = 0.9824760693770188
Iteration 30, Training loss = 0.9809864948575313
Iteration 40, Training loss = 0.9726917296648026
Iteration 50, Training loss = 0.9640011065281354
Iteration 60, Training loss = 0.954555957363202
Iteration 70, Training loss = 0.9451978200903306
Iteration 80, Training loss = 0.9274661096815879
Iteration 90, Training loss = 0.9143079347335376
Iteration 100, Training loss = 0.8917904456074421
Iteration 110, Training loss = 0.8589788308510413
Iteration 120, Training loss = 0.819864979157081
Iteration 130, Training loss = 0.7716202913568571
Iteration 140, Training loss = 0.7167123957322195
Iteration 150, Training loss = 0.6521340777667669
Iteration 160, Training loss = 0.5877609241467255
Iteration 170, Training loss = 0.5306333097127768
Iteration 180, Training loss = 0.48049170781786626
Iteration 190, Training loss = 0.44751393451140475
Iteration 200, Training loss = 0.4195410811270659
Iteration 210, Training loss = 0.4054658074791615
Iteration 220, Training loss = 0.3947257108699817
Iteration 230, Training loss = 0.38846539935240376
Iteration 240, Training loss = 0.384130398527934
Iteration 250, Training loss = 0.38119743893352837
Iteration 260, Training loss = 0.37733998292913806
Iteration 270, Training loss = 0.37374528784018296
Iteration 280, Training loss = 0.3710452669228499
Iteration 290, Training loss = 0.3680860319962868
Model training time: 50.271674156188965
Device: cuda
Iteration 0, Training loss = 1.0517126356179898
Iteration 10, Training loss = 1.002282722065082
Iteration 20, Training loss = 0.9929509168634048
Iteration 30, Training loss = 0.9855717586783262
Iteration 40, Training loss = 0.980810250227268
Iteration 50, Training loss = 0.9781466304109647
Iteration 60, Training loss = 0.9739180023853595
Iteration 70, Training loss = 0.9624666004226758
Iteration 80, Training loss = 0.956413715504683
Iteration 90, Training loss = 0.9402380998318012
Iteration 100, Training loss = 0.9239907407989869
Iteration 110, Training loss = 0.9033697310548562
Iteration 120, Training loss = 0.872214783842747
Iteration 130, Training loss = 0.8368855594442441
Iteration 140, Training loss = 0.7882423532696871
Iteration 150, Training loss = 0.7332218598860961
Iteration 160, Training loss = 0.6624622029753832
Iteration 170, Training loss = 0.5889235528615805
Iteration 180, Training loss = 0.5244039136629838
Iteration 190, Training loss = 0.46898405024638545
Iteration 200, Training loss = 0.4271661533186069
Iteration 210, Training loss = 0.4011584253838429
Iteration 220, Training loss = 0.38439226537369764
Iteration 230, Training loss = 0.3737574196778811
Iteration 240, Training loss = 0.37030510575725484
Iteration 250, Training loss = 0.36624418972776485
Iteration 260, Training loss = 0.3609168129758193
Iteration 270, Training loss = 0.35737002067841017
Iteration 280, Training loss = 0.35468958934339195
Iteration 290, Training loss = 0.35238719402024377
Model training time: 50.91295838356018
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0910541352171164
Iteration 10, Training loss = 0.3633854046750527
Iteration 20, Training loss = 0.3064253721386194
Iteration 30, Training loss = 0.29075002999833
Iteration 40, Training loss = 0.28149197007027954
Iteration 50, Training loss = 0.2758941653256233
Iteration 60, Training loss = 0.27080115489661694
Iteration 70, Training loss = 0.26843941311996716
Iteration 80, Training loss = 0.26580122786645705
Iteration 90, Training loss = 0.2641650042854823
Iteration 100, Training loss = 0.2612756768671366
Iteration 110, Training loss = 0.26308544281010443
Iteration 120, Training loss = 0.2590280261177283
Iteration 130, Training loss = 0.2594347748045738
Iteration 140, Training loss = 0.2573830880797826
Iteration 150, Training loss = 0.2554129674457587
Iteration 160, Training loss = 0.25648099613877445
Iteration 170, Training loss = 0.2550928404984566
Iteration 180, Training loss = 0.25308187463535714
Iteration 190, Training loss = 0.25323902084850347
Iteration 200, Training loss = 0.25327825216719735
Iteration 210, Training loss = 0.2515131434282431
Iteration 220, Training loss = 0.2536425500248487
Iteration 230, Training loss = 0.25024528686816877
Iteration 240, Training loss = 0.2510620028926776
Iteration 250, Training loss = 0.24917163391812489
Iteration 260, Training loss = 0.24927046665778527
Iteration 270, Training loss = 0.24878495243879464
Iteration 280, Training loss = 0.24949443569550148
Iteration 290, Training loss = 0.2500556932332424
Model training time: 57.11080861091614
Device: cuda
Iteration 0, Training loss = 1.0514825209975243
Iteration 10, Training loss = 0.33629265527885693
Iteration 20, Training loss = 0.30373496003448963
Iteration 30, Training loss = 0.29149515425356537
Iteration 40, Training loss = 0.28122544202667016
Iteration 50, Training loss = 0.27350759534881663
Iteration 60, Training loss = 0.26828012328881484
Iteration 70, Training loss = 0.2649331744760275
Iteration 80, Training loss = 0.26211988997574037
Iteration 90, Training loss = 0.26108494186057496
Iteration 100, Training loss = 0.26048833704911745
Iteration 110, Training loss = 0.25706766502788436
Iteration 120, Training loss = 0.2555174303169434
Iteration 130, Training loss = 0.2547304561504951
Iteration 140, Training loss = 0.2546577128366782
Iteration 150, Training loss = 0.25375830203008193
Iteration 160, Training loss = 0.25216457519966823
Iteration 170, Training loss = 0.250598347101074
Iteration 180, Training loss = 0.24958272865758493
Iteration 190, Training loss = 0.24944730289280415
Iteration 200, Training loss = 0.2512126980492702
Iteration 210, Training loss = 0.24719538253087264
Iteration 220, Training loss = 0.24780382115680438
Iteration 230, Training loss = 0.24650717856219181
Iteration 240, Training loss = 0.2471049936631551
Iteration 250, Training loss = 0.2502831161881869
Iteration 260, Training loss = 0.24460736418572757
Iteration 270, Training loss = 0.24422727295985588
Iteration 280, Training loss = 0.24384366921507394
Iteration 290, Training loss = 0.2422911420177955
Model training time: 57.60489535331726
Device: cuda
Iteration 0, Training loss = 0.9909782593066876
Iteration 10, Training loss = 0.3298134052982697
Iteration 20, Training loss = 0.29833416411509883
Iteration 30, Training loss = 0.2879809592492305
Iteration 40, Training loss = 0.2821490644262387
Iteration 50, Training loss = 0.27827094968121785
Iteration 60, Training loss = 0.27409622417046475
Iteration 70, Training loss = 0.2728887206086746
Iteration 80, Training loss = 0.26614553968493754
Iteration 90, Training loss = 0.2638147540677052
Iteration 100, Training loss = 0.26259683760312885
Iteration 110, Training loss = 0.2589217738176768
Iteration 120, Training loss = 0.2586344126611948
Iteration 130, Training loss = 0.25523551331403166
Iteration 140, Training loss = 0.2531891559752134
Iteration 150, Training loss = 0.25138361350848126
Iteration 160, Training loss = 0.25071159248741776
Iteration 170, Training loss = 0.24817215909178442
Iteration 180, Training loss = 0.24772947410551402
Iteration 190, Training loss = 0.24716266996871966
Iteration 200, Training loss = 0.24586022043457398
Iteration 210, Training loss = 0.24390121621008104
Iteration 220, Training loss = 0.243626837260448
Iteration 230, Training loss = 0.24182450757003748
Iteration 240, Training loss = 0.23996771356234184
Iteration 250, Training loss = 0.24013307217795116
Iteration 260, Training loss = 0.2390523603042731
Iteration 270, Training loss = 0.23848334728525236
Iteration 280, Training loss = 0.23801031078283602
Iteration 290, Training loss = 0.2375392226072458
Model training time: 56.5279004573822
Device: cuda
Iteration 0, Training loss = 0.9929044854182464
Iteration 10, Training loss = 0.35199430427299094
Iteration 20, Training loss = 0.30393125088169026
Iteration 30, Training loss = 0.2885306521963615
Iteration 40, Training loss = 0.2791257973473806
Iteration 50, Training loss = 0.2717339008186872
Iteration 60, Training loss = 0.26883925296939337
Iteration 70, Training loss = 0.26575441429248226
Iteration 80, Training loss = 0.26300297777813214
Iteration 90, Training loss = 0.25983766036537975
Iteration 100, Training loss = 0.2595504288776563
Iteration 110, Training loss = 0.2570066942045322
Iteration 120, Training loss = 0.2549388834203665
Iteration 130, Training loss = 0.2524451924344668
Iteration 140, Training loss = 0.2538818128120441
Iteration 150, Training loss = 0.25157989819462484
Iteration 160, Training loss = 0.2523870834937462
Iteration 170, Training loss = 0.24825370555313733
Iteration 180, Training loss = 0.24783337983087853
Iteration 190, Training loss = 0.24691151541012984
Iteration 200, Training loss = 0.24614396376105455
Iteration 210, Training loss = 0.2440545349740065
Iteration 220, Training loss = 0.2441135673568799
Iteration 230, Training loss = 0.2425770927220583
Iteration 240, Training loss = 0.24190977015174353
Iteration 250, Training loss = 0.2406556521757291
Iteration 260, Training loss = 0.23962715454399586
Iteration 270, Training loss = 0.2384629719532453
Iteration 280, Training loss = 0.23748421740646547
Iteration 290, Training loss = 0.23666824486393195
Model training time: 56.24665117263794
Device: cuda
Iteration 0, Training loss = 0.9936650263575407
Iteration 10, Training loss = 0.34685060582481897
Iteration 20, Training loss = 0.29977520655554074
Iteration 30, Training loss = 0.2852366085235889
Iteration 40, Training loss = 0.27850471866818577
Iteration 50, Training loss = 0.27188067897581136
Iteration 60, Training loss = 0.2700399783654855
Iteration 70, Training loss = 0.26546413336808866
Iteration 80, Training loss = 0.2642825980885671
Iteration 90, Training loss = 0.2619564821227239
Iteration 100, Training loss = 0.2612414232526834
Iteration 110, Training loss = 0.2595934382138344
Iteration 120, Training loss = 0.25750932183403236
Iteration 130, Training loss = 0.2563462432903739
Iteration 140, Training loss = 0.256460672745911
Iteration 150, Training loss = 0.2545793993541828
Iteration 160, Training loss = 0.25374627801088184
Iteration 170, Training loss = 0.25239532641493356
Iteration 180, Training loss = 0.2525376339371388
Iteration 190, Training loss = 0.25059888354287696
Iteration 200, Training loss = 0.25064357914603674
Iteration 210, Training loss = 0.24844641997837102
Iteration 220, Training loss = 0.25034621802087015
Iteration 230, Training loss = 0.24784476252702567
Iteration 240, Training loss = 0.24928601530308908
Iteration 250, Training loss = 0.2467155000911309
Iteration 260, Training loss = 0.24648194287258846
Iteration 270, Training loss = 0.2476929145363661
Iteration 280, Training loss = 0.24424021891676462
Iteration 290, Training loss = 0.2434832162152116
Model training time: 56.851001262664795
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.031732293848808
Iteration 10, Training loss = 0.9945318567065092
Iteration 20, Training loss = 0.9924869864032819
Iteration 30, Training loss = 0.9934588252351835
Iteration 40, Training loss = 0.984148285136773
Iteration 50, Training loss = 0.9875711861711282
Iteration 60, Training loss = 0.9846787034318998
Iteration 70, Training loss = 0.971396301801388
Iteration 80, Training loss = 0.9693472110308133
Iteration 90, Training loss = 0.9564229539380624
Iteration 100, Training loss = 0.9568390846252441
Iteration 110, Training loss = 0.9368726869042103
Iteration 120, Training loss = 0.926182261453225
Iteration 130, Training loss = 0.9100258671320401
Iteration 140, Training loss = 0.8882727823578395
Iteration 150, Training loss = 0.8608682894935975
Iteration 160, Training loss = 0.829961071220728
Iteration 170, Training loss = 0.7945445798910581
Iteration 180, Training loss = 0.7511970337766868
Iteration 190, Training loss = 0.6973953138177211
Iteration 200, Training loss = 0.6426309731144172
Iteration 210, Training loss = 0.5863267203363088
Iteration 220, Training loss = 0.5335451897520286
Iteration 230, Training loss = 0.4886930940242914
Iteration 240, Training loss = 0.4530177150781338
Iteration 250, Training loss = 0.4263175310423741
Iteration 260, Training loss = 0.40666066482663155
Iteration 270, Training loss = 0.39251929550216746
Iteration 280, Training loss = 0.38712399567549044
Iteration 290, Training loss = 0.3756817997648166
Model training time: 51.41433119773865
Device: cuda
Iteration 0, Training loss = 1.0185306994960859
Iteration 10, Training loss = 1.0052910447120667
Iteration 20, Training loss = 1.000824619944279
Iteration 30, Training loss = 1.0001037332874079
Iteration 40, Training loss = 1.0040832196290677
Iteration 50, Training loss = 0.9979822354821059
Iteration 60, Training loss = 0.9969541029288218
Iteration 70, Training loss = 0.9981563882185862
Iteration 80, Training loss = 0.9991498010662886
Iteration 90, Training loss = 0.9960816949605942
Iteration 100, Training loss = 0.9920455641471423
Iteration 110, Training loss = 0.9923077443471322
Iteration 120, Training loss = 0.9892077296972275
Iteration 130, Training loss = 0.987452262869248
Iteration 140, Training loss = 0.9823040990875318
Iteration 150, Training loss = 0.9822031379892275
Iteration 160, Training loss = 0.9805363044142723
Iteration 170, Training loss = 0.97692288343723
Iteration 180, Training loss = 0.9752449416197263
Iteration 190, Training loss = 0.9718750474544672
Iteration 200, Training loss = 0.9711046579938668
Iteration 210, Training loss = 0.9641691870414294
Iteration 220, Training loss = 0.9555379427396334
Iteration 230, Training loss = 0.9487298090870564
Iteration 240, Training loss = 0.9402223481581762
Iteration 250, Training loss = 0.9388859965480291
Iteration 260, Training loss = 0.9259302570269659
Iteration 270, Training loss = 0.9050858519398249
Iteration 280, Training loss = 0.8886664028351123
Iteration 290, Training loss = 0.8731581975634282
Model training time: 50.750511169433594
Device: cuda
Iteration 0, Training loss = 1.021336780144618
Iteration 10, Training loss = 1.0004306085980856
Iteration 20, Training loss = 1.0001670815623724
Iteration 30, Training loss = 0.9915326260603391
Iteration 40, Training loss = 0.9885149070849786
Iteration 50, Training loss = 0.9836750443165119
Iteration 60, Training loss = 0.9773095032343497
Iteration 70, Training loss = 0.9696907853850951
Iteration 80, Training loss = 0.9659708090699636
Iteration 90, Training loss = 0.949511003608887
Iteration 100, Training loss = 0.935622095488585
Iteration 110, Training loss = 0.9216161863161967
Iteration 120, Training loss = 0.8978723138570786
Iteration 130, Training loss = 0.8676781740326148
Iteration 140, Training loss = 0.8312715698893254
Iteration 150, Training loss = 0.78505344918141
Iteration 160, Training loss = 0.731546316582423
Iteration 170, Training loss = 0.6714233953792316
Iteration 180, Training loss = 0.6038156168965193
Iteration 190, Training loss = 0.5433888578644166
Iteration 200, Training loss = 0.48816653126134324
Iteration 210, Training loss = 0.4477598830484427
Iteration 220, Training loss = 0.42098524450109553
Iteration 230, Training loss = 0.39856563250605875
Iteration 240, Training loss = 0.38638652803806156
Iteration 250, Training loss = 0.3784048654712163
Iteration 260, Training loss = 0.3721725753007027
Iteration 270, Training loss = 0.3709385387886029
Iteration 280, Training loss = 0.36407931851079833
Iteration 290, Training loss = 0.36203149835077614
Model training time: 51.215856313705444
Device: cuda
Iteration 0, Training loss = 1.205407130603607
Iteration 10, Training loss = 0.9975722437867751
Iteration 20, Training loss = 1.0003401551109095
Iteration 30, Training loss = 0.9979123186606628
Iteration 40, Training loss = 0.9988399388698431
Iteration 50, Training loss = 0.9935167300013396
Iteration 60, Training loss = 0.9949491579945271
Iteration 70, Training loss = 0.9950461691388717
Iteration 80, Training loss = 0.9921122262111077
Iteration 90, Training loss = 0.9964840257397065
Iteration 100, Training loss = 0.9903744522195596
Iteration 110, Training loss = 0.9896636066528467
Iteration 120, Training loss = 0.9892373027709814
Iteration 130, Training loss = 0.9853358091070101
Iteration 140, Training loss = 0.9820748235170658
Iteration 150, Training loss = 0.9776769142884475
Iteration 160, Training loss = 0.9774103491352155
Iteration 170, Training loss = 0.97220695993075
Iteration 180, Training loss = 0.9613121942831919
Iteration 190, Training loss = 0.9580572132880871
Iteration 200, Training loss = 0.9457284899858328
Iteration 210, Training loss = 0.9315696123700875
Iteration 220, Training loss = 0.9204515786125109
Iteration 230, Training loss = 0.8992496774746821
Iteration 240, Training loss = 0.8737829172840486
Iteration 250, Training loss = 0.8418531538202212
Iteration 260, Training loss = 0.8008690940646025
Iteration 270, Training loss = 0.7538433109338467
Iteration 280, Training loss = 0.6994438847670188
Iteration 290, Training loss = 0.6397914095566823
Model training time: 52.07440686225891
Device: cuda
Iteration 0, Training loss = 1.0936676728037686
Iteration 10, Training loss = 1.0053131631933725
Iteration 20, Training loss = 1.006268805036178
Iteration 30, Training loss = 0.9974879255661597
Iteration 40, Training loss = 0.99596651414266
Iteration 50, Training loss = 0.992612650188116
Iteration 60, Training loss = 0.995231141264622
Iteration 70, Training loss = 0.9916068808390543
Iteration 80, Training loss = 0.9870085475536493
Iteration 90, Training loss = 0.9890196031102767
Iteration 100, Training loss = 0.9804868216697986
Iteration 110, Training loss = 0.9808967302624996
Iteration 120, Training loss = 0.9750261060320414
Iteration 130, Training loss = 0.9670359096848048
Iteration 140, Training loss = 0.9600839809729502
Iteration 150, Training loss = 0.9566276044799731
Iteration 160, Training loss = 0.9410603178235201
Iteration 170, Training loss = 0.9309308425738261
Iteration 180, Training loss = 0.9138373956084251
Iteration 190, Training loss = 0.8941020363798509
Iteration 200, Training loss = 0.8719021070462006
Iteration 210, Training loss = 0.8370964842346998
Iteration 220, Training loss = 0.8061461351238765
Iteration 230, Training loss = 0.7550716236806833
Iteration 240, Training loss = 0.7024394855476342
Iteration 250, Training loss = 0.6472655027531661
Iteration 260, Training loss = 0.5867068776144431
Iteration 270, Training loss = 0.5330826700306855
Iteration 280, Training loss = 0.4844522928962341
Iteration 290, Training loss = 0.4437812220018644
Model training time: 54.85263133049011
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0420743668308625
Iteration 10, Training loss = 0.322313224323667
Iteration 20, Training loss = 0.2993702877026338
Iteration 30, Training loss = 0.29034120947695696
Iteration 40, Training loss = 0.2818085929522148
Iteration 50, Training loss = 0.27647361230964845
Iteration 60, Training loss = 0.2680617022113158
Iteration 70, Training loss = 0.265453320569717
Iteration 80, Training loss = 0.2630149611773399
Iteration 90, Training loss = 0.2613965611045177
Iteration 100, Training loss = 0.2580060302638091
Iteration 110, Training loss = 0.25649881921708584
Iteration 120, Training loss = 0.2565065593673633
Iteration 130, Training loss = 0.2534521262233074
Iteration 140, Training loss = 0.25280240937494314
Iteration 150, Training loss = 0.25186197416713607
Iteration 160, Training loss = 0.2528760524896475
Iteration 170, Training loss = 0.24882190402310628
Iteration 180, Training loss = 0.24766029546467158
Iteration 190, Training loss = 0.24612327450169966
Iteration 200, Training loss = 0.24647217902999657
Iteration 210, Training loss = 0.24589940246481162
Iteration 220, Training loss = 0.24498711469081733
Iteration 230, Training loss = 0.24348641129640433
Iteration 240, Training loss = 0.2433025255226172
Iteration 250, Training loss = 0.2428142802359966
Iteration 260, Training loss = 0.24119172173623854
Iteration 270, Training loss = 0.23999418733784786
Iteration 280, Training loss = 0.24203419900284365
Iteration 290, Training loss = 0.23824783063565308
Model training time: 60.950220823287964
Device: cuda
Iteration 0, Training loss = 1.0083500863267825
Iteration 10, Training loss = 0.3179448659603412
Iteration 20, Training loss = 0.3013282287865877
Iteration 30, Training loss = 0.29098199057177854
Iteration 40, Training loss = 0.28430890377897483
Iteration 50, Training loss = 0.27652088094216126
Iteration 60, Training loss = 0.27230681092120135
Iteration 70, Training loss = 0.2680675450425882
Iteration 80, Training loss = 0.26527548538377654
Iteration 90, Training loss = 0.261797741198769
Iteration 100, Training loss = 0.2588900248161875
Iteration 110, Training loss = 0.25675476106026995
Iteration 120, Training loss = 0.25334179508858
Iteration 130, Training loss = 0.25298128176767093
Iteration 140, Training loss = 0.25272473520957506
Iteration 150, Training loss = 0.24961184051174384
Iteration 160, Training loss = 0.24799800119720972
Iteration 170, Training loss = 0.2467803253004184
Iteration 180, Training loss = 0.2459207374889117
Iteration 190, Training loss = 0.24619337558173215
Iteration 200, Training loss = 0.24394349023126638
Iteration 210, Training loss = 0.242713768250094
Iteration 220, Training loss = 0.24204754686126342
Iteration 230, Training loss = 0.24246687012223098
Iteration 240, Training loss = 0.24349167914344713
Iteration 250, Training loss = 0.23954752474450147
Iteration 260, Training loss = 0.2396091311596907
Iteration 270, Training loss = 0.2384947148653177
Iteration 280, Training loss = 0.23835685118459737
Iteration 290, Training loss = 0.23577519570692226
Model training time: 57.54910945892334
Device: cuda
Iteration 0, Training loss = 1.2186749921395228
Iteration 10, Training loss = 0.37678431232388204
Iteration 20, Training loss = 0.3026640767661425
Iteration 30, Training loss = 0.2848948218788092
Iteration 40, Training loss = 0.27685048115941197
Iteration 50, Training loss = 0.2708934066960445
Iteration 60, Training loss = 0.2670057520556908
Iteration 70, Training loss = 0.2656495382006352
Iteration 80, Training loss = 0.262605850943006
Iteration 90, Training loss = 0.2655719189116588
Iteration 100, Training loss = 0.2584537756271087
Iteration 110, Training loss = 0.2566956293124419
Iteration 120, Training loss = 0.2557978228880809
Iteration 130, Training loss = 0.25458787696865887
Iteration 140, Training loss = 0.2540273845482331
Iteration 150, Training loss = 0.25175871728704524
Iteration 160, Training loss = 0.2489382831666332
Iteration 170, Training loss = 0.2491662295965048
Iteration 180, Training loss = 0.24918383374237096
Iteration 190, Training loss = 0.2467464661368957
Iteration 200, Training loss = 0.24778522207186773
Iteration 210, Training loss = 0.24483319701483616
Iteration 220, Training loss = 0.24295329309713382
Iteration 230, Training loss = 0.24227206222712994
Iteration 240, Training loss = 0.2423038059988847
Iteration 250, Training loss = 0.24025439356382078
Iteration 260, Training loss = 0.24136172485752747
Iteration 270, Training loss = 0.23843653729328743
Iteration 280, Training loss = 0.2381526931690482
Iteration 290, Training loss = 0.23615339770913124
Model training time: 57.028403520584106
Device: cuda
Iteration 0, Training loss = 0.9966411292552948
Iteration 10, Training loss = 0.3352958865177173
Iteration 20, Training loss = 0.29827912696279013
Iteration 30, Training loss = 0.28625555558559984
Iteration 40, Training loss = 0.28011190919921947
Iteration 50, Training loss = 0.27589600103405804
Iteration 60, Training loss = 0.2732325901205723
Iteration 70, Training loss = 0.2699537691302024
Iteration 80, Training loss = 0.26744826840093505
Iteration 90, Training loss = 0.2661682701168152
Iteration 100, Training loss = 0.2657137752438967
Iteration 110, Training loss = 0.26460692214851195
Iteration 120, Training loss = 0.26212695240974426
Iteration 130, Training loss = 0.26079855701671195
Iteration 140, Training loss = 0.2593921788323384
Iteration 150, Training loss = 0.25765785488944787
Iteration 160, Training loss = 0.2570788845993005
Iteration 170, Training loss = 0.25636136374221397
Iteration 180, Training loss = 0.2551863712186997
Iteration 190, Training loss = 0.2554622587676232
Iteration 200, Training loss = 0.25402871462015003
Iteration 210, Training loss = 0.2521047264050979
Iteration 220, Training loss = 0.25244592345104766
Iteration 230, Training loss = 0.25372573991234487
Iteration 240, Training loss = 0.25041952607436824
Iteration 250, Training loss = 0.25029934154680145
Iteration 260, Training loss = 0.2496270898443002
Iteration 270, Training loss = 0.24804048075412327
Iteration 280, Training loss = 0.24835910906012243
Iteration 290, Training loss = 0.2479659361908069
Model training time: 56.663705587387085
Device: cuda
Iteration 0, Training loss = 0.9962812622006123
Iteration 10, Training loss = 0.31383620331493706
Iteration 20, Training loss = 0.29440542940910047
Iteration 30, Training loss = 0.28400691942526746
Iteration 40, Training loss = 0.27590086984519774
Iteration 50, Training loss = 0.2699345378921582
Iteration 60, Training loss = 0.2662421026482032
Iteration 70, Training loss = 0.26367695901829463
Iteration 80, Training loss = 0.25916945110433376
Iteration 90, Training loss = 0.2583259978833107
Iteration 100, Training loss = 0.25600617421934235
Iteration 110, Training loss = 0.2544609432896742
Iteration 120, Training loss = 0.25311681914788026
Iteration 130, Training loss = 0.2528204913609303
Iteration 140, Training loss = 0.2508411000554378
Iteration 150, Training loss = 0.25043363432185006
Iteration 160, Training loss = 0.2490356070204423
Iteration 170, Training loss = 0.24948894877273303
Iteration 180, Training loss = 0.24716718265643486
Iteration 190, Training loss = 0.24712610158782738
Iteration 200, Training loss = 0.2463179540175658
Iteration 210, Training loss = 0.2455745439689893
Iteration 220, Training loss = 0.24370622391311023
Iteration 230, Training loss = 0.2433688279527884
Iteration 240, Training loss = 0.24159230012446642
Iteration 250, Training loss = 0.24212925580258554
Iteration 260, Training loss = 0.24077522811981347
Iteration 270, Training loss = 0.239515872242359
Iteration 280, Training loss = 0.23914764577952716
Iteration 290, Training loss = 0.23828055272595242
Model training time: 56.30125975608826
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9938612918441112
Iteration 10, Training loss = 0.9929231410989394
Iteration 20, Training loss = 0.9873438947475873
Iteration 30, Training loss = 0.9807958909525321
Iteration 40, Training loss = 0.9794985239322369
Iteration 50, Training loss = 0.9782025269590892
Iteration 60, Training loss = 0.967692990715687
Iteration 70, Training loss = 0.9632680547925142
Iteration 80, Training loss = 0.9495882609715829
Iteration 90, Training loss = 0.9373560507710164
Iteration 100, Training loss = 0.9220404235216287
Iteration 110, Training loss = 0.900203906572782
Iteration 120, Training loss = 0.8720035948432409
Iteration 130, Training loss = 0.8374730583566886
Iteration 140, Training loss = 0.7995872308428471
Iteration 150, Training loss = 0.7394336751447275
Iteration 160, Training loss = 0.6794451225835544
Iteration 170, Training loss = 0.6144825317538701
Iteration 180, Training loss = 0.5542351001730332
Iteration 190, Training loss = 0.501233397768094
Iteration 200, Training loss = 0.45933830394194675
Iteration 210, Training loss = 0.4293310112104966
Iteration 220, Training loss = 0.4122527869274983
Iteration 230, Training loss = 0.39976966524353397
Iteration 240, Training loss = 0.39047420110840064
Iteration 250, Training loss = 0.38519766468268174
Iteration 260, Training loss = 0.3815222492871376
Iteration 270, Training loss = 0.3774410609442454
Iteration 280, Training loss = 0.3742560214148118
Iteration 290, Training loss = 0.3720840699970722
Model training time: 51.046234130859375
Device: cuda
Iteration 0, Training loss = 1.0086966930673673
Iteration 10, Training loss = 0.9940773357565587
Iteration 20, Training loss = 0.988383140701514
Iteration 30, Training loss = 0.9810722957436855
Iteration 40, Training loss = 0.9780925592550864
Iteration 50, Training loss = 0.9685671851038933
Iteration 60, Training loss = 0.9601313362901027
Iteration 70, Training loss = 0.9528287488680619
Iteration 80, Training loss = 0.9379436815014253
Iteration 90, Training loss = 0.9171050706735024
Iteration 100, Training loss = 0.8954353854060173
Iteration 110, Training loss = 0.8700966863678052
Iteration 120, Training loss = 0.8310917037037703
Iteration 130, Training loss = 0.7883004970275439
Iteration 140, Training loss = 0.7286065243757688
Iteration 150, Training loss = 0.6657126660530384
Iteration 160, Training loss = 0.6004345531647022
Iteration 170, Training loss = 0.5351194939934291
Iteration 180, Training loss = 0.48353173211216927
Iteration 190, Training loss = 0.4401594549417496
Iteration 200, Training loss = 0.4107542618249471
Iteration 210, Training loss = 0.3958158395611323
Iteration 220, Training loss = 0.38342913856300026
Iteration 230, Training loss = 0.37614153454510063
Iteration 240, Training loss = 0.3720895726806842
Iteration 250, Training loss = 0.3669255261237805
Iteration 260, Training loss = 0.36275426441660297
Iteration 270, Training loss = 0.3605908207022227
Iteration 280, Training loss = 0.3581977920750013
Iteration 290, Training loss = 0.3549233620556501
Model training time: 51.5119526386261
Device: cuda
Iteration 0, Training loss = 1.0051008715079381
Iteration 10, Training loss = 0.9965102684039336
Iteration 20, Training loss = 1.0048628650032556
Iteration 30, Training loss = 0.9999647616193845
Iteration 40, Training loss = 0.9951511197365247
Iteration 50, Training loss = 0.9901142441309415
Iteration 60, Training loss = 0.9882149054453924
Iteration 70, Training loss = 0.9862831515761522
Iteration 80, Training loss = 0.9797673930342381
Iteration 90, Training loss = 0.9727624998642848
Iteration 100, Training loss = 0.9701780166763526
Iteration 110, Training loss = 0.965416377553573
Iteration 120, Training loss = 0.9566417210377179
Iteration 130, Training loss = 0.9437507551449996
Iteration 140, Training loss = 0.9361428366257594
Iteration 150, Training loss = 0.9157746376899573
Iteration 160, Training loss = 0.8907202075307186
Iteration 170, Training loss = 0.8644156261132314
Iteration 180, Training loss = 0.8320972415117117
Iteration 190, Training loss = 0.7830505210619706
Iteration 200, Training loss = 0.7336583613203123
Iteration 210, Training loss = 0.6683500810311391
Iteration 220, Training loss = 0.6087531200968302
Iteration 230, Training loss = 0.546684056520462
Iteration 240, Training loss = 0.4926714670772736
Iteration 250, Training loss = 0.44805175409867215
Iteration 260, Training loss = 0.4218007701520736
Iteration 270, Training loss = 0.40020979969547343
Iteration 280, Training loss = 0.3874819058065231
Iteration 290, Training loss = 0.37834488987349546
Model training time: 50.169312953948975
Device: cuda
Iteration 0, Training loss = 1.0766710937023163
Iteration 10, Training loss = 1.0021029269466033
Iteration 20, Training loss = 1.0033433483197138
Iteration 30, Training loss = 0.9995802600796406
Iteration 40, Training loss = 0.9998648052032177
Iteration 50, Training loss = 0.9978871185045975
Iteration 60, Training loss = 0.9958033441351011
Iteration 70, Training loss = 0.9915695024224428
Iteration 80, Training loss = 0.9920516174573165
Iteration 90, Training loss = 0.9894254815119964
Iteration 100, Training loss = 0.9870210759914838
Iteration 110, Training loss = 0.9833052129699633
Iteration 120, Training loss = 0.9762664752510878
Iteration 130, Training loss = 0.9720205039932177
Iteration 140, Training loss = 0.9672974520004712
Iteration 150, Training loss = 0.9581200056351148
Iteration 160, Training loss = 0.9499038818937081
Iteration 170, Training loss = 0.9381201330285805
Iteration 180, Training loss = 0.9171995984820219
Iteration 190, Training loss = 0.8976510109809729
Iteration 200, Training loss = 0.8687951490283012
Iteration 210, Training loss = 0.8347864494873927
Iteration 220, Training loss = 0.7897051280507674
Iteration 230, Training loss = 0.7373697144480852
Iteration 240, Training loss = 0.6738938674903833
Iteration 250, Training loss = 0.6102072725502344
Iteration 260, Training loss = 0.5472513520373747
Iteration 270, Training loss = 0.4902900721018131
Iteration 280, Training loss = 0.4498276306459537
Iteration 290, Training loss = 0.4187955799011084
Model training time: 50.21176743507385
Device: cuda
Iteration 0, Training loss = 1.0224350375624804
Iteration 10, Training loss = 1.002906386095744
Iteration 20, Training loss = 1.001502309854214
Iteration 30, Training loss = 0.9997094158942883
Iteration 40, Training loss = 0.9964474605826231
Iteration 50, Training loss = 0.9943835574847001
Iteration 60, Training loss = 0.9914019749714778
Iteration 70, Training loss = 0.9889917620099508
Iteration 80, Training loss = 0.9883765818981024
Iteration 90, Training loss = 0.9875311077787325
Iteration 100, Training loss = 0.9822300368776689
Iteration 110, Training loss = 0.9821260405274538
Iteration 120, Training loss = 0.9763375864579127
Iteration 130, Training loss = 0.9716988458083227
Iteration 140, Training loss = 0.965333104133606
Iteration 150, Training loss = 0.9547609449006044
Iteration 160, Training loss = 0.9509462123880019
Iteration 170, Training loss = 0.9333319188310549
Iteration 180, Training loss = 0.9186756834387779
Iteration 190, Training loss = 0.9045551734474989
Iteration 200, Training loss = 0.8789549567378484
Iteration 210, Training loss = 0.8507832821745139
Iteration 220, Training loss = 0.8171636576835926
Iteration 230, Training loss = 0.7744699206489783
Iteration 240, Training loss = 0.7194201805843756
Iteration 250, Training loss = 0.6652599937067583
Iteration 260, Training loss = 0.6114521247263138
Iteration 270, Training loss = 0.5580945318708053
Iteration 280, Training loss = 0.5126075475261762
Iteration 290, Training loss = 0.476026385736007
Model training time: 50.3289213180542
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9539707291584748
Iteration 10, Training loss = 0.29745111986994743
Iteration 20, Training loss = 0.277594251701465
Iteration 30, Training loss = 0.26891024654301315
Iteration 40, Training loss = 0.2625205025363427
Iteration 50, Training loss = 0.2579205341350574
Iteration 60, Training loss = 0.2513744137608088
Iteration 70, Training loss = 0.24889466352760792
Iteration 80, Training loss = 0.2435982500274594
Iteration 90, Training loss = 0.24132218770682812
Model training time: 19.063014268875122
Device: cuda
Iteration 0, Training loss = 0.9524936825037003
Iteration 10, Training loss = 0.2985872828329985
Iteration 20, Training loss = 0.27776690085346883
Iteration 30, Training loss = 0.26664647545952064
Iteration 40, Training loss = 0.2612955081634797
Iteration 50, Training loss = 0.2569759962363885
Iteration 60, Training loss = 0.2539873207704379
Iteration 70, Training loss = 0.25202671667704213
Iteration 80, Training loss = 0.2502149489636605
Iteration 90, Training loss = 0.24910533929673526
Model training time: 18.97399640083313
Device: cuda
Iteration 0, Training loss = 1.0374472800355692
Iteration 10, Training loss = 0.3001346436257546
Iteration 20, Training loss = 0.285774026495906
Iteration 30, Training loss = 0.27704608913224477
Iteration 40, Training loss = 0.2616220838748492
Iteration 50, Training loss = 0.2542529210734826
Iteration 60, Training loss = 0.24960274106034866
Iteration 70, Training loss = 0.24699415194873625
Iteration 80, Training loss = 0.24418881062704784
Iteration 90, Training loss = 0.24525943828316835
Model training time: 18.62354040145874
Device: cuda
Iteration 0, Training loss = 0.9561177612497256
Iteration 10, Training loss = 0.2955416736121361
Iteration 20, Training loss = 0.2740867215280349
Iteration 30, Training loss = 0.26387438049110085
Iteration 40, Training loss = 0.25872738673709905
Iteration 50, Training loss = 0.2554090702189849
Iteration 60, Training loss = 0.24977209361699912
Iteration 70, Training loss = 0.24614205082448629
Iteration 80, Training loss = 0.24262467119842768
Iteration 90, Training loss = 0.24069920941614187
Model training time: 18.79026746749878
Device: cuda
Iteration 0, Training loss = 0.9785423966554495
Iteration 10, Training loss = 0.29297470888839316
Iteration 20, Training loss = 0.2828320120103084
Iteration 30, Training loss = 0.26750826477431333
Iteration 40, Training loss = 0.2598761727030461
Iteration 50, Training loss = 0.25435840610701305
Iteration 60, Training loss = 0.2507259970387587
Iteration 70, Training loss = 0.2467929865591801
Iteration 80, Training loss = 0.2429571393160866
Iteration 90, Training loss = 0.24129247163923886
Model training time: 19.049420833587646
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1307464849490385
Iteration 10, Training loss = 0.997866731423598
Iteration 20, Training loss = 0.99061251947513
Iteration 30, Training loss = 0.9899986965151933
Iteration 40, Training loss = 0.9829789319863687
Iteration 50, Training loss = 0.9707685952576307
Iteration 60, Training loss = 0.9708148613572121
Iteration 70, Training loss = 0.9486021577165678
Iteration 80, Training loss = 0.9192311918506255
Iteration 90, Training loss = 0.8768079395477588
Model training time: 16.887887954711914
Device: cuda
Iteration 0, Training loss = 1.0340409152782881
Iteration 10, Training loss = 1.0019386210120642
Iteration 20, Training loss = 0.9914618082917653
Iteration 30, Training loss = 0.9982120068027422
Iteration 40, Training loss = 0.9840938695348226
Iteration 50, Training loss = 0.9782126242151628
Iteration 60, Training loss = 0.9631543216797022
Iteration 70, Training loss = 0.9456857786728785
Iteration 80, Training loss = 0.9188712755074868
Iteration 90, Training loss = 0.8730695075713671
Model training time: 17.118554830551147
Device: cuda
Iteration 0, Training loss = 1.0030339589485755
Iteration 10, Training loss = 0.995672538303412
Iteration 20, Training loss = 0.9848497641774324
Iteration 30, Training loss = 0.9795841265183228
Iteration 40, Training loss = 0.964560021001559
Iteration 50, Training loss = 0.9470195535283822
Iteration 60, Training loss = 0.9125916883349419
Iteration 70, Training loss = 0.8619065531171285
Iteration 80, Training loss = 0.7759545600185027
Iteration 90, Training loss = 0.6629933964174527
Model training time: 17.208927869796753
Device: cuda
Iteration 0, Training loss = 1.0081454996879284
Iteration 10, Training loss = 1.0046337774166694
Iteration 20, Training loss = 1.000743999504126
Iteration 30, Training loss = 0.9987771539733961
Iteration 40, Training loss = 1.0016214326024055
Iteration 50, Training loss = 0.9949593635705801
Iteration 60, Training loss = 0.9957367571500632
Iteration 70, Training loss = 0.986939593576468
Iteration 80, Training loss = 0.9841571874343432
Iteration 90, Training loss = 0.9755172207951546
Model training time: 17.18342614173889
Device: cuda
Iteration 0, Training loss = 1.004842141499886
Iteration 10, Training loss = 0.9905216923126807
Iteration 20, Training loss = 0.978424958884716
Iteration 30, Training loss = 0.9705227556136938
Iteration 40, Training loss = 0.9441786534511126
Iteration 50, Training loss = 0.9184260993049695
Iteration 60, Training loss = 0.8626985386587106
Iteration 70, Training loss = 0.7875713247519273
Iteration 80, Training loss = 0.6721372673144708
Iteration 90, Training loss = 0.5445086706716281
Model training time: 17.190119981765747
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0217655473030531
Iteration 10, Training loss = 0.30832809682648915
Iteration 20, Training loss = 0.2890432191869387
Iteration 30, Training loss = 0.2752437104399388
Iteration 40, Training loss = 0.26618551942878044
Iteration 50, Training loss = 0.263015025223677
Iteration 60, Training loss = 0.25915949132580024
Iteration 70, Training loss = 0.25601012646578825
Iteration 80, Training loss = 0.2551057060750631
Iteration 90, Training loss = 0.25304544760057557
Model training time: 19.3114070892334
Device: cuda
Iteration 0, Training loss = 1.024309525696131
Iteration 10, Training loss = 0.29985627202460396
Iteration 20, Training loss = 0.27895933489959973
Iteration 30, Training loss = 0.26700282662820357
Iteration 40, Training loss = 0.2618483938276768
Iteration 50, Training loss = 0.25917072551181686
Iteration 60, Training loss = 0.2563811685316838
Iteration 70, Training loss = 0.2548284063545557
Iteration 80, Training loss = 0.251962041339049
Iteration 90, Training loss = 0.2536284616933419
Model training time: 18.78224802017212
Device: cuda
Iteration 0, Training loss = 0.9626681134104729
Iteration 10, Training loss = 0.3018912880753095
Iteration 20, Training loss = 0.28304677069760287
Iteration 30, Training loss = 0.2707505519860066
Iteration 40, Training loss = 0.2684742430081734
Iteration 50, Training loss = 0.26093090039033157
Iteration 60, Training loss = 0.25749129658708203
Iteration 70, Training loss = 0.2556213876948907
Iteration 80, Training loss = 0.2547846499543924
Iteration 90, Training loss = 0.253456931704512
Model training time: 19.009753465652466
Device: cuda
Iteration 0, Training loss = 1.0205093931693296
Iteration 10, Training loss = 0.29899016638787895
Iteration 20, Training loss = 0.2809180681808637
Iteration 30, Training loss = 0.26939646469858974
Iteration 40, Training loss = 0.26290027209772515
Iteration 50, Training loss = 0.25806379303909266
Iteration 60, Training loss = 0.2539878962561488
Iteration 70, Training loss = 0.2507633541065913
Iteration 80, Training loss = 0.24623468109908012
Iteration 90, Training loss = 0.24460911306624228
Model training time: 18.572150945663452
Device: cuda
Iteration 0, Training loss = 0.9864162986095135
Iteration 10, Training loss = 0.29983422045524305
Iteration 20, Training loss = 0.28087829511899215
Iteration 30, Training loss = 0.26810745178506923
Iteration 40, Training loss = 0.26052014028223663
Iteration 50, Training loss = 0.2578735208282104
Iteration 60, Training loss = 0.25175825363168347
Iteration 70, Training loss = 0.2490262475151282
Iteration 80, Training loss = 0.2466187859670474
Iteration 90, Training loss = 0.24420758351110494
Model training time: 18.864177227020264
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0305499388621404
Iteration 10, Training loss = 0.9916445635832273
Iteration 20, Training loss = 0.9890864159052188
Iteration 30, Training loss = 0.9813793731423525
Iteration 40, Training loss = 0.9748386167562925
Iteration 50, Training loss = 0.9594641958291714
Iteration 60, Training loss = 0.9367052425558751
Iteration 70, Training loss = 0.9008930910092133
Iteration 80, Training loss = 0.8432098718789908
Iteration 90, Training loss = 0.7551252394914627
Model training time: 16.748809814453125
Device: cuda
Iteration 0, Training loss = 1.0049618751956866
Iteration 10, Training loss = 0.9971204921603203
Iteration 20, Training loss = 0.9918030735391837
Iteration 30, Training loss = 0.9882246920695672
Iteration 40, Training loss = 0.9831507887977821
Iteration 50, Training loss = 0.9766492952521031
Iteration 60, Training loss = 0.9702229637366074
Iteration 70, Training loss = 0.9492389811919286
Iteration 80, Training loss = 0.9231158512142988
Iteration 90, Training loss = 0.8837520950115644
Model training time: 16.76625919342041
Device: cuda
Iteration 0, Training loss = 1.1294901445508003
Iteration 10, Training loss = 0.9917112227815849
Iteration 20, Training loss = 0.98945114417718
Iteration 30, Training loss = 0.9882970790450389
Iteration 40, Training loss = 0.9815954485764871
Iteration 50, Training loss = 0.9704734671574372
Iteration 60, Training loss = 0.9615276690859061
Iteration 70, Training loss = 0.9440805894824175
Iteration 80, Training loss = 0.9068598432036546
Iteration 90, Training loss = 0.8588608830020978
Model training time: 16.8274142742157
Device: cuda
Iteration 0, Training loss = 1.0121397834557753
Iteration 10, Training loss = 0.9864466763459719
Iteration 20, Training loss = 0.9793505456585151
Iteration 30, Training loss = 0.9658170554500359
Iteration 40, Training loss = 0.9478406098026496
Iteration 50, Training loss = 0.9156161426351621
Iteration 60, Training loss = 0.8613705159379885
Iteration 70, Training loss = 0.7756989781673138
Iteration 80, Training loss = 0.6583345609788711
Iteration 90, Training loss = 0.5320088221476629
Model training time: 16.700950622558594
Device: cuda
Iteration 0, Training loss = 1.0055113268586306
Iteration 10, Training loss = 0.9932268771987695
Iteration 20, Training loss = 0.9907704087404104
Iteration 30, Training loss = 0.9896839679433749
Iteration 40, Training loss = 0.9804803912456219
Iteration 50, Training loss = 0.9730036293084805
Iteration 60, Training loss = 0.9575828795249646
Iteration 70, Training loss = 0.93478495742266
Iteration 80, Training loss = 0.897532561650643
Iteration 90, Training loss = 0.8401542283021487
Model training time: 17.00036883354187
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.008189014517344
Iteration 10, Training loss = 0.3017693001490373
Iteration 20, Training loss = 0.28850531348815334
Iteration 30, Training loss = 0.2771614959750038
Iteration 40, Training loss = 0.2673593098297715
Iteration 50, Training loss = 0.2642243759563336
Iteration 60, Training loss = 0.2607015109119507
Iteration 70, Training loss = 0.2574338073340746
Iteration 80, Training loss = 0.25687986340087193
Iteration 90, Training loss = 0.2507046052756218
Model training time: 18.716583013534546
Device: cuda
Iteration 0, Training loss = 0.9791751538331692
Iteration 10, Training loss = 0.29944331170274663
Iteration 20, Training loss = 0.2858403522807818
Iteration 30, Training loss = 0.2752357510706553
Iteration 40, Training loss = 0.2672664248026334
Iteration 50, Training loss = 0.2640154962069713
Iteration 60, Training loss = 0.2610136653081729
Iteration 70, Training loss = 0.2587395320431544
Iteration 80, Training loss = 0.25830621659182584
Iteration 90, Training loss = 0.2570738507291445
Model training time: 19.197518587112427
Device: cuda
Iteration 0, Training loss = 1.005590061728771
Iteration 10, Training loss = 0.2988231246574567
Iteration 20, Training loss = 0.2780502225057437
Iteration 30, Training loss = 0.2696057131084112
Iteration 40, Training loss = 0.2647607008425089
Iteration 50, Training loss = 0.2588665525500591
Iteration 60, Training loss = 0.2549567630227942
Iteration 70, Training loss = 0.25430951597025764
Iteration 80, Training loss = 0.2522367350757122
Iteration 90, Training loss = 0.2507552547523609
Model training time: 18.845258712768555
Device: cuda
Iteration 0, Training loss = 1.1207565539158308
Iteration 10, Training loss = 0.3021776376721951
Iteration 20, Training loss = 0.27783504142784154
Iteration 30, Training loss = 0.2680358544278603
Iteration 40, Training loss = 0.26204191907667196
Iteration 50, Training loss = 0.2615536803809496
Iteration 60, Training loss = 0.2561863460219823
Iteration 70, Training loss = 0.25446354841383606
Iteration 80, Training loss = 0.25183805192892367
Iteration 90, Training loss = 0.24975753883616283
Model training time: 19.104654550552368
Device: cuda
Iteration 0, Training loss = 0.9846312501109563
Iteration 10, Training loss = 0.2995542035653041
Iteration 20, Training loss = 0.2874802244970432
Iteration 30, Training loss = 0.27613321792047757
Iteration 40, Training loss = 0.2667127581169972
Iteration 50, Training loss = 0.2598024893265504
Iteration 60, Training loss = 0.2544112494931771
Iteration 70, Training loss = 0.2516067659912201
Iteration 80, Training loss = 0.24897898060198015
Iteration 90, Training loss = 0.24757796210738328
Model training time: 18.9286630153656
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9981374161747786
Iteration 10, Training loss = 0.9892116618844179
Iteration 20, Training loss = 0.9815323261114267
Iteration 30, Training loss = 0.9773526036968598
Iteration 40, Training loss = 0.9575430000057588
Iteration 50, Training loss = 0.9395212823381791
Iteration 60, Training loss = 0.9084066278659381
Iteration 70, Training loss = 0.8494014768646314
Iteration 80, Training loss = 0.7572754088502663
Iteration 90, Training loss = 0.6363065589505893
Model training time: 17.14950156211853
Device: cuda
Iteration 0, Training loss = 1.008273071394517
Iteration 10, Training loss = 0.9961072550370142
Iteration 20, Training loss = 0.9972150113720161
Iteration 30, Training loss = 0.990247923307694
Iteration 40, Training loss = 0.9920503858190316
Iteration 50, Training loss = 0.9867444428113791
Iteration 60, Training loss = 0.9892185221497829
Iteration 70, Training loss = 0.9826058246768438
Iteration 80, Training loss = 0.9737956844843351
Iteration 90, Training loss = 0.9688756781128737
Model training time: 16.795909881591797
Device: cuda
Iteration 0, Training loss = 1.2396494419528887
Iteration 10, Training loss = 1.0007323651359632
Iteration 20, Training loss = 0.997151139837045
Iteration 30, Training loss = 0.9970314445403906
Iteration 40, Training loss = 0.9939721822738647
Iteration 50, Training loss = 0.9951427470032985
Iteration 60, Training loss = 0.9930814418655175
Iteration 70, Training loss = 0.9884050488471985
Iteration 80, Training loss = 0.9846667951116195
Iteration 90, Training loss = 0.9837907839279908
Model training time: 16.97817039489746
Device: cuda
Iteration 0, Training loss = 1.0613844509308155
Iteration 10, Training loss = 1.0039140364298453
Iteration 20, Training loss = 1.0008771339288125
Iteration 30, Training loss = 1.0044451275697122
Iteration 40, Training loss = 0.9988605655156649
Iteration 50, Training loss = 0.9943967358424113
Iteration 60, Training loss = 0.9904540599538729
Iteration 70, Training loss = 0.9857754340538611
Iteration 80, Training loss = 0.9787701528805953
Iteration 90, Training loss = 0.9748234897851944
Model training time: 17.02612280845642
Device: cuda
Iteration 0, Training loss = 1.0510837206473718
Iteration 10, Training loss = 0.9944339824410585
Iteration 20, Training loss = 0.9923018486453936
Iteration 30, Training loss = 0.9902829143863457
Iteration 40, Training loss = 0.9901900824445945
Iteration 50, Training loss = 0.9731106867010777
Iteration 60, Training loss = 0.9623047468753961
Iteration 70, Training loss = 0.9395427488936827
Iteration 80, Training loss = 0.9107367281730359
Iteration 90, Training loss = 0.853461548112906
Model training time: 16.791842222213745
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.127122887625144
Iteration 10, Training loss = 0.30991269576434904
Iteration 20, Training loss = 0.2839517884242993
Iteration 30, Training loss = 0.27246016177993554
Iteration 40, Training loss = 0.2639956855430053
Iteration 50, Training loss = 0.25788736164283294
Iteration 60, Training loss = 0.2569068143717371
Iteration 70, Training loss = 0.25172297541911787
Iteration 80, Training loss = 0.24922700355259272
Iteration 90, Training loss = 0.2461310590688999
Iteration 100, Training loss = 0.2432914308917064
Iteration 110, Training loss = 0.24023640485337147
Iteration 120, Training loss = 0.23832987105617157
Iteration 130, Training loss = 0.23684423617445505
Iteration 140, Training loss = 0.23552196186322433
Iteration 150, Training loss = 0.23240415909542486
Iteration 160, Training loss = 0.23044971333673367
Iteration 170, Training loss = 0.23030133488086554
Iteration 180, Training loss = 0.22697962089799917
Iteration 190, Training loss = 0.22471173558957303
Model training time: 38.287731647491455
Device: cuda
Iteration 0, Training loss = 0.9523021303690397
Iteration 10, Training loss = 0.2975845793978526
Iteration 20, Training loss = 0.28155672851090247
Iteration 30, Training loss = 0.2714069104538514
Iteration 40, Training loss = 0.26699305698275566
Iteration 50, Training loss = 0.2613495823282462
Iteration 60, Training loss = 0.25774191319942474
Iteration 70, Training loss = 0.2569558982952283
Iteration 80, Training loss = 0.25279089765479934
Iteration 90, Training loss = 0.25108810284963023
Iteration 100, Training loss = 0.24987129523203924
Iteration 110, Training loss = 0.2473253465902347
Iteration 120, Training loss = 0.24688175191672948
Iteration 130, Training loss = 0.24508476386276576
Iteration 140, Training loss = 0.24305224218047583
Iteration 150, Training loss = 0.24109982512891293
Iteration 160, Training loss = 0.2411823941824528
Iteration 170, Training loss = 0.23836448401785815
Iteration 180, Training loss = 0.2371004349910296
Iteration 190, Training loss = 0.2347072305587622
Model training time: 37.898746728897095
Device: cuda
Iteration 0, Training loss = 0.9423351007012221
Iteration 10, Training loss = 0.29297450170494044
Iteration 20, Training loss = 0.2791837377903553
Iteration 30, Training loss = 0.2676366753876209
Iteration 40, Training loss = 0.2600143233743998
Iteration 50, Training loss = 0.25669119163201404
Iteration 60, Training loss = 0.2523117566911074
Iteration 70, Training loss = 0.24803984380112246
Iteration 80, Training loss = 0.24582639981347781
Iteration 90, Training loss = 0.24227205033485705
Iteration 100, Training loss = 0.24121201869386893
Iteration 110, Training loss = 0.23900268979084033
Iteration 120, Training loss = 0.23594555201438758
Iteration 130, Training loss = 0.23383874546449918
Iteration 140, Training loss = 0.23385231812986043
Iteration 150, Training loss = 0.23155978307701075
Iteration 160, Training loss = 0.22999388480988833
Iteration 170, Training loss = 0.22799823425996762
Iteration 180, Training loss = 0.22701483305830222
Iteration 190, Training loss = 0.225970742221062
Model training time: 37.50691819190979
Device: cuda
Iteration 0, Training loss = 0.9352760194585874
Iteration 10, Training loss = 0.2932657212592088
Iteration 20, Training loss = 0.28140402169754875
Iteration 30, Training loss = 0.2682697056577756
Iteration 40, Training loss = 0.26520475558936596
Iteration 50, Training loss = 0.25939657682409656
Iteration 60, Training loss = 0.2591394871616593
Iteration 70, Training loss = 0.25187703107412046
Iteration 80, Training loss = 0.24864766646463138
Iteration 90, Training loss = 0.24746573874010488
Iteration 100, Training loss = 0.2429863321236693
Iteration 110, Training loss = 0.241379741865855
Iteration 120, Training loss = 0.23872330498236877
Iteration 130, Training loss = 0.2368757312114422
Iteration 140, Training loss = 0.23552423815887708
Iteration 150, Training loss = 0.23231907231876484
Iteration 160, Training loss = 0.232570596325856
Iteration 170, Training loss = 0.23151034933443254
Iteration 180, Training loss = 0.22913727909326553
Iteration 190, Training loss = 0.2280146200209856
Model training time: 37.591747522354126
Device: cuda
Iteration 0, Training loss = 0.9416941536160616
Iteration 10, Training loss = 0.29837902148182577
Iteration 20, Training loss = 0.27929904727408517
Iteration 30, Training loss = 0.26797693566634107
Iteration 40, Training loss = 0.2617147626498571
Iteration 50, Training loss = 0.2558550942116059
Iteration 60, Training loss = 0.25294506177306175
Iteration 70, Training loss = 0.24748683692171022
Iteration 80, Training loss = 0.2453683831084233
Iteration 90, Training loss = 0.24228489986405924
Iteration 100, Training loss = 0.23974698982559717
Iteration 110, Training loss = 0.23819683062342498
Iteration 120, Training loss = 0.23627361225394103
Iteration 130, Training loss = 0.23447495627288634
Iteration 140, Training loss = 0.2323622919905644
Iteration 150, Training loss = 0.23116029856296685
Iteration 160, Training loss = 0.23033883436941183
Iteration 170, Training loss = 0.22801486030220985
Iteration 180, Training loss = 0.22747194616553876
Iteration 190, Training loss = 0.2246602550148964
Model training time: 37.85193943977356
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0991433784365654
Iteration 10, Training loss = 0.9937346250965045
Iteration 20, Training loss = 0.9886294081807137
Iteration 30, Training loss = 0.9867038337083963
Iteration 40, Training loss = 0.9735973322620759
Iteration 50, Training loss = 0.956697383752236
Iteration 60, Training loss = 0.9358236079032605
Iteration 70, Training loss = 0.8950918162098298
Iteration 80, Training loss = 0.8281993504900199
Iteration 90, Training loss = 0.7288643517173253
Iteration 100, Training loss = 0.6010797447883166
Iteration 110, Training loss = 0.48815270885825157
Iteration 120, Training loss = 0.4221850805557691
Iteration 130, Training loss = 0.3960363002350697
Iteration 140, Training loss = 0.3829637154077108
Iteration 150, Training loss = 0.3764813258670844
Iteration 160, Training loss = 0.3693751079531816
Iteration 170, Training loss = 0.3651145981768003
Iteration 180, Training loss = 0.3586203266794865
Iteration 190, Training loss = 0.35521614580200267
Model training time: 33.42227101325989
Device: cuda
Iteration 0, Training loss = 0.9977752543412722
Iteration 10, Training loss = 0.9921932575794367
Iteration 20, Training loss = 0.9900091238892995
Iteration 30, Training loss = 0.9786094421377549
Iteration 40, Training loss = 0.9710418645005959
Iteration 50, Training loss = 0.9557978470738118
Iteration 60, Training loss = 0.9353994836027806
Iteration 70, Training loss = 0.9016117109702184
Iteration 80, Training loss = 0.8504009928840858
Iteration 90, Training loss = 0.7739670007274702
Iteration 100, Training loss = 0.6746758632361889
Iteration 110, Training loss = 0.5601820702163073
Iteration 120, Training loss = 0.470963444847327
Iteration 130, Training loss = 0.41939477823101556
Iteration 140, Training loss = 0.3967788600577758
Iteration 150, Training loss = 0.3840777160456547
Iteration 160, Training loss = 0.37511629023804116
Iteration 170, Training loss = 0.36835636771642244
Iteration 180, Training loss = 0.36649362542308295
Iteration 190, Training loss = 0.3586156118947726
Model training time: 34.01154947280884
Device: cuda
Iteration 0, Training loss = 0.995333466392297
Iteration 10, Training loss = 0.9870277946958175
Iteration 20, Training loss = 0.987542473925994
Iteration 30, Training loss = 0.9778034056608493
Iteration 40, Training loss = 0.9668915930848855
Iteration 50, Training loss = 0.9455523227269833
Iteration 60, Training loss = 0.9160321985299771
Iteration 70, Training loss = 0.8631440968467639
Iteration 80, Training loss = 0.7786029290694457
Iteration 90, Training loss = 0.659107058094098
Iteration 100, Training loss = 0.5339213752975831
Iteration 110, Training loss = 0.4390576507609624
Iteration 120, Training loss = 0.3975637019253694
Iteration 130, Training loss = 0.3799172334659558
Iteration 140, Training loss = 0.3687377958916701
Iteration 150, Training loss = 0.36245320178568363
Iteration 160, Training loss = 0.3585405023052142
Iteration 170, Training loss = 0.35144594168433774
Iteration 180, Training loss = 0.3490517907417737
Iteration 190, Training loss = 0.3434060566986983
Model training time: 33.756641149520874
Device: cuda
Iteration 0, Training loss = 0.9958072104133092
Iteration 10, Training loss = 0.9925778565498499
Iteration 20, Training loss = 0.9835400438079467
Iteration 30, Training loss = 0.9827675968408585
Iteration 40, Training loss = 0.9651169960315411
Iteration 50, Training loss = 0.9436450422956393
Iteration 60, Training loss = 0.9108630103560594
Iteration 70, Training loss = 0.8524546692004571
Iteration 80, Training loss = 0.762685029552533
Iteration 90, Training loss = 0.6299108693806025
Iteration 100, Training loss = 0.5048688936692017
Iteration 110, Training loss = 0.42423578151143515
Iteration 120, Training loss = 0.3914921037279643
Iteration 130, Training loss = 0.37718939551940334
Iteration 140, Training loss = 0.37046204364070523
Iteration 150, Training loss = 0.3636303102740875
Iteration 160, Training loss = 0.3584495440412026
Iteration 170, Training loss = 0.3550107704045681
Iteration 180, Training loss = 0.3506532128040607
Iteration 190, Training loss = 0.3472364079207182
Model training time: 33.64827108383179
Device: cuda
Iteration 0, Training loss = 1.0728477967473178
Iteration 10, Training loss = 1.0026557915485823
Iteration 20, Training loss = 0.9955038892535063
Iteration 30, Training loss = 0.9924033060669899
Iteration 40, Training loss = 0.9896980655881075
Iteration 50, Training loss = 0.9895455000492243
Iteration 60, Training loss = 0.9813266843557358
Iteration 70, Training loss = 0.9724120893157445
Iteration 80, Training loss = 0.9596425495468653
Iteration 90, Training loss = 0.9375860479015571
Iteration 100, Training loss = 0.9063821933590449
Iteration 110, Training loss = 0.8536524382921366
Iteration 120, Training loss = 0.7784585167582219
Iteration 130, Training loss = 0.6753213749482081
Iteration 140, Training loss = 0.5568285461228627
Iteration 150, Training loss = 0.47148650454787105
Iteration 160, Training loss = 0.423875343054533
Iteration 170, Training loss = 0.40272667215993774
Iteration 180, Training loss = 0.38565946370363235
Iteration 190, Training loss = 0.37512321369006085
Model training time: 33.26836800575256
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9818161296156737
Iteration 10, Training loss = 0.3061469132797076
Iteration 20, Training loss = 0.278690991923213
Iteration 30, Training loss = 0.26883027315712893
Iteration 40, Training loss = 0.2639909011240189
Iteration 50, Training loss = 0.26031081421444047
Iteration 60, Training loss = 0.2572103079694968
Iteration 70, Training loss = 0.254106458563071
Iteration 80, Training loss = 0.2534921761029042
Iteration 90, Training loss = 0.25051809059312713
Iteration 100, Training loss = 0.251437275742109
Iteration 110, Training loss = 0.2486885622716867
Iteration 120, Training loss = 0.24667885792083466
Iteration 130, Training loss = 0.2450647444392626
Iteration 140, Training loss = 0.24417122166890365
Iteration 150, Training loss = 0.2432754306265941
Iteration 160, Training loss = 0.24144663518437973
Iteration 170, Training loss = 0.23991991500728405
Iteration 180, Training loss = 0.24071771193009156
Iteration 190, Training loss = 0.23750804228564867
Model training time: 37.233678340911865
Device: cuda
Iteration 0, Training loss = 0.9514330281661108
Iteration 10, Training loss = 0.30247166423270333
Iteration 20, Training loss = 0.2838507637094993
Iteration 30, Training loss = 0.2691135387867689
Iteration 40, Training loss = 0.2643995243482865
Iteration 50, Training loss = 0.26112675867401636
Iteration 60, Training loss = 0.2578079038514541
Iteration 70, Training loss = 0.2543175101566773
Iteration 80, Training loss = 0.2518911143908134
Iteration 90, Training loss = 0.2510017514801942
Iteration 100, Training loss = 0.24650742342838874
Iteration 110, Training loss = 0.24548270051869062
Iteration 120, Training loss = 0.24553492923195547
Iteration 130, Training loss = 0.24269522425646967
Iteration 140, Training loss = 0.24245009284753066
Iteration 150, Training loss = 0.24117716373159334
Iteration 160, Training loss = 0.2375099823022118
Iteration 170, Training loss = 0.23689063882025388
Iteration 180, Training loss = 0.23588677424077803
Iteration 190, Training loss = 0.23382971091912344
Model training time: 37.37774634361267
Device: cuda
Iteration 0, Training loss = 0.9794642569927069
Iteration 10, Training loss = 0.2989119106473831
Iteration 20, Training loss = 0.2782554868608713
Iteration 30, Training loss = 0.2670435246366721
Iteration 40, Training loss = 0.25992423124038255
Iteration 50, Training loss = 0.25604769816765416
Iteration 60, Training loss = 0.2512854281812906
Iteration 70, Training loss = 0.2481077746129953
Iteration 80, Training loss = 0.24621074818647826
Iteration 90, Training loss = 0.24284489381198698
Iteration 100, Training loss = 0.24162821049013963
Iteration 110, Training loss = 0.23835244216024876
Iteration 120, Training loss = 0.23843389004468918
Iteration 130, Training loss = 0.23492912375009978
Iteration 140, Training loss = 0.23302629833611158
Iteration 150, Training loss = 0.23759559477464512
Iteration 160, Training loss = 0.2315756260918883
Iteration 170, Training loss = 0.2296388684413754
Iteration 180, Training loss = 0.22851897725978723
Iteration 190, Training loss = 0.2286184006012403
Model training time: 37.588093280792236
Device: cuda
Iteration 0, Training loss = 0.9910351840349344
Iteration 10, Training loss = 0.3028190415352583
Iteration 20, Training loss = 0.2804891284411916
Iteration 30, Training loss = 0.2709191544697835
Iteration 40, Training loss = 0.26274206317388094
Iteration 50, Training loss = 0.2553813589306978
Iteration 60, Training loss = 0.2522495873272419
Iteration 70, Training loss = 0.2480821259892904
Iteration 80, Training loss = 0.24678456854934877
Iteration 90, Training loss = 0.24351861241918343
Iteration 100, Training loss = 0.24650696134911135
Iteration 110, Training loss = 0.24016830912576273
Iteration 120, Training loss = 0.23886856637321985
Iteration 130, Training loss = 0.2384637571298159
Iteration 140, Training loss = 0.2348722222332771
Iteration 150, Training loss = 0.23419591297323888
Iteration 160, Training loss = 0.2323634410993411
Iteration 170, Training loss = 0.2307472602249338
Iteration 180, Training loss = 0.23100089066876814
Iteration 190, Training loss = 0.22866030553212532
Model training time: 37.662726402282715
Device: cuda
Iteration 0, Training loss = 0.9919725432991982
Iteration 10, Training loss = 0.2980081539314527
Iteration 20, Training loss = 0.274633029045967
Iteration 30, Training loss = 0.263897320183997
Iteration 40, Training loss = 0.2596468593065555
Iteration 50, Training loss = 0.2573642283678055
Iteration 60, Training loss = 0.2540392824090444
Iteration 70, Training loss = 0.25345186344706094
Iteration 80, Training loss = 0.2495328550441907
Iteration 90, Training loss = 0.2490912526845932
Iteration 100, Training loss = 0.2469854220891228
Iteration 110, Training loss = 0.24666317638296348
Iteration 120, Training loss = 0.24500365813191122
Iteration 130, Training loss = 0.24263734155549452
Iteration 140, Training loss = 0.2418378532792513
Iteration 150, Training loss = 0.2410108128992411
Iteration 160, Training loss = 0.23935581156267569
Iteration 170, Training loss = 0.2380920396401332
Iteration 180, Training loss = 0.2399796379300264
Iteration 190, Training loss = 0.2353587792469905
Model training time: 37.87174105644226
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0219186865366423
Iteration 10, Training loss = 0.999571789915745
Iteration 20, Training loss = 0.9917625796336395
Iteration 30, Training loss = 0.9911596408257117
Iteration 40, Training loss = 0.9816195443272591
Iteration 50, Training loss = 0.9681490482046053
Iteration 60, Training loss = 0.9470645046004882
Iteration 70, Training loss = 0.915071458197557
Iteration 80, Training loss = 0.8661054275356806
Iteration 90, Training loss = 0.7881521783196009
Iteration 100, Training loss = 0.6722107446537569
Iteration 110, Training loss = 0.5535647029486986
Iteration 120, Training loss = 0.45942377041165644
Iteration 130, Training loss = 0.4144614353202857
Iteration 140, Training loss = 0.3954205077428084
Iteration 150, Training loss = 0.38182142548836195
Iteration 160, Training loss = 0.3709530910620323
Iteration 170, Training loss = 0.3676907872924438
Iteration 180, Training loss = 0.35944175791855043
Iteration 190, Training loss = 0.3539896859572484
Model training time: 33.64231872558594
Device: cuda
Iteration 0, Training loss = 1.013616568193986
Iteration 10, Training loss = 0.9996732226931132
Iteration 20, Training loss = 0.9996180706299268
Iteration 30, Training loss = 0.9928827953453248
Iteration 40, Training loss = 0.9985290255684119
Iteration 50, Training loss = 0.9893124338525993
Iteration 60, Training loss = 0.9849873861441245
Iteration 70, Training loss = 0.9795785941756688
Iteration 80, Training loss = 0.9731255299769915
Iteration 90, Training loss = 0.9571447865321085
Iteration 100, Training loss = 0.929944783449173
Iteration 110, Training loss = 0.8968047087009137
Iteration 120, Training loss = 0.8418640763713763
Iteration 130, Training loss = 0.755255039781332
Iteration 140, Training loss = 0.6500937990271128
Iteration 150, Training loss = 0.5485465125395701
Iteration 160, Training loss = 0.4685061456492314
Iteration 170, Training loss = 0.4254585481606997
Iteration 180, Training loss = 0.3989581881234279
Iteration 190, Training loss = 0.386089239269495
Model training time: 33.74740719795227
Device: cuda
Iteration 0, Training loss = 1.1388553312191596
Iteration 10, Training loss = 0.998854955801597
Iteration 20, Training loss = 0.9922882009011048
Iteration 30, Training loss = 0.992729858137094
Iteration 40, Training loss = 0.9890243714818587
Iteration 50, Training loss = 0.9813747079326556
Iteration 60, Training loss = 0.9755063991133983
Iteration 70, Training loss = 0.966952366897693
Iteration 80, Training loss = 0.9556112558795855
Iteration 90, Training loss = 0.9309474940483387
Iteration 100, Training loss = 0.8904949819239286
Iteration 110, Training loss = 0.8324216076960931
Iteration 120, Training loss = 0.7455156542933904
Iteration 130, Training loss = 0.635852721448128
Iteration 140, Training loss = 0.5297363148285792
Iteration 150, Training loss = 0.4509790235987076
Iteration 160, Training loss = 0.40646873887341756
Iteration 170, Training loss = 0.3852996609818477
Iteration 180, Training loss = 0.3732711390520518
Iteration 190, Training loss = 0.3665525082212228
Model training time: 34.13058876991272
Device: cuda
Iteration 0, Training loss = 1.0492278853288064
Iteration 10, Training loss = 1.0025324936096485
Iteration 20, Training loss = 0.994787783576892
Iteration 30, Training loss = 0.9940174726339487
Iteration 40, Training loss = 0.9940978340231456
Iteration 50, Training loss = 0.9893267114575093
Iteration 60, Training loss = 0.9811584462340062
Iteration 70, Training loss = 0.9730374704186733
Iteration 80, Training loss = 0.9597803491812485
Iteration 90, Training loss = 0.9382645963476255
Iteration 100, Training loss = 0.8883912958777868
Iteration 110, Training loss = 0.8130750214824309
Iteration 120, Training loss = 0.7028952785409414
Iteration 130, Training loss = 0.5704774289177015
Iteration 140, Training loss = 0.4643236135060971
Iteration 150, Training loss = 0.40593788772821426
Iteration 160, Training loss = 0.37977733907218164
Iteration 170, Training loss = 0.36654148207834136
Iteration 180, Training loss = 0.35862419037864757
Iteration 190, Training loss = 0.3540904294126309
Model training time: 34.868449449539185
Device: cuda
Iteration 0, Training loss = 1.0246133093650525
Iteration 10, Training loss = 1.0012361006094859
Iteration 20, Training loss = 0.9965823613680326
Iteration 30, Training loss = 1.0032669643943126
Iteration 40, Training loss = 0.993819935963704
Iteration 50, Training loss = 0.989358602808072
Iteration 60, Training loss = 0.9885814545246271
Iteration 70, Training loss = 0.9890091700049547
Iteration 80, Training loss = 0.9833989750880462
Iteration 90, Training loss = 0.9714660873779883
Iteration 100, Training loss = 0.9582268091348501
Iteration 110, Training loss = 0.9455034939142374
Iteration 120, Training loss = 0.9179690250983605
Iteration 130, Training loss = 0.8723338693380356
Iteration 140, Training loss = 0.8063981911310782
Iteration 150, Training loss = 0.7082574877601403
Iteration 160, Training loss = 0.5966347954594172
Iteration 170, Training loss = 0.4997492338029238
Iteration 180, Training loss = 0.4346564787511642
Iteration 190, Training loss = 0.4009169966268998
Model training time: 36.562846660614014
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9935420419161136
Iteration 10, Training loss = 0.29670030222489285
Iteration 20, Training loss = 0.2796154421969102
Iteration 30, Training loss = 0.27111751242325854
Iteration 40, Training loss = 0.26834710587102634
Iteration 50, Training loss = 0.2636611589158957
Iteration 60, Training loss = 0.26092734918571436
Iteration 70, Training loss = 0.25793442072776646
Iteration 80, Training loss = 0.25722249697607297
Iteration 90, Training loss = 0.2546070180116938
Iteration 100, Training loss = 0.2534104881521601
Iteration 110, Training loss = 0.25327835919765324
Iteration 120, Training loss = 0.25166172870936304
Iteration 130, Training loss = 0.2522638741020973
Iteration 140, Training loss = 0.25341855734586716
Iteration 150, Training loss = 0.2524536558641837
Iteration 160, Training loss = 0.2524433151747172
Iteration 170, Training loss = 0.250160564453556
Iteration 180, Training loss = 0.2502907170699193
Iteration 190, Training loss = 0.24753542942926288
Model training time: 41.08104753494263
Device: cuda
Iteration 0, Training loss = 0.9717641776570907
Iteration 10, Training loss = 0.29685130457465464
Iteration 20, Training loss = 0.2827413513396795
Iteration 30, Training loss = 0.2677334831454433
Iteration 40, Training loss = 0.2614061217755079
Iteration 50, Training loss = 0.2589716762304306
Iteration 60, Training loss = 0.25513199251145124
Iteration 70, Training loss = 0.2530914106621192
Iteration 80, Training loss = 0.2506450992793991
Iteration 90, Training loss = 0.2491017454662002
Iteration 100, Training loss = 0.24829154848479307
Iteration 110, Training loss = 0.2442036374257161
Iteration 120, Training loss = 0.24145967284074196
Iteration 130, Training loss = 0.24016824837487477
Iteration 140, Training loss = 0.23894860136967438
Iteration 150, Training loss = 0.23755785865852466
Iteration 160, Training loss = 0.2355800852036247
Iteration 170, Training loss = 0.2354889358752049
Iteration 180, Training loss = 0.23440845095767424
Iteration 190, Training loss = 0.23141466073978406
Model training time: 38.78363847732544
Device: cuda
Iteration 0, Training loss = 1.0157142682717397
Iteration 10, Training loss = 0.2949858554280721
Iteration 20, Training loss = 0.283214000125344
Iteration 30, Training loss = 0.27305773445046866
Iteration 40, Training loss = 0.2654958185381614
Iteration 50, Training loss = 0.2590173318122442
Iteration 60, Training loss = 0.25631288868876606
Iteration 70, Training loss = 0.2547755065159156
Iteration 80, Training loss = 0.25152900184576327
Iteration 90, Training loss = 0.25032205335222757
Iteration 100, Training loss = 0.249938517379073
Iteration 110, Training loss = 0.2481761026697663
Iteration 120, Training loss = 0.24870846621119058
Iteration 130, Training loss = 0.2461287917999121
Iteration 140, Training loss = 0.24537563739487758
Iteration 150, Training loss = 0.2447585926319544
Iteration 160, Training loss = 0.2432529595322334
Iteration 170, Training loss = 0.24434485752135515
Iteration 180, Training loss = 0.24248365284158632
Iteration 190, Training loss = 0.24368606364497772
Model training time: 37.8919837474823
Device: cuda
Iteration 0, Training loss = 0.9713101346905415
Iteration 10, Training loss = 0.2965460756363777
Iteration 20, Training loss = 0.280410997569561
Iteration 30, Training loss = 0.27125264947804123
Iteration 40, Training loss = 0.2644538018279351
Iteration 50, Training loss = 0.2600088201176662
Iteration 60, Training loss = 0.2567433977069763
Iteration 70, Training loss = 0.2532755139355476
Iteration 80, Training loss = 0.25202673186476415
Iteration 90, Training loss = 0.24854625639720604
Iteration 100, Training loss = 0.24823068440533602
Iteration 110, Training loss = 0.24452010241265482
Iteration 120, Training loss = 0.24264852394564793
Iteration 130, Training loss = 0.2414494794435226
Iteration 140, Training loss = 0.24055294420283574
Iteration 150, Training loss = 0.24077543788231337
Iteration 160, Training loss = 0.23716084874020174
Iteration 170, Training loss = 0.23626012197480753
Iteration 180, Training loss = 0.23520197117557892
Iteration 190, Training loss = 0.2337782200998985
Model training time: 37.78304433822632
Device: cuda
Iteration 0, Training loss = 1.1923221968687499
Iteration 10, Training loss = 0.30560768338350147
Iteration 20, Training loss = 0.2855484573027262
Iteration 30, Training loss = 0.2684185804369358
Iteration 40, Training loss = 0.2600017605492702
Iteration 50, Training loss = 0.2551749466130367
Iteration 60, Training loss = 0.25182671630038667
Iteration 70, Training loss = 0.24946005599429974
Iteration 80, Training loss = 0.24710045625957158
Iteration 90, Training loss = 0.24439448834611818
Iteration 100, Training loss = 0.24296798184514046
Iteration 110, Training loss = 0.24151928909122944
Iteration 120, Training loss = 0.2402956530165214
Iteration 130, Training loss = 0.23970495021113983
Iteration 140, Training loss = 0.23789598405934298
Iteration 150, Training loss = 0.23904872284485743
Iteration 160, Training loss = 0.23738412308291748
Iteration 170, Training loss = 0.2352032121270895
Iteration 180, Training loss = 0.2360649871138426
Iteration 190, Training loss = 0.23480079050820607
Model training time: 37.60250782966614
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.003568148956849
Iteration 10, Training loss = 0.9984281670588714
Iteration 20, Training loss = 0.9944602554807296
Iteration 30, Training loss = 0.9942261003530942
Iteration 40, Training loss = 0.9862846244986241
Iteration 50, Training loss = 0.9826217356782693
Iteration 60, Training loss = 0.9723937024290745
Iteration 70, Training loss = 0.9553183216888171
Iteration 80, Training loss = 0.9354161367966578
Iteration 90, Training loss = 0.9035381571604655
Iteration 100, Training loss = 0.8321476418238419
Iteration 110, Training loss = 0.7404977163443198
Iteration 120, Training loss = 0.6194957328530458
Iteration 130, Training loss = 0.511418743775441
Iteration 140, Training loss = 0.4406797977594229
Iteration 150, Training loss = 0.4027981494481747
Iteration 160, Training loss = 0.385495154473644
Iteration 170, Training loss = 0.37671432443536246
Iteration 180, Training loss = 0.36743139217679316
Iteration 190, Training loss = 0.35952777257905555
Model training time: 33.50732612609863
Device: cuda
Iteration 0, Training loss = 1.0018570503363242
Iteration 10, Training loss = 1.0047608040846312
Iteration 20, Training loss = 0.9926182220761592
Iteration 30, Training loss = 0.9886491562311466
Iteration 40, Training loss = 0.9835888921068265
Iteration 50, Training loss = 0.9784508192768464
Iteration 60, Training loss = 0.9658922117490035
Iteration 70, Training loss = 0.9474446974121608
Iteration 80, Training loss = 0.922355415729376
Iteration 90, Training loss = 0.8726893118940867
Iteration 100, Training loss = 0.7961302898251094
Iteration 110, Training loss = 0.6812478920015005
Iteration 120, Training loss = 0.5541224170189637
Iteration 130, Training loss = 0.45420462265610695
Iteration 140, Training loss = 0.40405855227548343
Iteration 150, Training loss = 0.3855928171139497
Iteration 160, Training loss = 0.3740186427648251
Iteration 170, Training loss = 0.36945932186566866
Iteration 180, Training loss = 0.36351563623891425
Iteration 190, Training loss = 0.360036662994669
Model training time: 33.55614233016968
Device: cuda
Iteration 0, Training loss = 1.0128837067347307
Iteration 10, Training loss = 0.997576273404635
Iteration 20, Training loss = 0.9902974796982912
Iteration 30, Training loss = 0.9796548182001481
Iteration 40, Training loss = 0.9716752194441282
Iteration 50, Training loss = 0.9614920593225039
Iteration 60, Training loss = 0.937267694908839
Iteration 70, Training loss = 0.9024204800908382
Iteration 80, Training loss = 0.8462672405518018
Iteration 90, Training loss = 0.7558123509471233
Iteration 100, Training loss = 0.6370885354968218
Iteration 110, Training loss = 0.5168352115612763
Iteration 120, Training loss = 0.4385194514806454
Iteration 130, Training loss = 0.3966284104837821
Iteration 140, Training loss = 0.3806384320442493
Iteration 150, Training loss = 0.3726742138656286
Iteration 160, Training loss = 0.3627375387228452
Iteration 170, Training loss = 0.3594640683955871
Iteration 180, Training loss = 0.3518789648436583
Iteration 190, Training loss = 0.35000408641420877
Model training time: 33.467262744903564
Device: cuda
Iteration 0, Training loss = 1.0322568685962603
Iteration 10, Training loss = 0.9939867109060287
Iteration 20, Training loss = 0.9917752456206542
Iteration 30, Training loss = 0.9836149376172286
Iteration 40, Training loss = 0.9796750155779032
Iteration 50, Training loss = 0.9757229874913509
Iteration 60, Training loss = 0.9598673788400797
Iteration 70, Training loss = 0.9408633491167655
Iteration 80, Training loss = 0.9076314195990562
Iteration 90, Training loss = 0.8609780881267327
Iteration 100, Training loss = 0.7836498801524823
Iteration 110, Training loss = 0.6813744510022494
Iteration 120, Training loss = 0.5604299157857895
Iteration 130, Training loss = 0.4618079404418285
Iteration 140, Training loss = 0.4088977538049221
Iteration 150, Training loss = 0.38851301873532623
Iteration 160, Training loss = 0.3743722630807987
Iteration 170, Training loss = 0.36554846769342053
Iteration 180, Training loss = 0.358562201834642
Iteration 190, Training loss = 0.35268935647148353
Model training time: 33.51734685897827
Device: cuda
Iteration 0, Training loss = 1.0141205128568869
Iteration 10, Training loss = 0.9957656722802383
Iteration 20, Training loss = 0.9982176159436886
Iteration 30, Training loss = 0.9908075309716738
Iteration 40, Training loss = 0.984574077794185
Iteration 50, Training loss = 0.9762896293630967
Iteration 60, Training loss = 0.9628871278120921
Iteration 70, Training loss = 0.934723012149334
Iteration 80, Training loss = 0.8946088466506738
Iteration 90, Training loss = 0.834751098201825
Iteration 100, Training loss = 0.7354407161474228
Iteration 110, Training loss = 0.6151095049885603
Iteration 120, Training loss = 0.5004209431891258
Iteration 130, Training loss = 0.42916994198010516
Iteration 140, Training loss = 0.3948359036674866
Iteration 150, Training loss = 0.3834322529056898
Iteration 160, Training loss = 0.3712758341660866
Iteration 170, Training loss = 0.3649480255463949
Iteration 180, Training loss = 0.36101692212888825
Iteration 190, Training loss = 0.35819085357853997
Model training time: 33.242520809173584
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9724852855388935
Iteration 10, Training loss = 0.2958954321936919
Iteration 20, Training loss = 0.27647580134753996
Iteration 30, Training loss = 0.266688888749251
Iteration 40, Training loss = 0.26217305287718773
Iteration 50, Training loss = 0.25928950753922647
Iteration 60, Training loss = 0.2571056023813211
Iteration 70, Training loss = 0.25497041943554694
Iteration 80, Training loss = 0.2522723222008118
Iteration 90, Training loss = 0.2508593767594833
Iteration 100, Training loss = 0.24735492238631615
Iteration 110, Training loss = 0.24553728461838686
Iteration 120, Training loss = 0.24181271673968205
Iteration 130, Training loss = 0.24048964134775674
Iteration 140, Training loss = 0.23889813357247755
Iteration 150, Training loss = 0.2372359185694502
Iteration 160, Training loss = 0.23494948625851136
Iteration 170, Training loss = 0.2353926942182275
Iteration 180, Training loss = 0.23265239266821972
Iteration 190, Training loss = 0.2307583555006064
Iteration 200, Training loss = 0.22743936255574226
Iteration 210, Training loss = 0.22629198116751817
Iteration 220, Training loss = 0.2257515343192678
Iteration 230, Training loss = 0.22509561923260873
Iteration 240, Training loss = 0.2232543583959341
Iteration 250, Training loss = 0.2211082181535088
Iteration 260, Training loss = 0.22267425719362038
Iteration 270, Training loss = 0.22297578032773274
Iteration 280, Training loss = 0.21992460452020168
Iteration 290, Training loss = 0.21858771696973306
Model training time: 56.883801221847534
Device: cuda
Iteration 0, Training loss = 0.9754439408962543
Iteration 10, Training loss = 0.29681812341396624
Iteration 20, Training loss = 0.28022309521643013
Iteration 30, Training loss = 0.26903541357471394
Iteration 40, Training loss = 0.26454229982426536
Iteration 50, Training loss = 0.26037471626813596
Iteration 60, Training loss = 0.2576206825100459
Iteration 70, Training loss = 0.2557810056381501
Iteration 80, Training loss = 0.25352200306952
Iteration 90, Training loss = 0.25178189332095474
Iteration 100, Training loss = 0.24959119072613808
Iteration 110, Training loss = 0.2499328057926435
Iteration 120, Training loss = 0.24964062597316045
Iteration 130, Training loss = 0.245628832696149
Iteration 140, Training loss = 0.24455782083364633
Iteration 150, Training loss = 0.24171046926998174
Iteration 160, Training loss = 0.24002428431637013
Iteration 170, Training loss = 0.2390812708494755
Iteration 180, Training loss = 0.23731864623438853
Iteration 190, Training loss = 0.2344638747521318
Iteration 200, Training loss = 0.2336660070488086
Iteration 210, Training loss = 0.2321553579889811
Iteration 220, Training loss = 0.23342737541175806
Iteration 230, Training loss = 0.22952407498199207
Iteration 240, Training loss = 0.2280413851571771
Iteration 250, Training loss = 0.2274472710604851
Iteration 260, Training loss = 0.22800650977744505
Iteration 270, Training loss = 0.2256835653948096
Iteration 280, Training loss = 0.22635460988833353
Iteration 290, Training loss = 0.22327879114219776
Model training time: 55.51422619819641
Device: cuda
Iteration 0, Training loss = 0.9637978667250047
Iteration 10, Training loss = 0.29711558383244735
Iteration 20, Training loss = 0.27829060373971093
Iteration 30, Training loss = 0.26730022185410446
Iteration 40, Training loss = 0.26373311418753403
Iteration 50, Training loss = 0.2608207445591688
Iteration 60, Training loss = 0.25838504860607475
Iteration 70, Training loss = 0.2504497137493812
Iteration 80, Training loss = 0.24637867990308082
Iteration 90, Training loss = 0.24308214888263208
Iteration 100, Training loss = 0.24020893348810765
Iteration 110, Training loss = 0.2389238402247429
Iteration 120, Training loss = 0.23671518165904742
Iteration 130, Training loss = 0.23465861924565756
Iteration 140, Training loss = 0.2325832094424046
Iteration 150, Training loss = 0.23075385914685634
Iteration 160, Training loss = 0.23024207955369583
Iteration 170, Training loss = 0.2286806506319688
Iteration 180, Training loss = 0.22666507959365845
Iteration 190, Training loss = 0.2268283782670131
Iteration 200, Training loss = 0.22302007474578345
Iteration 210, Training loss = 0.22206825311653888
Iteration 220, Training loss = 0.22087174095213413
Iteration 230, Training loss = 0.21907333026711756
Iteration 240, Training loss = 0.21804046172362107
Iteration 250, Training loss = 0.21716415932258734
Iteration 260, Training loss = 0.21648098600025362
Iteration 270, Training loss = 0.21525205299258232
Iteration 280, Training loss = 0.21333358064293861
Iteration 290, Training loss = 0.21245221521418828
Model training time: 55.31873059272766
Device: cuda
Iteration 0, Training loss = 1.0098105783645923
Iteration 10, Training loss = 0.2985534258186817
Iteration 20, Training loss = 0.27788406739441246
Iteration 30, Training loss = 0.2696799167360251
Iteration 40, Training loss = 0.2622985295378245
Iteration 50, Training loss = 0.25998109106260997
Iteration 60, Training loss = 0.2586394132902989
Iteration 70, Training loss = 0.25135813975849974
Iteration 80, Training loss = 0.2484899779351858
Iteration 90, Training loss = 0.24739703693642065
Iteration 100, Training loss = 0.24587679124222353
Iteration 110, Training loss = 0.24581724075743785
Iteration 120, Training loss = 0.24178190980679715
Iteration 130, Training loss = 0.24269340588496283
Iteration 140, Training loss = 0.23881857183117133
Iteration 150, Training loss = 0.2373799064125006
Iteration 160, Training loss = 0.23612111907165784
Iteration 170, Training loss = 0.2337223646016075
Iteration 180, Training loss = 0.23255882498163444
Iteration 190, Training loss = 0.23004103838824308
Iteration 200, Training loss = 0.22957306290761784
Iteration 210, Training loss = 0.2270328105212404
Iteration 220, Training loss = 0.229156692369053
Iteration 230, Training loss = 0.22652229444625285
Iteration 240, Training loss = 0.22388975795071858
Iteration 250, Training loss = 0.2225156561113321
Iteration 260, Training loss = 0.2242249697446823
Iteration 270, Training loss = 0.22171663944251263
Iteration 280, Training loss = 0.22232522543233174
Iteration 290, Training loss = 0.2204041277559904
Model training time: 56.10584211349487
Device: cuda
Iteration 0, Training loss = 0.9349141396009005
Iteration 10, Training loss = 0.29543899931013584
Iteration 20, Training loss = 0.2809064961396731
Iteration 30, Training loss = 0.26787318676137006
Iteration 40, Training loss = 0.26090161244456583
Iteration 50, Training loss = 0.25765240435990006
Iteration 60, Training loss = 0.25612699569991004
Iteration 70, Training loss = 0.2524172475274939
Iteration 80, Training loss = 0.25057440107831586
Iteration 90, Training loss = 0.24641805013211873
Iteration 100, Training loss = 0.24516431929973456
Iteration 110, Training loss = 0.24352865795103404
Iteration 120, Training loss = 0.2403545518620656
Iteration 130, Training loss = 0.23842288489238575
Iteration 140, Training loss = 0.23743272400819337
Iteration 150, Training loss = 0.23451614372718793
Iteration 160, Training loss = 0.23385864510559118
Iteration 170, Training loss = 0.23197088591181314
Iteration 180, Training loss = 0.23048700893727633
Iteration 190, Training loss = 0.22719475254416466
Iteration 200, Training loss = 0.22589318432773536
Iteration 210, Training loss = 0.22411310070982346
Iteration 220, Training loss = 0.22272006493921465
Iteration 230, Training loss = 0.22519896709575102
Iteration 240, Training loss = 0.22036340152128384
Iteration 250, Training loss = 0.21998058180682933
Iteration 260, Training loss = 0.21871655548994356
Iteration 270, Training loss = 0.2169000063664638
Iteration 280, Training loss = 0.21511379901606303
Iteration 290, Training loss = 0.21437195846094534
Model training time: 56.077593088150024
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0146104189065785
Iteration 10, Training loss = 0.9875999081593293
Iteration 20, Training loss = 0.9825574480570279
Iteration 30, Training loss = 0.9734682864867724
Iteration 40, Training loss = 0.9550078350764054
Iteration 50, Training loss = 0.9333871316451293
Iteration 60, Training loss = 0.8936000254291755
Iteration 70, Training loss = 0.8272479170790086
Iteration 80, Training loss = 0.7242184246961887
Iteration 90, Training loss = 0.5978308139512172
Iteration 100, Training loss = 0.4912838311149524
Iteration 110, Training loss = 0.42972324387385297
Iteration 120, Training loss = 0.4050968083051535
Iteration 130, Training loss = 0.394919377250167
Iteration 140, Training loss = 0.3897602423452414
Iteration 150, Training loss = 0.38014756329357624
Iteration 160, Training loss = 0.37444138140059435
Iteration 170, Training loss = 0.3678634728376682
Iteration 180, Training loss = 0.3636917050641317
Iteration 190, Training loss = 0.35963474514965826
Iteration 200, Training loss = 0.35658917174889493
Iteration 210, Training loss = 0.35147628145149123
Iteration 220, Training loss = 0.35030262960264313
Iteration 230, Training loss = 0.3466993398391284
Iteration 240, Training loss = 0.3473186345341114
Iteration 250, Training loss = 0.343537931688703
Iteration 260, Training loss = 0.3410726825778301
Iteration 270, Training loss = 0.3388205642023912
Iteration 280, Training loss = 0.335809052706911
Iteration 290, Training loss = 0.33368710041619265
Model training time: 50.67368268966675
Device: cuda
Iteration 0, Training loss = 1.0066760004712985
Iteration 10, Training loss = 0.9876656371813554
Iteration 20, Training loss = 0.9820515444645515
Iteration 30, Training loss = 0.9701257015650089
Iteration 40, Training loss = 0.956813497038988
Iteration 50, Training loss = 0.9335017708631662
Iteration 60, Training loss = 0.8915352855737393
Iteration 70, Training loss = 0.829603604399241
Iteration 80, Training loss = 0.7243087363357728
Iteration 90, Training loss = 0.6020875762288387
Iteration 100, Training loss = 0.4874438475817442
Iteration 110, Training loss = 0.42457539244340015
Iteration 120, Training loss = 0.39737934418595755
Iteration 130, Training loss = 0.3835874987909427
Iteration 140, Training loss = 0.376120602282194
Iteration 150, Training loss = 0.36865627507750803
Iteration 160, Training loss = 0.36273838207125664
Iteration 170, Training loss = 0.35849438020243096
Iteration 180, Training loss = 0.356469440775422
Iteration 190, Training loss = 0.3508895184271611
Iteration 200, Training loss = 0.34910703565065676
Iteration 210, Training loss = 0.34606198149804884
Iteration 220, Training loss = 0.3412411627001487
Iteration 230, Training loss = 0.33862995341993296
Iteration 240, Training loss = 0.3378838633115475
Iteration 250, Training loss = 0.3355084065921031
Iteration 260, Training loss = 0.3370404515701991
Iteration 270, Training loss = 0.3318368185024995
Iteration 280, Training loss = 0.3299436051971637
Iteration 290, Training loss = 0.32973075199585694
Model training time: 48.855637550354004
Device: cuda
Iteration 0, Training loss = 1.016428145651634
Iteration 10, Training loss = 0.9833660532648747
Iteration 20, Training loss = 0.9767352806834074
Iteration 30, Training loss = 0.9564360698255209
Iteration 40, Training loss = 0.9382584857252928
Iteration 50, Training loss = 0.903921623642628
Iteration 60, Training loss = 0.8502571605719053
Iteration 70, Training loss = 0.7641530667360013
Iteration 80, Training loss = 0.6440282618770232
Iteration 90, Training loss = 0.5291339671955659
Iteration 100, Training loss = 0.4414201524968331
Iteration 110, Training loss = 0.40198520198464394
Iteration 120, Training loss = 0.3841470219194889
Iteration 130, Training loss = 0.3727620512676927
Iteration 140, Training loss = 0.36826447999248135
Iteration 150, Training loss = 0.3609153816046623
Iteration 160, Training loss = 0.3588281746667165
Iteration 170, Training loss = 0.3535142050912747
Iteration 180, Training loss = 0.3522068697672624
Iteration 190, Training loss = 0.3494323629599351
Iteration 200, Training loss = 0.34458353384756124
Iteration 210, Training loss = 0.34000561185754263
Iteration 220, Training loss = 0.3372044672186558
Iteration 230, Training loss = 0.33588313655211377
Iteration 240, Training loss = 0.335594314078872
Iteration 250, Training loss = 0.33174933736714035
Iteration 260, Training loss = 0.32911871882298815
Iteration 270, Training loss = 0.3293535660665769
Iteration 280, Training loss = 0.32695981229727084
Iteration 290, Training loss = 0.3261076163213987
Model training time: 48.74731683731079
Device: cuda
Iteration 0, Training loss = 1.0061502370696802
Iteration 10, Training loss = 0.9943747033293431
Iteration 20, Training loss = 0.9885339977649542
Iteration 30, Training loss = 0.9896077559544489
Iteration 40, Training loss = 0.9816702604293823
Iteration 50, Training loss = 0.975144817852057
Iteration 60, Training loss = 0.9623548302512902
Iteration 70, Training loss = 0.939421551158795
Iteration 80, Training loss = 0.9046324136165472
Iteration 90, Training loss = 0.8500109945352261
Iteration 100, Training loss = 0.759732833561989
Iteration 110, Training loss = 0.6409528427399122
Iteration 120, Training loss = 0.5152610087623963
Iteration 130, Training loss = 0.43385602648441607
Iteration 140, Training loss = 0.3965185324733074
Iteration 150, Training loss = 0.3814767375588417
Iteration 160, Training loss = 0.3708376746911269
Iteration 170, Training loss = 0.36327454490730393
Iteration 180, Training loss = 0.357641924602481
Iteration 190, Training loss = 0.3532421210637459
Iteration 200, Training loss = 0.34939820505678654
Iteration 210, Training loss = 0.34620025152197254
Iteration 220, Training loss = 0.3424999521901974
Iteration 230, Training loss = 0.33984698383853984
Iteration 240, Training loss = 0.34164650852863604
Iteration 250, Training loss = 0.3341788209688205
Iteration 260, Training loss = 0.3333771016735297
Iteration 270, Training loss = 0.33141015025858694
Iteration 280, Training loss = 0.32969495080984557
Iteration 290, Training loss = 0.32833056080226714
Model training time: 49.25641369819641
Device: cuda
Iteration 0, Training loss = 1.1057051924558787
Iteration 10, Training loss = 0.9958701414557604
Iteration 20, Training loss = 0.9921825379133224
Iteration 30, Training loss = 0.9897395183260624
Iteration 40, Training loss = 0.9780380175663874
Iteration 50, Training loss = 0.9639149485872343
Iteration 60, Training loss = 0.9479802634853584
Iteration 70, Training loss = 0.9193262480772458
Iteration 80, Training loss = 0.8733024029777601
Iteration 90, Training loss = 0.7972437017239057
Iteration 100, Training loss = 0.6855557578114363
Iteration 110, Training loss = 0.5590103214176801
Iteration 120, Training loss = 0.4544755260531719
Iteration 130, Training loss = 0.4005453953376183
Iteration 140, Training loss = 0.3780462651585157
Iteration 150, Training loss = 0.3666244255235562
Iteration 160, Training loss = 0.3609348065578021
Iteration 170, Training loss = 0.35434589334405386
Iteration 180, Training loss = 0.3531533936755015
Iteration 190, Training loss = 0.3465417715219351
Iteration 200, Training loss = 0.3421930636350925
Iteration 210, Training loss = 0.3391227273700329
Iteration 220, Training loss = 0.33740165586081833
Iteration 230, Training loss = 0.3351602928283123
Iteration 240, Training loss = 0.33296132130691636
Iteration 250, Training loss = 0.32907539204909253
Iteration 260, Training loss = 0.3303700936241792
Iteration 270, Training loss = 0.3281290723154178
Iteration 280, Training loss = 0.3255634784985047
Iteration 290, Training loss = 0.3254183277201194
Model training time: 49.901209354400635
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.999492405125728
Iteration 10, Training loss = 0.3059214876534847
Iteration 20, Training loss = 0.2804761677980423
Iteration 30, Training loss = 0.26854512009483117
Iteration 40, Training loss = 0.261460198662602
Iteration 50, Training loss = 0.2560827670475611
Iteration 60, Training loss = 0.25565461757091373
Iteration 70, Training loss = 0.25019111212056416
Iteration 80, Training loss = 0.24771397933363914
Iteration 90, Training loss = 0.24512665160000324
Iteration 100, Training loss = 0.24346639961004257
Iteration 110, Training loss = 0.24158099881158426
Iteration 120, Training loss = 0.24002202270695797
Iteration 130, Training loss = 0.23807899345858738
Iteration 140, Training loss = 0.23723843965965968
Iteration 150, Training loss = 0.23483333865610453
Iteration 160, Training loss = 0.23390354559971735
Iteration 170, Training loss = 0.23183185105713514
Iteration 180, Training loss = 0.23115608712228444
Iteration 190, Training loss = 0.2296123094856739
Iteration 200, Training loss = 0.22874937407099283
Iteration 210, Training loss = 0.22669968309883887
Iteration 220, Training loss = 0.2262461714159984
Iteration 230, Training loss = 0.2244838484777854
Iteration 240, Training loss = 0.22287126601888582
Iteration 250, Training loss = 0.2209696899383114
Iteration 260, Training loss = 0.22175769894742048
Iteration 270, Training loss = 0.22165048666871512
Iteration 280, Training loss = 0.22067438782407686
Iteration 290, Training loss = 0.21767337584438232
Model training time: 56.40795421600342
Device: cuda
Iteration 0, Training loss = 0.9606215598491522
Iteration 10, Training loss = 0.30044557354771173
Iteration 20, Training loss = 0.2847610760766726
Iteration 30, Training loss = 0.27092560882178635
Iteration 40, Training loss = 0.26298877525214964
Iteration 50, Training loss = 0.2617322917167957
Iteration 60, Training loss = 0.2551353354580127
Iteration 70, Training loss = 0.2525012425791759
Iteration 80, Training loss = 0.2520809918642044
Iteration 90, Training loss = 0.2508789385454013
Iteration 100, Training loss = 0.2486666887998581
Iteration 110, Training loss = 0.2470488536816377
Iteration 120, Training loss = 0.24533092775023901
Iteration 130, Training loss = 0.24512360345285672
Iteration 140, Training loss = 0.24266540373747164
Iteration 150, Training loss = 0.24138736853805873
Iteration 160, Training loss = 0.23978309233028156
Iteration 170, Training loss = 0.23857446473378402
Iteration 180, Training loss = 0.23729142661278063
Iteration 190, Training loss = 0.23618174244004947
Iteration 200, Training loss = 0.23458548477635935
Iteration 210, Training loss = 0.2338213060910885
Iteration 220, Training loss = 0.23174362242794955
Iteration 230, Training loss = 0.23261547848009145
Iteration 240, Training loss = 0.23058738695600858
Iteration 250, Training loss = 0.2316451254658974
Iteration 260, Training loss = 0.22947435811735117
Iteration 270, Training loss = 0.22820796602620527
Iteration 280, Training loss = 0.228304665392408
Iteration 290, Training loss = 0.22964824205980852
Model training time: 55.62891387939453
Device: cuda
Iteration 0, Training loss = 1.1046518740745692
Iteration 10, Training loss = 0.3049894244625018
Iteration 20, Training loss = 0.2776923474784081
Iteration 30, Training loss = 0.26375266909599304
Iteration 40, Training loss = 0.25522259386399615
Iteration 50, Training loss = 0.2500260459402433
Iteration 60, Training loss = 0.24638011607413107
Iteration 70, Training loss = 0.24242894294170234
Iteration 80, Training loss = 0.24068756179454234
Iteration 90, Training loss = 0.23889558833952135
Iteration 100, Training loss = 0.23766451773162073
Iteration 110, Training loss = 0.23639230229533637
Iteration 120, Training loss = 0.23706895471192324
Iteration 130, Training loss = 0.23315412622804826
Iteration 140, Training loss = 0.2318868635652157
Iteration 150, Training loss = 0.23080330084149653
Iteration 160, Training loss = 0.2310558480138962
Iteration 170, Training loss = 0.22838940500066832
Iteration 180, Training loss = 0.22752099226300532
Iteration 190, Training loss = 0.22650084257698977
Iteration 200, Training loss = 0.2257093477707643
Iteration 210, Training loss = 0.22486088854762223
Iteration 220, Training loss = 0.22593692747446206
Iteration 230, Training loss = 0.22402757549515137
Iteration 240, Training loss = 0.2236856664172732
Iteration 250, Training loss = 0.22259073317624056
Iteration 260, Training loss = 0.2212911043316126
Iteration 270, Training loss = 0.22293150496597475
Iteration 280, Training loss = 0.21979883026618224
Iteration 290, Training loss = 0.22007350423015082
Model training time: 55.85556387901306
Device: cuda
Iteration 0, Training loss = 0.9746447096650417
Iteration 10, Training loss = 0.30259077867063194
Iteration 20, Training loss = 0.2822923223273112
Iteration 30, Training loss = 0.27104656699185187
Iteration 40, Training loss = 0.26515818086381143
Iteration 50, Training loss = 0.260786529057301
Iteration 60, Training loss = 0.25735905293661815
Iteration 70, Training loss = 0.2578887583401341
Iteration 80, Training loss = 0.2527242951954787
Iteration 90, Training loss = 0.2500917977438523
Iteration 100, Training loss = 0.24764548793721658
Iteration 110, Training loss = 0.2465139479877857
Iteration 120, Training loss = 0.24465857002024466
Iteration 130, Training loss = 0.24120098237807935
Iteration 140, Training loss = 0.24117599298747686
Iteration 150, Training loss = 0.23821892804251268
Iteration 160, Training loss = 0.23685781034426048
Iteration 170, Training loss = 0.23770521552516863
Iteration 180, Training loss = 0.23537321087832636
Iteration 190, Training loss = 0.23391086808764017
Iteration 200, Training loss = 0.23193279467523098
Iteration 210, Training loss = 0.23224166171768537
Iteration 220, Training loss = 0.23007272327175507
Iteration 230, Training loss = 0.23056573048233986
Iteration 240, Training loss = 0.22789093701598737
Iteration 250, Training loss = 0.22750025249731082
Iteration 260, Training loss = 0.2277406477727569
Iteration 270, Training loss = 0.2257881727642738
Iteration 280, Training loss = 0.22462887682307225
Iteration 290, Training loss = 0.22339734427917463
Model training time: 55.60767221450806
Device: cuda
Iteration 0, Training loss = 0.9845085533765646
Iteration 10, Training loss = 0.2965806754162678
Iteration 20, Training loss = 0.2793042515046321
Iteration 30, Training loss = 0.2680226263518517
Iteration 40, Training loss = 0.26148168126551
Iteration 50, Training loss = 0.25601405270684224
Iteration 60, Training loss = 0.2537010655953334
Iteration 70, Training loss = 0.2504886265557546
Iteration 80, Training loss = 0.24769850376133734
Iteration 90, Training loss = 0.24469989294616076
Iteration 100, Training loss = 0.2442715186625719
Iteration 110, Training loss = 0.2410870399326086
Iteration 120, Training loss = 0.23902965480318436
Iteration 130, Training loss = 0.23876244173600122
Iteration 140, Training loss = 0.23640162841631815
Iteration 150, Training loss = 0.2352665737271309
Iteration 160, Training loss = 0.23379603415154493
Iteration 170, Training loss = 0.23606247153992838
Iteration 180, Training loss = 0.23301406892446372
Iteration 190, Training loss = 0.23224860730652624
Iteration 200, Training loss = 0.23210593441931102
Iteration 210, Training loss = 0.2302164170317925
Iteration 220, Training loss = 0.22986894931930762
Iteration 230, Training loss = 0.22924230691905206
Iteration 240, Training loss = 0.22933000426452893
Iteration 250, Training loss = 0.2282475088364803
Iteration 260, Training loss = 0.22755328114502704
Iteration 270, Training loss = 0.22617252479092434
Iteration 280, Training loss = 0.22503169504209206
Iteration 290, Training loss = 0.2244462721909468
Model training time: 56.33850622177124
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0292294409412603
Iteration 10, Training loss = 0.9962328895926476
Iteration 20, Training loss = 0.9905682315046971
Iteration 30, Training loss = 0.9895704388618469
Iteration 40, Training loss = 0.9766303902635207
Iteration 50, Training loss = 0.9651294545485423
Iteration 60, Training loss = 0.9481369543534058
Iteration 70, Training loss = 0.9159902786979308
Iteration 80, Training loss = 0.8664588429606878
Iteration 90, Training loss = 0.7928531353290265
Iteration 100, Training loss = 0.6887175807586083
Iteration 110, Training loss = 0.5788905523144282
Iteration 120, Training loss = 0.48266906520495045
Iteration 130, Training loss = 0.4269093369635252
Iteration 140, Training loss = 0.3998990927178126
Iteration 150, Training loss = 0.3872316012588831
Iteration 160, Training loss = 0.3754864903883292
Iteration 170, Training loss = 0.3690791701754698
Iteration 180, Training loss = 0.36047519299273306
Iteration 190, Training loss = 0.35709837236656594
Iteration 200, Training loss = 0.3513294141739607
Iteration 210, Training loss = 0.34966556097452456
Iteration 220, Training loss = 0.3426956203407966
Iteration 230, Training loss = 0.3391075200186326
Iteration 240, Training loss = 0.33840055548800874
Iteration 250, Training loss = 0.3344743846414181
Iteration 260, Training loss = 0.3347431025826014
Iteration 270, Training loss = 0.3314565805575022
Iteration 280, Training loss = 0.3288057442181386
Iteration 290, Training loss = 0.32690284945643866
Model training time: 50.694814682006836
Device: cuda
Iteration 0, Training loss = 1.0303157361654134
Iteration 10, Training loss = 0.9987787667375344
Iteration 20, Training loss = 0.9904089105816988
Iteration 30, Training loss = 0.9855334735833682
Iteration 40, Training loss = 0.9753959396710763
Iteration 50, Training loss = 0.9682131862411132
Iteration 60, Training loss = 0.948565216591725
Iteration 70, Training loss = 0.9204242384204497
Iteration 80, Training loss = 0.8710648440397702
Iteration 90, Training loss = 0.7966670737816737
Iteration 100, Training loss = 0.6863846093989335
Iteration 110, Training loss = 0.5672106155409262
Iteration 120, Training loss = 0.466266629501031
Iteration 130, Training loss = 0.4126216103681005
Iteration 140, Training loss = 0.39034142818015355
Iteration 150, Training loss = 0.3793404802966576
Iteration 160, Training loss = 0.3734991034636131
Iteration 170, Training loss = 0.36641474068164825
Iteration 180, Training loss = 0.3603778707866485
Iteration 190, Training loss = 0.3547411479342442
Iteration 200, Training loss = 0.35093689738557887
Iteration 210, Training loss = 0.34839584964972276
Iteration 220, Training loss = 0.3442021096841647
Iteration 230, Training loss = 0.3405980570958211
Iteration 240, Training loss = 0.3393686250425302
Iteration 250, Training loss = 0.3364407941699028
Iteration 260, Training loss = 0.3335519733910377
Iteration 270, Training loss = 0.3313077297061682
Iteration 280, Training loss = 0.3290583724872424
Iteration 290, Training loss = 0.32871686194378597
Model training time: 49.85194993019104
Device: cuda
Iteration 0, Training loss = 1.0002213762356684
Iteration 10, Training loss = 1.0012193316450486
Iteration 20, Training loss = 0.9958660774506055
Iteration 30, Training loss = 0.9896781530517799
Iteration 40, Training loss = 0.9779584144170468
Iteration 50, Training loss = 0.9712342127011373
Iteration 60, Training loss = 0.9484890286739056
Iteration 70, Training loss = 0.9214534083238015
Iteration 80, Training loss = 0.8709644342844303
Iteration 90, Training loss = 0.7860630848086797
Iteration 100, Training loss = 0.6726094246483766
Iteration 110, Training loss = 0.550580394669221
Iteration 120, Training loss = 0.45660919323563576
Iteration 130, Training loss = 0.4064242851275664
Iteration 140, Training loss = 0.3857209367247728
Iteration 150, Training loss = 0.3706815146769469
Iteration 160, Training loss = 0.3643524433271243
Iteration 170, Training loss = 0.35821262259895986
Iteration 180, Training loss = 0.3530316093506721
Iteration 190, Training loss = 0.34637190817067254
Iteration 200, Training loss = 0.34257829963014674
Iteration 210, Training loss = 0.3383808084405385
Iteration 220, Training loss = 0.3346995429064219
Iteration 230, Training loss = 0.33273862001414484
Iteration 240, Training loss = 0.330631879516519
Iteration 250, Training loss = 0.3277462418549336
Iteration 260, Training loss = 0.3254137800003474
Iteration 270, Training loss = 0.32383402136082834
Iteration 280, Training loss = 0.3231203525971908
Iteration 290, Training loss = 0.3215532646729396
Model training time: 50.4206120967865
Device: cuda
Iteration 0, Training loss = 1.0386209659851515
Iteration 10, Training loss = 0.999470517039299
Iteration 20, Training loss = 0.9920363328777827
Iteration 30, Training loss = 0.9920642186815922
Iteration 40, Training loss = 0.9917648428907762
Iteration 50, Training loss = 0.9860394310492736
Iteration 60, Training loss = 0.9821692389937547
Iteration 70, Training loss = 0.9718542500184133
Iteration 80, Training loss = 0.9556886777281761
Iteration 90, Training loss = 0.9452508378487366
Iteration 100, Training loss = 0.9044582918286324
Iteration 110, Training loss = 0.8574422374367714
Iteration 120, Training loss = 0.770560004390203
Iteration 130, Training loss = 0.6581327599974779
Iteration 140, Training loss = 0.537178460508585
Iteration 150, Training loss = 0.4489457852278764
Iteration 160, Training loss = 0.40663556639964765
Iteration 170, Training loss = 0.38453479225818926
Iteration 180, Training loss = 0.3781739587967212
Iteration 190, Training loss = 0.36566941855618584
Iteration 200, Training loss = 0.35880990512669086
Iteration 210, Training loss = 0.3537331659060258
Iteration 220, Training loss = 0.3508113518070716
Iteration 230, Training loss = 0.3451522492731993
Iteration 240, Training loss = 0.3430218445853545
Iteration 250, Training loss = 0.3387386194215371
Iteration 260, Training loss = 0.33564596279309344
Iteration 270, Training loss = 0.33237941419849026
Iteration 280, Training loss = 0.33068553931438005
Iteration 290, Training loss = 0.3282373885695751
Model training time: 49.67432928085327
Device: cuda
Iteration 0, Training loss = 1.2506133610239396
Iteration 10, Training loss = 1.0007647625528848
Iteration 20, Training loss = 0.9907210864699804
Iteration 30, Training loss = 0.9951932057738304
Iteration 40, Training loss = 0.9858803961139458
Iteration 50, Training loss = 0.9827120911616546
Iteration 60, Training loss = 0.9780622749374464
Iteration 70, Training loss = 0.969869155723315
Iteration 80, Training loss = 0.9562404912251693
Iteration 90, Training loss = 0.9388008736647092
Iteration 100, Training loss = 0.9074521832741224
Iteration 110, Training loss = 0.8747010414416974
Iteration 120, Training loss = 0.8109015659070932
Iteration 130, Training loss = 0.7304269803258089
Iteration 140, Training loss = 0.6312313687342864
Iteration 150, Training loss = 0.5287481784247435
Iteration 160, Training loss = 0.4503867905586958
Iteration 170, Training loss = 0.4120100232271048
Iteration 180, Training loss = 0.38771501556038857
Iteration 190, Training loss = 0.3764639382178967
Iteration 200, Training loss = 0.3679236534696359
Iteration 210, Training loss = 0.36088485184770364
Iteration 220, Training loss = 0.35583933184926325
Iteration 230, Training loss = 0.3496132397021239
Iteration 240, Training loss = 0.3468226654311785
Iteration 250, Training loss = 0.3465596857265784
Iteration 260, Training loss = 0.3389924233062909
Iteration 270, Training loss = 0.33617032792132634
Iteration 280, Training loss = 0.33323849006914175
Iteration 290, Training loss = 0.33070125359182173
Model training time: 50.28467035293579
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9723018963749592
Iteration 10, Training loss = 0.3004206077983746
Iteration 20, Training loss = 0.28411981019263083
Iteration 30, Training loss = 0.2731350247676556
Iteration 40, Training loss = 0.2654397381612888
Iteration 50, Training loss = 0.2607275125785516
Iteration 60, Training loss = 0.2579896905674384
Iteration 70, Training loss = 0.2548674875153945
Iteration 80, Training loss = 0.25427986251620144
Iteration 90, Training loss = 0.2520783686867127
Iteration 100, Training loss = 0.24984469508322385
Iteration 110, Training loss = 0.24850608508747357
Iteration 120, Training loss = 0.24764331129307932
Iteration 130, Training loss = 0.24754962124503577
Iteration 140, Training loss = 0.24550565976936084
Iteration 150, Training loss = 0.24310029634776023
Iteration 160, Training loss = 0.24592295489632165
Iteration 170, Training loss = 0.24206601742368478
Iteration 180, Training loss = 0.23961937757065663
Iteration 190, Training loss = 0.24061694719757026
Iteration 200, Training loss = 0.23691346095158503
Iteration 210, Training loss = 0.2368917468075569
Iteration 220, Training loss = 0.23633589051090753
Iteration 230, Training loss = 0.23644763732758853
Iteration 240, Training loss = 0.23370510339736938
Iteration 250, Training loss = 0.2321026252152828
Iteration 260, Training loss = 0.23191480797070724
Iteration 270, Training loss = 0.23047102529268998
Iteration 280, Training loss = 0.22981855975320706
Iteration 290, Training loss = 0.2276473561874949
Model training time: 57.38091015815735
Device: cuda
Iteration 0, Training loss = 0.9759901498372738
Iteration 10, Training loss = 0.2993184832426218
Iteration 20, Training loss = 0.28384000779344487
Iteration 30, Training loss = 0.2701958158554939
Iteration 40, Training loss = 0.26357696993419755
Iteration 50, Training loss = 0.2580642501035562
Iteration 60, Training loss = 0.2543978364421771
Iteration 70, Training loss = 0.2488667987860166
Iteration 80, Training loss = 0.24646725691854954
Iteration 90, Training loss = 0.24419283465697214
Iteration 100, Training loss = 0.24135367421863171
Iteration 110, Training loss = 0.24123363855939645
Iteration 120, Training loss = 0.23919772098843867
Iteration 130, Training loss = 0.23641265771136835
Iteration 140, Training loss = 0.23545965652626294
Iteration 150, Training loss = 0.2349535649499068
Iteration 160, Training loss = 0.23102152533829212
Iteration 170, Training loss = 0.22903864794912246
Iteration 180, Training loss = 0.22886863250571948
Iteration 190, Training loss = 0.22577980633538502
Iteration 200, Training loss = 0.22590052580031064
Iteration 210, Training loss = 0.2248231555120303
Iteration 220, Training loss = 0.2226786046073987
Iteration 230, Training loss = 0.22262722640656507
Iteration 240, Training loss = 0.218550266411442
Iteration 250, Training loss = 0.22104308102279902
Iteration 260, Training loss = 0.2161556931499105
Iteration 270, Training loss = 0.216074380880365
Iteration 280, Training loss = 0.21511672953000435
Iteration 290, Training loss = 0.2136999053450731
Model training time: 55.99285173416138
Device: cuda
Iteration 0, Training loss = 0.9600021094083786
Iteration 10, Training loss = 0.2954804079177288
Iteration 20, Training loss = 0.27975818858696866
Iteration 30, Training loss = 0.2678058773565751
Iteration 40, Training loss = 0.26098463268807304
Iteration 50, Training loss = 0.2582581235239139
Iteration 60, Training loss = 0.2542708200904039
Iteration 70, Training loss = 0.2553530703656949
Iteration 80, Training loss = 0.24934289504129153
Iteration 90, Training loss = 0.2458699675849997
Iteration 100, Training loss = 0.24503278424246952
Iteration 110, Training loss = 0.24196213799027297
Iteration 120, Training loss = 0.239641868437712
Iteration 130, Training loss = 0.23990784872036713
Iteration 140, Training loss = 0.23825953342020512
Iteration 150, Training loss = 0.23864581242490274
Iteration 160, Training loss = 0.23485454759345606
Iteration 170, Training loss = 0.23337597156373355
Iteration 180, Training loss = 0.23330202182898155
Iteration 190, Training loss = 0.23091474676934573
Iteration 200, Training loss = 0.23112818527107054
Iteration 210, Training loss = 0.2312361943320586
Iteration 220, Training loss = 0.2280171804416638
Iteration 230, Training loss = 0.2268389222713617
Iteration 240, Training loss = 0.22609595811137786
Iteration 250, Training loss = 0.22581641633923238
Iteration 260, Training loss = 0.22512479599278706
Iteration 270, Training loss = 0.22316974122077227
Iteration 280, Training loss = 0.22267670843463677
Iteration 290, Training loss = 0.22236116545704696
Model training time: 56.67250061035156
Device: cuda
Iteration 0, Training loss = 0.964617908000946
Iteration 10, Training loss = 0.29778901530573
Iteration 20, Training loss = 0.2838625447967878
Iteration 30, Training loss = 0.27078749096164334
Iteration 40, Training loss = 0.2641687242744061
Iteration 50, Training loss = 0.25626054549446475
Iteration 60, Training loss = 0.2531347708919874
Iteration 70, Training loss = 0.25208609379254854
Iteration 80, Training loss = 0.2466573862788769
Iteration 90, Training loss = 0.24374907162900156
Iteration 100, Training loss = 0.24174626099948698
Iteration 110, Training loss = 0.2405175562374867
Iteration 120, Training loss = 0.23922847411953485
Iteration 130, Training loss = 0.2373709620621342
Iteration 140, Training loss = 0.23698596799602875
Iteration 150, Training loss = 0.23399228387727186
Iteration 160, Training loss = 0.23278063936875418
Iteration 170, Training loss = 0.2311826033087877
Iteration 180, Training loss = 0.2298114849254489
Iteration 190, Training loss = 0.22867768415464804
Iteration 200, Training loss = 0.22709241251532847
Iteration 210, Training loss = 0.2269301893046269
Iteration 220, Training loss = 0.22592962318314955
Iteration 230, Training loss = 0.22391298900430018
Iteration 240, Training loss = 0.22255251112465674
Iteration 250, Training loss = 0.22276233222622138
Iteration 260, Training loss = 0.22221464104950428
Iteration 270, Training loss = 0.22031266836879346
Iteration 280, Training loss = 0.21943518426269293
Iteration 290, Training loss = 0.22059367683071357
Model training time: 56.00579380989075
Device: cuda
Iteration 0, Training loss = 0.9679438718236409
Iteration 10, Training loss = 0.2950641307979822
Iteration 20, Training loss = 0.2794494115962432
Iteration 30, Training loss = 0.2692528311163187
Iteration 40, Training loss = 0.26241960061284214
Iteration 50, Training loss = 0.26086847785000616
Iteration 60, Training loss = 0.2558983307904922
Iteration 70, Training loss = 0.2561756680504634
Iteration 80, Training loss = 0.2517752864469702
Iteration 90, Training loss = 0.2505888408766343
Iteration 100, Training loss = 0.25139354814130527
Iteration 110, Training loss = 0.24916493692077124
Iteration 120, Training loss = 0.24740901947594607
Iteration 130, Training loss = 0.2475476858134453
Iteration 140, Training loss = 0.24793000920460775
Iteration 150, Training loss = 0.24260516937535542
Iteration 160, Training loss = 0.24159359788665405
Iteration 170, Training loss = 0.2403752405482989
Iteration 180, Training loss = 0.23984098391464123
Iteration 190, Training loss = 0.23725333523291808
Iteration 200, Training loss = 0.236134871840477
Iteration 210, Training loss = 0.23598734934169513
Iteration 220, Training loss = 0.23408221718497002
Iteration 230, Training loss = 0.23304025692721972
Iteration 240, Training loss = 0.23231247081779516
Iteration 250, Training loss = 0.23007892201153132
Iteration 260, Training loss = 0.2300624344497919
Iteration 270, Training loss = 0.2287239175862991
Iteration 280, Training loss = 0.22864838188084272
Iteration 290, Training loss = 0.2266843574694716
Model training time: 56.81535482406616
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0452785457556064
Iteration 10, Training loss = 0.9946467309043958
Iteration 20, Training loss = 0.9920760261324736
Iteration 30, Training loss = 0.9912974461913109
Iteration 40, Training loss = 0.9874870788592559
Iteration 50, Training loss = 0.9830647494930488
Iteration 60, Training loss = 0.9799420953943179
Iteration 70, Training loss = 0.9642749182306803
Iteration 80, Training loss = 0.950661436869548
Iteration 90, Training loss = 0.9210456787393644
Iteration 100, Training loss = 0.8805328114674642
Iteration 110, Training loss = 0.8240448362552203
Iteration 120, Training loss = 0.7346583094734412
Iteration 130, Training loss = 0.6288518693584663
Iteration 140, Training loss = 0.5300027759602437
Iteration 150, Training loss = 0.4585891469166829
Iteration 160, Training loss = 0.42152309589661086
Iteration 170, Training loss = 0.4018360276061755
Iteration 180, Training loss = 0.3902021675155713
Iteration 190, Training loss = 0.3775622301663344
Iteration 200, Training loss = 0.3705698743176002
Iteration 210, Training loss = 0.36347643543894476
Iteration 220, Training loss = 0.35740636117183244
Iteration 230, Training loss = 0.35511418159764546
Iteration 240, Training loss = 0.3489875276214801
Iteration 250, Training loss = 0.34506107580203277
Iteration 260, Training loss = 0.34095881349192214
Iteration 270, Training loss = 0.33670727579066384
Iteration 280, Training loss = 0.33457835548772263
Iteration 290, Training loss = 0.33179392479360104
Model training time: 49.48535656929016
Device: cuda
Iteration 0, Training loss = 0.9976722706968968
Iteration 10, Training loss = 0.990001770739372
Iteration 20, Training loss = 0.9837976900430826
Iteration 30, Training loss = 0.9668819033182584
Iteration 40, Training loss = 0.9509849450909175
Iteration 50, Training loss = 0.9219477102160454
Iteration 60, Training loss = 0.8734709821068324
Iteration 70, Training loss = 0.7969635361089156
Iteration 80, Training loss = 0.6858933762862132
Iteration 90, Training loss = 0.5642446213616774
Iteration 100, Training loss = 0.4677944022875566
Iteration 110, Training loss = 0.413356691885453
Iteration 120, Training loss = 0.38827928026708275
Iteration 130, Training loss = 0.37620512353113067
Iteration 140, Training loss = 0.3685638953286868
Iteration 150, Training loss = 0.3599696365686563
Iteration 160, Training loss = 0.3552340002587208
Iteration 170, Training loss = 0.35088736778841567
Iteration 180, Training loss = 0.34946907526598525
Iteration 190, Training loss = 0.34201390081300187
Iteration 200, Training loss = 0.33932095751739466
Iteration 210, Training loss = 0.34026688867463517
Iteration 220, Training loss = 0.3341331216864861
Iteration 230, Training loss = 0.33403420276366746
Iteration 240, Training loss = 0.32813446352688164
Iteration 250, Training loss = 0.32727653060394984
Iteration 260, Training loss = 0.3259356686702141
Iteration 270, Training loss = 0.32499777095822185
Iteration 280, Training loss = 0.32281860007116425
Iteration 290, Training loss = 0.32103171271200365
Model training time: 53.09764099121094
Device: cuda
Iteration 0, Training loss = 1.0127532487878432
Iteration 10, Training loss = 0.996437203998749
Iteration 20, Training loss = 0.9971018691475575
Iteration 30, Training loss = 0.9910525358640231
Iteration 40, Training loss = 0.9883151799440384
Iteration 50, Training loss = 0.9878659168115029
Iteration 60, Training loss = 0.9815038528579932
Iteration 70, Training loss = 0.9754354225901457
Iteration 80, Training loss = 0.9568088335486559
Iteration 90, Training loss = 0.929757528580152
Iteration 100, Training loss = 0.8965613727386181
Iteration 110, Training loss = 0.8341184579409086
Iteration 120, Training loss = 0.7446072955544178
Iteration 130, Training loss = 0.6305727715102526
Iteration 140, Training loss = 0.5162139004812791
Iteration 150, Training loss = 0.4396496747548764
Iteration 160, Training loss = 0.39878125961583394
Iteration 170, Training loss = 0.3799368337943004
Iteration 180, Training loss = 0.37119209637435585
Iteration 190, Training loss = 0.36282582953572273
Iteration 200, Training loss = 0.3569890237771548
Iteration 210, Training loss = 0.3519678059965372
Iteration 220, Training loss = 0.34719412874143857
Iteration 230, Training loss = 0.3447286713008697
Iteration 240, Training loss = 0.3395042789097016
Iteration 250, Training loss = 0.3377011393984923
Iteration 260, Training loss = 0.3351199966497146
Iteration 270, Training loss = 0.3316646937567454
Iteration 280, Training loss = 0.3292148888397675
Iteration 290, Training loss = 0.3277893027720543
Model training time: 53.79863262176514
Device: cuda
Iteration 0, Training loss = 1.0870360909746244
Iteration 10, Training loss = 0.9967926436891923
Iteration 20, Training loss = 0.9925517967114081
Iteration 30, Training loss = 0.9876451480847138
Iteration 40, Training loss = 0.9794416909034436
Iteration 50, Training loss = 0.9692910749178666
Iteration 60, Training loss = 0.9416060321606122
Iteration 70, Training loss = 0.9122717380523682
Iteration 80, Training loss = 0.8575572858636196
Iteration 90, Training loss = 0.7795535767307649
Iteration 100, Training loss = 0.6667048449699695
Iteration 110, Training loss = 0.5514557163875836
Iteration 120, Training loss = 0.4643031069292472
Iteration 130, Training loss = 0.41373540747624177
Iteration 140, Training loss = 0.3932384057686879
Iteration 150, Training loss = 0.37752850296405643
Iteration 160, Training loss = 0.3694562387581055
Iteration 170, Training loss = 0.36174848002309984
Iteration 180, Training loss = 0.35640684543893886
Iteration 190, Training loss = 0.35012076350931937
Iteration 200, Training loss = 0.345405360397238
Iteration 210, Training loss = 0.3408383142489653
Iteration 220, Training loss = 0.33768223498303157
Iteration 230, Training loss = 0.33393078426328987
Iteration 240, Training loss = 0.3351342467447886
Iteration 250, Training loss = 0.3286746208770917
Iteration 260, Training loss = 0.329179511190607
Iteration 270, Training loss = 0.3264351680588264
Iteration 280, Training loss = 0.32401826017751145
Iteration 290, Training loss = 0.3220643395414719
Model training time: 50.6251175403595
Device: cuda
Iteration 0, Training loss = 1.0034004564468677
Iteration 10, Training loss = 0.990769081390821
Iteration 20, Training loss = 0.9895567246354543
Iteration 30, Training loss = 0.978791492489668
Iteration 40, Training loss = 0.9668606364956269
Iteration 50, Training loss = 0.9551751103538734
Iteration 60, Training loss = 0.9245423571421549
Iteration 70, Training loss = 0.8905899536151153
Iteration 80, Training loss = 0.8421539429288644
Iteration 90, Training loss = 0.7651694010083492
Iteration 100, Training loss = 0.6692859263947377
Iteration 110, Training loss = 0.5654213706461283
Iteration 120, Training loss = 0.4805414049098125
Iteration 130, Training loss = 0.43099947159106916
Iteration 140, Training loss = 0.40088050282345367
Iteration 150, Training loss = 0.3865664694458246
Iteration 160, Training loss = 0.3810601555384122
Iteration 170, Training loss = 0.3729493881647403
Iteration 180, Training loss = 0.36658743482369643
Iteration 190, Training loss = 0.36309504666580605
Iteration 200, Training loss = 0.35651903986357725
Iteration 210, Training loss = 0.35718291725676793
Iteration 220, Training loss = 0.3497178696382504
Iteration 230, Training loss = 0.34697346346309554
Iteration 240, Training loss = 0.34264551303707635
Iteration 250, Training loss = 0.3416230936463063
Iteration 260, Training loss = 0.3388330808912332
Iteration 270, Training loss = 0.3362807319141351
Iteration 280, Training loss = 0.33392015586678797
Iteration 290, Training loss = 0.33016135815817577
Model training time: 49.622302532196045
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9871329619334295
Iteration 10, Training loss = 0.35089078488258213
Iteration 20, Training loss = 0.30839508743240285
Iteration 30, Training loss = 0.29793047790343946
Iteration 40, Training loss = 0.2908832342005693
Iteration 50, Training loss = 0.28420015940299403
Iteration 60, Training loss = 0.2791808622960861
Iteration 70, Training loss = 0.27589660768325514
Iteration 80, Training loss = 0.27254417366706407
Iteration 90, Training loss = 0.2700333664050469
Model training time: 14.257006168365479
Device: cuda
Iteration 0, Training loss = 0.9981240698924432
Iteration 10, Training loss = 0.32796107691067916
Iteration 20, Training loss = 0.30476830126001286
Iteration 30, Training loss = 0.29818255683550465
Iteration 40, Training loss = 0.2928067442889397
Iteration 50, Training loss = 0.28667606527988726
Iteration 60, Training loss = 0.281751483392257
Iteration 70, Training loss = 0.27692197607113767
Iteration 80, Training loss = 0.2735742273238989
Iteration 90, Training loss = 0.2688465889256734
Model training time: 14.218413591384888
Device: cuda
Iteration 0, Training loss = 0.9997991300546206
Iteration 10, Training loss = 0.3393107005036794
Iteration 20, Training loss = 0.3043822697721995
Iteration 30, Training loss = 0.2945250218304304
Iteration 40, Training loss = 0.2879324879210729
Iteration 50, Training loss = 0.28232902116500413
Iteration 60, Training loss = 0.27665990218520164
Iteration 70, Training loss = 0.27093680890706867
Iteration 80, Training loss = 0.2675447211815761
Iteration 90, Training loss = 0.2654087096452713
Model training time: 14.022070169448853
Device: cuda
Iteration 0, Training loss = 0.987827983040076
Iteration 10, Training loss = 0.33511435326475364
Iteration 20, Training loss = 0.3034306440788966
Iteration 30, Training loss = 0.2938906252384186
Iteration 40, Training loss = 0.2878344783989283
Iteration 50, Training loss = 0.282098776445939
Iteration 60, Training loss = 0.27636850567964405
Iteration 70, Training loss = 0.2732858437185104
Iteration 80, Training loss = 0.2691461518406868
Iteration 90, Training loss = 0.2660814013618689
Model training time: 14.389527559280396
Device: cuda
Iteration 0, Training loss = 0.9954796513685813
Iteration 10, Training loss = 0.3354734036211784
Iteration 20, Training loss = 0.3024391219593011
Iteration 30, Training loss = 0.29423351843769735
Iteration 40, Training loss = 0.288004040144957
Iteration 50, Training loss = 0.28399811025995475
Iteration 60, Training loss = 0.2776577432568257
Iteration 70, Training loss = 0.2730346941030942
Iteration 80, Training loss = 0.26879765953008944
Iteration 90, Training loss = 0.2655798712602028
Model training time: 14.163435935974121
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9998605950520589
Iteration 10, Training loss = 0.9983818897834191
Iteration 20, Training loss = 0.9943325794660128
Iteration 30, Training loss = 0.993581202167731
Iteration 40, Training loss = 0.9929761496874002
Iteration 50, Training loss = 0.9899687652404492
Iteration 60, Training loss = 0.9878796407809625
Iteration 70, Training loss = 0.9860756970368899
Iteration 80, Training loss = 0.9839140818669245
Iteration 90, Training loss = 0.9815760965530689
Model training time: 13.01814079284668
Device: cuda
Iteration 0, Training loss = 1.2149725395899553
Iteration 10, Training loss = 0.9999559429975656
Iteration 20, Training loss = 0.9978087968551196
Iteration 30, Training loss = 0.9960761608985754
Iteration 40, Training loss = 0.9946520087810663
Iteration 50, Training loss = 0.9932502978123151
Iteration 60, Training loss = 0.9913792002659577
Iteration 70, Training loss = 0.9917229826633747
Iteration 80, Training loss = 0.9894790695263789
Iteration 90, Training loss = 0.9862849448735898
Model training time: 13.324312686920166
Device: cuda
Iteration 0, Training loss = 1.1461803110746236
Iteration 10, Training loss = 0.9958591151695985
Iteration 20, Training loss = 0.9916304143575522
Iteration 30, Training loss = 0.9904470076927772
Iteration 40, Training loss = 0.9900046002406341
Iteration 50, Training loss = 0.9857822622244174
Iteration 60, Training loss = 0.9830046307582122
Iteration 70, Training loss = 0.9821771268661206
Iteration 80, Training loss = 0.977993966295169
Iteration 90, Training loss = 0.974745091337424
Model training time: 13.052720308303833
Device: cuda
Iteration 0, Training loss = 1.0089005449643502
Iteration 10, Training loss = 1.0012650352257948
Iteration 20, Training loss = 0.9985121545883325
Iteration 30, Training loss = 0.9964437702527413
Iteration 40, Training loss = 0.9975723910790223
Iteration 50, Training loss = 0.9981511441560892
Iteration 60, Training loss = 0.9937634548315635
Iteration 70, Training loss = 0.9924875119557748
Iteration 80, Training loss = 0.9919364280425585
Iteration 90, Training loss = 0.9898103074385569
Model training time: 13.099069833755493
Device: cuda
Iteration 0, Training loss = 1.6381467122298021
Iteration 10, Training loss = 1.0073676980458772
Iteration 20, Training loss = 1.0047421088585486
Iteration 30, Training loss = 1.0023521127609105
Iteration 40, Training loss = 1.001942216203763
Iteration 50, Training loss = 1.0012832857095277
Iteration 60, Training loss = 0.9994643938082916
Iteration 70, Training loss = 0.9991328028532175
Iteration 80, Training loss = 0.9985158386138769
Iteration 90, Training loss = 0.9986750529362605
Model training time: 12.890637397766113
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.070938569995073
Iteration 10, Training loss = 0.571481361412085
Iteration 20, Training loss = 0.3716483706465134
Iteration 30, Training loss = 0.3227323308013953
Iteration 40, Training loss = 0.30108635414105195
Iteration 50, Training loss = 0.28902501011124027
Iteration 60, Training loss = 0.2805444340293224
Iteration 70, Training loss = 0.2749872434024627
Iteration 80, Training loss = 0.27007600693748546
Iteration 90, Training loss = 0.2662625318536392
Model training time: 14.157594442367554
Device: cuda
Iteration 0, Training loss = 0.9963098603945512
Iteration 10, Training loss = 0.38489388158688176
Iteration 20, Training loss = 0.31970742172919786
Iteration 30, Training loss = 0.30419252468989444
Iteration 40, Training loss = 0.296535761310504
Iteration 50, Training loss = 0.2931181857219109
Iteration 60, Training loss = 0.2881203225025764
Iteration 70, Training loss = 0.2843544947413298
Iteration 80, Training loss = 0.2786751644542584
Iteration 90, Training loss = 0.27507550383989626
Model training time: 14.150398969650269
Device: cuda
Iteration 0, Training loss = 1.046728905576926
Iteration 10, Training loss = 0.5217466348638902
Iteration 20, Training loss = 0.35499686346604276
Iteration 30, Training loss = 0.31520799776682484
Iteration 40, Training loss = 0.29777901992201805
Iteration 50, Training loss = 0.28636578814341473
Iteration 60, Training loss = 0.28044870409827966
Iteration 70, Training loss = 0.27544885501265526
Iteration 80, Training loss = 0.2719385348833524
Iteration 90, Training loss = 0.2687562397466256
Model training time: 14.074983835220337
Device: cuda
Iteration 0, Training loss = 1.010893246302238
Iteration 10, Training loss = 0.41119713680102277
Iteration 20, Training loss = 0.32633780802671725
Iteration 30, Training loss = 0.3024052788431828
Iteration 40, Training loss = 0.2918463203196342
Iteration 50, Training loss = 0.28470987282120264
Iteration 60, Training loss = 0.2792083036441069
Iteration 70, Training loss = 0.2733346304068199
Iteration 80, Training loss = 0.26884808076115757
Iteration 90, Training loss = 0.2657475617642586
Model training time: 14.22263503074646
Device: cuda
Iteration 0, Training loss = 1.0584699362516403
Iteration 10, Training loss = 0.5053388459178118
Iteration 20, Training loss = 0.35592686442228466
Iteration 30, Training loss = 0.319955850736453
Iteration 40, Training loss = 0.3032436545651693
Iteration 50, Training loss = 0.28961127881820387
Iteration 60, Training loss = 0.28126027463720393
Iteration 70, Training loss = 0.27370993764354634
Iteration 80, Training loss = 0.2702866153648266
Iteration 90, Training loss = 0.2647123070290455
Model training time: 14.095195293426514
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.5789585411548615
Iteration 10, Training loss = 1.0086055959646518
Iteration 20, Training loss = 1.0012404735271747
Iteration 30, Training loss = 0.9999926949922855
Iteration 40, Training loss = 1.0021573087343802
Iteration 50, Training loss = 0.9988808459960498
Iteration 60, Training loss = 1.000854486456284
Iteration 70, Training loss = 1.0017253573124225
Iteration 80, Training loss = 0.9997465782440625
Iteration 90, Training loss = 0.9985498338937759
Model training time: 13.067803382873535
Device: cuda
Iteration 0, Training loss = 1.053543651333222
Iteration 10, Training loss = 1.0078088331681032
Iteration 20, Training loss = 1.0063423835314238
Iteration 30, Training loss = 1.0067582508692374
Iteration 40, Training loss = 1.003530313189213
Iteration 50, Training loss = 1.0044905096292496
Iteration 60, Training loss = 1.0013223187281535
Iteration 70, Training loss = 1.0034832094724362
Iteration 80, Training loss = 1.0029844825084393
Iteration 90, Training loss = 1.0028902441263199
Model training time: 12.977706909179688
Device: cuda
Iteration 0, Training loss = 1.2269026006643589
Iteration 10, Training loss = 1.0034884970921736
Iteration 20, Training loss = 1.000700980424881
Iteration 30, Training loss = 1.001274369083918
Iteration 40, Training loss = 1.0000648693396494
Iteration 50, Training loss = 0.9980414257599757
Iteration 60, Training loss = 0.998699772816438
Iteration 70, Training loss = 0.9967562980376757
Iteration 80, Training loss = 0.9953968754181495
Iteration 90, Training loss = 0.9953142691117066
Model training time: 13.161170244216919
Device: cuda
Iteration 0, Training loss = 1.1047217077933824
Iteration 10, Training loss = 1.0031974006157656
Iteration 20, Training loss = 1.0010097645796263
Iteration 30, Training loss = 1.0004588606265874
Iteration 40, Training loss = 0.9993398487567902
Iteration 50, Training loss = 1.0011051560823734
Iteration 60, Training loss = 0.9984487845347478
Iteration 70, Training loss = 1.0035197413884676
Iteration 80, Training loss = 0.9987198939690223
Iteration 90, Training loss = 0.9981019244744227
Model training time: 12.854031324386597
Device: cuda
Iteration 0, Training loss = 1.5048415592083564
Iteration 10, Training loss = 1.0003587202383921
Iteration 20, Training loss = 0.9969967981943717
Iteration 30, Training loss = 0.9976847355182354
Iteration 40, Training loss = 0.9968963299806302
Iteration 50, Training loss = 0.9967974963096472
Iteration 60, Training loss = 0.9955103695392609
Iteration 70, Training loss = 0.9951000878444085
Iteration 80, Training loss = 0.9940571039915085
Iteration 90, Training loss = 0.9946899448449795
Model training time: 13.278115272521973
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0004756049467967
Iteration 10, Training loss = 0.36536306085494846
Iteration 20, Training loss = 0.3113092350272032
Iteration 30, Training loss = 0.29939723301392335
Iteration 40, Training loss = 0.294583299698738
Iteration 50, Training loss = 0.2894159690118753
Iteration 60, Training loss = 0.2832969163472836
Iteration 70, Training loss = 0.279012817889452
Iteration 80, Training loss = 0.27469537063286853
Iteration 90, Training loss = 0.27109882493431753
Model training time: 14.144937515258789
Device: cuda
Iteration 0, Training loss = 1.0207594747726734
Iteration 10, Training loss = 0.37152224893753344
Iteration 20, Training loss = 0.3130001422877495
Iteration 30, Training loss = 0.2979665275376577
Iteration 40, Training loss = 0.28945501893758774
Iteration 50, Training loss = 0.28384228165333086
Iteration 60, Training loss = 0.27900457898011577
Iteration 70, Training loss = 0.2751520009568104
Iteration 80, Training loss = 0.27155516382593375
Iteration 90, Training loss = 0.26924475397054964
Model training time: 14.21782898902893
Device: cuda
Iteration 0, Training loss = 1.023564385680052
Iteration 10, Training loss = 0.37974530573074633
Iteration 20, Training loss = 0.30982819400154626
Iteration 30, Training loss = 0.29765967107736147
Iteration 40, Training loss = 0.29114985122130466
Iteration 50, Training loss = 0.28650967003061223
Iteration 60, Training loss = 0.2814566707954957
Iteration 70, Training loss = 0.2780458494447745
Iteration 80, Training loss = 0.27545392885804176
Iteration 90, Training loss = 0.27338215192923176
Model training time: 14.185223817825317
Device: cuda
Iteration 0, Training loss = 0.9966651121011147
Iteration 10, Training loss = 0.3402917981147766
Iteration 20, Training loss = 0.30415864145526517
Iteration 30, Training loss = 0.2940804112989169
Iteration 40, Training loss = 0.2881377660311185
Iteration 50, Training loss = 0.28232289535494953
Iteration 60, Training loss = 0.2780447911757689
Iteration 70, Training loss = 0.27173179072829395
Iteration 80, Training loss = 0.26751292038422364
Iteration 90, Training loss = 0.26392245808473
Model training time: 14.260233640670776
Device: cuda
Iteration 0, Training loss = 1.0024415988188524
Iteration 10, Training loss = 0.34155596792697906
Iteration 20, Training loss = 0.30546816094563556
Iteration 30, Training loss = 0.29642370916329897
Iteration 40, Training loss = 0.29035350107229674
Iteration 50, Training loss = 0.28467637673020363
Iteration 60, Training loss = 0.27933113792767894
Iteration 70, Training loss = 0.27288304326625973
Iteration 80, Training loss = 0.2688409167413528
Iteration 90, Training loss = 0.2656150044730076
Model training time: 14.227142333984375
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0737983802190194
Iteration 10, Training loss = 1.0012707263231277
Iteration 20, Training loss = 1.000293954060628
Iteration 30, Training loss = 1.000036677488914
Iteration 40, Training loss = 0.999246400136214
Iteration 50, Training loss = 0.9985468376141328
Iteration 60, Training loss = 0.9985429976995175
Iteration 70, Training loss = 0.9957101574310889
Iteration 80, Training loss = 0.9969536054592866
Iteration 90, Training loss = 0.9963208173330014
Model training time: 13.0408456325531
Device: cuda
Iteration 0, Training loss = 1.0136916006986911
Iteration 10, Training loss = 0.9956780683535796
Iteration 20, Training loss = 0.9975744233681605
Iteration 30, Training loss = 0.995594057899255
Iteration 40, Training loss = 0.9929714157031133
Iteration 50, Training loss = 0.9928267345978663
Iteration 60, Training loss = 0.9904015820759994
Iteration 70, Training loss = 0.988735522215183
Iteration 80, Training loss = 0.9894424516421098
Iteration 90, Training loss = 0.9869343615495242
Model training time: 13.000989198684692
Device: cuda
Iteration 0, Training loss = 1.0431395230384974
Iteration 10, Training loss = 0.9895272977077044
Iteration 20, Training loss = 0.9878818450065759
Iteration 30, Training loss = 0.9864709125115321
Iteration 40, Training loss = 0.9846824762912897
Iteration 50, Training loss = 0.9834778217168955
Iteration 60, Training loss = 0.9784804376272055
Iteration 70, Training loss = 0.9765547273250726
Iteration 80, Training loss = 0.9734712988138199
Iteration 90, Training loss = 0.971816897392273
Model training time: 13.209803104400635
Device: cuda
Iteration 0, Training loss = 1.0499785118378127
Iteration 10, Training loss = 0.9934739906054276
Iteration 20, Training loss = 0.9915017909728564
Iteration 30, Training loss = 0.9913922892167017
Iteration 40, Training loss = 0.9905553276722248
Iteration 50, Training loss = 0.9873514542212853
Iteration 60, Training loss = 0.9862052293924185
Iteration 70, Training loss = 0.9804998429921957
Iteration 80, Training loss = 0.9824561155759372
Iteration 90, Training loss = 0.9772558647852677
Model training time: 13.296198606491089
Device: cuda
Iteration 0, Training loss = 1.121496169612958
Iteration 10, Training loss = 1.003051214493238
Iteration 20, Training loss = 1.0019441854495268
Iteration 30, Training loss = 1.001256430378327
Iteration 40, Training loss = 1.000419706106186
Iteration 50, Training loss = 0.9985985434972323
Iteration 60, Training loss = 0.9984435989306524
Iteration 70, Training loss = 0.9955035310525161
Iteration 80, Training loss = 0.9952745598096114
Iteration 90, Training loss = 0.9957438237391986
Model training time: 13.159714698791504
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.982573552773549
Iteration 10, Training loss = 0.3384952863248495
Iteration 20, Training loss = 0.30745287010302913
Iteration 30, Training loss = 0.3001207262277603
Iteration 40, Training loss = 0.294613097436153
Iteration 50, Training loss = 0.2905045188963413
Iteration 60, Training loss = 0.2848818009862533
Iteration 70, Training loss = 0.27959631841916305
Iteration 80, Training loss = 0.275332238811713
Iteration 90, Training loss = 0.2721517894130487
Iteration 100, Training loss = 0.2683177667741592
Iteration 110, Training loss = 0.2662920974768125
Iteration 120, Training loss = 0.26363109109493404
Iteration 130, Training loss = 0.26129744803676236
Iteration 140, Training loss = 0.2603153104965503
Iteration 150, Training loss = 0.2574274611587708
Iteration 160, Training loss = 0.25530322325917393
Iteration 170, Training loss = 0.25332036786354506
Iteration 180, Training loss = 0.2522072491164391
Iteration 190, Training loss = 0.2503871229978708
Model training time: 28.12276816368103
Device: cuda
Iteration 0, Training loss = 0.9971891045570374
Iteration 10, Training loss = 0.34439245611429214
Iteration 20, Training loss = 0.3079421164897772
Iteration 30, Training loss = 0.29909089093024915
Iteration 40, Training loss = 0.2937704413556136
Iteration 50, Training loss = 0.28820187512498635
Iteration 60, Training loss = 0.28288579330994534
Iteration 70, Training loss = 0.27816985748135126
Iteration 80, Training loss = 0.27296213948955905
Iteration 90, Training loss = 0.2680167994247033
Iteration 100, Training loss = 0.2661407392185468
Iteration 110, Training loss = 0.2628828350168008
Iteration 120, Training loss = 0.2600635324533169
Iteration 130, Training loss = 0.25899560950123346
Iteration 140, Training loss = 0.25667189434170723
Iteration 150, Training loss = 0.2545098180954273
Iteration 160, Training loss = 0.25240794110756654
Iteration 170, Training loss = 0.25153007157720053
Iteration 180, Training loss = 0.2501902694885547
Iteration 190, Training loss = 0.24823026473705584
Model training time: 28.056980848312378
Device: cuda
Iteration 0, Training loss = 1.0784119883408914
Iteration 10, Training loss = 0.3818154134429418
Iteration 20, Training loss = 0.3115350750203316
Iteration 30, Training loss = 0.2987254657424413
Iteration 40, Training loss = 0.2924562386022164
Iteration 50, Training loss = 0.285773434318029
Iteration 60, Training loss = 0.28103093287119496
Iteration 70, Training loss = 0.274555349865785
Iteration 80, Training loss = 0.2700862930371211
Iteration 90, Training loss = 0.2675300466899688
Iteration 100, Training loss = 0.2628886421712545
Iteration 110, Training loss = 0.2608354567335202
Iteration 120, Training loss = 0.2579801950890284
Iteration 130, Training loss = 0.25624746542710525
Iteration 140, Training loss = 0.25416760977644187
Iteration 150, Training loss = 0.2527066030754493
Iteration 160, Training loss = 0.2509307451546192
Iteration 170, Training loss = 0.24816049033632645
Iteration 180, Training loss = 0.24773037806153297
Iteration 190, Training loss = 0.2466966280570397
Model training time: 28.827336072921753
Device: cuda
Iteration 0, Training loss = 1.006735669878813
Iteration 10, Training loss = 0.34712592741617787
Iteration 20, Training loss = 0.3069757314828726
Iteration 30, Training loss = 0.29591561452700543
Iteration 40, Training loss = 0.29021486238791394
Iteration 50, Training loss = 0.28310902474018246
Iteration 60, Training loss = 0.2774897747888015
Iteration 70, Training loss = 0.2730270598370295
Iteration 80, Training loss = 0.26809201332239
Iteration 90, Training loss = 0.264958126613727
Iteration 100, Training loss = 0.2637050091647185
Iteration 110, Training loss = 0.26176870786226714
Iteration 120, Training loss = 0.25869398248883396
Iteration 130, Training loss = 0.25767001051169175
Iteration 140, Training loss = 0.2561056808783458
Iteration 150, Training loss = 0.25417153709209883
Iteration 160, Training loss = 0.253138768271758
Iteration 170, Training loss = 0.2521011964059793
Iteration 180, Training loss = 0.25137922454338807
Iteration 190, Training loss = 0.2497104758826586
Model training time: 28.076883792877197
Device: cuda
Iteration 0, Training loss = 1.025710343168332
Iteration 10, Training loss = 0.3598862359156975
Iteration 20, Training loss = 0.30976428062869954
Iteration 30, Training loss = 0.2978382405753319
Iteration 40, Training loss = 0.2902669393672393
Iteration 50, Training loss = 0.28372761091360676
Iteration 60, Training loss = 0.27549252114616907
Iteration 70, Training loss = 0.26935840111512405
Iteration 80, Training loss = 0.2655548235544792
Iteration 90, Training loss = 0.26342851095474684
Iteration 100, Training loss = 0.26064301282167435
Iteration 110, Training loss = 0.2594536431133747
Iteration 120, Training loss = 0.2578664250098742
Iteration 130, Training loss = 0.25658549540317976
Iteration 140, Training loss = 0.2550093551667837
Iteration 150, Training loss = 0.25362549980099386
Iteration 160, Training loss = 0.25236550775858074
Iteration 170, Training loss = 0.2514263064815448
Iteration 180, Training loss = 0.2506073581484648
Iteration 190, Training loss = 0.24862236672869095
Model training time: 27.9747896194458
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1012441321061208
Iteration 10, Training loss = 0.9970110207796097
Iteration 20, Training loss = 0.9956108732865407
Iteration 30, Training loss = 0.9954312672981849
Iteration 40, Training loss = 0.9936436093770541
Iteration 50, Training loss = 0.9928776415494772
Iteration 60, Training loss = 0.991571829869197
Iteration 70, Training loss = 0.9914810141691794
Iteration 80, Training loss = 0.990025330048341
Iteration 90, Training loss = 0.989495343886889
Iteration 100, Training loss = 0.9874744140184842
Iteration 110, Training loss = 0.9850664723377961
Iteration 120, Training loss = 0.9824827577059085
Iteration 130, Training loss = 0.9808330856836759
Iteration 140, Training loss = 0.9807720528199122
Iteration 150, Training loss = 0.9773174146047006
Iteration 160, Training loss = 0.9751084206195978
Iteration 170, Training loss = 0.9697682765813974
Iteration 180, Training loss = 0.9679296956612513
Iteration 190, Training loss = 0.9638973806913083
Model training time: 25.75548529624939
Device: cuda
Iteration 0, Training loss = 1.1845174913222973
Iteration 10, Training loss = 1.0006342014441123
Iteration 20, Training loss = 0.9985210207792429
Iteration 30, Training loss = 0.9954436879891616
Iteration 40, Training loss = 0.9941431157864057
Iteration 50, Training loss = 0.9929366478553185
Iteration 60, Training loss = 0.9912309784155625
Iteration 70, Training loss = 0.9894476338074758
Iteration 80, Training loss = 0.9861458207552249
Iteration 90, Training loss = 0.9856208562850952
Iteration 100, Training loss = 0.9846256398237668
Iteration 110, Training loss = 0.9817498142902668
Iteration 120, Training loss = 0.9771956514853698
Iteration 130, Training loss = 0.974815886754256
Iteration 140, Training loss = 0.9728584725123185
Iteration 150, Training loss = 0.9674434180443103
Iteration 160, Training loss = 0.9630631426206002
Iteration 170, Training loss = 0.9593589729987658
Iteration 180, Training loss = 0.954921196286495
Iteration 190, Training loss = 0.9498781768175272
Model training time: 26.107136249542236
Device: cuda
Iteration 0, Training loss = 1.0164763904534853
Iteration 10, Training loss = 0.9970637342104545
Iteration 20, Training loss = 0.9954867798548478
Iteration 30, Training loss = 0.9930636023099606
Iteration 40, Training loss = 0.9894886716054037
Iteration 50, Training loss = 0.9892431027614154
Iteration 60, Training loss = 0.984239222911688
Iteration 70, Training loss = 0.9811841753812937
Iteration 80, Training loss = 0.9768795004257789
Iteration 90, Training loss = 0.9713129286582654
Iteration 100, Training loss = 0.9703577218147424
Iteration 110, Training loss = 0.96450430727922
Iteration 120, Training loss = 0.9608567723861108
Iteration 130, Training loss = 0.9534468020384128
Iteration 140, Training loss = 0.9479783177375793
Iteration 150, Training loss = 0.9403327818100269
Iteration 160, Training loss = 0.9342145644701444
Iteration 170, Training loss = 0.9269436632211392
Iteration 180, Training loss = 0.9149120094684454
Iteration 190, Training loss = 0.9021278012257355
Model training time: 26.27824378013611
Device: cuda
Iteration 0, Training loss = 1.1375909046484873
Iteration 10, Training loss = 1.0017657486292033
Iteration 20, Training loss = 0.9999152284402114
Iteration 30, Training loss = 0.9982116910127493
Iteration 40, Training loss = 0.9982561847338309
Iteration 50, Training loss = 0.9968769733722394
Iteration 60, Training loss = 0.9956085739227442
Iteration 70, Training loss = 0.9947237544334852
Iteration 80, Training loss = 0.9934959021898416
Iteration 90, Training loss = 0.9920529826329305
Iteration 100, Training loss = 0.9931005606284509
Iteration 110, Training loss = 0.9911176172586588
Iteration 120, Training loss = 0.9893755053098385
Iteration 130, Training loss = 0.9860816059204248
Iteration 140, Training loss = 0.9846614908713561
Iteration 150, Training loss = 0.9829031997002088
Iteration 160, Training loss = 0.9822830271262389
Iteration 170, Training loss = 0.9790467894994296
Iteration 180, Training loss = 0.9749928884781324
Iteration 190, Training loss = 0.9738668971336805
Model training time: 26.135504961013794
Device: cuda
Iteration 0, Training loss = 1.0018295508164625
Iteration 10, Training loss = 0.9926323157090408
Iteration 20, Training loss = 0.9912042101988425
Iteration 30, Training loss = 0.9897610820256747
Iteration 40, Training loss = 0.988193507377918
Iteration 50, Training loss = 0.9860458305248847
Iteration 60, Training loss = 0.9836989079530423
Iteration 70, Training loss = 0.9806131881016952
Iteration 80, Training loss = 0.9765731359903629
Iteration 90, Training loss = 0.9757854663408719
Iteration 100, Training loss = 0.9699055598332331
Iteration 110, Training loss = 0.9666460878573931
Iteration 120, Training loss = 0.9634025280292218
Iteration 130, Training loss = 0.9565164744853973
Iteration 140, Training loss = 0.9524242992584522
Iteration 150, Training loss = 0.9464592188596725
Iteration 160, Training loss = 0.9388998769796811
Iteration 170, Training loss = 0.930627629160881
Iteration 180, Training loss = 0.9209379393320817
Iteration 190, Training loss = 0.9109922166054065
Model training time: 26.03501296043396
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9981285505569898
Iteration 10, Training loss = 0.4041130898090509
Iteration 20, Training loss = 0.3281047146480817
Iteration 30, Training loss = 0.3065252464551192
Iteration 40, Training loss = 0.29667527543810696
Iteration 50, Training loss = 0.28887276723980904
Iteration 60, Training loss = 0.2825801664820084
Iteration 70, Training loss = 0.2784519585279318
Iteration 80, Training loss = 0.27428920882252544
Iteration 90, Training loss = 0.27047634898469997
Iteration 100, Training loss = 0.268281401349948
Iteration 110, Training loss = 0.2657706026847546
Iteration 120, Training loss = 0.2631449091892976
Iteration 130, Training loss = 0.26156319964390534
Iteration 140, Training loss = 0.2605912997745551
Iteration 150, Training loss = 0.2592167387214991
Iteration 160, Training loss = 0.25774896001586545
Iteration 170, Training loss = 0.2565157482257256
Iteration 180, Training loss = 0.2548789975161736
Iteration 190, Training loss = 0.25323191944223183
Model training time: 28.03390669822693
Device: cuda
Iteration 0, Training loss = 1.0270688648407276
Iteration 10, Training loss = 0.44619549409701276
Iteration 20, Training loss = 0.32401635153935504
Iteration 30, Training loss = 0.30179328242173564
Iteration 40, Training loss = 0.2924999055954126
Iteration 50, Training loss = 0.28628702519031674
Iteration 60, Training loss = 0.28244683834222645
Iteration 70, Training loss = 0.2780074391227502
Iteration 80, Training loss = 0.2757153081206175
Iteration 90, Training loss = 0.2738752499795877
Iteration 100, Training loss = 0.27078100035970026
Iteration 110, Training loss = 0.26987848488184124
Iteration 120, Training loss = 0.2667133848254497
Iteration 130, Training loss = 0.2648059086730847
Iteration 140, Training loss = 0.2641647294736825
Iteration 150, Training loss = 0.26318257187421507
Iteration 160, Training loss = 0.2617962503662476
Iteration 170, Training loss = 0.2607776038348675
Iteration 180, Training loss = 0.259786214106358
Iteration 190, Training loss = 0.2588097544816824
Model training time: 28.495588541030884
Device: cuda
Iteration 0, Training loss = 1.4488578897256117
Iteration 10, Training loss = 0.811654772896033
Iteration 20, Training loss = 0.4465981899545743
Iteration 30, Training loss = 0.3594193080296883
Iteration 40, Training loss = 0.32089494770536053
Iteration 50, Training loss = 0.3004650166974618
Iteration 60, Training loss = 0.2887566089630127
Iteration 70, Training loss = 0.28054534959105343
Iteration 80, Training loss = 0.27565533123337305
Iteration 90, Training loss = 0.27170005010870785
Iteration 100, Training loss = 0.26817900429551417
Iteration 110, Training loss = 0.2664020253488651
Iteration 120, Training loss = 0.2641746267103232
Iteration 130, Training loss = 0.2630715992015142
Iteration 140, Training loss = 0.26127518770786434
Iteration 150, Training loss = 0.2594266227231576
Iteration 160, Training loss = 0.25922042675889456
Iteration 170, Training loss = 0.2572118737376653
Iteration 180, Training loss = 0.25692479283763814
Iteration 190, Training loss = 0.2547715154404824
Model training time: 28.48405408859253
Device: cuda
Iteration 0, Training loss = 1.0188621397201831
Iteration 10, Training loss = 0.43563368228765637
Iteration 20, Training loss = 0.32150459919984525
Iteration 30, Training loss = 0.2998030005166164
Iteration 40, Training loss = 0.2909127499621648
Iteration 50, Training loss = 0.28381786294854605
Iteration 60, Training loss = 0.2790830149673499
Iteration 70, Training loss = 0.2748265957029966
Iteration 80, Training loss = 0.27217961160036236
Iteration 90, Training loss = 0.26923810002895504
Iteration 100, Training loss = 0.2674226869757359
Iteration 110, Training loss = 0.26553533455500233
Iteration 120, Training loss = 0.2639488586439536
Iteration 130, Training loss = 0.2632350643666891
Iteration 140, Training loss = 0.26161450892686844
Iteration 150, Training loss = 0.2607881816533896
Iteration 160, Training loss = 0.2597360404638144
Iteration 170, Training loss = 0.2599520047123616
Iteration 180, Training loss = 0.2583804815434493
Iteration 190, Training loss = 0.25621631598243344
Model training time: 28.72132968902588
Device: cuda
Iteration 0, Training loss = 0.9919939705958734
Iteration 10, Training loss = 0.42386636023338026
Iteration 20, Training loss = 0.3342174876194734
Iteration 30, Training loss = 0.3078672636586886
Iteration 40, Training loss = 0.2931837087067274
Iteration 50, Training loss = 0.28479737931719196
Iteration 60, Training loss = 0.2801426096031299
Iteration 70, Training loss = 0.2759589747740672
Iteration 80, Training loss = 0.27251356467604637
Iteration 90, Training loss = 0.2692452004322639
Iteration 100, Training loss = 0.26683620420786053
Iteration 110, Training loss = 0.2644584250564759
Iteration 120, Training loss = 0.26219340041279793
Iteration 130, Training loss = 0.2613091107744437
Iteration 140, Training loss = 0.25999137128774935
Iteration 150, Training loss = 0.25886948607288873
Iteration 160, Training loss = 0.25689861493615
Iteration 170, Training loss = 0.25587371507516277
Iteration 180, Training loss = 0.2559081513721209
Iteration 190, Training loss = 0.2540612670664604
Model training time: 28.2186119556427
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.3071307161679635
Iteration 10, Training loss = 1.0066734540920992
Iteration 20, Training loss = 1.0048664819735746
Iteration 30, Training loss = 1.003485313974894
Iteration 40, Training loss = 1.000659038241093
Iteration 50, Training loss = 1.0004003265729318
Iteration 60, Training loss = 1.0012444785008063
Iteration 70, Training loss = 0.999840470460745
Iteration 80, Training loss = 1.000223912871801
Iteration 90, Training loss = 0.9992231222299429
Iteration 100, Training loss = 0.9993600031504264
Iteration 110, Training loss = 0.998655752493785
Iteration 120, Training loss = 0.9960676672366949
Iteration 130, Training loss = 0.9963450855933703
Iteration 140, Training loss = 0.9968157903506205
Iteration 150, Training loss = 0.9967402380246383
Iteration 160, Training loss = 0.9950228860745063
Iteration 170, Training loss = 0.9932708453673583
Iteration 180, Training loss = 0.9937229213806299
Iteration 190, Training loss = 0.9941240892960475
Model training time: 26.115783214569092
Device: cuda
Iteration 0, Training loss = 1.011142680278191
Iteration 10, Training loss = 0.9971111806539389
Iteration 20, Training loss = 0.9964535030034872
Iteration 30, Training loss = 0.9941672189877584
Iteration 40, Training loss = 0.992415937093588
Iteration 50, Training loss = 0.9913750749367934
Iteration 60, Training loss = 0.9904762013600423
Iteration 70, Training loss = 0.9872789841431838
Iteration 80, Training loss = 0.9843922337660422
Iteration 90, Training loss = 0.9819934838093244
Iteration 100, Training loss = 0.9823722621569266
Iteration 110, Training loss = 0.978446547801678
Iteration 120, Training loss = 0.9773745525341767
Iteration 130, Training loss = 0.9752587286325601
Iteration 140, Training loss = 0.9696023544439902
Iteration 150, Training loss = 0.9681802988052368
Iteration 160, Training loss = 0.964692004598104
Iteration 170, Training loss = 0.9584254828783182
Iteration 180, Training loss = 0.9528277527827483
Iteration 190, Training loss = 0.9506532056973531
Model training time: 26.032984972000122
Device: cuda
Iteration 0, Training loss = 1.157665376479809
Iteration 10, Training loss = 1.0009041462953274
Iteration 20, Training loss = 0.9995718380579581
Iteration 30, Training loss = 0.997824225288171
Iteration 40, Training loss = 0.9941975967242167
Iteration 50, Training loss = 0.992233992769168
Iteration 60, Training loss = 0.9894449642071357
Iteration 70, Training loss = 0.9887770884312116
Iteration 80, Training loss = 0.9873880480344479
Iteration 90, Training loss = 0.9859055234835699
Iteration 100, Training loss = 0.9811465465105497
Iteration 110, Training loss = 0.9795085673148816
Iteration 120, Training loss = 0.9768948394518632
Iteration 130, Training loss = 0.9735224843025208
Iteration 140, Training loss = 0.9696273792248505
Iteration 150, Training loss = 0.9660329990662061
Iteration 160, Training loss = 0.9631685259250494
Iteration 170, Training loss = 0.958058156646215
Iteration 180, Training loss = 0.9519178122282028
Iteration 190, Training loss = 0.9447981589115583
Model training time: 25.98852562904358
Device: cuda
Iteration 0, Training loss = 1.1926828026771545
Iteration 10, Training loss = 1.003252034003918
Iteration 20, Training loss = 1.0000352423924666
Iteration 30, Training loss = 0.998166126700548
Iteration 40, Training loss = 0.9977263682163678
Iteration 50, Training loss = 0.9964607197504777
Iteration 60, Training loss = 0.9968287761394794
Iteration 70, Training loss = 0.9963895460734
Iteration 80, Training loss = 0.995591959128013
Iteration 90, Training loss = 0.9938299988324826
Iteration 100, Training loss = 0.995405559356396
Iteration 110, Training loss = 0.993325839822109
Iteration 120, Training loss = 0.9921025026303071
Iteration 130, Training loss = 0.9931897429319528
Iteration 140, Training loss = 0.9906644867016718
Iteration 150, Training loss = 0.98896069251574
Iteration 160, Training loss = 0.9882675925126443
Iteration 170, Training loss = 0.9870140357659414
Iteration 180, Training loss = 0.9862331805320886
Iteration 190, Training loss = 0.983788407765902
Model training time: 25.86171555519104
Device: cuda
Iteration 0, Training loss = 1.6477869244722219
Iteration 10, Training loss = 1.00615377838795
Iteration 20, Training loss = 1.0010379678927934
Iteration 30, Training loss = 1.000329539179802
Iteration 40, Training loss = 0.999047298844044
Iteration 50, Training loss = 0.998459578706668
Iteration 60, Training loss = 0.9991018164616364
Iteration 70, Training loss = 0.9975210737723571
Iteration 80, Training loss = 0.9967747800625287
Iteration 90, Training loss = 0.9973038492294458
Iteration 100, Training loss = 0.9971102223946497
Iteration 110, Training loss = 0.9981717387071023
Iteration 120, Training loss = 0.994827573116009
Iteration 130, Training loss = 0.9972702665970876
Iteration 140, Training loss = 0.9965881246786851
Iteration 150, Training loss = 0.9955796610850555
Iteration 160, Training loss = 0.994863592661344
Iteration 170, Training loss = 0.9954797602616824
Iteration 180, Training loss = 0.993974316578645
Iteration 190, Training loss = 0.9936152226649798
Model training time: 26.13420820236206
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.057904770741096
Iteration 10, Training loss = 0.3692813438291733
Iteration 20, Training loss = 0.3135853544450723
Iteration 30, Training loss = 0.30142615363001823
Iteration 40, Training loss = 0.2954215971896282
Iteration 50, Training loss = 0.28862960235430646
Iteration 60, Training loss = 0.28289179102732587
Iteration 70, Training loss = 0.2774099615904001
Iteration 80, Training loss = 0.27282041827073467
Iteration 90, Training loss = 0.2693871104946503
Iteration 100, Training loss = 0.2662110752784289
Iteration 110, Training loss = 0.26409066210572535
Iteration 120, Training loss = 0.2618228816069089
Iteration 130, Training loss = 0.26003715424583507
Iteration 140, Training loss = 0.25827081959981185
Iteration 150, Training loss = 0.257723317696498
Iteration 160, Training loss = 0.2552153648665318
Iteration 170, Training loss = 0.2543125313061934
Iteration 180, Training loss = 0.2534349042062576
Iteration 190, Training loss = 0.25255691174131173
Model training time: 28.153472423553467
Device: cuda
Iteration 0, Training loss = 1.0706251229231174
Iteration 10, Training loss = 0.40536955228218663
Iteration 20, Training loss = 0.3177504144035853
Iteration 30, Training loss = 0.29874761173358333
Iteration 40, Training loss = 0.29068595514847684
Iteration 50, Training loss = 0.2840153511900168
Iteration 60, Training loss = 0.2795024843743214
Iteration 70, Training loss = 0.27644469331090266
Iteration 80, Training loss = 0.2731686303248772
Iteration 90, Training loss = 0.2710235554438371
Iteration 100, Training loss = 0.2693810798227787
Iteration 110, Training loss = 0.26735652782596075
Iteration 120, Training loss = 0.2663297481261767
Iteration 130, Training loss = 0.26445316723906076
Iteration 140, Training loss = 0.26300676424915975
Iteration 150, Training loss = 0.2614439513820868
Iteration 160, Training loss = 0.26136286098223466
Iteration 170, Training loss = 0.26007167851695645
Iteration 180, Training loss = 0.25886309347473657
Iteration 190, Training loss = 0.258490403684286
Model training time: 28.31520390510559
Device: cuda
Iteration 0, Training loss = 1.0194162875413895
Iteration 10, Training loss = 0.3554650883261974
Iteration 20, Training loss = 0.3071350928109426
Iteration 30, Training loss = 0.295489141287712
Iteration 40, Training loss = 0.28965020408997166
Iteration 50, Training loss = 0.28463670124228185
Iteration 60, Training loss = 0.2796312619287234
Iteration 70, Training loss = 0.27630989683362156
Iteration 80, Training loss = 0.2722839300449078
Iteration 90, Training loss = 0.27012319232408816
Iteration 100, Training loss = 0.2678176239132881
Iteration 110, Training loss = 0.26632219495681614
Iteration 120, Training loss = 0.2643997563192478
Iteration 130, Training loss = 0.26322731805535465
Iteration 140, Training loss = 0.2608017935775794
Iteration 150, Training loss = 0.25917793437838554
Iteration 160, Training loss = 0.2583845899655269
Iteration 170, Training loss = 0.25614599912212443
Iteration 180, Training loss = 0.2551802004186007
Iteration 190, Training loss = 0.2539883287480244
Model training time: 28.074854135513306
Device: cuda
Iteration 0, Training loss = 0.999175985272114
Iteration 10, Training loss = 0.36952277502188313
Iteration 20, Training loss = 0.30936479511169285
Iteration 30, Training loss = 0.2941276944027497
Iteration 40, Training loss = 0.2875893723506194
Iteration 50, Training loss = 0.2814872588675756
Iteration 60, Training loss = 0.274645500171643
Iteration 70, Training loss = 0.2698029102041171
Iteration 80, Training loss = 0.2673318956333857
Iteration 90, Training loss = 0.26407233654306483
Iteration 100, Training loss = 0.26139565815146154
Iteration 110, Training loss = 0.2591910127263803
Iteration 120, Training loss = 0.25777967704030186
Iteration 130, Training loss = 0.25575176683756023
Iteration 140, Training loss = 0.2538547736520951
Iteration 150, Training loss = 0.2524002936597054
Iteration 160, Training loss = 0.2514618383004115
Iteration 170, Training loss = 0.24930653233940786
Iteration 180, Training loss = 0.24848025779311472
Iteration 190, Training loss = 0.24761140174590623
Model training time: 28.062941312789917
Device: cuda
Iteration 0, Training loss = 1.0011079930342162
Iteration 10, Training loss = 0.3541791570874361
Iteration 20, Training loss = 0.3077359279760948
Iteration 30, Training loss = 0.2966596487049873
Iteration 40, Training loss = 0.29096127903232205
Iteration 50, Training loss = 0.2852267041229285
Iteration 60, Training loss = 0.2797812710587795
Iteration 70, Training loss = 0.2754792940731232
Iteration 80, Training loss = 0.2720756229872887
Iteration 90, Training loss = 0.2687052651666678
Iteration 100, Training loss = 0.26637480637201894
Iteration 110, Training loss = 0.26459613184516245
Iteration 120, Training loss = 0.2621604645481476
Iteration 130, Training loss = 0.26057517098692745
Iteration 140, Training loss = 0.25924794289928216
Iteration 150, Training loss = 0.25835457845376086
Iteration 160, Training loss = 0.25684669327277404
Iteration 170, Training loss = 0.25434723181220203
Iteration 180, Training loss = 0.2537803019468601
Iteration 190, Training loss = 0.25221715982143694
Model training time: 28.567020416259766
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.03584351677161
Iteration 10, Training loss = 0.998958483338356
Iteration 20, Training loss = 0.995375951895347
Iteration 30, Training loss = 0.9955929196797885
Iteration 40, Training loss = 0.9936964752582403
Iteration 50, Training loss = 0.995375700868093
Iteration 60, Training loss = 0.9938238217280462
Iteration 70, Training loss = 0.9928012112012277
Iteration 80, Training loss = 0.9935437738895416
Iteration 90, Training loss = 0.9920176966832235
Iteration 100, Training loss = 0.9912529553358371
Iteration 110, Training loss = 0.98987071789228
Iteration 120, Training loss = 0.9890367583586619
Iteration 130, Training loss = 0.9898479110919512
Iteration 140, Training loss = 0.9873626002898583
Iteration 150, Training loss = 0.9850376683932084
Iteration 160, Training loss = 0.9878806380125192
Iteration 170, Training loss = 0.984037709923891
Iteration 180, Training loss = 0.9834857078698965
Iteration 190, Training loss = 0.9818713240898572
Model training time: 26.48320770263672
Device: cuda
Iteration 0, Training loss = 1.008097191269581
Iteration 10, Training loss = 1.007514007962667
Iteration 20, Training loss = 1.0051064628821154
Iteration 30, Training loss = 1.005541205406189
Iteration 40, Training loss = 1.00163850073631
Iteration 50, Training loss = 1.0022962448688655
Iteration 60, Training loss = 1.0006310103031306
Iteration 70, Training loss = 0.9995006150924243
Iteration 80, Training loss = 0.9965483981829423
Iteration 90, Training loss = 0.9984116233312167
Iteration 100, Training loss = 0.9979016001407917
Iteration 110, Training loss = 0.9944799106854659
Iteration 120, Training loss = 0.9933180958032608
Iteration 130, Training loss = 0.992022802050297
Iteration 140, Training loss = 0.9908435069597684
Iteration 150, Training loss = 0.9914109397393006
Iteration 160, Training loss = 0.9887439333475553
Iteration 170, Training loss = 0.988865729707938
Iteration 180, Training loss = 0.9894042049462979
Iteration 190, Training loss = 0.9838036229977241
Model training time: 26.58953833580017
Device: cuda
Iteration 0, Training loss = 1.196376995398448
Iteration 10, Training loss = 1.000978483603551
Iteration 20, Training loss = 0.9975571873096319
Iteration 30, Training loss = 0.9974425870638627
Iteration 40, Training loss = 0.9968467767422016
Iteration 50, Training loss = 0.9978948556459867
Iteration 60, Training loss = 0.9950661177818592
Iteration 70, Training loss = 0.9948522070279489
Iteration 80, Training loss = 0.993630411533209
Iteration 90, Training loss = 0.9931065703813846
Iteration 100, Training loss = 0.9931875879947956
Iteration 110, Training loss = 0.9911808245457135
Iteration 120, Training loss = 0.9893838923711044
Iteration 130, Training loss = 0.9898345046318494
Iteration 140, Training loss = 0.9894504409569961
Iteration 150, Training loss = 0.9894987230117505
Iteration 160, Training loss = 0.9855841914048562
Iteration 170, Training loss = 0.9839591269309704
Iteration 180, Training loss = 0.984706597832533
Iteration 190, Training loss = 0.9816679977453672
Model training time: 25.69563317298889
Device: cuda
Iteration 0, Training loss = 1.0058330194308207
Iteration 10, Training loss = 1.0040546025221164
Iteration 20, Training loss = 1.002898165812859
Iteration 30, Training loss = 1.004246324300766
Iteration 40, Training loss = 1.002351355094176
Iteration 50, Training loss = 0.9994078828738286
Iteration 60, Training loss = 1.000393617611665
Iteration 70, Training loss = 0.9993313138301556
Iteration 80, Training loss = 0.9999596350468122
Iteration 90, Training loss = 0.9987339434715418
Iteration 100, Training loss = 0.9976377682043955
Iteration 110, Training loss = 0.9972628011153295
Iteration 120, Training loss = 0.997364743397786
Iteration 130, Training loss = 0.9964287154949628
Iteration 140, Training loss = 0.9963120520114899
Iteration 150, Training loss = 0.9940714343236043
Iteration 160, Training loss = 0.9943149938033178
Iteration 170, Training loss = 0.9939323549087231
Iteration 180, Training loss = 0.9925583899021149
Iteration 190, Training loss = 0.9910278962208674
Model training time: 26.177845001220703
Device: cuda
Iteration 0, Training loss = 1.010044855567125
Iteration 10, Training loss = 1.0072456919229948
Iteration 20, Training loss = 1.0080152681240668
Iteration 30, Training loss = 1.0052662904445941
Iteration 40, Training loss = 1.0041507413754096
Iteration 50, Training loss = 1.0039780930830882
Iteration 60, Training loss = 1.0015795551813567
Iteration 70, Training loss = 1.0012248743038912
Iteration 80, Training loss = 1.000338953274947
Iteration 90, Training loss = 0.9985043773284326
Iteration 100, Training loss = 1.0005068710217109
Iteration 110, Training loss = 0.9998826751342187
Iteration 120, Training loss = 0.9957380375036826
Iteration 130, Training loss = 0.9979746949214202
Iteration 140, Training loss = 0.9946974813938141
Iteration 150, Training loss = 0.9952165610515155
Iteration 160, Training loss = 0.9925592014422784
Iteration 170, Training loss = 0.9904940197101006
Iteration 180, Training loss = 0.9910059789052377
Iteration 190, Training loss = 0.9890945313068537
Model training time: 25.98993158340454
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9896614712018234
Iteration 10, Training loss = 0.34334113276921785
Iteration 20, Training loss = 0.3092070339390865
Iteration 30, Training loss = 0.3005502928908055
Iteration 40, Training loss = 0.2944038756764852
Iteration 50, Training loss = 0.28934768644663006
Iteration 60, Training loss = 0.282302934389848
Iteration 70, Training loss = 0.2784333908214019
Iteration 80, Training loss = 0.27294193695370966
Iteration 90, Training loss = 0.26952572338856184
Iteration 100, Training loss = 0.26644557915054834
Iteration 110, Training loss = 0.264119455161003
Iteration 120, Training loss = 0.26246288074896884
Iteration 130, Training loss = 0.2598765019613963
Iteration 140, Training loss = 0.25815694120067817
Iteration 150, Training loss = 0.2566937603629552
Iteration 160, Training loss = 0.2552733596127767
Iteration 170, Training loss = 0.25504334156329816
Iteration 180, Training loss = 0.25404263774936015
Iteration 190, Training loss = 0.25279201108675736
Iteration 200, Training loss = 0.25135790613981396
Iteration 210, Training loss = 0.25118911696168095
Iteration 220, Training loss = 0.24962071214730924
Iteration 230, Training loss = 0.24826322008784002
Iteration 240, Training loss = 0.24749882204028276
Iteration 250, Training loss = 0.24638202213324034
Iteration 260, Training loss = 0.2459480266731519
Iteration 270, Training loss = 0.24509960527603442
Iteration 280, Training loss = 0.24314668631324401
Iteration 290, Training loss = 0.24300677730486944
Model training time: 42.47354340553284
Device: cuda
Iteration 0, Training loss = 0.9912855602227725
Iteration 10, Training loss = 0.34147972957446027
Iteration 20, Training loss = 0.3065357491947137
Iteration 30, Training loss = 0.2974350366454858
Iteration 40, Training loss = 0.29173487740067333
Iteration 50, Training loss = 0.2873859743659313
Iteration 60, Training loss = 0.2817647445660371
Iteration 70, Training loss = 0.277271108271984
Iteration 80, Training loss = 0.27334813888256365
Iteration 90, Training loss = 0.2696582826857383
Iteration 100, Training loss = 0.26703867172965634
Iteration 110, Training loss = 0.2651896052635633
Iteration 120, Training loss = 0.2637133165620841
Iteration 130, Training loss = 0.2618361310316966
Iteration 140, Training loss = 0.26011621579527855
Iteration 150, Training loss = 0.258925111247943
Iteration 160, Training loss = 0.257592514443856
Iteration 170, Training loss = 0.2560548320985757
Iteration 180, Training loss = 0.25593131150190646
Iteration 190, Training loss = 0.25407510279462886
Iteration 200, Training loss = 0.2532613973778028
Iteration 210, Training loss = 0.2518687895857371
Iteration 220, Training loss = 0.2511606706449619
Iteration 230, Training loss = 0.25071579905656666
Iteration 240, Training loss = 0.2500297682216534
Iteration 250, Training loss = 0.24828830332710192
Iteration 260, Training loss = 0.24764293632828271
Iteration 270, Training loss = 0.24680115253879473
Iteration 280, Training loss = 0.246194827155425
Iteration 290, Training loss = 0.245316781103611
Model training time: 42.233980655670166
Device: cuda
Iteration 0, Training loss = 0.9837940610372103
Iteration 10, Training loss = 0.33731013536453247
Iteration 20, Training loss = 0.30289173842622685
Iteration 30, Training loss = 0.2925608172439612
Iteration 40, Training loss = 0.28659438685728955
Iteration 50, Training loss = 0.28115226471653354
Iteration 60, Training loss = 0.2752613310630505
Iteration 70, Training loss = 0.271514327193682
Iteration 80, Training loss = 0.2681290002969595
Iteration 90, Training loss = 0.26461482535188013
Iteration 100, Training loss = 0.26462350671107954
Iteration 110, Training loss = 0.2611652452212114
Iteration 120, Training loss = 0.2591284696872418
Iteration 130, Training loss = 0.2582657546378099
Iteration 140, Training loss = 0.25534041598439217
Iteration 150, Training loss = 0.25431642366143375
Iteration 160, Training loss = 0.2525516848724622
Iteration 170, Training loss = 0.2513816219109755
Iteration 180, Training loss = 0.2504758531084427
Iteration 190, Training loss = 0.24855657437672982
Iteration 200, Training loss = 0.24825731693552092
Iteration 210, Training loss = 0.24619138527375
Iteration 220, Training loss = 0.2452111954872425
Iteration 230, Training loss = 0.24278554807488734
Iteration 240, Training loss = 0.24259445300469032
Iteration 250, Training loss = 0.24176717922091484
Iteration 260, Training loss = 0.24021850927517965
Iteration 270, Training loss = 0.23948051436589315
Iteration 280, Training loss = 0.2375238758440201
Iteration 290, Training loss = 0.23677885704315627
Model training time: 42.4952757358551
Device: cuda
Iteration 0, Training loss = 1.060906096146657
Iteration 10, Training loss = 0.4078163934441713
Iteration 20, Training loss = 0.3197406309728439
Iteration 30, Training loss = 0.2971076386479231
Iteration 40, Training loss = 0.28619390840713793
Iteration 50, Training loss = 0.28024230983394843
Iteration 60, Training loss = 0.27613580140929955
Iteration 70, Training loss = 0.27360734515465224
Iteration 80, Training loss = 0.27176537851874644
Iteration 90, Training loss = 0.2696104416480431
Iteration 100, Training loss = 0.2681557501737888
Iteration 110, Training loss = 0.2667190713378099
Iteration 120, Training loss = 0.2656810905497808
Iteration 130, Training loss = 0.26455708507161874
Iteration 140, Training loss = 0.2624892540849172
Iteration 150, Training loss = 0.26176070737150997
Iteration 160, Training loss = 0.26072765313662016
Iteration 170, Training loss = 0.26036223797844005
Iteration 180, Training loss = 0.25906049001675385
Iteration 190, Training loss = 0.2581661438139585
Iteration 200, Training loss = 0.2574301321919148
Iteration 210, Training loss = 0.25666673424152225
Iteration 220, Training loss = 0.2564714416288413
Iteration 230, Training loss = 0.2557149472144934
Iteration 240, Training loss = 0.2545045998234015
Iteration 250, Training loss = 0.25414744277413076
Iteration 260, Training loss = 0.2530429520859168
Iteration 270, Training loss = 0.25283166565574133
Iteration 280, Training loss = 0.2514410187991766
Iteration 290, Training loss = 0.2508899185519952
Model training time: 42.76741337776184
Device: cuda
Iteration 0, Training loss = 0.9843706878331991
Iteration 10, Training loss = 0.33517399029089856
Iteration 20, Training loss = 0.30510354385926175
Iteration 30, Training loss = 0.2968444827084358
Iteration 40, Training loss = 0.29033036071520585
Iteration 50, Training loss = 0.28390397446659893
Iteration 60, Training loss = 0.2780233383751832
Iteration 70, Training loss = 0.2722904275243099
Iteration 80, Training loss = 0.2682863175868988
Iteration 90, Training loss = 0.2652781324890944
Iteration 100, Training loss = 0.2635910820502501
Iteration 110, Training loss = 0.2604082946020823
Iteration 120, Training loss = 0.25855793116184383
Iteration 130, Training loss = 0.2568750146489877
Iteration 140, Training loss = 0.2559995539486408
Iteration 150, Training loss = 0.25380789001400655
Iteration 160, Training loss = 0.2529255552933766
Iteration 170, Training loss = 0.2505783335520671
Iteration 180, Training loss = 0.24906367579331765
Iteration 190, Training loss = 0.2485889821098401
Iteration 200, Training loss = 0.24737360500372374
Iteration 210, Training loss = 0.2458990393922879
Iteration 220, Training loss = 0.24484925487866768
Iteration 230, Training loss = 0.2439188352571084
Iteration 240, Training loss = 0.2429974706700215
Iteration 250, Training loss = 0.24206750524731782
Iteration 260, Training loss = 0.2407979632799442
Iteration 270, Training loss = 0.24042909076580635
Iteration 280, Training loss = 0.23927326357135406
Iteration 290, Training loss = 0.23812859333478487
Model training time: 42.561368227005005
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0626262712937136
Iteration 10, Training loss = 0.9949328498198435
Iteration 20, Training loss = 0.9920937373087957
Iteration 30, Training loss = 0.9905678079678462
Iteration 40, Training loss = 0.9882999910758092
Iteration 50, Training loss = 0.9884939824159329
Iteration 60, Training loss = 0.9842959229762738
Iteration 70, Training loss = 0.9841184822412637
Iteration 80, Training loss = 0.9799529073330072
Iteration 90, Training loss = 0.9788284955116419
Iteration 100, Training loss = 0.9757698499239408
Iteration 110, Training loss = 0.9737616674258158
Iteration 120, Training loss = 0.9708326837191215
Iteration 130, Training loss = 0.9670655543987567
Iteration 140, Training loss = 0.9628485544369771
Iteration 150, Training loss = 0.9564365973839393
Iteration 160, Training loss = 0.9528021204930085
Iteration 170, Training loss = 0.9463609285079516
Iteration 180, Training loss = 0.9407338775121249
Iteration 190, Training loss = 0.9340025324087876
Iteration 200, Training loss = 0.9243123164543738
Iteration 210, Training loss = 0.9154836684465408
Iteration 220, Training loss = 0.9036192171848737
Iteration 230, Training loss = 0.8915453667824085
Iteration 240, Training loss = 0.8787239721188178
Iteration 250, Training loss = 0.8619544758246496
Iteration 260, Training loss = 0.8440516969332328
Iteration 270, Training loss = 0.8249423274627099
Iteration 280, Training loss = 0.8019130516510743
Iteration 290, Training loss = 0.7766231974730124
Model training time: 39.073320150375366
Device: cuda
Iteration 0, Training loss = 1.0983667029784276
Iteration 10, Training loss = 0.9994935496495321
Iteration 20, Training loss = 0.9984554946422577
Iteration 30, Training loss = 0.996159688784526
Iteration 40, Training loss = 0.9953809609779944
Iteration 50, Training loss = 0.994349832718189
Iteration 60, Training loss = 0.993515758560254
Iteration 70, Training loss = 0.9913083532681832
Iteration 80, Training loss = 0.9901991933584213
Iteration 90, Training loss = 0.9892172503929871
Iteration 100, Training loss = 0.9872916512764417
Iteration 110, Training loss = 0.9873961279025445
Iteration 120, Training loss = 0.9837506929269204
Iteration 130, Training loss = 0.9819488617090079
Iteration 140, Training loss = 0.980746634877645
Iteration 150, Training loss = 0.9766091188559165
Iteration 160, Training loss = 0.9756724845904571
Iteration 170, Training loss = 0.9733245533246261
Iteration 180, Training loss = 0.9683416818196957
Iteration 190, Training loss = 0.9647676245524333
Iteration 200, Training loss = 0.9629677602877984
Iteration 210, Training loss = 0.9558066301620923
Iteration 220, Training loss = 0.9501722099689337
Iteration 230, Training loss = 0.945364582997102
Iteration 240, Training loss = 0.9393179221795156
Iteration 250, Training loss = 0.9313355684280396
Iteration 260, Training loss = 0.9214216207082455
Iteration 270, Training loss = 0.9141268753088437
Iteration 280, Training loss = 0.9012602097713031
Iteration 290, Training loss = 0.8880888877006677
Model training time: 39.2091805934906
Device: cuda
Iteration 0, Training loss = 0.9923158001441222
Iteration 10, Training loss = 0.9904765750353153
Iteration 20, Training loss = 0.9886157157329413
Iteration 30, Training loss = 0.9862113583546418
Iteration 40, Training loss = 0.9847132185330758
Iteration 50, Training loss = 0.9847606328817514
Iteration 60, Training loss = 0.9799029815655488
Iteration 70, Training loss = 0.9792886192982013
Iteration 80, Training loss = 0.9727445588662074
Iteration 90, Training loss = 0.9721089842227789
Iteration 100, Training loss = 0.9691339949002633
Iteration 110, Training loss = 0.9638287940850625
Iteration 120, Training loss = 0.9581136909814981
Iteration 130, Training loss = 0.9552894337819173
Iteration 140, Training loss = 0.9515816145218335
Iteration 150, Training loss = 0.9436774437244122
Iteration 160, Training loss = 0.9355978197776355
Iteration 170, Training loss = 0.9283317098250756
Iteration 180, Training loss = 0.9174139499664307
Iteration 190, Training loss = 0.9077867372677877
Iteration 200, Training loss = 0.8947900155415902
Iteration 210, Training loss = 0.8844297528266907
Iteration 220, Training loss = 0.867367182786648
Iteration 230, Training loss = 0.8503828644752502
Iteration 240, Training loss = 0.8307950909321125
Iteration 250, Training loss = 0.8091003677019706
Iteration 260, Training loss = 0.7861960851229154
Iteration 270, Training loss = 0.7593878622238452
Iteration 280, Training loss = 0.7338004971926029
Iteration 290, Training loss = 0.7017554921599535
Model training time: 38.90627980232239
Device: cuda
Iteration 0, Training loss = 1.3872147706838756
Iteration 10, Training loss = 0.9973345914712319
Iteration 20, Training loss = 0.9945369454530569
Iteration 30, Training loss = 0.992355730671149
Iteration 40, Training loss = 0.9937271510179226
Iteration 50, Training loss = 0.9920203662835635
Iteration 60, Training loss = 0.9907657481156863
Iteration 70, Training loss = 0.9891562908887863
Iteration 80, Training loss = 0.9878064336685034
Iteration 90, Training loss = 0.9885396235264264
Iteration 100, Training loss = 0.9845768923942859
Iteration 110, Training loss = 0.9832252229635532
Iteration 120, Training loss = 0.9812873819699655
Iteration 130, Training loss = 0.9785315669499911
Iteration 140, Training loss = 0.97925454722001
Iteration 150, Training loss = 0.9746732757641718
Iteration 160, Training loss = 0.9726405074963203
Iteration 170, Training loss = 0.9690456447693018
Iteration 180, Training loss = 0.9683232078185449
Iteration 190, Training loss = 0.9620860345088519
Iteration 200, Training loss = 0.9573899232424222
Iteration 210, Training loss = 0.955934254022745
Iteration 220, Training loss = 0.9486231345396775
Iteration 230, Training loss = 0.9437746680699862
Iteration 240, Training loss = 0.9340465320990636
Iteration 250, Training loss = 0.9256028957091845
Iteration 260, Training loss = 0.9172038825658652
Iteration 270, Training loss = 0.907140103670267
Iteration 280, Training loss = 0.8942283586813853
Iteration 290, Training loss = 0.8812718379956025
Model training time: 39.21067261695862
Device: cuda
Iteration 0, Training loss = 1.0087103912463555
Iteration 10, Training loss = 1.000247218287908
Iteration 20, Training loss = 0.9966946496413305
Iteration 30, Training loss = 0.9946356851321
Iteration 40, Training loss = 0.9935494500857133
Iteration 50, Training loss = 0.9914319755939337
Iteration 60, Training loss = 0.9881735008496505
Iteration 70, Training loss = 0.985325894676722
Iteration 80, Training loss = 0.9838609959070499
Iteration 90, Training loss = 0.9801401702257303
Iteration 100, Training loss = 0.9781345449961149
Iteration 110, Training loss = 0.9739608053977673
Iteration 120, Training loss = 0.970473628777724
Iteration 130, Training loss = 0.9659459923322384
Iteration 140, Training loss = 0.9635515568348078
Iteration 150, Training loss = 0.9557774353485841
Iteration 160, Training loss = 0.952231553884653
Iteration 170, Training loss = 0.9454299555375025
Iteration 180, Training loss = 0.9378366264013144
Iteration 190, Training loss = 0.9292139468284754
Iteration 200, Training loss = 0.9196477303138146
Iteration 210, Training loss = 0.9096721880711042
Iteration 220, Training loss = 0.8986987677904276
Iteration 230, Training loss = 0.8843923692519848
Iteration 240, Training loss = 0.8686267469937985
Iteration 250, Training loss = 0.851959045116718
Iteration 260, Training loss = 0.8312880752178339
Iteration 270, Training loss = 0.8087626317372689
Iteration 280, Training loss = 0.78597013766949
Iteration 290, Training loss = 0.7589931740210607
Model training time: 39.70694422721863
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.081823928998067
Iteration 10, Training loss = 0.5005059924263221
Iteration 20, Training loss = 0.3406185642457925
Iteration 30, Training loss = 0.3100869916379452
Iteration 40, Training loss = 0.2987278625369072
Iteration 50, Training loss = 0.2914685861995587
Iteration 60, Training loss = 0.2861007070885255
Iteration 70, Training loss = 0.28141511489565557
Iteration 80, Training loss = 0.2775629091148193
Iteration 90, Training loss = 0.27461611221616083
Iteration 100, Training loss = 0.27217157213733745
Iteration 110, Training loss = 0.26976976457696694
Iteration 120, Training loss = 0.268881700359858
Iteration 130, Training loss = 0.2671002777150044
Iteration 140, Training loss = 0.265589742992933
Iteration 150, Training loss = 0.2639007135652579
Iteration 160, Training loss = 0.263551572481027
Iteration 170, Training loss = 0.26061589786639583
Iteration 180, Training loss = 0.25983522173303825
Iteration 190, Training loss = 0.25887461149921787
Iteration 200, Training loss = 0.2577438262792734
Iteration 210, Training loss = 0.25775919281519377
Iteration 220, Training loss = 0.25482626918416756
Iteration 230, Training loss = 0.255154983068888
Iteration 240, Training loss = 0.25452061312702984
Iteration 250, Training loss = 0.2534118750347541
Iteration 260, Training loss = 0.25235904352023053
Iteration 270, Training loss = 0.2521547907246993
Iteration 280, Training loss = 0.25065828315340555
Iteration 290, Training loss = 0.2505931040415397
Model training time: 45.28290867805481
Device: cuda
Iteration 0, Training loss = 0.9963583739904257
Iteration 10, Training loss = 0.38487186569433945
Iteration 20, Training loss = 0.3191773599156967
Iteration 30, Training loss = 0.30295055502882373
Iteration 40, Training loss = 0.2934243160371597
Iteration 50, Training loss = 0.28634981266581094
Iteration 60, Training loss = 0.28054367349697995
Iteration 70, Training loss = 0.27629748817819816
Iteration 80, Training loss = 0.27271768995202506
Iteration 90, Training loss = 0.26928843844395417
Iteration 100, Training loss = 0.2669890070190796
Iteration 110, Training loss = 0.265725856503615
Iteration 120, Training loss = 0.2631634379235598
Iteration 130, Training loss = 0.2610476274903004
Iteration 140, Training loss = 0.2605595849454403
Iteration 150, Training loss = 0.2585072649212984
Iteration 160, Training loss = 0.25811799644277644
Iteration 170, Training loss = 0.2568263391462656
Iteration 180, Training loss = 0.25622093276335645
Iteration 190, Training loss = 0.25479677606087464
Iteration 200, Training loss = 0.2553705914089313
Iteration 210, Training loss = 0.2536898599221156
Iteration 220, Training loss = 0.2537797580544765
Iteration 230, Training loss = 0.25266607420948833
Iteration 240, Training loss = 0.25216566914549243
Iteration 250, Training loss = 0.25072382447811276
Iteration 260, Training loss = 0.25067730620503426
Iteration 270, Training loss = 0.25039754435420036
Iteration 280, Training loss = 0.24948925582262185
Iteration 290, Training loss = 0.249075776969011
Model training time: 45.54771661758423
Device: cuda
Iteration 0, Training loss = 1.0215734839439392
Iteration 10, Training loss = 0.4105697423219681
Iteration 20, Training loss = 0.3225774300786165
Iteration 30, Training loss = 0.3005733097401949
Iteration 40, Training loss = 0.2920421794630014
Iteration 50, Training loss = 0.28631080142580545
Iteration 60, Training loss = 0.28081384387153846
Iteration 70, Training loss = 0.27676133152384025
Iteration 80, Training loss = 0.27276099817110944
Iteration 90, Training loss = 0.26900250665270364
Iteration 100, Training loss = 0.2659869400354532
Iteration 110, Training loss = 0.26407075787966067
Iteration 120, Training loss = 0.26250718247431976
Iteration 130, Training loss = 0.26041100403437245
Iteration 140, Training loss = 0.2595028137931457
Iteration 150, Training loss = 0.25761719735769123
Iteration 160, Training loss = 0.2562670905429583
Iteration 170, Training loss = 0.2548982658638404
Iteration 180, Training loss = 0.25464508739801556
Iteration 190, Training loss = 0.2517392672598362
Iteration 200, Training loss = 0.25175416899415165
Iteration 210, Training loss = 0.24997757661801118
Iteration 220, Training loss = 0.24866405960458976
Iteration 230, Training loss = 0.24846619711472437
Iteration 240, Training loss = 0.24666084836308771
Iteration 250, Training loss = 0.24576445468343222
Iteration 260, Training loss = 0.24523874916709387
Iteration 270, Training loss = 0.24425052880094603
Iteration 280, Training loss = 0.24234907816235834
Iteration 290, Training loss = 0.24169358869011587
Model training time: 42.500086069107056
Device: cuda
Iteration 0, Training loss = 1.002050159069208
Iteration 10, Training loss = 0.4029606477572368
Iteration 20, Training loss = 0.321977436542511
Iteration 30, Training loss = 0.30307825912649816
Iteration 40, Training loss = 0.2935835220492803
Iteration 50, Training loss = 0.28746710173212564
Iteration 60, Training loss = 0.28234577694764507
Iteration 70, Training loss = 0.27854887367441106
Iteration 80, Training loss = 0.27330114721105647
Iteration 90, Training loss = 0.2689517673391562
Iteration 100, Training loss = 0.2659161208340755
Iteration 110, Training loss = 0.2634679451584816
Iteration 120, Training loss = 0.26254570856690407
Iteration 130, Training loss = 0.2603654612142306
Iteration 140, Training loss = 0.2593795886406532
Iteration 150, Training loss = 0.25756268260570675
Iteration 160, Training loss = 0.2562439444546516
Iteration 170, Training loss = 0.2552258475468709
Iteration 180, Training loss = 0.2539531144385154
Iteration 190, Training loss = 0.2534155719555341
Iteration 200, Training loss = 0.2512248089680305
Iteration 210, Training loss = 0.2508967788173602
Iteration 220, Training loss = 0.25036293640732765
Iteration 230, Training loss = 0.24937968529187715
Iteration 240, Training loss = 0.24785356997297361
Iteration 250, Training loss = 0.2464951520355848
Iteration 260, Training loss = 0.24609349582057732
Iteration 270, Training loss = 0.24597218346137267
Iteration 280, Training loss = 0.2443832021493178
Iteration 290, Training loss = 0.24294107063458517
Model training time: 42.32547473907471
Device: cuda
Iteration 0, Training loss = 1.4302714214875147
Iteration 10, Training loss = 0.8400750974049935
Iteration 20, Training loss = 0.4347944649366232
Iteration 30, Training loss = 0.34581272246745914
Iteration 40, Training loss = 0.3120377774421985
Iteration 50, Training loss = 0.2949531559760754
Iteration 60, Training loss = 0.2826790190660037
Iteration 70, Training loss = 0.27379401572621787
Iteration 80, Training loss = 0.26804009395150036
Iteration 90, Training loss = 0.26292285638359875
Iteration 100, Training loss = 0.2599084755549064
Iteration 110, Training loss = 0.25762932346417355
Iteration 120, Training loss = 0.256026747994698
Iteration 130, Training loss = 0.25524042030939686
Iteration 140, Training loss = 0.2527453535451339
Iteration 150, Training loss = 0.25156434539418954
Iteration 160, Training loss = 0.25024133490828365
Iteration 170, Training loss = 0.2495128414951838
Iteration 180, Training loss = 0.2490833347233442
Iteration 190, Training loss = 0.24790492825783217
Iteration 200, Training loss = 0.24684183614758345
Iteration 210, Training loss = 0.24592669652058527
Iteration 220, Training loss = 0.24478761450602457
Iteration 230, Training loss = 0.24389151798991057
Iteration 240, Training loss = 0.24310837772030097
Iteration 250, Training loss = 0.24271645167699227
Iteration 260, Training loss = 0.24161193577142862
Iteration 270, Training loss = 0.24049672570366126
Iteration 280, Training loss = 0.2401226839193931
Iteration 290, Training loss = 0.23889010399580002
Model training time: 42.050607681274414
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.012089124092689
Iteration 10, Training loss = 1.000945549744826
Iteration 20, Training loss = 0.9988216757774353
Iteration 30, Training loss = 1.0005520524886937
Iteration 40, Training loss = 0.9992539619023983
Iteration 50, Training loss = 0.997036514373926
Iteration 60, Training loss = 0.9980970139686878
Iteration 70, Training loss = 0.9954168842389033
Iteration 80, Training loss = 0.9949077253158276
Iteration 90, Training loss = 0.9947694803659732
Iteration 100, Training loss = 0.9947807708611855
Iteration 110, Training loss = 0.9944508190338428
Iteration 120, Training loss = 0.9935256093740463
Iteration 130, Training loss = 0.9921235384849402
Iteration 140, Training loss = 0.992737754033162
Iteration 150, Training loss = 0.9902997590028323
Iteration 160, Training loss = 0.990819880595574
Iteration 170, Training loss = 0.9895365375738877
Iteration 180, Training loss = 0.9868004264739844
Iteration 190, Training loss = 0.9864663802660428
Iteration 200, Training loss = 0.9857752242913613
Iteration 210, Training loss = 0.983797448185774
Iteration 220, Training loss = 0.9825811053697879
Iteration 230, Training loss = 0.9810061741333741
Iteration 240, Training loss = 0.9791124657942698
Iteration 250, Training loss = 0.9779535073500413
Iteration 260, Training loss = 0.9746757046534464
Iteration 270, Training loss = 0.9723038902649512
Iteration 280, Training loss = 0.9690740383588351
Iteration 290, Training loss = 0.9666342964539161
Model training time: 38.88265633583069
Device: cuda
Iteration 0, Training loss = 1.0539511556808765
Iteration 10, Training loss = 0.9949048321980697
Iteration 20, Training loss = 0.9946636924376855
Iteration 30, Training loss = 0.9945426801076303
Iteration 40, Training loss = 0.9939914609377201
Iteration 50, Training loss = 0.9936564060357901
Iteration 60, Training loss = 0.9913294131939228
Iteration 70, Training loss = 0.9891886436022245
Iteration 80, Training loss = 0.9881354788174996
Iteration 90, Training loss = 0.9871289214262595
Iteration 100, Training loss = 0.9861803077734433
Iteration 110, Training loss = 0.9843971419792908
Iteration 120, Training loss = 0.981297166301654
Iteration 130, Training loss = 0.9813994501645749
Iteration 140, Training loss = 0.979076658303921
Iteration 150, Training loss = 0.9772653694336231
Iteration 160, Training loss = 0.9755139717688928
Iteration 170, Training loss = 0.9709071562840388
Iteration 180, Training loss = 0.9689037731060615
Iteration 190, Training loss = 0.9647810206963465
Iteration 200, Training loss = 0.9612917728148974
Iteration 210, Training loss = 0.9583108367828223
Iteration 220, Training loss = 0.9545445293188095
Iteration 230, Training loss = 0.9486025411349076
Iteration 240, Training loss = 0.9438877908083109
Iteration 250, Training loss = 0.9369411972852854
Iteration 260, Training loss = 0.9293205955853829
Iteration 270, Training loss = 0.923903858432403
Iteration 280, Training loss = 0.9135877822454159
Iteration 290, Training loss = 0.904813540669588
Model training time: 39.22973561286926
Device: cuda
Iteration 0, Training loss = 1.0060891509056091
Iteration 10, Training loss = 1.0014299544004293
Iteration 20, Training loss = 0.9994528156060439
Iteration 30, Training loss = 0.999282561815702
Iteration 40, Training loss = 0.9961703671858861
Iteration 50, Training loss = 0.9959617704153061
Iteration 60, Training loss = 0.9926842095760199
Iteration 70, Training loss = 0.9918396163445252
Iteration 80, Training loss = 0.9896352978853079
Iteration 90, Training loss = 0.987526800770026
Iteration 100, Training loss = 0.9870391912185229
Iteration 110, Training loss = 0.9844388801317948
Iteration 120, Training loss = 0.9815384894609451
Iteration 130, Training loss = 0.9784926646030866
Iteration 140, Training loss = 0.9763120619150308
Iteration 150, Training loss = 0.9727311283349991
Iteration 160, Training loss = 0.9694639558975513
Iteration 170, Training loss = 0.9673077475566131
Iteration 180, Training loss = 0.9626632115015616
Iteration 190, Training loss = 0.9584866819473413
Iteration 200, Training loss = 0.9523771691780824
Iteration 210, Training loss = 0.94842473589457
Iteration 220, Training loss = 0.9418364155751008
Iteration 230, Training loss = 0.93536432660543
Iteration 240, Training loss = 0.9269293902012018
Iteration 250, Training loss = 0.9191605586272019
Iteration 260, Training loss = 0.9083818575510612
Iteration 270, Training loss = 0.89926989032672
Iteration 280, Training loss = 0.8868982310478504
Iteration 290, Training loss = 0.8725572045032794
Model training time: 39.03599572181702
Device: cuda
Iteration 0, Training loss = 1.0486405812777007
Iteration 10, Training loss = 1.0040168876831348
Iteration 20, Training loss = 1.0019505883638675
Iteration 30, Training loss = 1.0016054235971892
Iteration 40, Training loss = 1.0004846900701523
Iteration 50, Training loss = 1.0008510557504802
Iteration 60, Training loss = 0.998860933459722
Iteration 70, Training loss = 0.9986372062793145
Iteration 80, Training loss = 0.9968394820506756
Iteration 90, Training loss = 0.9957170497912627
Iteration 100, Training loss = 0.9967020864670093
Iteration 110, Training loss = 0.9950638917776254
Iteration 120, Training loss = 0.9945441977335856
Iteration 130, Training loss = 0.9927978504162568
Iteration 140, Training loss = 0.9913222182255524
Iteration 150, Training loss = 0.9923344655678823
Iteration 160, Training loss = 0.9908944677848083
Iteration 170, Training loss = 0.9901538319312609
Iteration 180, Training loss = 0.9894001885102346
Iteration 190, Training loss = 0.9881483350808804
Iteration 200, Training loss = 0.9861461680669051
Iteration 210, Training loss = 0.9846994636150507
Iteration 220, Training loss = 0.9823002585997949
Iteration 230, Training loss = 0.9786357489915994
Iteration 240, Training loss = 0.9762339718066729
Iteration 250, Training loss = 0.9759077773644373
Iteration 260, Training loss = 0.9731434549276645
Iteration 270, Training loss = 0.9708898308185431
Iteration 280, Training loss = 0.9673562382276242
Iteration 290, Training loss = 0.9647651062561915
Model training time: 39.96175980567932
Device: cuda
Iteration 0, Training loss = 1.0255090903777342
Iteration 10, Training loss = 1.0044074620191867
Iteration 20, Training loss = 1.0040708562502494
Iteration 30, Training loss = 1.0023248688532755
Iteration 40, Training loss = 1.0028451818686266
Iteration 50, Training loss = 0.998756026992431
Iteration 60, Training loss = 0.9987704341228192
Iteration 70, Training loss = 0.9981008882705982
Iteration 80, Training loss = 0.9978762842141665
Iteration 90, Training loss = 0.9953687294171407
Iteration 100, Training loss = 0.9939841261276832
Iteration 110, Training loss = 0.9927240598660249
Iteration 120, Training loss = 0.9919913812325551
Iteration 130, Training loss = 0.9904584896105987
Iteration 140, Training loss = 0.9905985628183072
Iteration 150, Training loss = 0.9869055151939392
Iteration 160, Training loss = 0.9873953759670258
Iteration 170, Training loss = 0.9871517133254272
Iteration 180, Training loss = 0.9828280061483383
Iteration 190, Training loss = 0.9813354577009494
Iteration 200, Training loss = 0.9785058406683115
Iteration 210, Training loss = 0.9770579051512939
Iteration 220, Training loss = 0.9736146250596414
Iteration 230, Training loss = 0.9713049565370266
Iteration 240, Training loss = 0.9667284202117187
Iteration 250, Training loss = 0.9647405606049758
Iteration 260, Training loss = 0.9609152307877173
Iteration 270, Training loss = 0.9557224603799673
Iteration 280, Training loss = 0.9495932998565527
Iteration 290, Training loss = 0.9420162932230876
Model training time: 39.28589344024658
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9991397914978174
Iteration 10, Training loss = 0.36843282786699444
Iteration 20, Training loss = 0.3161312949198943
Iteration 30, Training loss = 0.30250707612587857
Iteration 40, Training loss = 0.295120508911518
Iteration 50, Training loss = 0.2890320477577356
Iteration 60, Training loss = 0.28452936244698673
Iteration 70, Training loss = 0.2798115169772735
Iteration 80, Training loss = 0.2770803418870156
Iteration 90, Training loss = 0.2739681635911648
Iteration 100, Training loss = 0.2724456726931609
Iteration 110, Training loss = 0.2690988022547502
Iteration 120, Training loss = 0.267194758527554
Iteration 130, Training loss = 0.2642452401610521
Iteration 140, Training loss = 0.2635207703480354
Iteration 150, Training loss = 0.2621833819609422
Iteration 160, Training loss = 0.26095840592796987
Iteration 170, Training loss = 0.25898414506362033
Iteration 180, Training loss = 0.2583054705308034
Iteration 190, Training loss = 0.2566302685210338
Iteration 200, Training loss = 0.255573907150672
Iteration 210, Training loss = 0.2552957520461999
Iteration 220, Training loss = 0.2536650116627033
Iteration 230, Training loss = 0.2528877476086983
Iteration 240, Training loss = 0.2526596217201306
Iteration 250, Training loss = 0.25097419722722125
Iteration 260, Training loss = 0.25156741073498357
Iteration 270, Training loss = 0.24939415546563956
Iteration 280, Training loss = 0.24846021000009316
Iteration 290, Training loss = 0.24748938645307833
Model training time: 42.416805028915405
Device: cuda
Iteration 0, Training loss = 1.019555668418224
Iteration 10, Training loss = 0.3607070199572123
Iteration 20, Training loss = 0.3102187832387594
Iteration 30, Training loss = 0.29907859575289947
Iteration 40, Training loss = 0.2921287603676319
Iteration 50, Training loss = 0.2863376260950015
Iteration 60, Training loss = 0.2817523814737797
Iteration 70, Training loss = 0.2770915346649977
Iteration 80, Training loss = 0.27304895193530965
Iteration 90, Training loss = 0.27050329400942874
Iteration 100, Training loss = 0.267117442133335
Iteration 110, Training loss = 0.2651625923239268
Iteration 120, Training loss = 0.2631260801393252
Iteration 130, Training loss = 0.2623754318516988
Iteration 140, Training loss = 0.26083738987262434
Iteration 150, Training loss = 0.2597461190934365
Iteration 160, Training loss = 0.25892099222311604
Iteration 170, Training loss = 0.2570893583962551
Iteration 180, Training loss = 0.25638369155617863
Iteration 190, Training loss = 0.25556241147793257
Iteration 200, Training loss = 0.25508487912324757
Iteration 210, Training loss = 0.2541983485794984
Iteration 220, Training loss = 0.25366592263946164
Iteration 230, Training loss = 0.2527332285848948
Iteration 240, Training loss = 0.252344533514518
Iteration 250, Training loss = 0.25112975245484936
Iteration 260, Training loss = 0.2513915119835964
Iteration 270, Training loss = 0.2509887774403279
Iteration 280, Training loss = 0.25010165428886044
Iteration 290, Training loss = 0.24954176808779055
Model training time: 43.12060236930847
Device: cuda
Iteration 0, Training loss = 0.9919232187362818
Iteration 10, Training loss = 0.35080544822491133
Iteration 20, Training loss = 0.3078539881568689
Iteration 30, Training loss = 0.2964915965612118
Iteration 40, Training loss = 0.2898825461474749
Iteration 50, Training loss = 0.28553445350665313
Iteration 60, Training loss = 0.27956525809489763
Iteration 70, Training loss = 0.2759236266406683
Iteration 80, Training loss = 0.2730080479612717
Iteration 90, Training loss = 0.2711705086896053
Iteration 100, Training loss = 0.26868228471049893
Iteration 110, Training loss = 0.2667370977310034
Iteration 120, Training loss = 0.2645058331008141
Iteration 130, Training loss = 0.2626687577710702
Iteration 140, Training loss = 0.2617711152594823
Iteration 150, Training loss = 0.26050556995547736
Iteration 160, Training loss = 0.25864473758981776
Iteration 170, Training loss = 0.25765052609718764
Iteration 180, Training loss = 0.2558872192524947
Iteration 190, Training loss = 0.25627339688631207
Iteration 200, Training loss = 0.25517648028639645
Iteration 210, Training loss = 0.2532996019491783
Iteration 220, Training loss = 0.253050912458163
Iteration 230, Training loss = 0.2524617024912284
Iteration 240, Training loss = 0.2516910473887737
Iteration 250, Training loss = 0.25131531747487873
Iteration 260, Training loss = 0.2504598469688342
Iteration 270, Training loss = 0.2498046113894536
Iteration 280, Training loss = 0.25001708389474797
Iteration 290, Training loss = 0.24968805679908165
Model training time: 42.352659463882446
Device: cuda
Iteration 0, Training loss = 0.9941756014640515
Iteration 10, Training loss = 0.33638626451675707
Iteration 20, Training loss = 0.3045860789716244
Iteration 30, Training loss = 0.29594594412125075
Iteration 40, Training loss = 0.2892994313285901
Iteration 50, Training loss = 0.2849891300384815
Iteration 60, Training loss = 0.27848852282533276
Iteration 70, Training loss = 0.27599194502601254
Iteration 80, Training loss = 0.27065440467917
Iteration 90, Training loss = 0.2675912274191013
Iteration 100, Training loss = 0.2649283896271999
Iteration 110, Training loss = 0.26392830965610653
Iteration 120, Training loss = 0.26147300004959106
Iteration 130, Training loss = 0.2600309673983317
Iteration 140, Training loss = 0.2589340152648779
Iteration 150, Training loss = 0.25687160142339194
Iteration 160, Training loss = 0.2572011798620224
Iteration 170, Training loss = 0.25399289853297746
Iteration 180, Training loss = 0.2524545252896272
Iteration 190, Training loss = 0.2510507843242242
Iteration 200, Training loss = 0.2501321899203154
Iteration 210, Training loss = 0.2497458099745787
Iteration 220, Training loss = 0.24797092722012445
Iteration 230, Training loss = 0.2474031843818151
Iteration 240, Training loss = 0.24580378859088972
Iteration 250, Training loss = 0.24483299713868362
Iteration 260, Training loss = 0.24458075715945318
Iteration 270, Training loss = 0.2429143863801773
Iteration 280, Training loss = 0.24295337956685287
Iteration 290, Training loss = 0.24248003558470652
Model training time: 42.85973525047302
Device: cuda
Iteration 0, Training loss = 0.9943142235279083
Iteration 10, Training loss = 0.34072418682850325
Iteration 20, Training loss = 0.3056904346897052
Iteration 30, Training loss = 0.29633377435115665
Iteration 40, Training loss = 0.28961532935500145
Iteration 50, Training loss = 0.28469563390199953
Iteration 60, Training loss = 0.2796857308309812
Iteration 70, Training loss = 0.2743212331372958
Iteration 80, Training loss = 0.2704828610787025
Iteration 90, Training loss = 0.26743193945059407
Iteration 100, Training loss = 0.2652082176735768
Iteration 110, Training loss = 0.2626276875917728
Iteration 120, Training loss = 0.2610512782747929
Iteration 130, Training loss = 0.260562956906282
Iteration 140, Training loss = 0.2588891905660813
Iteration 150, Training loss = 0.25758371215600234
Iteration 160, Training loss = 0.25678695165194
Iteration 170, Training loss = 0.2554619934123296
Iteration 180, Training loss = 0.25406450921526325
Iteration 190, Training loss = 0.25397148355841637
Iteration 200, Training loss = 0.2535030380464517
Iteration 210, Training loss = 0.2517631168548877
Iteration 220, Training loss = 0.2509041136273971
Iteration 230, Training loss = 0.2501343603317554
Iteration 240, Training loss = 0.24945107016425866
Iteration 250, Training loss = 0.2500359470454546
Iteration 260, Training loss = 0.24807041396315282
Iteration 270, Training loss = 0.2467369273878061
Iteration 280, Training loss = 0.24694435556347555
Iteration 290, Training loss = 0.24607268606240934
Model training time: 42.43399262428284
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0035284700301976
Iteration 10, Training loss = 0.9976769456496606
Iteration 20, Training loss = 0.9962713122367859
Iteration 30, Training loss = 0.9946180811295142
Iteration 40, Training loss = 0.9944515411670392
Iteration 50, Training loss = 0.9920362245578033
Iteration 60, Training loss = 0.989881192262356
Iteration 70, Training loss = 0.9880152356166106
Iteration 80, Training loss = 0.9847921855174578
Iteration 90, Training loss = 0.9844860033347056
Iteration 100, Training loss = 0.9825792610645294
Iteration 110, Training loss = 0.979998013147941
Iteration 120, Training loss = 0.9751081191576444
Iteration 130, Training loss = 0.9716456509553469
Iteration 140, Training loss = 0.9687559902667999
Iteration 150, Training loss = 0.9646210888257394
Iteration 160, Training loss = 0.961420748096246
Iteration 170, Training loss = 0.9534251208488758
Iteration 180, Training loss = 0.9518607018085626
Iteration 190, Training loss = 0.942076136286442
Iteration 200, Training loss = 0.9348146158915299
Iteration 210, Training loss = 0.9280512046355468
Iteration 220, Training loss = 0.9188427581236913
Iteration 230, Training loss = 0.9069111805695754
Iteration 240, Training loss = 0.8938591044682723
Iteration 250, Training loss = 0.8801945356222299
Iteration 260, Training loss = 0.8645218450289506
Iteration 270, Training loss = 0.8480690809396597
Iteration 280, Training loss = 0.8274893772143584
Iteration 290, Training loss = 0.8044384981577213
Model training time: 39.692320346832275
Device: cuda
Iteration 0, Training loss = 1.2689266800880432
Iteration 10, Training loss = 1.0016055290515606
Iteration 20, Training loss = 0.9999176493057838
Iteration 30, Training loss = 0.9998367451704465
Iteration 40, Training loss = 0.9977542608976364
Iteration 50, Training loss = 0.9960737984914046
Iteration 60, Training loss = 0.994424758049158
Iteration 70, Training loss = 0.9948214154977065
Iteration 80, Training loss = 0.9943303809716151
Iteration 90, Training loss = 0.9935082311813648
Iteration 100, Training loss = 0.9911060333251953
Iteration 110, Training loss = 0.990425705909729
Iteration 120, Training loss = 0.98889594238538
Iteration 130, Training loss = 0.9871964362951425
Iteration 140, Training loss = 0.9869443865922781
Iteration 150, Training loss = 0.9858013953153903
Iteration 160, Training loss = 0.9838083260334455
Iteration 170, Training loss = 0.9811684351701003
Iteration 180, Training loss = 0.975708082318306
Iteration 190, Training loss = 0.977118228490536
Iteration 200, Training loss = 0.9748370395256922
Iteration 210, Training loss = 0.9701299976844054
Iteration 220, Training loss = 0.9675318220487008
Iteration 230, Training loss = 0.9619276706988995
Iteration 240, Training loss = 0.9616336696423017
Iteration 250, Training loss = 0.955111909371156
Iteration 260, Training loss = 0.9501175066599479
Iteration 270, Training loss = 0.9454329621333343
Iteration 280, Training loss = 0.9380110399081156
Iteration 290, Training loss = 0.9313622506765219
Model training time: 39.38210916519165
Device: cuda
Iteration 0, Training loss = 1.0976222868149097
Iteration 10, Training loss = 1.0010689932566423
Iteration 20, Training loss = 1.0015889005019114
Iteration 30, Training loss = 0.9976281477854803
Iteration 40, Training loss = 0.9981640290755492
Iteration 50, Training loss = 0.9938057236946546
Iteration 60, Training loss = 0.9930437184297122
Iteration 70, Training loss = 0.9919615972500581
Iteration 80, Training loss = 0.9876026052695054
Iteration 90, Training loss = 0.987858078800715
Iteration 100, Training loss = 0.9848674042866781
Iteration 110, Training loss = 0.9824235061040292
Iteration 120, Training loss = 0.980206256875625
Iteration 130, Training loss = 0.9771802918269084
Iteration 140, Training loss = 0.9746128653104489
Iteration 150, Training loss = 0.9701716682085624
Iteration 160, Training loss = 0.9669648191103568
Iteration 170, Training loss = 0.9619818719533774
Iteration 180, Training loss = 0.9576790871528479
Iteration 190, Training loss = 0.951543539762497
Iteration 200, Training loss = 0.9459163386088151
Iteration 210, Training loss = 0.9392686795729858
Iteration 220, Training loss = 0.9342345904845458
Iteration 230, Training loss = 0.9238471044943883
Iteration 240, Training loss = 0.916004771223435
Iteration 250, Training loss = 0.9036224266657462
Iteration 260, Training loss = 0.8909660233901098
Iteration 270, Training loss = 0.8780718732338685
Iteration 280, Training loss = 0.864335739841828
Iteration 290, Training loss = 0.8452976724276176
Model training time: 39.486029863357544
Device: cuda
Iteration 0, Training loss = 1.1617436810181692
Iteration 10, Training loss = 0.9960257617326883
Iteration 20, Training loss = 0.9946411985617417
Iteration 30, Training loss = 0.9924922470863049
Iteration 40, Training loss = 0.990831556228491
Iteration 50, Training loss = 0.9921615914656565
Iteration 60, Training loss = 0.9884286156067481
Iteration 70, Training loss = 0.9887919105016268
Iteration 80, Training loss = 0.9871303679851385
Iteration 90, Training loss = 0.9848431119552026
Iteration 100, Training loss = 0.9832812536221284
Iteration 110, Training loss = 0.9827821724689924
Iteration 120, Training loss = 0.9810122377597369
Iteration 130, Training loss = 0.9782680284518462
Iteration 140, Training loss = 0.9778284075168463
Iteration 150, Training loss = 0.973400456401018
Iteration 160, Training loss = 0.9712576293028318
Iteration 170, Training loss = 0.9669309934744468
Iteration 180, Training loss = 0.963438494847371
Iteration 190, Training loss = 0.9603654020107709
Iteration 200, Training loss = 0.9554542773045026
Iteration 210, Training loss = 0.9499900192022324
Iteration 220, Training loss = 0.9461641346032803
Iteration 230, Training loss = 0.9393269213346335
Iteration 240, Training loss = 0.9317884433727998
Iteration 250, Training loss = 0.9235110305822812
Iteration 260, Training loss = 0.9137799774224942
Iteration 270, Training loss = 0.9063888822610562
Iteration 280, Training loss = 0.8929569136637908
Iteration 290, Training loss = 0.8810605601622508
Model training time: 39.57147526741028
Device: cuda
Iteration 0, Training loss = 1.0791894518412077
Iteration 10, Training loss = 0.993870368370643
Iteration 20, Training loss = 0.9929728175585086
Iteration 30, Training loss = 0.9927060695794913
Iteration 40, Training loss = 0.9901236994908407
Iteration 50, Training loss = 0.9903484273415345
Iteration 60, Training loss = 0.9899319020601419
Iteration 70, Training loss = 0.9872938772806754
Iteration 80, Training loss = 0.9841970709654001
Iteration 90, Training loss = 0.9844395999725049
Iteration 100, Training loss = 0.979847150353285
Iteration 110, Training loss = 0.9777275541654
Iteration 120, Training loss = 0.975839397081962
Iteration 130, Training loss = 0.973245844244957
Iteration 140, Training loss = 0.9704472571611404
Iteration 150, Training loss = 0.968812665114036
Iteration 160, Training loss = 0.962749373454314
Iteration 170, Training loss = 0.9591362121013495
Iteration 180, Training loss = 0.9546267241239548
Iteration 190, Training loss = 0.9502806537426435
Iteration 200, Training loss = 0.943174869968341
Iteration 210, Training loss = 0.9363616212056234
Iteration 220, Training loss = 0.9273054232964149
Iteration 230, Training loss = 0.9188362531937085
Iteration 240, Training loss = 0.9078338673481574
Iteration 250, Training loss = 0.8955490761078321
Iteration 260, Training loss = 0.883325959627445
Iteration 270, Training loss = 0.8705328232966937
Iteration 280, Training loss = 0.8513217705946702
Iteration 290, Training loss = 0.8327506562838187
Model training time: 39.22685718536377
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9907876505301549
Iteration 10, Training loss = 0.3164705020877031
Iteration 20, Training loss = 0.29224945031679594
Iteration 30, Training loss = 0.2793458399291222
Iteration 40, Training loss = 0.2710249785047311
Iteration 50, Training loss = 0.2662461463075418
Iteration 60, Training loss = 0.26389410031529575
Iteration 70, Training loss = 0.25983956771401256
Iteration 80, Training loss = 0.25740728412683195
Iteration 90, Training loss = 0.2559485722046632
Model training time: 14.437790870666504
Device: cuda
Iteration 0, Training loss = 1.1049207895994186
Iteration 10, Training loss = 0.32404575009758657
Iteration 20, Training loss = 0.29698955095731294
Iteration 30, Training loss = 0.2826644583390309
Iteration 40, Training loss = 0.27000492782546925
Iteration 50, Training loss = 0.2609046795047246
Iteration 60, Training loss = 0.2563333267775866
Iteration 70, Training loss = 0.2528400564422974
Iteration 80, Training loss = 0.24919344370181745
Iteration 90, Training loss = 0.2459021359682083
Model training time: 14.382884502410889
Device: cuda
Iteration 0, Training loss = 0.961424239552938
Iteration 10, Training loss = 0.30285598595555013
Iteration 20, Training loss = 0.2901603765785694
Iteration 30, Training loss = 0.28323323967365116
Iteration 40, Training loss = 0.2734294570982456
Iteration 50, Training loss = 0.2676168940961361
Iteration 60, Training loss = 0.2635423048184468
Iteration 70, Training loss = 0.2594327496794554
Iteration 80, Training loss = 0.25642474884024036
Iteration 90, Training loss = 0.2545755368012648
Model training time: 14.079402446746826
Device: cuda
Iteration 0, Training loss = 1.2003555848048284
Iteration 10, Training loss = 0.3415525048398055
Iteration 20, Training loss = 0.2895305294256944
Iteration 30, Training loss = 0.27690577048521775
Iteration 40, Training loss = 0.2702358809228127
Iteration 50, Training loss = 0.2663902227695172
Iteration 60, Training loss = 0.2635566426011232
Iteration 70, Training loss = 0.2612832555404076
Iteration 80, Training loss = 0.2595437527276002
Iteration 90, Training loss = 0.2573976293206215
Model training time: 14.443830251693726
Device: cuda
Iteration 0, Training loss = 0.9781959790449876
Iteration 10, Training loss = 0.3033890976355626
Iteration 20, Training loss = 0.28885056938116366
Iteration 30, Training loss = 0.2778591513633728
Iteration 40, Training loss = 0.26997516963344353
Iteration 50, Training loss = 0.2621718261104364
Iteration 60, Training loss = 0.2573778328414147
Iteration 70, Training loss = 0.254517725453927
Iteration 80, Training loss = 0.25199664212190187
Iteration 90, Training loss = 0.24983953312039375
Model training time: 14.2371826171875
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9947354942560196
Iteration 10, Training loss = 0.988218645636852
Iteration 20, Training loss = 0.9831628936987656
Iteration 30, Training loss = 0.9769804466229218
Iteration 40, Training loss = 0.9721819552091452
Iteration 50, Training loss = 0.9643598806399566
Iteration 60, Training loss = 0.9557888989265149
Iteration 70, Training loss = 0.9450054535498986
Iteration 80, Training loss = 0.9311440529731604
Iteration 90, Training loss = 0.9125594233091061
Model training time: 13.108094930648804
Device: cuda
Iteration 0, Training loss = 1.006794321995515
Iteration 10, Training loss = 0.9987358003854752
Iteration 20, Training loss = 0.9985310905254804
Iteration 30, Training loss = 0.9951030348355954
Iteration 40, Training loss = 0.9935646641712922
Iteration 50, Training loss = 0.9917160616471217
Iteration 60, Training loss = 0.9878633927840453
Iteration 70, Training loss = 0.9885813387540671
Iteration 80, Training loss = 0.9826578383262341
Iteration 90, Training loss = 0.978818416595459
Model training time: 13.428990840911865
Device: cuda
Iteration 0, Training loss = 1.1745087469999607
Iteration 10, Training loss = 0.9880047635390208
Iteration 20, Training loss = 0.9827217218967584
Iteration 30, Training loss = 0.9787983367076287
Iteration 40, Training loss = 0.9742800570451297
Iteration 50, Training loss = 0.9676138804509089
Iteration 60, Training loss = 0.9597479552030563
Iteration 70, Training loss = 0.952120515016409
Iteration 80, Training loss = 0.9395182258807696
Iteration 90, Training loss = 0.9230107321189001
Model training time: 13.252739667892456
Device: cuda
Iteration 0, Training loss = 1.0991747138591914
Iteration 10, Training loss = 1.0005661776432624
Iteration 20, Training loss = 1.0005262986971781
Iteration 30, Training loss = 0.9962594830072843
Iteration 40, Training loss = 0.9937152553063172
Iteration 50, Training loss = 0.9938632662479694
Iteration 60, Training loss = 0.9911063989767661
Iteration 70, Training loss = 0.9877651127485129
Iteration 80, Training loss = 0.9874255955219269
Iteration 90, Training loss = 0.9818820506334305
Model training time: 13.45090913772583
Device: cuda
Iteration 0, Training loss = 1.0265765270361533
Iteration 10, Training loss = 0.9890522337876834
Iteration 20, Training loss = 0.9852326672810775
Iteration 30, Training loss = 0.9784700996600665
Iteration 40, Training loss = 0.9723817568558913
Iteration 50, Training loss = 0.9616013570473745
Iteration 60, Training loss = 0.9526129032556827
Iteration 70, Training loss = 0.9385620389993374
Iteration 80, Training loss = 0.9230528175830841
Iteration 90, Training loss = 0.9034681297265567
Model training time: 13.14984679222107
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0679435592431288
Iteration 10, Training loss = 0.350381634842891
Iteration 20, Training loss = 0.3073266056867746
Iteration 30, Training loss = 0.2896135159983085
Iteration 40, Training loss = 0.27767378149124294
Iteration 50, Training loss = 0.2687604186626581
Iteration 60, Training loss = 0.2646814762399747
Iteration 70, Training loss = 0.2613096781648122
Iteration 80, Training loss = 0.2585751124872611
Iteration 90, Training loss = 0.25673467178757375
Model training time: 14.381327867507935
Device: cuda
Iteration 0, Training loss = 1.0074613805000598
Iteration 10, Training loss = 0.3444968582345889
Iteration 20, Training loss = 0.29458844289183617
Iteration 30, Training loss = 0.2803711618941564
Iteration 40, Training loss = 0.2736766541806551
Iteration 50, Training loss = 0.2697156547353818
Iteration 60, Training loss = 0.2651520107801144
Iteration 70, Training loss = 0.2609013889271479
Iteration 80, Training loss = 0.25849310108102286
Iteration 90, Training loss = 0.2555017749277445
Model training time: 14.276789903640747
Device: cuda
Iteration 0, Training loss = 0.9877807532365506
Iteration 10, Training loss = 0.33202221072637117
Iteration 20, Training loss = 0.2942390163930563
Iteration 30, Training loss = 0.2790156588531457
Iteration 40, Training loss = 0.2714716625901369
Iteration 50, Training loss = 0.2653086552253136
Iteration 60, Training loss = 0.26294039906217503
Iteration 70, Training loss = 0.259586107558929
Iteration 80, Training loss = 0.2568594847734158
Iteration 90, Training loss = 0.2551749019095531
Model training time: 14.297486543655396
Device: cuda
Iteration 0, Training loss = 1.0682339748510947
Iteration 10, Training loss = 0.3657563231312312
Iteration 20, Training loss = 0.30322528372590357
Iteration 30, Training loss = 0.2833457864247836
Iteration 40, Training loss = 0.27378530628406084
Iteration 50, Training loss = 0.26826958472912127
Iteration 60, Training loss = 0.26459604358443845
Iteration 70, Training loss = 0.2621698502737742
Iteration 80, Training loss = 0.2585163930287728
Iteration 90, Training loss = 0.2557055577635765
Model training time: 14.33247709274292
Device: cuda
Iteration 0, Training loss = 0.9856787782448989
Iteration 10, Training loss = 0.3180310774881106
Iteration 20, Training loss = 0.29098434259112066
Iteration 30, Training loss = 0.279412737259498
Iteration 40, Training loss = 0.27049381973651737
Iteration 50, Training loss = 0.2638601821202498
Iteration 60, Training loss = 0.2604488833592488
Iteration 70, Training loss = 0.25699073563401514
Iteration 80, Training loss = 0.2546884772869257
Iteration 90, Training loss = 0.25290777906775475
Model training time: 14.186594009399414
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9973690383709394
Iteration 10, Training loss = 0.9947810803468411
Iteration 20, Training loss = 0.9940477254299017
Iteration 30, Training loss = 0.9896672379512054
Iteration 40, Training loss = 0.9886849373579025
Iteration 50, Training loss = 0.9874610236057868
Iteration 60, Training loss = 0.9833194086184869
Iteration 70, Training loss = 0.9798109531402588
Iteration 80, Training loss = 0.9759681362372178
Iteration 90, Training loss = 0.9722221722969642
Model training time: 13.100781917572021
Device: cuda
Iteration 0, Training loss = 1.0202832909730764
Iteration 10, Training loss = 1.0038688733027532
Iteration 20, Training loss = 1.0021586303527539
Iteration 30, Training loss = 1.0003228611671007
Iteration 40, Training loss = 0.9969723064165849
Iteration 50, Training loss = 0.9941947368475107
Iteration 60, Training loss = 0.9916338290159519
Iteration 70, Training loss = 0.9877077879814001
Iteration 80, Training loss = 0.9842846852082473
Iteration 90, Training loss = 0.9794003825921279
Model training time: 13.150533437728882
Device: cuda
Iteration 0, Training loss = 1.028764049594219
Iteration 10, Training loss = 1.007203390965095
Iteration 20, Training loss = 1.0053255271453123
Iteration 30, Training loss = 1.003691241145134
Iteration 40, Training loss = 1.0029523005852332
Iteration 50, Training loss = 1.000800107534115
Iteration 60, Training loss = 1.0019255452431166
Iteration 70, Training loss = 1.0031329576785748
Iteration 80, Training loss = 0.9994245458107728
Iteration 90, Training loss = 0.9990976705000951
Model training time: 13.116923809051514
Device: cuda
Iteration 0, Training loss = 1.04917627458389
Iteration 10, Training loss = 1.0072584943129466
Iteration 20, Training loss = 1.0050286856981425
Iteration 30, Training loss = 1.0054719665875802
Iteration 40, Training loss = 1.0006116846433053
Iteration 50, Training loss = 0.9998829032366092
Iteration 60, Training loss = 0.996575326873706
Iteration 70, Training loss = 0.9942947408327689
Iteration 80, Training loss = 0.9926231572261224
Iteration 90, Training loss = 0.9930708030095468
Model training time: 13.251631021499634
Device: cuda
Iteration 0, Training loss = 1.018938808487012
Iteration 10, Training loss = 0.990183908205766
Iteration 20, Training loss = 0.9882905655182325
Iteration 30, Training loss = 0.985575618652197
Iteration 40, Training loss = 0.9788916133917295
Iteration 50, Training loss = 0.9742484757533441
Iteration 60, Training loss = 0.9694724254883252
Iteration 70, Training loss = 0.9620129122183874
Iteration 80, Training loss = 0.9540168597148015
Iteration 90, Training loss = 0.9426100850105286
Model training time: 13.23281478881836
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0356595504742403
Iteration 10, Training loss = 0.31456539407372475
Iteration 20, Training loss = 0.2916182202215378
Iteration 30, Training loss = 0.28009019190302265
Iteration 40, Training loss = 0.27210096842967546
Iteration 50, Training loss = 0.26801715647945035
Iteration 60, Training loss = 0.26519862619730145
Iteration 70, Training loss = 0.26241377225288975
Iteration 80, Training loss = 0.26015308556648403
Iteration 90, Training loss = 0.2594708318893726
Model training time: 14.094512701034546
Device: cuda
Iteration 0, Training loss = 0.982193934229704
Iteration 10, Training loss = 0.3096152750345377
Iteration 20, Training loss = 0.2925569764696635
Iteration 30, Training loss = 0.2826001692849856
Iteration 40, Training loss = 0.2753158011115514
Iteration 50, Training loss = 0.2702671636182528
Iteration 60, Training loss = 0.26721791149331975
Iteration 70, Training loss = 0.2630332877429632
Iteration 80, Training loss = 0.26026205030771404
Iteration 90, Training loss = 0.2586315285701018
Model training time: 14.229889154434204
Device: cuda
Iteration 0, Training loss = 1.0670522348238871
Iteration 10, Training loss = 0.33528904215647626
Iteration 20, Training loss = 0.290485517336772
Iteration 30, Training loss = 0.27742518140719485
Iteration 40, Training loss = 0.2706952072106875
Iteration 50, Training loss = 0.26692425173062545
Iteration 60, Training loss = 0.26355918926688343
Iteration 70, Training loss = 0.26149799445500743
Iteration 80, Training loss = 0.2601798821527224
Iteration 90, Training loss = 0.25623278577740377
Model training time: 14.277814865112305
Device: cuda
Iteration 0, Training loss = 0.9976433618710592
Iteration 10, Training loss = 0.30755421691215956
Iteration 20, Training loss = 0.28725026404628384
Iteration 30, Training loss = 0.27761370660020757
Iteration 40, Training loss = 0.2695552993279237
Iteration 50, Training loss = 0.265318620376862
Iteration 60, Training loss = 0.26092154905200005
Iteration 70, Training loss = 0.25837365652506167
Iteration 80, Training loss = 0.25625604075881153
Iteration 90, Training loss = 0.2542986726531616
Model training time: 14.166181087493896
Device: cuda
Iteration 0, Training loss = 1.0210415663627477
Iteration 10, Training loss = 0.31563107382792693
Iteration 20, Training loss = 0.28995461418078494
Iteration 30, Training loss = 0.27405365728414977
Iteration 40, Training loss = 0.26549199424110925
Iteration 50, Training loss = 0.2607375414898762
Iteration 60, Training loss = 0.2570764565697083
Iteration 70, Training loss = 0.25458660463874155
Iteration 80, Training loss = 0.2528209858215772
Iteration 90, Training loss = 0.24851695104287222
Model training time: 14.17138957977295
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0179977061656804
Iteration 10, Training loss = 1.0021815747022629
Iteration 20, Training loss = 1.0007893718205965
Iteration 30, Training loss = 0.9998221374475039
Iteration 40, Training loss = 0.9993338011778318
Iteration 50, Training loss = 0.9996446932737644
Iteration 60, Training loss = 1.0004846350504801
Iteration 70, Training loss = 0.9978389785839961
Iteration 80, Training loss = 0.9966493008228449
Iteration 90, Training loss = 0.9961871447471472
Model training time: 13.029414892196655
Device: cuda
Iteration 0, Training loss = 1.0292550084682612
Iteration 10, Training loss = 0.9989385283910311
Iteration 20, Training loss = 0.9960455103562429
Iteration 30, Training loss = 0.9933267361842669
Iteration 40, Training loss = 0.9918023267617593
Iteration 50, Training loss = 0.9881490285579975
Iteration 60, Training loss = 0.9873011811421468
Iteration 70, Training loss = 0.983939602971077
Iteration 80, Training loss = 0.9795749095770029
Iteration 90, Training loss = 0.9744237741598716
Model training time: 13.27012848854065
Device: cuda
Iteration 0, Training loss = 1.027236703496713
Iteration 10, Training loss = 0.9964300692081451
Iteration 20, Training loss = 0.9950616371173125
Iteration 30, Training loss = 0.9932473978170981
Iteration 40, Training loss = 0.9896812782837794
Iteration 50, Training loss = 0.9857249122399551
Iteration 60, Training loss = 0.9832004297238129
Iteration 70, Training loss = 0.9768259834784728
Iteration 80, Training loss = 0.9709703876422002
Iteration 90, Training loss = 0.9630073320407134
Model training time: 13.268749475479126
Device: cuda
Iteration 0, Training loss = 1.0319504978565068
Iteration 10, Training loss = 1.001863915186662
Iteration 20, Training loss = 1.0027668281243398
Iteration 30, Training loss = 1.0008016308912864
Iteration 40, Training loss = 0.9978416630854974
Iteration 50, Training loss = 0.9961909548594401
Iteration 60, Training loss = 0.9947384125911273
Iteration 70, Training loss = 0.992945589698278
Iteration 80, Training loss = 0.9910236367812524
Iteration 90, Training loss = 0.9889525885765369
Model training time: 13.069523334503174
Device: cuda
Iteration 0, Training loss = 1.012157841370656
Iteration 10, Training loss = 0.9972204130429488
Iteration 20, Training loss = 0.9967093410400244
Iteration 30, Training loss = 0.9917239569700681
Iteration 40, Training loss = 0.9886691157634442
Iteration 50, Training loss = 0.9858616670736899
Iteration 60, Training loss = 0.9817190731947238
Iteration 70, Training loss = 0.9763777015300897
Iteration 80, Training loss = 0.9708844274282455
Iteration 90, Training loss = 0.9639127323260674
Model training time: 13.162530899047852
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9867101380458245
Iteration 10, Training loss = 0.3175170519031011
Iteration 20, Training loss = 0.29331008344888687
Iteration 30, Training loss = 0.27939293590875774
Iteration 40, Training loss = 0.2723511907343681
Iteration 50, Training loss = 0.26684668545539564
Iteration 60, Training loss = 0.26358582451939583
Iteration 70, Training loss = 0.2611884211118405
Iteration 80, Training loss = 0.2585526696191384
Iteration 90, Training loss = 0.2563705046016436
Iteration 100, Training loss = 0.2537232047090164
Iteration 110, Training loss = 0.25060875427264434
Iteration 120, Training loss = 0.24863998419963396
Iteration 130, Training loss = 0.2474376060641729
Iteration 140, Training loss = 0.2439196201471182
Iteration 150, Training loss = 0.24237133877781722
Iteration 160, Training loss = 0.2411660445997348
Iteration 170, Training loss = 0.23982567999225396
Iteration 180, Training loss = 0.23825762048363686
Iteration 190, Training loss = 0.2365118580368849
Model training time: 28.30732488632202
Device: cuda
Iteration 0, Training loss = 1.0206278344759574
Iteration 10, Training loss = 0.3298795564243427
Iteration 20, Training loss = 0.2914676806674554
Iteration 30, Training loss = 0.27746221910302454
Iteration 40, Training loss = 0.26972901792480397
Iteration 50, Training loss = 0.2661966013793762
Iteration 60, Training loss = 0.2630509602335783
Iteration 70, Training loss = 0.25915572305138296
Iteration 80, Training loss = 0.25598697221049893
Iteration 90, Training loss = 0.25423787706173384
Iteration 100, Training loss = 0.2523064836859703
Iteration 110, Training loss = 0.2507078375380773
Iteration 120, Training loss = 0.24934129416942596
Iteration 130, Training loss = 0.24818916704792243
Iteration 140, Training loss = 0.24702692633638015
Iteration 150, Training loss = 0.24514752225233957
Iteration 160, Training loss = 0.2440075292610205
Iteration 170, Training loss = 0.2432066869850342
Iteration 180, Training loss = 0.24098305461498407
Iteration 190, Training loss = 0.2391742029442237
Model training time: 28.00281834602356
Device: cuda
Iteration 0, Training loss = 1.0177974609228282
Iteration 10, Training loss = 0.31282265942830306
Iteration 20, Training loss = 0.2853821928684528
Iteration 30, Training loss = 0.27582796204548615
Iteration 40, Training loss = 0.2672652051999019
Iteration 50, Training loss = 0.2641534690673535
Iteration 60, Training loss = 0.25808299905978715
Iteration 70, Training loss = 0.2560735960992483
Iteration 80, Training loss = 0.2509855521986118
Iteration 90, Training loss = 0.24955816022478616
Iteration 100, Training loss = 0.24629109295514914
Iteration 110, Training loss = 0.24445719357866508
Iteration 120, Training loss = 0.2420764955190512
Iteration 130, Training loss = 0.2410074767584984
Iteration 140, Training loss = 0.2390722853059952
Iteration 150, Training loss = 0.23791796054977637
Iteration 160, Training loss = 0.23654231772972986
Iteration 170, Training loss = 0.23482698832566923
Iteration 180, Training loss = 0.233794710957087
Iteration 190, Training loss = 0.23329428296822768
Model training time: 28.224843740463257
Device: cuda
Iteration 0, Training loss = 0.9912575930356979
Iteration 10, Training loss = 0.3040819440323573
Iteration 20, Training loss = 0.28727439790964127
Iteration 30, Training loss = 0.27807525258797866
Iteration 40, Training loss = 0.2694836874993948
Iteration 50, Training loss = 0.2651262887968467
Iteration 60, Training loss = 0.261224040045188
Iteration 70, Training loss = 0.2580454226296682
Iteration 80, Training loss = 0.2564038018194529
Iteration 90, Training loss = 0.2543826490067519
Iteration 100, Training loss = 0.250420810511479
Iteration 110, Training loss = 0.24891575970328772
Iteration 120, Training loss = 0.24516956680096114
Iteration 130, Training loss = 0.24391133366869047
Iteration 140, Training loss = 0.24207331555394027
Iteration 150, Training loss = 0.24063649143163973
Iteration 160, Training loss = 0.23906461235422355
Iteration 170, Training loss = 0.23768301250842902
Iteration 180, Training loss = 0.23803207822717154
Iteration 190, Training loss = 0.2363438015946975
Model training time: 28.42086887359619
Device: cuda
Iteration 0, Training loss = 1.1452231132067168
Iteration 10, Training loss = 0.3239212093444971
Iteration 20, Training loss = 0.28839144110679626
Iteration 30, Training loss = 0.2741020026688392
Iteration 40, Training loss = 0.26581419001405054
Iteration 50, Training loss = 0.26154522827038396
Iteration 60, Training loss = 0.2578431723209528
Iteration 70, Training loss = 0.2552338041938268
Iteration 80, Training loss = 0.2525790368135159
Iteration 90, Training loss = 0.2509969492944387
Iteration 100, Training loss = 0.2486639146048289
Iteration 110, Training loss = 0.24764019451462305
Iteration 120, Training loss = 0.24636815316401994
Iteration 130, Training loss = 0.24461675807833672
Iteration 140, Training loss = 0.24324849706429702
Iteration 150, Training loss = 0.24218010529875755
Iteration 160, Training loss = 0.24004863059291473
Iteration 170, Training loss = 0.23996037903886575
Iteration 180, Training loss = 0.23765382170677185
Iteration 190, Training loss = 0.2370594126673845
Model training time: 28.206114292144775
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0813761124244103
Iteration 10, Training loss = 0.9897901324125437
Iteration 20, Training loss = 0.9868088777248676
Iteration 30, Training loss = 0.9802260570801221
Iteration 40, Training loss = 0.9750827573812925
Iteration 50, Training loss = 0.9655007399045504
Iteration 60, Training loss = 0.9584249086104907
Iteration 70, Training loss = 0.9475167794869497
Iteration 80, Training loss = 0.9351799052495223
Iteration 90, Training loss = 0.9177335214156371
Iteration 100, Training loss = 0.8938333415068113
Iteration 110, Training loss = 0.8655797621378531
Iteration 120, Training loss = 0.8273040789824265
Iteration 130, Training loss = 0.7780306855073342
Iteration 140, Training loss = 0.7186073385752164
Iteration 150, Training loss = 0.6505359938511481
Iteration 160, Training loss = 0.5803944755059022
Iteration 170, Training loss = 0.5171987517521932
Iteration 180, Training loss = 0.4656390536289949
Iteration 190, Training loss = 0.42777527009065336
Model training time: 25.95027232170105
Device: cuda
Iteration 0, Training loss = 1.1656719010609846
Iteration 10, Training loss = 0.9975022387046081
Iteration 20, Training loss = 0.9954288556025579
Iteration 30, Training loss = 0.9948237389326096
Iteration 40, Training loss = 0.9921986357523844
Iteration 50, Training loss = 0.9903466518108661
Iteration 60, Training loss = 0.9877122147725179
Iteration 70, Training loss = 0.9863111388224822
Iteration 80, Training loss = 0.9830395533488347
Iteration 90, Training loss = 0.978371649980545
Iteration 100, Training loss = 0.9710167898581579
Iteration 110, Training loss = 0.9668845557249509
Iteration 120, Training loss = 0.9575421638213671
Iteration 130, Training loss = 0.9473782445375736
Iteration 140, Training loss = 0.9346112035788022
Iteration 150, Training loss = 0.9170624693998923
Iteration 160, Training loss = 0.8960671138304931
Iteration 170, Training loss = 0.8682763782831339
Iteration 180, Training loss = 0.8314625116494986
Iteration 190, Training loss = 0.7872003740989245
Model training time: 26.092687129974365
Device: cuda
Iteration 0, Training loss = 1.014422206924512
Iteration 10, Training loss = 0.9976037156123382
Iteration 20, Training loss = 0.9968550755427434
Iteration 30, Training loss = 0.9914254798338964
Iteration 40, Training loss = 0.991097608437905
Iteration 50, Training loss = 0.985222557416329
Iteration 60, Training loss = 0.9818198359929599
Iteration 70, Training loss = 0.9772264304069372
Iteration 80, Training loss = 0.9706354530958029
Iteration 90, Training loss = 0.9634647713257716
Iteration 100, Training loss = 0.9549303582081428
Iteration 110, Training loss = 0.9432334292393464
Iteration 120, Training loss = 0.9275846825196192
Iteration 130, Training loss = 0.9082143971553216
Iteration 140, Training loss = 0.8838571367355493
Iteration 150, Training loss = 0.8483623392306842
Iteration 160, Training loss = 0.8053573679465514
Iteration 170, Training loss = 0.7518306225538254
Iteration 180, Training loss = 0.6877103333289807
Iteration 190, Training loss = 0.6183200174799333
Model training time: 26.033594846725464
Device: cuda
Iteration 0, Training loss = 1.0067412761541514
Iteration 10, Training loss = 1.0029987841844559
Iteration 20, Training loss = 1.001554617514977
Iteration 30, Training loss = 1.0010679845626538
Iteration 40, Training loss = 0.9987844526767731
Iteration 50, Training loss = 0.9981117214147861
Iteration 60, Training loss = 0.9948664124195392
Iteration 70, Training loss = 0.9950073384321653
Iteration 80, Training loss = 0.9949028480511445
Iteration 90, Training loss = 0.9921301500155375
Iteration 100, Training loss = 0.989504045018783
Iteration 110, Training loss = 0.9859904646873474
Iteration 120, Training loss = 0.9831723513511511
Iteration 130, Training loss = 0.979119687126233
Iteration 140, Training loss = 0.9751634116356189
Iteration 150, Training loss = 0.9687897115945816
Iteration 160, Training loss = 0.9622585269121023
Iteration 170, Training loss = 0.9547443722303097
Iteration 180, Training loss = 0.9439516938649691
Iteration 190, Training loss = 0.9291611210658
Model training time: 26.22406268119812
Device: cuda
Iteration 0, Training loss = 1.0240749785533318
Iteration 10, Training loss = 0.998828575015068
Iteration 20, Training loss = 0.996447626214761
Iteration 30, Training loss = 0.9934124384935086
Iteration 40, Training loss = 0.9899388884122555
Iteration 50, Training loss = 0.986139408670939
Iteration 60, Training loss = 0.9817027529844871
Iteration 70, Training loss = 0.977066043477792
Iteration 80, Training loss = 0.9721281746259103
Iteration 90, Training loss = 0.9625039776930442
Iteration 100, Training loss = 0.9528128275504479
Iteration 110, Training loss = 0.9403730963285153
Iteration 120, Training loss = 0.9244050257481061
Iteration 130, Training loss = 0.9059396431996272
Iteration 140, Training loss = 0.8776565377528851
Iteration 150, Training loss = 0.8431800386080375
Iteration 160, Training loss = 0.8013328222128061
Iteration 170, Training loss = 0.7472885503218725
Iteration 180, Training loss = 0.6882849851479897
Iteration 190, Training loss = 0.6222904169788728
Model training time: 26.088775157928467
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9909329150731747
Iteration 10, Training loss = 0.3334206033211488
Iteration 20, Training loss = 0.2961054934332004
Iteration 30, Training loss = 0.27959185810043263
Iteration 40, Training loss = 0.2722841687500477
Iteration 50, Training loss = 0.26604016211170417
Iteration 60, Training loss = 0.2632537166086527
Iteration 70, Training loss = 0.2596711120926417
Iteration 80, Training loss = 0.25852735426563483
Iteration 90, Training loss = 0.2547182503801126
Iteration 100, Training loss = 0.2530966131733014
Iteration 110, Training loss = 0.25117462099744725
Iteration 120, Training loss = 0.24905272487264413
Iteration 130, Training loss = 0.24781932022709113
Iteration 140, Training loss = 0.244840196118905
Iteration 150, Training loss = 0.24328445041408905
Iteration 160, Training loss = 0.24107499649891487
Iteration 170, Training loss = 0.24015872123149726
Iteration 180, Training loss = 0.23952611306538948
Iteration 190, Training loss = 0.23781868672141662
Model training time: 28.34733009338379
Device: cuda
Iteration 0, Training loss = 1.5378798498557165
Iteration 10, Training loss = 0.4273268889922362
Iteration 20, Training loss = 0.3114242052229551
Iteration 30, Training loss = 0.2903704345226288
Iteration 40, Training loss = 0.27846958631506336
Iteration 50, Training loss = 0.2697475589811802
Iteration 60, Training loss = 0.26332868005220705
Iteration 70, Training loss = 0.2592438399218596
Iteration 80, Training loss = 0.25574537137380016
Iteration 90, Training loss = 0.2529390537394927
Iteration 100, Training loss = 0.2512145761687022
Iteration 110, Training loss = 0.24831853663692108
Iteration 120, Training loss = 0.24693458871199533
Iteration 130, Training loss = 0.24427213261906916
Iteration 140, Training loss = 0.24295504133288676
Iteration 150, Training loss = 0.24208735875212228
Iteration 160, Training loss = 0.239997719342892
Iteration 170, Training loss = 0.23910631927160117
Iteration 180, Training loss = 0.23804840674767128
Iteration 190, Training loss = 0.23757973657204554
Model training time: 28.3820321559906
Device: cuda
Iteration 0, Training loss = 1.7345564525861006
Iteration 10, Training loss = 0.6892723555748279
Iteration 20, Training loss = 0.36616248007004076
Iteration 30, Training loss = 0.30077216057823253
Iteration 40, Training loss = 0.2816724149653545
Iteration 50, Training loss = 0.27446818580994237
Iteration 60, Training loss = 0.26950054644392085
Iteration 70, Training loss = 0.26668196056897825
Iteration 80, Training loss = 0.264194650718799
Iteration 90, Training loss = 0.261568891027799
Iteration 100, Training loss = 0.2593024651018473
Iteration 110, Training loss = 0.2585733438340517
Iteration 120, Training loss = 0.2570304529598126
Iteration 130, Training loss = 0.2558276243507862
Iteration 140, Training loss = 0.254616878926754
Iteration 150, Training loss = 0.2543927892469443
Iteration 160, Training loss = 0.2524714306570016
Iteration 170, Training loss = 0.2517711508732576
Iteration 180, Training loss = 0.2511019391509203
Iteration 190, Training loss = 0.2495645651450524
Model training time: 28.16674041748047
Device: cuda
Iteration 0, Training loss = 0.9959214914303559
Iteration 10, Training loss = 0.3141075579019693
Iteration 20, Training loss = 0.2898996529670862
Iteration 30, Training loss = 0.27823556673068267
Iteration 40, Training loss = 0.2685350308624598
Iteration 50, Training loss = 0.26410165858956486
Iteration 60, Training loss = 0.2602126065355081
Iteration 70, Training loss = 0.2576506988933453
Iteration 80, Training loss = 0.255619431917484
Iteration 90, Training loss = 0.25352434154886466
Iteration 100, Training loss = 0.25246015993448406
Iteration 110, Training loss = 0.2507169329776214
Iteration 120, Training loss = 0.2502019485602012
Iteration 130, Training loss = 0.24907176483135957
Iteration 140, Training loss = 0.24932682886719704
Iteration 150, Training loss = 0.24717838861621344
Iteration 160, Training loss = 0.24668716897185033
Iteration 170, Training loss = 0.24515924545434806
Iteration 180, Training loss = 0.24377125616257006
Iteration 190, Training loss = 0.24331658935317627
Model training time: 28.708383798599243
Device: cuda
Iteration 0, Training loss = 0.9768267434376937
Iteration 10, Training loss = 0.32672485050100547
Iteration 20, Training loss = 0.2950841122521804
Iteration 30, Training loss = 0.2789391282086189
Iteration 40, Training loss = 0.270172426620355
Iteration 50, Training loss = 0.26692565788443273
Iteration 60, Training loss = 0.2622712982388643
Iteration 70, Training loss = 0.2601240804562202
Iteration 80, Training loss = 0.2582896463572979
Iteration 90, Training loss = 0.2554487075943213
Iteration 100, Training loss = 0.2546157713692922
Iteration 110, Training loss = 0.2525020018219948
Iteration 120, Training loss = 0.2522062728038201
Iteration 130, Training loss = 0.25031219594753706
Iteration 140, Training loss = 0.25022081658244133
Iteration 150, Training loss = 0.2482370616724858
Iteration 160, Training loss = 0.24882387484495455
Iteration 170, Training loss = 0.24739223231489843
Iteration 180, Training loss = 0.24567036130107367
Iteration 190, Training loss = 0.2457702263043477
Model training time: 28.551645755767822
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0004091503528447
Iteration 10, Training loss = 0.99705515228785
Iteration 20, Training loss = 0.9957694915624765
Iteration 30, Training loss = 0.9935530538742359
Iteration 40, Training loss = 0.9906929032160685
Iteration 50, Training loss = 0.9883131247300369
Iteration 60, Training loss = 0.9869830344731991
Iteration 70, Training loss = 0.9854645190330652
Iteration 80, Training loss = 0.9805197555285233
Iteration 90, Training loss = 0.9757329661112565
Iteration 100, Training loss = 0.9714593703930194
Iteration 110, Training loss = 0.9639992908789561
Iteration 120, Training loss = 0.9559106288047937
Iteration 130, Training loss = 0.9468874312364138
Iteration 140, Training loss = 0.9350746973202779
Iteration 150, Training loss = 0.9175129383802414
Iteration 160, Training loss = 0.8987786999115577
Iteration 170, Training loss = 0.873030254474053
Iteration 180, Training loss = 0.8430236486288217
Iteration 190, Training loss = 0.8019478504474347
Model training time: 26.497871160507202
Device: cuda
Iteration 0, Training loss = 1.0267700025668511
Iteration 10, Training loss = 0.9903983657176678
Iteration 20, Training loss = 0.9879441398840684
Iteration 30, Training loss = 0.9849418103694916
Iteration 40, Training loss = 0.9791786601910224
Iteration 50, Training loss = 0.9761019475184954
Iteration 60, Training loss = 0.9693597509310796
Iteration 70, Training loss = 0.962729214475705
Iteration 80, Training loss = 0.9563933943326657
Iteration 90, Training loss = 0.9458680542615744
Iteration 100, Training loss = 0.9328310191631317
Iteration 110, Training loss = 0.9179200415427868
Iteration 120, Training loss = 0.8994020808201569
Iteration 130, Training loss = 0.8766113072633743
Iteration 140, Training loss = 0.8481965798598069
Iteration 150, Training loss = 0.8118293732404709
Iteration 160, Training loss = 0.7702139604550141
Iteration 170, Training loss = 0.7196492621531854
Iteration 180, Training loss = 0.6668430973704045
Iteration 190, Training loss = 0.6106300124755273
Model training time: 26.895606517791748
Device: cuda
Iteration 0, Training loss = 1.3011838232095425
Iteration 10, Training loss = 0.9986444734610044
Iteration 20, Training loss = 0.999489101079794
Iteration 30, Training loss = 0.997647056212792
Iteration 40, Training loss = 0.9966089370159003
Iteration 50, Training loss = 0.9961962906213907
Iteration 60, Training loss = 0.9974028903704423
Iteration 70, Training loss = 0.9969060271978378
Iteration 80, Training loss = 0.9952887331063931
Iteration 90, Training loss = 0.9945594462064596
Iteration 100, Training loss = 0.9929076742667419
Iteration 110, Training loss = 0.991997710787333
Iteration 120, Training loss = 0.9918881196242112
Iteration 130, Training loss = 0.9901498326888452
Iteration 140, Training loss = 0.9881355120585515
Iteration 150, Training loss = 0.9860445524637516
Iteration 160, Training loss = 0.9836754649877548
Iteration 170, Training loss = 0.9797875778033183
Iteration 180, Training loss = 0.977817219037276
Iteration 190, Training loss = 0.9708597740301719
Model training time: 26.56909704208374
Device: cuda
Iteration 0, Training loss = 1.014674506508387
Iteration 10, Training loss = 1.000059221799557
Iteration 20, Training loss = 0.9961236554842728
Iteration 30, Training loss = 0.9941409940903003
Iteration 40, Training loss = 0.9915979561897424
Iteration 50, Training loss = 0.989835935143324
Iteration 60, Training loss = 0.9867978852528793
Iteration 70, Training loss = 0.9832647878390092
Iteration 80, Training loss = 0.9808600900264887
Iteration 90, Training loss = 0.9721208833731138
Iteration 100, Training loss = 0.9680643792335804
Iteration 110, Training loss = 0.9611817082533469
Iteration 120, Training loss = 0.9518749633660684
Iteration 130, Training loss = 0.9405485139443324
Iteration 140, Training loss = 0.9252140120818064
Iteration 150, Training loss = 0.9092822785560901
Iteration 160, Training loss = 0.8875047277945739
Iteration 170, Training loss = 0.8580690645254575
Iteration 180, Training loss = 0.8230292155192449
Iteration 190, Training loss = 0.7802000573048224
Model training time: 26.305543184280396
Device: cuda
Iteration 0, Training loss = 1.2857556366003478
Iteration 10, Training loss = 0.9924540542639219
Iteration 20, Training loss = 0.9897593213961675
Iteration 30, Training loss = 0.9870112526875275
Iteration 40, Training loss = 0.9848170899427854
Iteration 50, Training loss = 0.979773794229214
Iteration 60, Training loss = 0.9755757600069046
Iteration 70, Training loss = 0.9688586890697479
Iteration 80, Training loss = 0.9623155479247754
Iteration 90, Training loss = 0.9526248837892826
Iteration 100, Training loss = 0.9396698635358077
Iteration 110, Training loss = 0.9256379902362823
Iteration 120, Training loss = 0.9040859032135743
Iteration 130, Training loss = 0.8775013937399938
Iteration 140, Training loss = 0.8435857857649143
Iteration 150, Training loss = 0.8026726165643105
Iteration 160, Training loss = 0.751665310217784
Iteration 170, Training loss = 0.6950591412874368
Iteration 180, Training loss = 0.633345154615549
Iteration 190, Training loss = 0.5729602288741332
Model training time: 26.079923152923584
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9807622696344669
Iteration 10, Training loss = 0.30913105664344936
Iteration 20, Training loss = 0.2962529467275509
Iteration 30, Training loss = 0.2855242144029874
Iteration 40, Training loss = 0.2751831102829713
Iteration 50, Training loss = 0.26747865315813285
Iteration 60, Training loss = 0.2635215778763478
Iteration 70, Training loss = 0.2603539744248757
Iteration 80, Training loss = 0.2579390968267734
Iteration 90, Training loss = 0.2560568056427516
Iteration 100, Training loss = 0.2560053957769504
Iteration 110, Training loss = 0.25402942901620496
Iteration 120, Training loss = 0.253459450430595
Iteration 130, Training loss = 0.2517525155383807
Iteration 140, Training loss = 0.25018947944045067
Iteration 150, Training loss = 0.24908307653207046
Iteration 160, Training loss = 0.24948346385589013
Iteration 170, Training loss = 0.24813181190536573
Iteration 180, Training loss = 0.24738085613800928
Iteration 190, Training loss = 0.24627048751482597
Model training time: 28.77002453804016
Device: cuda
Iteration 0, Training loss = 0.9671838283538818
Iteration 10, Training loss = 0.3127961087112243
Iteration 20, Training loss = 0.295099483946195
Iteration 30, Training loss = 0.28458257105488044
Iteration 40, Training loss = 0.2776131469469804
Iteration 50, Training loss = 0.2720860423376927
Iteration 60, Training loss = 0.26797985199552316
Iteration 70, Training loss = 0.26406565136634386
Iteration 80, Training loss = 0.2617539425308888
Iteration 90, Training loss = 0.2593549593136861
Iteration 100, Training loss = 0.2579030959078899
Iteration 110, Training loss = 0.2562800465294948
Iteration 120, Training loss = 0.2553881128820089
Iteration 130, Training loss = 0.25453688806066144
Iteration 140, Training loss = 0.25314514682843137
Iteration 150, Training loss = 0.25256361124607235
Iteration 160, Training loss = 0.2522597731306003
Iteration 170, Training loss = 0.2509911315372357
Iteration 180, Training loss = 0.25041094049811363
Iteration 190, Training loss = 0.2502194276222816
Model training time: 30.35045623779297
Device: cuda
Iteration 0, Training loss = 1.0546739204571798
Iteration 10, Training loss = 0.3279779977523364
Iteration 20, Training loss = 0.2978009716249429
Iteration 30, Training loss = 0.2879269060034018
Iteration 40, Training loss = 0.27995698669782054
Iteration 50, Training loss = 0.27207936031314045
Iteration 60, Training loss = 0.26532021766671765
Iteration 70, Training loss = 0.2602288622695666
Iteration 80, Training loss = 0.25680011912034106
Iteration 90, Training loss = 0.25467817886517596
Iteration 100, Training loss = 0.2514331005513668
Iteration 110, Training loss = 0.24920026470835394
Iteration 120, Training loss = 0.24673588631244806
Iteration 130, Training loss = 0.2436450057877944
Iteration 140, Training loss = 0.24208126016534293
Iteration 150, Training loss = 0.23930767321815857
Iteration 160, Training loss = 0.23845546663953707
Iteration 170, Training loss = 0.23619386553764343
Iteration 180, Training loss = 0.23440283021101585
Iteration 190, Training loss = 0.23336811850850397
Model training time: 31.32115936279297
Device: cuda
Iteration 0, Training loss = 0.9867849934559602
Iteration 10, Training loss = 0.3057792418851302
Iteration 20, Training loss = 0.2920157319078079
Iteration 30, Training loss = 0.28390874054569465
Iteration 40, Training loss = 0.27577663356295
Iteration 50, Training loss = 0.2707499563694
Iteration 60, Training loss = 0.2652374571905686
Iteration 70, Training loss = 0.2620426897819226
Iteration 80, Training loss = 0.25942345422047836
Iteration 90, Training loss = 0.258008915071304
Iteration 100, Training loss = 0.2552347627396767
Iteration 110, Training loss = 0.25300184837900674
Iteration 120, Training loss = 0.2516896208891502
Iteration 130, Training loss = 0.2502852529287338
Iteration 140, Training loss = 0.24903840027176416
Iteration 150, Training loss = 0.24812073117265335
Iteration 160, Training loss = 0.2471143904213722
Iteration 170, Training loss = 0.24621564493729517
Iteration 180, Training loss = 0.24509119271085814
Iteration 190, Training loss = 0.24376176269008562
Model training time: 30.005418062210083
Device: cuda
Iteration 0, Training loss = 1.0211147264792368
Iteration 10, Training loss = 0.3279862656043126
Iteration 20, Training loss = 0.29043753932301813
Iteration 30, Training loss = 0.2788977473974228
Iteration 40, Training loss = 0.2711975548702937
Iteration 50, Training loss = 0.267810166741793
Iteration 60, Training loss = 0.2654651099672684
Iteration 70, Training loss = 0.26256896555423737
Iteration 80, Training loss = 0.2600354443375881
Iteration 90, Training loss = 0.26009896111029845
Iteration 100, Training loss = 0.25664863792749554
Iteration 110, Training loss = 0.2559708414169458
Iteration 120, Training loss = 0.25368243713791555
Iteration 130, Training loss = 0.25286742786948496
Iteration 140, Training loss = 0.2512325799235931
Iteration 150, Training loss = 0.250265722664503
Iteration 160, Training loss = 0.2504700187307138
Iteration 170, Training loss = 0.24811326741026
Iteration 180, Training loss = 0.248089705235683
Iteration 190, Training loss = 0.24740348469752532
Model training time: 28.926135301589966
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9937604138484368
Iteration 10, Training loss = 0.991878839639517
Iteration 20, Training loss = 0.9896045097937951
Iteration 30, Training loss = 0.9899653505820495
Iteration 40, Training loss = 0.9877014240393271
Iteration 50, Training loss = 0.9818196411316211
Iteration 60, Training loss = 0.979833457332391
Iteration 70, Training loss = 0.9755212928240116
Iteration 80, Training loss = 0.9702219458726736
Iteration 90, Training loss = 0.9631217867136002
Iteration 100, Training loss = 0.9561103651156793
Iteration 110, Training loss = 0.9455309200745362
Iteration 120, Training loss = 0.932772275346976
Iteration 130, Training loss = 0.9140967692320163
Iteration 140, Training loss = 0.8948292204966912
Iteration 150, Training loss = 0.8667976042399039
Iteration 160, Training loss = 0.8307777150319173
Iteration 170, Training loss = 0.7856586220172735
Iteration 180, Training loss = 0.732154504610942
Iteration 190, Training loss = 0.6716772363736079
Model training time: 26.273342609405518
Device: cuda
Iteration 0, Training loss = 1.0525396363093302
Iteration 10, Training loss = 1.003732979297638
Iteration 20, Training loss = 1.0007047962683897
Iteration 30, Training loss = 0.9992571748220004
Iteration 40, Training loss = 0.9974008580813041
Iteration 50, Training loss = 0.9957365496800497
Iteration 60, Training loss = 0.9926304209690827
Iteration 70, Training loss = 0.9926132491001716
Iteration 80, Training loss = 0.9880980723179303
Iteration 90, Training loss = 0.9845301061868668
Iteration 100, Training loss = 0.9825362482896218
Iteration 110, Training loss = 0.9780335105382479
Iteration 120, Training loss = 0.9739998292464477
Iteration 130, Training loss = 0.9676692233635829
Iteration 140, Training loss = 0.9614364183866061
Iteration 150, Training loss = 0.9515915742287269
Iteration 160, Training loss = 0.9427546193966498
Iteration 170, Training loss = 0.9294700954969113
Iteration 180, Training loss = 0.9127799112063187
Iteration 190, Training loss = 0.8919863242369431
Model training time: 26.161123514175415
Device: cuda
Iteration 0, Training loss = 1.044266673234793
Iteration 10, Training loss = 0.9958318575070455
Iteration 20, Training loss = 0.993878844838876
Iteration 30, Training loss = 0.9938169133204681
Iteration 40, Training loss = 0.9925203907948273
Iteration 50, Training loss = 0.9878127494683633
Iteration 60, Training loss = 0.9852730734990194
Iteration 70, Training loss = 0.9818167984485626
Iteration 80, Training loss = 0.9790115631543673
Iteration 90, Training loss = 0.9730728646883597
Iteration 100, Training loss = 0.9676537250096982
Iteration 110, Training loss = 0.9602636500046804
Iteration 120, Training loss = 0.950664670421527
Iteration 130, Training loss = 0.9396733377988522
Iteration 140, Training loss = 0.9252809492441324
Iteration 150, Training loss = 0.9039878237705964
Iteration 160, Training loss = 0.8784692677167746
Iteration 170, Training loss = 0.8478957620950845
Iteration 180, Training loss = 0.805716124864725
Iteration 190, Training loss = 0.7560793849138113
Model training time: 26.6125009059906
Device: cuda
Iteration 0, Training loss = 1.0083258461493712
Iteration 10, Training loss = 0.9911304987393893
Iteration 20, Training loss = 0.9869526750766314
Iteration 30, Training loss = 0.9838534662356744
Iteration 40, Training loss = 0.9793834491417959
Iteration 50, Training loss = 0.9718143676335995
Iteration 60, Training loss = 0.9676962792873383
Iteration 70, Training loss = 0.9598214855560889
Iteration 80, Training loss = 0.948254963526359
Iteration 90, Training loss = 0.9372772253476657
Iteration 100, Training loss = 0.9220825468118374
Iteration 110, Training loss = 0.9021754471155313
Iteration 120, Training loss = 0.8770100990167031
Iteration 130, Training loss = 0.8427797074501331
Iteration 140, Training loss = 0.8026488377497747
Iteration 150, Training loss = 0.7509521887852595
Iteration 160, Training loss = 0.6911791677658374
Iteration 170, Training loss = 0.6281543162006599
Iteration 180, Training loss = 0.5649352692640744
Iteration 190, Training loss = 0.5077827882308227
Model training time: 26.39968204498291
Device: cuda
Iteration 0, Training loss = 1.0214245433990772
Iteration 10, Training loss = 1.002083951464066
Iteration 20, Training loss = 1.0009787701643431
Iteration 30, Training loss = 0.9993023368028494
Iteration 40, Training loss = 0.9993231961360345
Iteration 50, Training loss = 0.9972699960836997
Iteration 60, Training loss = 0.9957954528240057
Iteration 70, Training loss = 0.9935056211856695
Iteration 80, Training loss = 0.9923351762386469
Iteration 90, Training loss = 0.9902073683647009
Iteration 100, Training loss = 0.9861923146706361
Iteration 110, Training loss = 0.9867658592187442
Iteration 120, Training loss = 0.9813399727527912
Iteration 130, Training loss = 0.9794607334412061
Iteration 140, Training loss = 0.9715889738156245
Iteration 150, Training loss = 0.9689396252998939
Iteration 160, Training loss = 0.9618017214995164
Iteration 170, Training loss = 0.9513315054086539
Iteration 180, Training loss = 0.94163128389762
Iteration 190, Training loss = 0.9276415671293552
Model training time: 26.610654592514038
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0169751827533429
Iteration 10, Training loss = 0.3122483784189591
Iteration 20, Training loss = 0.29567400079507095
Iteration 30, Training loss = 0.28336571242946845
Iteration 40, Training loss = 0.27310691945827925
Iteration 50, Training loss = 0.2654256087083083
Iteration 60, Training loss = 0.26093917110791576
Iteration 70, Training loss = 0.25644743643127954
Iteration 80, Training loss = 0.2539266287707366
Iteration 90, Training loss = 0.25135410863619584
Iteration 100, Training loss = 0.24795887905817765
Iteration 110, Training loss = 0.24651885634431472
Iteration 120, Training loss = 0.24470053957058832
Iteration 130, Training loss = 0.2437196969985962
Iteration 140, Training loss = 0.2422903317671556
Iteration 150, Training loss = 0.24090807913587645
Iteration 160, Training loss = 0.2387795402453496
Iteration 170, Training loss = 0.23772817821456835
Iteration 180, Training loss = 0.236688502992575
Iteration 190, Training loss = 0.23495779530360147
Iteration 200, Training loss = 0.2331148302898957
Iteration 210, Training loss = 0.23278126808313224
Iteration 220, Training loss = 0.2321249361221607
Iteration 230, Training loss = 0.2317050640972761
Iteration 240, Training loss = 0.22947944442813212
Iteration 250, Training loss = 0.22833130737909904
Iteration 260, Training loss = 0.22847949140346968
Iteration 270, Training loss = 0.2272116902929086
Iteration 280, Training loss = 0.22599957281580338
Iteration 290, Training loss = 0.22493508400825354
Model training time: 42.182432413101196
Device: cuda
Iteration 0, Training loss = 0.9969415561510966
Iteration 10, Training loss = 0.30463242502166676
Iteration 20, Training loss = 0.288703091442585
Iteration 30, Training loss = 0.2775851556887993
Iteration 40, Training loss = 0.26914864721206516
Iteration 50, Training loss = 0.26470843616586465
Iteration 60, Training loss = 0.2604217311510673
Iteration 70, Training loss = 0.25656795501708984
Iteration 80, Training loss = 0.254583075356025
Iteration 90, Training loss = 0.25281607101743037
Iteration 100, Training loss = 0.2507632403419568
Iteration 110, Training loss = 0.24940141290426254
Iteration 120, Training loss = 0.24884945870592043
Iteration 130, Training loss = 0.24722595111681864
Iteration 140, Training loss = 0.24636444850609854
Iteration 150, Training loss = 0.24354419885919645
Iteration 160, Training loss = 0.24243594534122026
Iteration 170, Training loss = 0.2403393261707746
Iteration 180, Training loss = 0.23884667026308867
Iteration 190, Training loss = 0.23766539417780363
Iteration 200, Training loss = 0.23599986015604094
Iteration 210, Training loss = 0.23516155693393487
Iteration 220, Training loss = 0.233554070099042
Iteration 230, Training loss = 0.23280239563721877
Iteration 240, Training loss = 0.2318372127528374
Iteration 250, Training loss = 0.23100134472434336
Iteration 260, Training loss = 0.23028708535891312
Iteration 270, Training loss = 0.22945175491846526
Iteration 280, Training loss = 0.22790780835426772
Iteration 290, Training loss = 0.22779251549106377
Model training time: 41.97867512702942
Device: cuda
Iteration 0, Training loss = 1.019750951574399
Iteration 10, Training loss = 0.33969263617809003
Iteration 20, Training loss = 0.2916465046314093
Iteration 30, Training loss = 0.2769867402429764
Iteration 40, Training loss = 0.27120546853313077
Iteration 50, Training loss = 0.267084443798432
Iteration 60, Training loss = 0.2635490922973706
Iteration 70, Training loss = 0.2608798174903943
Iteration 80, Training loss = 0.2575544073031499
Iteration 90, Training loss = 0.2557665932063873
Iteration 100, Training loss = 0.25496463964764887
Iteration 110, Training loss = 0.2515431553698503
Iteration 120, Training loss = 0.25198151572392535
Iteration 130, Training loss = 0.2493872934809098
Iteration 140, Training loss = 0.24799553763407928
Iteration 150, Training loss = 0.24712202984553117
Iteration 160, Training loss = 0.24556076268737131
Iteration 170, Training loss = 0.24471552010912162
Iteration 180, Training loss = 0.24331511528446123
Iteration 190, Training loss = 0.24179971132140893
Iteration 200, Training loss = 0.24146345945504996
Iteration 210, Training loss = 0.24047451179761153
Iteration 220, Training loss = 0.24028697094092002
Iteration 230, Training loss = 0.23944146558642387
Iteration 240, Training loss = 0.23653231045374504
Iteration 250, Training loss = 0.2363101183795012
Iteration 260, Training loss = 0.2340714750954738
Iteration 270, Training loss = 0.2338704836483185
Iteration 280, Training loss = 0.23239492223812983
Iteration 290, Training loss = 0.2307416876921287
Model training time: 42.408907890319824
Device: cuda
Iteration 0, Training loss = 0.9796748757362366
Iteration 10, Training loss = 0.30239470274402547
Iteration 20, Training loss = 0.288104185118125
Iteration 30, Training loss = 0.27841021292484724
Iteration 40, Training loss = 0.2719523118665585
Iteration 50, Training loss = 0.2659901976585388
Iteration 60, Training loss = 0.2613047200899858
Iteration 70, Training loss = 0.25684474064753604
Iteration 80, Training loss = 0.25486517439667994
Iteration 90, Training loss = 0.25223292439029765
Iteration 100, Training loss = 0.24918299053723997
Iteration 110, Training loss = 0.24680527786795908
Iteration 120, Training loss = 0.24563832093889898
Iteration 130, Training loss = 0.24221938877151564
Iteration 140, Training loss = 0.24066987834297693
Iteration 150, Training loss = 0.2392786440367882
Iteration 160, Training loss = 0.23858420808727926
Iteration 170, Training loss = 0.23696164700847405
Iteration 180, Training loss = 0.23592147345726305
Iteration 190, Training loss = 0.234559932580361
Iteration 200, Training loss = 0.23326792539312288
Iteration 210, Training loss = 0.23325447795482782
Iteration 220, Training loss = 0.23181172660910165
Iteration 230, Training loss = 0.23081180080771446
Iteration 240, Training loss = 0.23056111484766006
Iteration 250, Training loss = 0.2291153256709759
Iteration 260, Training loss = 0.22881850800835168
Iteration 270, Training loss = 0.22882012277841568
Iteration 280, Training loss = 0.22665122925088957
Iteration 290, Training loss = 0.2273010230408265
Model training time: 42.219013929367065
Device: cuda
Iteration 0, Training loss = 0.9780682692160974
Iteration 10, Training loss = 0.30382869134728724
Iteration 20, Training loss = 0.2904473798206219
Iteration 30, Training loss = 0.2797205144396195
Iteration 40, Training loss = 0.27067758859350133
Iteration 50, Training loss = 0.26552523758548957
Iteration 60, Training loss = 0.260762440183988
Iteration 70, Training loss = 0.25916029742130864
Iteration 80, Training loss = 0.2557712119932358
Iteration 90, Training loss = 0.25401762700997865
Iteration 100, Training loss = 0.25193676753686023
Iteration 110, Training loss = 0.2500588988455442
Iteration 120, Training loss = 0.24850332822937232
Iteration 130, Training loss = 0.24698862777306482
Iteration 140, Training loss = 0.2448926530778408
Iteration 150, Training loss = 0.2440262193290087
Iteration 160, Training loss = 0.2421693532512738
Iteration 170, Training loss = 0.24184753774450377
Iteration 180, Training loss = 0.2407767271193174
Iteration 190, Training loss = 0.23943486551825816
Iteration 200, Training loss = 0.23869270086288452
Iteration 210, Training loss = 0.23838794432007349
Iteration 220, Training loss = 0.2363471489113111
Iteration 230, Training loss = 0.2356236525453054
Iteration 240, Training loss = 0.23492389172315598
Iteration 250, Training loss = 0.2338657361956743
Iteration 260, Training loss = 0.23291947721288755
Iteration 270, Training loss = 0.2329453989290274
Iteration 280, Training loss = 0.23260928231936234
Iteration 290, Training loss = 0.2300475322856353
Model training time: 42.598817110061646
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0839066963929396
Iteration 10, Training loss = 0.9918825511748974
Iteration 20, Training loss = 0.99047018129092
Iteration 30, Training loss = 0.9856952348580728
Iteration 40, Training loss = 0.983312577009201
Iteration 50, Training loss = 0.9760292585079486
Iteration 60, Training loss = 0.9698840196316059
Iteration 70, Training loss = 0.9623023947844138
Iteration 80, Training loss = 0.9510150116223556
Iteration 90, Training loss = 0.9403132120004067
Iteration 100, Training loss = 0.9239525943994522
Iteration 110, Training loss = 0.9005330399825022
Iteration 120, Training loss = 0.8735213898695432
Iteration 130, Training loss = 0.8368109132234867
Iteration 140, Training loss = 0.793570590706972
Iteration 150, Training loss = 0.7374555021524429
Iteration 160, Training loss = 0.6751226622324723
Iteration 170, Training loss = 0.605008144218188
Iteration 180, Training loss = 0.5399171767326502
Iteration 190, Training loss = 0.48085431582652605
Iteration 200, Training loss = 0.4383310159811607
Iteration 210, Training loss = 0.40943995118141174
Iteration 220, Training loss = 0.3903352085214395
Iteration 230, Training loss = 0.3802185763533299
Iteration 240, Training loss = 0.3736817790911748
Iteration 250, Training loss = 0.3684190654983887
Iteration 260, Training loss = 0.3642696216702461
Iteration 270, Training loss = 0.36194108025385785
Iteration 280, Training loss = 0.35912300990178037
Iteration 290, Training loss = 0.3565542514507587
Model training time: 39.377084732055664
Device: cuda
Iteration 0, Training loss = 1.0025055098992128
Iteration 10, Training loss = 0.9945813417434692
Iteration 20, Training loss = 0.9929708827000397
Iteration 30, Training loss = 0.9931558874937204
Iteration 40, Training loss = 0.9861153570505289
Iteration 50, Training loss = 0.9832167247167001
Iteration 60, Training loss = 0.9787467832748706
Iteration 70, Training loss = 0.9747453343409759
Iteration 80, Training loss = 0.9664990477837049
Iteration 90, Training loss = 0.9585863225735151
Iteration 100, Training loss = 0.9495102075430063
Iteration 110, Training loss = 0.9390329661277624
Iteration 120, Training loss = 0.9184399270094358
Iteration 130, Training loss = 0.89777010679245
Iteration 140, Training loss = 0.8717079105285498
Iteration 150, Training loss = 0.833142563700676
Iteration 160, Training loss = 0.7875819481336154
Iteration 170, Training loss = 0.7329633052532489
Iteration 180, Training loss = 0.6675956254968276
Iteration 190, Training loss = 0.601867672915642
Iteration 200, Training loss = 0.5389098788683231
Iteration 210, Training loss = 0.4854468571451994
Iteration 220, Training loss = 0.4474270125994316
Iteration 230, Training loss = 0.41963384873591936
Iteration 240, Training loss = 0.4024594850265063
Iteration 250, Training loss = 0.3914450200704428
Iteration 260, Training loss = 0.38441725304493535
Iteration 270, Training loss = 0.38037824688049465
Iteration 280, Training loss = 0.37570478365971494
Iteration 290, Training loss = 0.3717801705575906
Model training time: 39.302621603012085
Device: cuda
Iteration 0, Training loss = 1.0167178098972027
Iteration 10, Training loss = 0.993094156567867
Iteration 20, Training loss = 0.989353746175766
Iteration 30, Training loss = 0.9839421120973734
Iteration 40, Training loss = 0.9772121023673278
Iteration 50, Training loss = 0.972279354929924
Iteration 60, Training loss = 0.9651180574527154
Iteration 70, Training loss = 0.9537573571388538
Iteration 80, Training loss = 0.9419364069516842
Iteration 90, Training loss = 0.9275730848312378
Iteration 100, Training loss = 0.9091154910050906
Iteration 110, Training loss = 0.8833252890751913
Iteration 120, Training loss = 0.8497869911102148
Iteration 130, Training loss = 0.8082633167505264
Iteration 140, Training loss = 0.7591182532218786
Iteration 150, Training loss = 0.701001926110341
Iteration 160, Training loss = 0.6370874001429632
Iteration 170, Training loss = 0.5710646194907335
Iteration 180, Training loss = 0.5127885181170243
Iteration 190, Training loss = 0.46348986488122207
Iteration 200, Training loss = 0.42841321860368436
Iteration 210, Training loss = 0.40563296583982617
Iteration 220, Training loss = 0.39024807111575055
Iteration 230, Training loss = 0.3809300185396121
Iteration 240, Training loss = 0.37476856261491776
Iteration 250, Training loss = 0.3704265986497586
Iteration 260, Training loss = 0.36521814648921674
Iteration 270, Training loss = 0.36195068863721996
Iteration 280, Training loss = 0.3583742013344398
Iteration 290, Training loss = 0.3544122384717831
Model training time: 39.59150290489197
Device: cuda
Iteration 0, Training loss = 1.0193299158261373
Iteration 10, Training loss = 0.9967735742147152
Iteration 20, Training loss = 0.9944083438469813
Iteration 30, Training loss = 0.9919429937234292
Iteration 40, Training loss = 0.9881880214581122
Iteration 50, Training loss = 0.9826597410898942
Iteration 60, Training loss = 0.9775893470415702
Iteration 70, Training loss = 0.9707994220348505
Iteration 80, Training loss = 0.9629047788106478
Iteration 90, Training loss = 0.9540283897748361
Iteration 100, Training loss = 0.9399927522127445
Iteration 110, Training loss = 0.9258415538531083
Iteration 120, Training loss = 0.9049830253307636
Iteration 130, Training loss = 0.878433516392341
Iteration 140, Training loss = 0.8449841542885854
Iteration 150, Training loss = 0.8004517417687637
Iteration 160, Training loss = 0.7477637494985874
Iteration 170, Training loss = 0.6874576371449691
Iteration 180, Training loss = 0.6216801267403823
Iteration 190, Training loss = 0.5585756261761372
Iteration 200, Training loss = 0.5035156705058538
Iteration 210, Training loss = 0.4619631411937567
Iteration 220, Training loss = 0.431497885630681
Iteration 230, Training loss = 0.4118211017205165
Iteration 240, Training loss = 0.39786013846214
Iteration 250, Training loss = 0.3899132311344147
Iteration 260, Training loss = 0.3828287474237956
Iteration 270, Training loss = 0.3780443754333716
Iteration 280, Training loss = 0.3731975730222005
Iteration 290, Training loss = 0.37019255413458896
Model training time: 39.08775615692139
Device: cuda
Iteration 0, Training loss = 1.1212984736149127
Iteration 10, Training loss = 0.999363688322214
Iteration 20, Training loss = 0.9961264683650091
Iteration 30, Training loss = 0.9933847509897672
Iteration 40, Training loss = 0.988463085431319
Iteration 50, Training loss = 0.9862622824999002
Iteration 60, Training loss = 0.9846184448553965
Iteration 70, Training loss = 0.9792601076456217
Iteration 80, Training loss = 0.9737990154669836
Iteration 90, Training loss = 0.9695361290986722
Iteration 100, Training loss = 0.9625566510053781
Iteration 110, Training loss = 0.9509554184400119
Iteration 120, Training loss = 0.9402433989139704
Iteration 130, Training loss = 0.9283730284525797
Iteration 140, Training loss = 0.9089858291240839
Iteration 150, Training loss = 0.8866510838270187
Iteration 160, Training loss = 0.8585718079255178
Iteration 170, Training loss = 0.8211273711461288
Iteration 180, Training loss = 0.778003549346557
Iteration 190, Training loss = 0.7258037122396322
Iteration 200, Training loss = 0.6698150027256745
Iteration 210, Training loss = 0.6100670345700704
Iteration 220, Training loss = 0.5522163791152147
Iteration 230, Training loss = 0.5029753022469007
Iteration 240, Training loss = 0.4652768602738014
Iteration 250, Training loss = 0.4356175053578157
Iteration 260, Training loss = 0.4148720113130716
Iteration 270, Training loss = 0.4015304514994988
Iteration 280, Training loss = 0.3910803445256673
Iteration 290, Training loss = 0.3843102804743327
Model training time: 39.09354758262634
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9887519169312257
Iteration 10, Training loss = 0.3228651686356618
Iteration 20, Training loss = 0.297368747397111
Iteration 30, Training loss = 0.2854503355920315
Iteration 40, Training loss = 0.2750946217431472
Iteration 50, Training loss = 0.26566759468271184
Iteration 60, Training loss = 0.26055842036238086
Iteration 70, Training loss = 0.25699721248104024
Iteration 80, Training loss = 0.2529146625445439
Iteration 90, Training loss = 0.2507844724907325
Iteration 100, Training loss = 0.24758640246895644
Iteration 110, Training loss = 0.24562871771363112
Iteration 120, Training loss = 0.2433932853432802
Iteration 130, Training loss = 0.24313485565093848
Iteration 140, Training loss = 0.24133693512815696
Iteration 150, Training loss = 0.23903418303682253
Iteration 160, Training loss = 0.23817620913569743
Iteration 170, Training loss = 0.23720683472660872
Iteration 180, Training loss = 0.23662733401243502
Iteration 190, Training loss = 0.23551516951276705
Iteration 200, Training loss = 0.2343734290737372
Iteration 210, Training loss = 0.23367002348487192
Iteration 220, Training loss = 0.23248764585990173
Iteration 230, Training loss = 0.2318205268910298
Iteration 240, Training loss = 0.23161676440101403
Iteration 250, Training loss = 0.23086499508756858
Iteration 260, Training loss = 0.23040507418604997
Iteration 270, Training loss = 0.22907769192869848
Iteration 280, Training loss = 0.22864487996468177
Iteration 290, Training loss = 0.2277188117687519
Model training time: 42.95785856246948
Device: cuda
Iteration 0, Training loss = 1.0470750010930574
Iteration 10, Training loss = 0.3556020718354445
Iteration 20, Training loss = 0.30577676829237205
Iteration 30, Training loss = 0.2906668638953796
Iteration 40, Training loss = 0.2805723396058266
Iteration 50, Training loss = 0.27019735225118124
Iteration 60, Training loss = 0.263896462149345
Iteration 70, Training loss = 0.25847438035102993
Iteration 80, Training loss = 0.25459692942408413
Iteration 90, Training loss = 0.2519456810103013
Iteration 100, Training loss = 0.2491185960288231
Iteration 110, Training loss = 0.24725064864525428
Iteration 120, Training loss = 0.2450228471022386
Iteration 130, Training loss = 0.24356274077525505
Iteration 140, Training loss = 0.24215853357544312
Iteration 150, Training loss = 0.24069797304960397
Iteration 160, Training loss = 0.23925160272763327
Iteration 170, Training loss = 0.23779270196190247
Iteration 180, Training loss = 0.23629578632804063
Iteration 190, Training loss = 0.2343357248375049
Iteration 200, Training loss = 0.23330391398989236
Iteration 210, Training loss = 0.231819344827762
Iteration 220, Training loss = 0.23103995334643584
Iteration 230, Training loss = 0.22942784772469446
Iteration 240, Training loss = 0.2284379925292272
Iteration 250, Training loss = 0.22815070014733535
Iteration 260, Training loss = 0.2275101121228475
Iteration 270, Training loss = 0.22708327380510476
Iteration 280, Training loss = 0.22514746939906707
Iteration 290, Training loss = 0.2258203187241004
Model training time: 42.55270552635193
Device: cuda
Iteration 0, Training loss = 0.9846107868047861
Iteration 10, Training loss = 0.3279862621655831
Iteration 20, Training loss = 0.2950444889183228
Iteration 30, Training loss = 0.2840197656590205
Iteration 40, Training loss = 0.2750522577418731
Iteration 50, Training loss = 0.2697484272603805
Iteration 60, Training loss = 0.26438627821894795
Iteration 70, Training loss = 0.26077414418642336
Iteration 80, Training loss = 0.25824199330348235
Iteration 90, Training loss = 0.2549300743983342
Iteration 100, Training loss = 0.25249196503024834
Iteration 110, Training loss = 0.2511717998064481
Iteration 120, Training loss = 0.25008184262193167
Iteration 130, Training loss = 0.24877257301257208
Iteration 140, Training loss = 0.24824051960156515
Iteration 150, Training loss = 0.24742938606784895
Iteration 160, Training loss = 0.24611493372000182
Iteration 170, Training loss = 0.2456257208608664
Iteration 180, Training loss = 0.24616105052141043
Iteration 190, Training loss = 0.2452188297533072
Iteration 200, Training loss = 0.2445850890989487
Iteration 210, Training loss = 0.24290899215982512
Iteration 220, Training loss = 0.24212492973758623
Iteration 230, Training loss = 0.24125311523675919
Iteration 240, Training loss = 0.23971199473509422
Iteration 250, Training loss = 0.23857318695921165
Iteration 260, Training loss = 0.23619296487707359
Iteration 270, Training loss = 0.23481373460246965
Iteration 280, Training loss = 0.23379065612187752
Iteration 290, Training loss = 0.23215970597588098
Model training time: 42.44110417366028
Device: cuda
Iteration 0, Training loss = 0.9795512075607593
Iteration 10, Training loss = 0.3171757622980155
Iteration 20, Training loss = 0.29248808324337006
Iteration 30, Training loss = 0.2827727310359478
Iteration 40, Training loss = 0.2749056260173137
Iteration 50, Training loss = 0.2690310418032683
Iteration 60, Training loss = 0.26276073260949206
Iteration 70, Training loss = 0.2581775357516912
Iteration 80, Training loss = 0.2556154768054302
Iteration 90, Training loss = 0.2515743543895391
Iteration 100, Training loss = 0.24952834070875093
Iteration 110, Training loss = 0.24618289447747743
Iteration 120, Training loss = 0.24504963939006513
Iteration 130, Training loss = 0.24347770758546317
Iteration 140, Training loss = 0.24174235990414253
Iteration 150, Training loss = 0.24051439876739794
Iteration 160, Training loss = 0.23880062624812126
Iteration 170, Training loss = 0.23854469765837377
Iteration 180, Training loss = 0.23655364089287245
Iteration 190, Training loss = 0.2370120043364855
Iteration 200, Training loss = 0.23474466112943795
Iteration 210, Training loss = 0.2336293295598947
Iteration 220, Training loss = 0.23294461317933524
Iteration 230, Training loss = 0.23202364461926314
Iteration 240, Training loss = 0.23178528908353585
Iteration 250, Training loss = 0.23112501175357744
Iteration 260, Training loss = 0.22999391618829507
Iteration 270, Training loss = 0.22972042400103349
Iteration 280, Training loss = 0.2298645723897677
Iteration 290, Training loss = 0.22861661303501862
Model training time: 42.665093421936035
Device: cuda
Iteration 0, Training loss = 0.9830008687881323
Iteration 10, Training loss = 0.3198552378095113
Iteration 20, Training loss = 0.2926272148123154
Iteration 30, Training loss = 0.28238445033247656
Iteration 40, Training loss = 0.2739630811489545
Iteration 50, Training loss = 0.2683454889517564
Iteration 60, Training loss = 0.264542921517904
Iteration 70, Training loss = 0.2620059956724827
Iteration 80, Training loss = 0.25880366879013866
Iteration 90, Training loss = 0.2555788786938557
Iteration 100, Training loss = 0.2532869244997318
Iteration 110, Training loss = 0.25082358374045444
Iteration 120, Training loss = 0.24903061126287168
Iteration 130, Training loss = 0.24680014613729256
Iteration 140, Training loss = 0.2450447389139579
Iteration 150, Training loss = 0.24340304359793663
Iteration 160, Training loss = 0.24199330634795702
Iteration 170, Training loss = 0.24204166004290947
Iteration 180, Training loss = 0.24040312577898687
Iteration 190, Training loss = 0.23878542706370354
Iteration 200, Training loss = 0.23814395671853653
Iteration 210, Training loss = 0.23810344590590551
Iteration 220, Training loss = 0.236647983869681
Iteration 230, Training loss = 0.2369038095841041
Iteration 240, Training loss = 0.2351793131003013
Iteration 250, Training loss = 0.2346337466285779
Iteration 260, Training loss = 0.23456558527854773
Iteration 270, Training loss = 0.23425024174726927
Iteration 280, Training loss = 0.2326139320547764
Iteration 290, Training loss = 0.2319237948037111
Model training time: 43.52506422996521
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0234602930454106
Iteration 10, Training loss = 0.9998570416982357
Iteration 20, Training loss = 0.9980337791717969
Iteration 30, Training loss = 0.9944070852719821
Iteration 40, Training loss = 0.993375804561835
Iteration 50, Training loss = 0.9915883403557998
Iteration 60, Training loss = 0.9917672230647161
Iteration 70, Training loss = 0.9871778705945382
Iteration 80, Training loss = 0.9847715817964994
Iteration 90, Training loss = 0.980088526239762
Iteration 100, Training loss = 0.9779513833614496
Iteration 110, Training loss = 0.971318842126773
Iteration 120, Training loss = 0.9628951687079209
Iteration 130, Training loss = 0.9553093921679717
Iteration 140, Training loss = 0.9447837219788477
Iteration 150, Training loss = 0.9320091524949441
Iteration 160, Training loss = 0.9150167864102584
Iteration 170, Training loss = 0.8927838767950351
Iteration 180, Training loss = 0.8667162026350315
Iteration 190, Training loss = 0.8299820904548352
Iteration 200, Training loss = 0.7863302872731135
Iteration 210, Training loss = 0.7358811704012064
Iteration 220, Training loss = 0.6791365983394476
Iteration 230, Training loss = 0.6170450586539048
Iteration 240, Training loss = 0.559864284327397
Iteration 250, Training loss = 0.5094766622552505
Iteration 260, Training loss = 0.47086147562815595
Iteration 270, Training loss = 0.44173722943434346
Iteration 280, Training loss = 0.4226425255720432
Iteration 290, Training loss = 0.4109876717512424
Model training time: 39.561607122421265
Device: cuda
Iteration 0, Training loss = 1.4701683292022119
Iteration 10, Training loss = 0.9943645562116916
Iteration 20, Training loss = 0.9918846293137624
Iteration 30, Training loss = 0.9866197384320773
Iteration 40, Training loss = 0.9839872786631951
Iteration 50, Training loss = 0.9797574774577067
Iteration 60, Training loss = 0.9766791623372298
Iteration 70, Training loss = 0.9715805569520364
Iteration 80, Training loss = 0.9614113741196119
Iteration 90, Training loss = 0.9529951489888705
Iteration 100, Training loss = 0.9425577016977164
Iteration 110, Training loss = 0.92538432433055
Iteration 120, Training loss = 0.9057403630935229
Iteration 130, Training loss = 0.8805585950613022
Iteration 140, Training loss = 0.848325503560213
Iteration 150, Training loss = 0.8071634288017566
Iteration 160, Training loss = 0.7581571145699575
Iteration 170, Training loss = 0.6997814797438108
Iteration 180, Training loss = 0.6386317839989295
Iteration 190, Training loss = 0.5778027618160615
Iteration 200, Training loss = 0.5218680523909055
Iteration 210, Training loss = 0.47674008229604137
Iteration 220, Training loss = 0.4414425818965985
Iteration 230, Training loss = 0.4188717036293103
Iteration 240, Training loss = 0.4013212403425804
Iteration 250, Training loss = 0.3900613790521255
Iteration 260, Training loss = 0.38272212273799455
Iteration 270, Training loss = 0.37657924168384993
Iteration 280, Training loss = 0.370418664927666
Iteration 290, Training loss = 0.36670444447260636
Model training time: 39.76644444465637
Device: cuda
Iteration 0, Training loss = 1.2306603307907398
Iteration 10, Training loss = 0.9789406095559781
Iteration 20, Training loss = 0.9710434010395637
Iteration 30, Training loss = 0.9655865683005407
Iteration 40, Training loss = 0.9570890011695715
Iteration 50, Training loss = 0.9459472470558606
Iteration 60, Training loss = 0.9338571864825028
Iteration 70, Training loss = 0.918059996687449
Iteration 80, Training loss = 0.8957571696776611
Iteration 90, Training loss = 0.86993619455741
Iteration 100, Training loss = 0.8388139915007812
Iteration 110, Training loss = 0.7965691582514689
Iteration 120, Training loss = 0.744920762685629
Iteration 130, Training loss = 0.6848601698875427
Iteration 140, Training loss = 0.6207746651310188
Iteration 150, Training loss = 0.5583032839573346
Iteration 160, Training loss = 0.4997599577674499
Iteration 170, Training loss = 0.45200624947364515
Iteration 180, Training loss = 0.41781325007860476
Iteration 190, Training loss = 0.3949965542325607
Iteration 200, Training loss = 0.3788411267674886
Iteration 210, Training loss = 0.36963471197164977
Iteration 220, Training loss = 0.36162038147449493
Iteration 230, Training loss = 0.35927337408065796
Iteration 240, Training loss = 0.35551887578689134
Iteration 250, Training loss = 0.35179922672418446
Iteration 260, Training loss = 0.3493875932808106
Iteration 270, Training loss = 0.34761679287140185
Iteration 280, Training loss = 0.34607466424886996
Iteration 290, Training loss = 0.34356957874619043
Model training time: 39.0333514213562
Device: cuda
Iteration 0, Training loss = 1.0466579370773756
Iteration 10, Training loss = 0.9997117542303525
Iteration 20, Training loss = 0.9987730154624352
Iteration 30, Training loss = 0.9988502023311762
Iteration 40, Training loss = 0.997119848544781
Iteration 50, Training loss = 0.9970736320202167
Iteration 60, Training loss = 0.9961072309659078
Iteration 70, Training loss = 0.9934993317494025
Iteration 80, Training loss = 0.9936373783991888
Iteration 90, Training loss = 0.9913493864811384
Iteration 100, Training loss = 0.9888620067101258
Iteration 110, Training loss = 0.9872137445669907
Iteration 120, Training loss = 0.9841067034464616
Iteration 130, Training loss = 0.979956818314699
Iteration 140, Training loss = 0.9770895609488854
Iteration 150, Training loss = 0.9711461926882083
Iteration 160, Training loss = 0.9669430427826368
Iteration 170, Training loss = 0.9552353884165103
Iteration 180, Training loss = 0.94505631006681
Iteration 190, Training loss = 0.929660425736354
Iteration 200, Training loss = 0.9131958461724795
Iteration 210, Training loss = 0.8912191471228232
Iteration 220, Training loss = 0.8639001559752685
Iteration 230, Training loss = 0.8267756368105228
Iteration 240, Training loss = 0.7819723888085439
Iteration 250, Training loss = 0.7320992981012051
Iteration 260, Training loss = 0.6733446029516367
Iteration 270, Training loss = 0.6134668668875327
Iteration 280, Training loss = 0.5564842894673347
Iteration 290, Training loss = 0.5078073017872297
Model training time: 39.4781014919281
Device: cuda
Iteration 0, Training loss = 1.1055648510272686
Iteration 10, Training loss = 0.9969236896588252
Iteration 20, Training loss = 0.9957103362450233
Iteration 30, Training loss = 0.995207734979116
Iteration 40, Training loss = 0.9959348921592419
Iteration 50, Training loss = 0.9916922679314246
Iteration 60, Training loss = 0.9915262529483209
Iteration 70, Training loss = 0.9914126166930566
Iteration 80, Training loss = 0.9883130960739576
Iteration 90, Training loss = 0.9868555275293497
Iteration 100, Training loss = 0.9852876995618527
Iteration 110, Training loss = 0.9843760682986333
Iteration 120, Training loss = 0.9796564349761376
Iteration 130, Training loss = 0.977181498820965
Iteration 140, Training loss = 0.9746340146431556
Iteration 150, Training loss = 0.967863645691138
Iteration 160, Training loss = 0.9631838225401365
Iteration 170, Training loss = 0.9567556060277499
Iteration 180, Training loss = 0.9496543143804257
Iteration 190, Training loss = 0.9401469780848577
Iteration 200, Training loss = 0.9292322271145307
Iteration 210, Training loss = 0.9155989358058343
Iteration 220, Training loss = 0.8993890125017899
Iteration 230, Training loss = 0.8764500870154455
Iteration 240, Training loss = 0.8506041249403586
Iteration 250, Training loss = 0.819161953834387
Iteration 260, Training loss = 0.781305546943958
Iteration 270, Training loss = 0.7379604337307123
Iteration 280, Training loss = 0.6909071665543777
Iteration 290, Training loss = 0.6398735275635352
Model training time: 39.62178945541382
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9808972466450471
Iteration 10, Training loss = 0.3105542470629399
Iteration 20, Training loss = 0.2967089113707726
Iteration 30, Training loss = 0.28580160754231304
Iteration 40, Training loss = 0.2747338623381578
Iteration 50, Training loss = 0.2678615012421058
Iteration 60, Training loss = 0.26288066202631366
Iteration 70, Training loss = 0.26112945119921976
Iteration 80, Training loss = 0.25680449776924574
Iteration 90, Training loss = 0.2542959245351645
Iteration 100, Training loss = 0.2512744673742698
Iteration 110, Training loss = 0.248851016163826
Iteration 120, Training loss = 0.2487610584268203
Iteration 130, Training loss = 0.24593344961221403
Iteration 140, Training loss = 0.24436065611930993
Iteration 150, Training loss = 0.24370912003975648
Iteration 160, Training loss = 0.24331117707949418
Iteration 170, Training loss = 0.24138079984829977
Iteration 180, Training loss = 0.2406027878706272
Iteration 190, Training loss = 0.24004948855592653
Iteration 200, Training loss = 0.2383711002767086
Iteration 210, Training loss = 0.2376165252465468
Iteration 220, Training loss = 0.23694593258775198
Iteration 230, Training loss = 0.23579470498057511
Iteration 240, Training loss = 0.2354212377506953
Iteration 250, Training loss = 0.23519815848423883
Iteration 260, Training loss = 0.23473302790751824
Iteration 270, Training loss = 0.23326042093909705
Iteration 280, Training loss = 0.2330362690755954
Iteration 290, Training loss = 0.23248152549450213
Model training time: 43.20449447631836
Device: cuda
Iteration 0, Training loss = 0.9935260438002073
Iteration 10, Training loss = 0.30967590843255705
Iteration 20, Training loss = 0.2932112216949463
Iteration 30, Training loss = 0.28273697999807507
Iteration 40, Training loss = 0.27526173196159875
Iteration 50, Training loss = 0.2698951598543387
Iteration 60, Training loss = 0.26596264799053854
Iteration 70, Training loss = 0.2623557648979701
Iteration 80, Training loss = 0.26131991669535637
Iteration 90, Training loss = 0.2589924120559142
Iteration 100, Training loss = 0.25764109366215193
Iteration 110, Training loss = 0.25532518413204414
Iteration 120, Training loss = 0.25369597856815046
Iteration 130, Training loss = 0.2524679876290835
Iteration 140, Training loss = 0.2517472267724
Iteration 150, Training loss = 0.25105568737937856
Iteration 160, Training loss = 0.24966420204593584
Iteration 170, Training loss = 0.24891792295070794
Iteration 180, Training loss = 0.24807541387585494
Iteration 190, Training loss = 0.24720421232856238
Iteration 200, Training loss = 0.2465840939145822
Iteration 210, Training loss = 0.2463938522224243
Iteration 220, Training loss = 0.24410106270359114
Iteration 230, Training loss = 0.242588865928925
Iteration 240, Training loss = 0.2424375664156217
Iteration 250, Training loss = 0.24162120142808327
Iteration 260, Training loss = 0.2399846865580632
Iteration 270, Training loss = 0.2390450803706279
Iteration 280, Training loss = 0.2381262595836933
Iteration 290, Training loss = 0.23783175876507393
Model training time: 43.08400845527649
Device: cuda
Iteration 0, Training loss = 1.0362128191269362
Iteration 10, Training loss = 0.31619158731057095
Iteration 20, Training loss = 0.29528673967489827
Iteration 30, Training loss = 0.28646711747233683
Iteration 40, Training loss = 0.2771448917113818
Iteration 50, Training loss = 0.2688664934382989
Iteration 60, Training loss = 0.26326058776332784
Iteration 70, Training loss = 0.258265384974388
Iteration 80, Training loss = 0.2555449814177476
Iteration 90, Training loss = 0.25227644924934095
Iteration 100, Training loss = 0.2500931480183051
Iteration 110, Training loss = 0.2482092326077131
Iteration 120, Training loss = 0.2470159619473494
Iteration 130, Training loss = 0.24337268993258476
Iteration 140, Training loss = 0.24185775191738054
Iteration 150, Training loss = 0.24080514563963965
Iteration 160, Training loss = 0.24027900111216766
Iteration 170, Training loss = 0.23842211153644782
Iteration 180, Training loss = 0.23757926231393448
Iteration 190, Training loss = 0.23627053717008004
Iteration 200, Training loss = 0.23481383690467247
Iteration 210, Training loss = 0.23420241933602554
Iteration 220, Training loss = 0.23300426653944528
Iteration 230, Training loss = 0.23131491473087898
Iteration 240, Training loss = 0.23092049073714477
Iteration 250, Training loss = 0.22979528571550661
Iteration 260, Training loss = 0.22826018184423447
Iteration 270, Training loss = 0.22751764666575652
Iteration 280, Training loss = 0.22647772299555632
Iteration 290, Training loss = 0.22569469133248696
Model training time: 42.694613456726074
Device: cuda
Iteration 0, Training loss = 0.986082714337569
Iteration 10, Training loss = 0.30482317994420344
Iteration 20, Training loss = 0.2907580790611414
Iteration 30, Training loss = 0.2832096511354813
Iteration 40, Training loss = 0.27631334186746526
Iteration 50, Training loss = 0.2696335052068417
Iteration 60, Training loss = 0.265403842410216
Iteration 70, Training loss = 0.26001604159290975
Iteration 80, Training loss = 0.2564314589477502
Iteration 90, Training loss = 0.2528661612707835
Iteration 100, Training loss = 0.2508880862822899
Iteration 110, Training loss = 0.2480807536496566
Iteration 120, Training loss = 0.24649111066873258
Iteration 130, Training loss = 0.24487693779743636
Iteration 140, Training loss = 0.2435377687215805
Iteration 150, Training loss = 0.24229610281494948
Iteration 160, Training loss = 0.24018061304321656
Iteration 170, Training loss = 0.23929744500380296
Iteration 180, Training loss = 0.23754791370951212
Iteration 190, Training loss = 0.2364894039928913
Iteration 200, Training loss = 0.23617305262730673
Iteration 210, Training loss = 0.23453598297559297
Iteration 220, Training loss = 0.23414742746032202
Iteration 230, Training loss = 0.23336005526093337
Iteration 240, Training loss = 0.23195921629667282
Iteration 250, Training loss = 0.2316105331365879
Iteration 260, Training loss = 0.2308451200907047
Iteration 270, Training loss = 0.23029018795261016
Iteration 280, Training loss = 0.2295843018935277
Iteration 290, Training loss = 0.22883613751484796
Model training time: 42.305315256118774
Device: cuda
Iteration 0, Training loss = 1.0833858996629715
Iteration 10, Training loss = 0.3265312219468447
Iteration 20, Training loss = 0.28753852013211983
Iteration 30, Training loss = 0.2740910855623392
Iteration 40, Training loss = 0.2697202073266873
Iteration 50, Training loss = 0.26384155394939274
Iteration 60, Training loss = 0.2609789887299904
Iteration 70, Training loss = 0.2594071119450606
Iteration 80, Training loss = 0.2574810173649054
Iteration 90, Training loss = 0.25633464925564253
Iteration 100, Training loss = 0.25515172802484953
Iteration 110, Training loss = 0.2534387879646741
Iteration 120, Training loss = 0.2528140244002526
Iteration 130, Training loss = 0.2510927932766768
Iteration 140, Training loss = 0.2508323493485267
Iteration 150, Training loss = 0.25000703134215796
Iteration 160, Training loss = 0.24909744125146133
Iteration 170, Training loss = 0.2486978224836863
Iteration 180, Training loss = 0.24766369278614336
Iteration 190, Training loss = 0.24728048363557228
Iteration 200, Training loss = 0.24557175802496764
Iteration 210, Training loss = 0.24559400144677895
Iteration 220, Training loss = 0.24511944559904245
Iteration 230, Training loss = 0.2437404362628093
Iteration 240, Training loss = 0.24393019137474206
Iteration 250, Training loss = 0.24362309248401567
Iteration 260, Training loss = 0.24257766971221337
Iteration 270, Training loss = 0.24307522005759752
Iteration 280, Training loss = 0.2411948786332057
Iteration 290, Training loss = 0.2412867170686905
Model training time: 42.27014994621277
{'activation_functions': ['sigmoid', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0242750828082745
Iteration 10, Training loss = 0.9986594216181681
Iteration 20, Training loss = 0.9987443765768638
Iteration 30, Training loss = 0.9964110175004373
Iteration 40, Training loss = 0.9951299612338727
Iteration 50, Training loss = 0.9932671842666773
Iteration 60, Training loss = 0.9924375816033437
Iteration 70, Training loss = 0.9909061846824793
Iteration 80, Training loss = 0.9866411926654669
Iteration 90, Training loss = 0.9849141962253131
Iteration 100, Training loss = 0.9815108329057693
Iteration 110, Training loss = 0.9790131357999948
Iteration 120, Training loss = 0.97460554425533
Iteration 130, Training loss = 0.9679240721922654
Iteration 140, Training loss = 0.9622713396182427
Iteration 150, Training loss = 0.9553889265427222
Iteration 160, Training loss = 0.9449062805909377
Iteration 170, Training loss = 0.9330552117182658
Iteration 180, Training loss = 0.9160957714686027
Iteration 190, Training loss = 0.8973439542146829
Iteration 200, Training loss = 0.8713908252807764
Iteration 210, Training loss = 0.8405086386662263
Iteration 220, Training loss = 0.8019857555627823
Iteration 230, Training loss = 0.7559196089322751
Iteration 240, Training loss = 0.7031719157328973
Iteration 250, Training loss = 0.6447757608615435
Iteration 260, Training loss = 0.5855232328176498
Iteration 270, Training loss = 0.5292874878415694
Iteration 280, Training loss = 0.4832918088023479
Iteration 290, Training loss = 0.4473201265701881
Model training time: 39.381715297698975
Device: cuda
Iteration 0, Training loss = 1.0329550894407125
Iteration 10, Training loss = 0.9980548631686431
Iteration 20, Training loss = 0.9972232293624145
Iteration 30, Training loss = 0.9965655161784246
Iteration 40, Training loss = 0.9949474999537835
Iteration 50, Training loss = 0.9938949541403697
Iteration 60, Training loss = 0.9933844827688657
Iteration 70, Training loss = 0.9924047577839631
Iteration 80, Training loss = 0.9909539073705673
Iteration 90, Training loss = 0.987881689117505
Iteration 100, Training loss = 0.9844810767815664
Iteration 110, Training loss = 0.9824012472079351
Iteration 120, Training loss = 0.9794520151156646
Iteration 130, Training loss = 0.9750862511304709
Iteration 140, Training loss = 0.9698674713189785
Iteration 150, Training loss = 0.9637833868081753
Iteration 160, Training loss = 0.955163734463545
Iteration 170, Training loss = 0.9469551810851464
Iteration 180, Training loss = 0.932982827608402
Iteration 190, Training loss = 0.9185061443310517
Iteration 200, Training loss = 0.896633682342676
Iteration 210, Training loss = 0.8723981575323985
Iteration 220, Training loss = 0.8417168660805776
Iteration 230, Training loss = 0.8026180817530706
Iteration 240, Training loss = 0.7579834782160245
Iteration 250, Training loss = 0.7041563380223054
Iteration 260, Training loss = 0.6487552976379027
Iteration 270, Training loss = 0.5934813756209153
Iteration 280, Training loss = 0.5397956107671444
Iteration 290, Training loss = 0.4957824590114447
Model training time: 39.61895728111267
Device: cuda
Iteration 0, Training loss = 1.2913874639914587
Iteration 10, Training loss = 0.9848258793354034
Iteration 20, Training loss = 0.9813921279632128
Iteration 30, Training loss = 0.9783385315766702
Iteration 40, Training loss = 0.971684034054096
Iteration 50, Training loss = 0.9666074193440951
Iteration 60, Training loss = 0.9583930029318883
Iteration 70, Training loss = 0.9523786054207728
Iteration 80, Training loss = 0.9403663140076858
Iteration 90, Training loss = 0.9252417878462718
Iteration 100, Training loss = 0.9062789254463636
Iteration 110, Training loss = 0.8831682445911261
Iteration 120, Training loss = 0.8567064840060014
Iteration 130, Training loss = 0.8213908637945468
Iteration 140, Training loss = 0.77941256073805
Iteration 150, Training loss = 0.7297215037620984
Iteration 160, Training loss = 0.675134254189638
Iteration 170, Training loss = 0.6155755571447886
Iteration 180, Training loss = 0.5587520140867966
Iteration 190, Training loss = 0.5056632006397614
Iteration 200, Training loss = 0.4629630973705879
Iteration 210, Training loss = 0.4292606350321036
Iteration 220, Training loss = 0.40625490018954646
Iteration 230, Training loss = 0.39050590705413085
Iteration 240, Training loss = 0.380136485856313
Iteration 250, Training loss = 0.3726852404383513
Iteration 260, Training loss = 0.36853858771232456
Iteration 270, Training loss = 0.36431828943582684
Iteration 280, Training loss = 0.3603502712570704
Iteration 290, Training loss = 0.35757947025390774
Model training time: 39.169108390808105
Device: cuda
Iteration 0, Training loss = 1.0034084641016447
Iteration 10, Training loss = 0.996623049561794
Iteration 20, Training loss = 0.9939562529325485
Iteration 30, Training loss = 0.9917928587931854
Iteration 40, Training loss = 0.9893445464280936
Iteration 50, Training loss = 0.9849075491611774
Iteration 60, Training loss = 0.983738457927337
Iteration 70, Training loss = 0.9780544031124848
Iteration 80, Training loss = 0.9730303230193945
Iteration 90, Training loss = 0.9656149939848826
Iteration 100, Training loss = 0.9587389654838122
Iteration 110, Training loss = 0.9491132944822311
Iteration 120, Training loss = 0.9346931473566935
Iteration 130, Training loss = 0.9171846829927884
Iteration 140, Training loss = 0.8959124581171916
Iteration 150, Training loss = 0.865368202328682
Iteration 160, Training loss = 0.8284390339484582
Iteration 170, Training loss = 0.7806175190668839
Iteration 180, Training loss = 0.7234143798167889
Iteration 190, Training loss = 0.656188044983607
Iteration 200, Training loss = 0.588213338301732
Iteration 210, Training loss = 0.5243041171477392
Iteration 220, Training loss = 0.4714230877848772
Iteration 230, Training loss = 0.4330530814253367
Iteration 240, Training loss = 0.4068207236436697
Iteration 250, Training loss = 0.3912898044173534
Iteration 260, Training loss = 0.38061211659358096
Iteration 270, Training loss = 0.3734047321172861
Iteration 280, Training loss = 0.36972967936442447
Iteration 290, Training loss = 0.3655547138590079
Model training time: 40.11900305747986
Device: cuda
Iteration 0, Training loss = 1.153228223323822
Iteration 10, Training loss = 0.9953006219405395
Iteration 20, Training loss = 0.9936481301601117
Iteration 30, Training loss = 0.9922850430011749
Iteration 40, Training loss = 0.9878872885153844
Iteration 50, Training loss = 0.9855670103659997
Iteration 60, Training loss = 0.9818928172955146
Iteration 70, Training loss = 0.9759828253434255
Iteration 80, Training loss = 0.971399032152616
Iteration 90, Training loss = 0.966477435368758
Iteration 100, Training loss = 0.9563426191990192
Iteration 110, Training loss = 0.946812595312412
Iteration 120, Training loss = 0.932008934708742
Iteration 130, Training loss = 0.9125743771974857
Iteration 140, Training loss = 0.8892945750401571
Iteration 150, Training loss = 0.8600153888647373
Iteration 160, Training loss = 0.8213047408140622
Iteration 170, Training loss = 0.7743791387631342
Iteration 180, Training loss = 0.7183222231956629
Iteration 190, Training loss = 0.6543009802699089
Iteration 200, Training loss = 0.5897930482259164
Iteration 210, Training loss = 0.5284966270510967
Iteration 220, Training loss = 0.4773074732376979
Iteration 230, Training loss = 0.4378532056625073
Iteration 240, Training loss = 0.4102259943118462
Iteration 250, Training loss = 0.3920294057864409
Iteration 260, Training loss = 0.3814006745815277
Iteration 270, Training loss = 0.37332203067266023
Iteration 280, Training loss = 0.36779342821011174
Iteration 290, Training loss = 0.36300168415674794
Model training time: 40.149083614349365
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6556464985698534
Iteration 10, Training loss = 0.25992907859727776
Iteration 20, Training loss = 0.2519659112967533
Iteration 30, Training loss = 0.24707175664982553
Iteration 40, Training loss = 0.24119360380542193
Iteration 50, Training loss = 0.2360567832974487
Iteration 60, Training loss = 0.23210715426731918
Iteration 70, Training loss = 0.22803134734564198
Iteration 80, Training loss = 0.2248303967259698
Iteration 90, Training loss = 0.22183869169352996
Model training time: 53.14891028404236
Device: cuda
Iteration 0, Training loss = 0.6229845370565142
Iteration 10, Training loss = 0.25913548657160984
Iteration 20, Training loss = 0.25394469023328137
Iteration 30, Training loss = 0.2505128343136345
Iteration 40, Training loss = 0.24720328520379112
Iteration 50, Training loss = 0.243752172951909
Iteration 60, Training loss = 0.23978527218608534
Iteration 70, Training loss = 0.2368858293528707
Iteration 80, Training loss = 0.231738492485686
Iteration 90, Training loss = 0.2275804608031999
Model training time: 53.27655553817749
Device: cuda
Iteration 0, Training loss = 0.650372973726679
Iteration 10, Training loss = 0.25544526952952507
Iteration 20, Training loss = 0.24698962110656225
Iteration 30, Training loss = 0.24151194562851372
Iteration 40, Training loss = 0.23704866447979833
Iteration 50, Training loss = 0.23380731742549463
Iteration 60, Training loss = 0.2299518170822908
Iteration 70, Training loss = 0.22702879937631743
Iteration 80, Training loss = 0.22472899831091808
Iteration 90, Training loss = 0.2226976633035819
Model training time: 51.81507754325867
Device: cuda
Iteration 0, Training loss = 0.678611038455663
Iteration 10, Training loss = 0.25563858677666934
Iteration 20, Training loss = 0.24703338256976218
Iteration 30, Training loss = 0.23914201552224218
Iteration 40, Training loss = 0.23438519179893175
Iteration 50, Training loss = 0.22872301414789356
Iteration 60, Training loss = 0.22507352712079343
Iteration 70, Training loss = 0.2220021575220272
Iteration 80, Training loss = 0.21869344802767254
Iteration 90, Training loss = 0.21610816191999155
Model training time: 52.669878005981445
Device: cuda
Iteration 0, Training loss = 0.7168536876795079
Iteration 10, Training loss = 0.25500861355092275
Iteration 20, Training loss = 0.24406533031645467
Iteration 30, Training loss = 0.236809396570589
Iteration 40, Training loss = 0.23071517613118844
Iteration 50, Training loss = 0.22655758357293382
Iteration 60, Training loss = 0.22225826879462665
Iteration 70, Training loss = 0.21822566945854457
Iteration 80, Training loss = 0.21619471848985186
Iteration 90, Training loss = 0.21364307349444012
Model training time: 52.07980918884277
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0546871386919414
Iteration 10, Training loss = 0.8834504283802272
Iteration 20, Training loss = 0.4501809547080255
Iteration 30, Training loss = 0.35691575172016754
Iteration 40, Training loss = 0.33086633936617044
Iteration 50, Training loss = 0.31741283179123236
Iteration 60, Training loss = 0.3087434906323077
Iteration 70, Training loss = 0.30317728043721026
Iteration 80, Training loss = 0.29915712575358283
Iteration 90, Training loss = 0.2960593985804057
Model training time: 45.14654541015625
Device: cuda
Iteration 0, Training loss = 1.0315393140183233
Iteration 10, Training loss = 0.9401738695601862
Iteration 20, Training loss = 0.6659758867420815
Iteration 30, Training loss = 0.4128325274146498
Iteration 40, Training loss = 0.36438619791018184
Iteration 50, Training loss = 0.33883980450560913
Iteration 60, Training loss = 0.3236867431145315
Iteration 70, Training loss = 0.31345414119540344
Iteration 80, Training loss = 0.3060359282838519
Iteration 90, Training loss = 0.30017444691632045
Model training time: 48.22306418418884
Device: cuda
Iteration 0, Training loss = 0.9894949339925522
Iteration 10, Training loss = 0.7838804489475185
Iteration 20, Training loss = 0.3943250836673718
Iteration 30, Training loss = 0.34976427013519024
Iteration 40, Training loss = 0.331298428283188
Iteration 50, Training loss = 0.3207112082232863
Iteration 60, Training loss = 0.31351072496348953
Iteration 70, Training loss = 0.30803666512772765
Iteration 80, Training loss = 0.3036837057902796
Iteration 90, Training loss = 0.2998148965813923
Model training time: 47.8070113658905
Device: cuda
Iteration 0, Training loss = 1.1087102174181627
Iteration 10, Training loss = 0.9540856544099766
Iteration 20, Training loss = 0.666129681590683
Iteration 30, Training loss = 0.37998071547426265
Iteration 40, Training loss = 0.34763248419934845
Iteration 50, Training loss = 0.33006373195757877
Iteration 60, Training loss = 0.3196298332645876
Iteration 70, Training loss = 0.31250906133117745
Iteration 80, Training loss = 0.30679115535777074
Iteration 90, Training loss = 0.3022333983395059
Model training time: 43.89057421684265
Device: cuda
Iteration 0, Training loss = 0.9886424021917163
Iteration 10, Training loss = 0.8269026232545081
Iteration 20, Training loss = 0.42222166829911617
Iteration 30, Training loss = 0.35525714214575493
Iteration 40, Training loss = 0.33411844754839637
Iteration 50, Training loss = 0.3215082059580535
Iteration 60, Training loss = 0.312741582586315
Iteration 70, Training loss = 0.30543672529750526
Iteration 80, Training loss = 0.2990474264175782
Iteration 90, Training loss = 0.29423910271168907
Model training time: 42.79685854911804
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9651969406420036
Iteration 10, Training loss = 0.27205987144757704
Iteration 20, Training loss = 0.25977132947479553
Iteration 30, Training loss = 0.25572632523772215
Iteration 40, Training loss = 0.2522456022858908
Iteration 50, Training loss = 0.25001427881221217
Iteration 60, Training loss = 0.24850133196710097
Iteration 70, Training loss = 0.2457851188812071
Iteration 80, Training loss = 0.24398330774059018
Iteration 90, Training loss = 0.24221632526804113
Model training time: 51.60398292541504
Device: cuda
Iteration 0, Training loss = 0.6758119471497455
Iteration 10, Training loss = 0.26299167081677594
Iteration 20, Training loss = 0.25316246389693264
Iteration 30, Training loss = 0.2489449476629279
Iteration 40, Training loss = 0.24316397623201838
Iteration 50, Training loss = 0.2382903387702406
Iteration 60, Training loss = 0.23351919067432747
Iteration 70, Training loss = 0.23101944495308196
Iteration 80, Training loss = 0.22884701199595056
Iteration 90, Training loss = 0.2267712921346143
Model training time: 50.844117641448975
Device: cuda
Iteration 0, Training loss = 0.7477176487806634
Iteration 10, Training loss = 0.2576872028915559
Iteration 20, Training loss = 0.2500650599761246
Iteration 30, Training loss = 0.2458356264933547
Iteration 40, Training loss = 0.24229335776794042
Iteration 50, Training loss = 0.23900858332689393
Iteration 60, Training loss = 0.23672945537665277
Iteration 70, Training loss = 0.2340500196554759
Iteration 80, Training loss = 0.2324309662146805
Iteration 90, Training loss = 0.23032848983461862
Model training time: 51.69583249092102
Device: cuda
Iteration 0, Training loss = 0.7279816309923699
Iteration 10, Training loss = 0.2593746373371264
Iteration 20, Training loss = 0.2509720394383331
Iteration 30, Training loss = 0.24745951775127692
Iteration 40, Training loss = 0.24467559876799871
Iteration 50, Training loss = 0.24178781696836538
Iteration 60, Training loss = 0.2403124464048889
Iteration 70, Training loss = 0.23894274895041098
Iteration 80, Training loss = 0.2376502172223159
Iteration 90, Training loss = 0.23598655104889707
Model training time: 52.618067264556885
Device: cuda
Iteration 0, Training loss = 0.7480365289397736
Iteration 10, Training loss = 0.25941891899697717
Iteration 20, Training loss = 0.2518743990174073
Iteration 30, Training loss = 0.24646430136072145
Iteration 40, Training loss = 0.24361271741315182
Iteration 50, Training loss = 0.24059277484261093
Iteration 60, Training loss = 0.23833409464099506
Iteration 70, Training loss = 0.23647518326093153
Iteration 80, Training loss = 0.23458922298999443
Iteration 90, Training loss = 0.23270965814951258
Model training time: 54.128002643585205
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0056363329038782
Iteration 10, Training loss = 0.9113203230839376
Iteration 20, Training loss = 0.507955304394334
Iteration 30, Training loss = 0.3724864089171477
Iteration 40, Training loss = 0.34659596774321205
Iteration 50, Training loss = 0.330516966560418
Iteration 60, Training loss = 0.31940593193431743
Iteration 70, Training loss = 0.3111445102975963
Iteration 80, Training loss = 0.30452070017198674
Iteration 90, Training loss = 0.29894305935206195
Model training time: 43.58616399765015
Device: cuda
Iteration 0, Training loss = 1.015594633165341
Iteration 10, Training loss = 0.9216589665730409
Iteration 20, Training loss = 0.5544114454534382
Iteration 30, Training loss = 0.3832268396676597
Iteration 40, Training loss = 0.34714328315442755
Iteration 50, Training loss = 0.3269471869991132
Iteration 60, Training loss = 0.31485846833513087
Iteration 70, Training loss = 0.3071752822355843
Iteration 80, Training loss = 0.301467324086165
Iteration 90, Training loss = 0.2967066344315723
Model training time: 42.9688560962677
Device: cuda
Iteration 0, Training loss = 0.9952801592990792
Iteration 10, Training loss = 0.8462561750671765
Iteration 20, Training loss = 0.4469416849405367
Iteration 30, Training loss = 0.3569262721564522
Iteration 40, Training loss = 0.3273789298519961
Iteration 50, Training loss = 0.31036112325820736
Iteration 60, Training loss = 0.2993278446230704
Iteration 70, Training loss = 0.2917771378744024
Iteration 80, Training loss = 0.28626766903709266
Iteration 90, Training loss = 0.2821055541568172
Model training time: 43.108508825302124
Device: cuda
Iteration 0, Training loss = 1.0001983520626734
Iteration 10, Training loss = 0.8923905449254173
Iteration 20, Training loss = 0.5855236455762068
Iteration 30, Training loss = 0.41828364453145433
Iteration 40, Training loss = 0.3718041907499835
Iteration 50, Training loss = 0.3417498820413977
Iteration 60, Training loss = 0.32279658166028685
Iteration 70, Training loss = 0.31055312755249315
Iteration 80, Training loss = 0.3020357051210311
Iteration 90, Training loss = 0.2960749583512761
Model training time: 43.27034044265747
Device: cuda
Iteration 0, Training loss = 0.9928614451001978
Iteration 10, Training loss = 0.8596949736084835
Iteration 20, Training loss = 0.44218348316649836
Iteration 30, Training loss = 0.3524316189580622
Iteration 40, Training loss = 0.3294120124985462
Iteration 50, Training loss = 0.3160787696158626
Iteration 60, Training loss = 0.3079627244654348
Iteration 70, Training loss = 0.30255035572565786
Iteration 80, Training loss = 0.2982085409233703
Iteration 90, Training loss = 0.29468619976797056
Model training time: 43.64155316352844
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6906249203852245
Iteration 10, Training loss = 0.25984367329546093
Iteration 20, Training loss = 0.2523096988369997
Iteration 30, Training loss = 0.2471570274082281
Iteration 40, Training loss = 0.24214939491656445
Iteration 50, Training loss = 0.23893477728919602
Iteration 60, Training loss = 0.23431359616629147
Iteration 70, Training loss = 0.22997171119685322
Iteration 80, Training loss = 0.22584516897833665
Iteration 90, Training loss = 0.22217087076918265
Model training time: 52.080079555511475
Device: cuda
Iteration 0, Training loss = 0.6802347889824006
Iteration 10, Training loss = 0.26245309177957493
Iteration 20, Training loss = 0.2558585226211074
Iteration 30, Training loss = 0.2507110794343181
Iteration 40, Training loss = 0.24723732749284325
Iteration 50, Training loss = 0.2430250515335986
Iteration 60, Training loss = 0.24055618237135773
Iteration 70, Training loss = 0.23816264319722935
Iteration 80, Training loss = 0.23603899507235385
Iteration 90, Training loss = 0.23388526427219047
Model training time: 52.59451699256897
Device: cuda
Iteration 0, Training loss = 0.6972622372408468
Iteration 10, Training loss = 0.2596107874128778
Iteration 20, Training loss = 0.24982715188807494
Iteration 30, Training loss = 0.24402897485664912
Iteration 40, Training loss = 0.2398995067067354
Iteration 50, Training loss = 0.2372798571051899
Iteration 60, Training loss = 0.23483857778566225
Iteration 70, Training loss = 0.23228225483588388
Iteration 80, Training loss = 0.22961408533065716
Iteration 90, Training loss = 0.22710083066046094
Model training time: 53.18396353721619
Device: cuda
Iteration 0, Training loss = 0.6564931715443983
Iteration 10, Training loss = 0.2590894177195235
Iteration 20, Training loss = 0.24871200673083704
Iteration 30, Training loss = 0.2438859032155815
Iteration 40, Training loss = 0.2400865348430292
Iteration 50, Training loss = 0.23598808576927924
Iteration 60, Training loss = 0.23342320784818174
Iteration 70, Training loss = 0.23030778278594444
Iteration 80, Training loss = 0.22850162813724098
Iteration 90, Training loss = 0.2264666858853064
Model training time: 51.468408823013306
Device: cuda
Iteration 0, Training loss = 0.6178293067231305
Iteration 10, Training loss = 0.26126799959752517
Iteration 20, Training loss = 0.25450461720438905
Iteration 30, Training loss = 0.24997164873681404
Iteration 40, Training loss = 0.24650046346623442
Iteration 50, Training loss = 0.2416221946838693
Iteration 60, Training loss = 0.2374052758493233
Iteration 70, Training loss = 0.23434667978390655
Iteration 80, Training loss = 0.22867191106658294
Iteration 90, Training loss = 0.22538657535147147
Model training time: 51.64331078529358
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9887315804098189
Iteration 10, Training loss = 0.7717262495660897
Iteration 20, Training loss = 0.4161889424263421
Iteration 30, Training loss = 0.3623995236673598
Iteration 40, Training loss = 0.3355893595347104
Iteration 50, Training loss = 0.3193942183227285
Iteration 60, Training loss = 0.30919673559019123
Iteration 70, Training loss = 0.3016221229410922
Iteration 80, Training loss = 0.2955929006271732
Iteration 90, Training loss = 0.2907282346205619
Model training time: 45.05404496192932
Device: cuda
Iteration 0, Training loss = 1.010265152044504
Iteration 10, Training loss = 0.9464541293373986
Iteration 20, Training loss = 0.6060140504531075
Iteration 30, Training loss = 0.38004779328589866
Iteration 40, Training loss = 0.3489984872302189
Iteration 50, Training loss = 0.33019513357205194
Iteration 60, Training loss = 0.31867589040255717
Iteration 70, Training loss = 0.3113458615527026
Iteration 80, Training loss = 0.30653438592649834
Iteration 90, Training loss = 0.30264998829898765
Model training time: 44.05777430534363
Device: cuda
Iteration 0, Training loss = 0.9792301548018005
Iteration 10, Training loss = 0.789452051495813
Iteration 20, Training loss = 0.39381565920305023
Iteration 30, Training loss = 0.3411075612462462
Iteration 40, Training loss = 0.31939294784539546
Iteration 50, Training loss = 0.30784281561507437
Iteration 60, Training loss = 0.3009324966057161
Iteration 70, Training loss = 0.29628175953470764
Iteration 80, Training loss = 0.2924315984615691
Iteration 90, Training loss = 0.2894844009245568
Model training time: 44.68999719619751
Device: cuda
Iteration 0, Training loss = 1.0498796386811116
Iteration 10, Training loss = 0.992903515011
Iteration 20, Training loss = 0.960682611921509
Iteration 30, Training loss = 0.748765530439035
Iteration 40, Training loss = 0.39832750987487037
Iteration 50, Training loss = 0.3481366471646773
Iteration 60, Training loss = 0.3250628440994905
Iteration 70, Training loss = 0.31358433478753156
Iteration 80, Training loss = 0.30625215428937724
Iteration 90, Training loss = 0.30131481021426204
Model training time: 45.19799780845642
Device: cuda
Iteration 0, Training loss = 1.019216434069465
Iteration 10, Training loss = 0.9080695356208534
Iteration 20, Training loss = 0.4902946402749484
Iteration 30, Training loss = 0.36322964984048656
Iteration 40, Training loss = 0.33363339101070355
Iteration 50, Training loss = 0.3179263801609344
Iteration 60, Training loss = 0.3092101049084063
Iteration 70, Training loss = 0.30394048608172025
Iteration 80, Training loss = 0.29956797777379685
Iteration 90, Training loss = 0.2957322878388747
Model training time: 45.367833852767944
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6432971991147602
Iteration 10, Training loss = 0.2577367636936629
Iteration 20, Training loss = 0.24832205569989457
Iteration 30, Training loss = 0.24284286981056158
Iteration 40, Training loss = 0.23722879036575484
Iteration 50, Training loss = 0.23275172021285095
Iteration 60, Training loss = 0.22851560194469248
Iteration 70, Training loss = 0.22483319293067183
Iteration 80, Training loss = 0.22190407180107824
Iteration 90, Training loss = 0.21954030978333286
Iteration 100, Training loss = 0.21682885005961897
Iteration 110, Training loss = 0.21430149176507537
Iteration 120, Training loss = 0.21257522529031694
Iteration 130, Training loss = 0.21098408111107264
Iteration 140, Training loss = 0.20931042922935822
Iteration 150, Training loss = 0.20703402693782533
Iteration 160, Training loss = 0.20622686818963679
Iteration 170, Training loss = 0.20462137173870285
Iteration 180, Training loss = 0.20351843278828025
Iteration 190, Training loss = 0.20263421879156737
Model training time: 105.10667824745178
Device: cuda
Iteration 0, Training loss = 0.7132120886163619
Iteration 10, Training loss = 0.25784177724587715
Iteration 20, Training loss = 0.2510271825293219
Iteration 30, Training loss = 0.24553010446756862
Iteration 40, Training loss = 0.24073441323010164
Iteration 50, Training loss = 0.23697464997918496
Iteration 60, Training loss = 0.23083932981003283
Iteration 70, Training loss = 0.2260794200851034
Iteration 80, Training loss = 0.22130344956030856
Iteration 90, Training loss = 0.2179623158878622
Iteration 100, Training loss = 0.2145990042363183
Iteration 110, Training loss = 0.21232860129479922
Iteration 120, Training loss = 0.21103977507598176
Iteration 130, Training loss = 0.2095731671259132
Iteration 140, Training loss = 0.2077216078813659
Iteration 150, Training loss = 0.2060924838263244
Iteration 160, Training loss = 0.20538112320925941
Iteration 170, Training loss = 0.20453619359841357
Iteration 180, Training loss = 0.20350735589464986
Iteration 190, Training loss = 0.2031779769564512
Model training time: 102.82028269767761
Device: cuda
Iteration 0, Training loss = 0.6186185879294578
Iteration 10, Training loss = 0.2560383497622342
Iteration 20, Training loss = 0.24843207827924815
Iteration 30, Training loss = 0.24324382177709666
Iteration 40, Training loss = 0.23761410184980306
Iteration 50, Training loss = 0.23403710862914817
Iteration 60, Training loss = 0.23092220906736488
Iteration 70, Training loss = 0.228723188668273
Iteration 80, Training loss = 0.22587705144690254
Iteration 90, Training loss = 0.2235208731460369
Iteration 100, Training loss = 0.2218644305089484
Iteration 110, Training loss = 0.21943942688351392
Iteration 120, Training loss = 0.217659114095547
Iteration 130, Training loss = 0.21635922053535683
Iteration 140, Training loss = 0.21489541460793762
Iteration 150, Training loss = 0.2138144937558122
Iteration 160, Training loss = 0.21259145849986458
Iteration 170, Training loss = 0.2109841333372974
Iteration 180, Training loss = 0.20918763050985395
Iteration 190, Training loss = 0.20820303893587203
Model training time: 102.93361115455627
Device: cuda
Iteration 0, Training loss = 0.6613081438200814
Iteration 10, Training loss = 0.2539504559273148
Iteration 20, Training loss = 0.24431978119198983
Iteration 30, Training loss = 0.23799067219723802
Iteration 40, Training loss = 0.2323764940367484
Iteration 50, Training loss = 0.22693048719740663
Iteration 60, Training loss = 0.2234608965404963
Iteration 70, Training loss = 0.21940236114705158
Iteration 80, Training loss = 0.21668949384088956
Iteration 90, Training loss = 0.2145088280733792
Iteration 100, Training loss = 0.2117809048035243
Iteration 110, Training loss = 0.21056348157564028
Iteration 120, Training loss = 0.20865459794282337
Iteration 130, Training loss = 0.2068616189513599
Iteration 140, Training loss = 0.20641251724691426
Iteration 150, Training loss = 0.20484967376432464
Iteration 160, Training loss = 0.2032683799201149
Iteration 170, Training loss = 0.2024111596835872
Iteration 180, Training loss = 0.20143886636626923
Iteration 190, Training loss = 0.20016450968911514
Model training time: 105.01858329772949
Device: cuda
Iteration 0, Training loss = 0.6339698861788318
Iteration 10, Training loss = 0.25869335167416646
Iteration 20, Training loss = 0.24949755036152593
Iteration 30, Training loss = 0.24216899400256736
Iteration 40, Training loss = 0.23647168903785237
Iteration 50, Training loss = 0.23101656628686926
Iteration 60, Training loss = 0.22667960781089908
Iteration 70, Training loss = 0.2207028278607433
Iteration 80, Training loss = 0.21610938956099618
Iteration 90, Training loss = 0.21334004841438217
Iteration 100, Training loss = 0.21241549932855672
Iteration 110, Training loss = 0.21071892118951888
Iteration 120, Training loss = 0.20856586279072425
Iteration 130, Training loss = 0.2070473680592911
Iteration 140, Training loss = 0.20671937490202325
Iteration 150, Training loss = 0.2045620761667412
Iteration 160, Training loss = 0.20323662937012194
Iteration 170, Training loss = 0.20311912931231263
Iteration 180, Training loss = 0.20170985390466004
Iteration 190, Training loss = 0.20146848116133173
Model training time: 102.64197015762329
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.037273786186306
Iteration 10, Training loss = 0.92863131182823
Iteration 20, Training loss = 0.5732202328218386
Iteration 30, Training loss = 0.35664292960709576
Iteration 40, Training loss = 0.3285726308822632
Iteration 50, Training loss = 0.312903239619501
Iteration 60, Training loss = 0.3039582961083324
Iteration 70, Training loss = 0.2982376869632603
Iteration 80, Training loss = 0.29442367314859397
Iteration 90, Training loss = 0.29113186453863726
Iteration 100, Training loss = 0.2884826664485885
Iteration 110, Training loss = 0.2859004774372168
Iteration 120, Training loss = 0.28377700874866063
Iteration 130, Training loss = 0.2818865276349947
Iteration 140, Training loss = 0.27978370692265236
Iteration 150, Training loss = 0.27790990475447935
Iteration 160, Training loss = 0.2761295477501128
Iteration 170, Training loss = 0.2742257191287692
Iteration 180, Training loss = 0.27246345765437685
Iteration 190, Training loss = 0.27078101098465285
Model training time: 87.29494094848633
Device: cuda
Iteration 0, Training loss = 1.2225822878518924
Iteration 10, Training loss = 0.9389701186167414
Iteration 20, Training loss = 0.6097639129467796
Iteration 30, Training loss = 0.38769460911467923
Iteration 40, Training loss = 0.34982894700173894
Iteration 50, Training loss = 0.32913403288792753
Iteration 60, Training loss = 0.31619280600158123
Iteration 70, Training loss = 0.3070804916917556
Iteration 80, Training loss = 0.3003229166383316
Iteration 90, Training loss = 0.29445509756881444
Iteration 100, Training loss = 0.2897847313749588
Iteration 110, Training loss = 0.28597977454379453
Iteration 120, Training loss = 0.2825501651185188
Iteration 130, Training loss = 0.27960376811301735
Iteration 140, Training loss = 0.2767948351132956
Iteration 150, Training loss = 0.2744856768853728
Iteration 160, Training loss = 0.2724300663349992
Iteration 170, Training loss = 0.2707863199472716
Iteration 180, Training loss = 0.26919148718040736
Iteration 190, Training loss = 0.26790991149499976
Model training time: 89.3648533821106
Device: cuda
Iteration 0, Training loss = 1.014513989987154
Iteration 10, Training loss = 0.9755521451301206
Iteration 20, Training loss = 0.8562167171415924
Iteration 30, Training loss = 0.42541736461781415
Iteration 40, Training loss = 0.3492087252239627
Iteration 50, Training loss = 0.32684466901255577
Iteration 60, Training loss = 0.31314539074104
Iteration 70, Training loss = 0.30396033991263505
Iteration 80, Training loss = 0.2983202085837036
Iteration 90, Training loss = 0.294647430861256
Iteration 100, Training loss = 0.2917283631085484
Iteration 110, Training loss = 0.2894863881008388
Iteration 120, Training loss = 0.287207926228895
Iteration 130, Training loss = 0.2852362570404718
Iteration 140, Training loss = 0.2834629781598329
Iteration 150, Training loss = 0.2818115985530629
Iteration 160, Training loss = 0.28021210899292415
Iteration 170, Training loss = 0.2787063556743592
Iteration 180, Training loss = 0.27730943500706995
Iteration 190, Training loss = 0.27594820785248253
Model training time: 94.93855381011963
Device: cuda
Iteration 0, Training loss = 1.131286434702954
Iteration 10, Training loss = 0.8836891918147736
Iteration 20, Training loss = 0.486922260527172
Iteration 30, Training loss = 0.37200611855013893
Iteration 40, Training loss = 0.3449544197277642
Iteration 50, Training loss = 0.32807716819261523
Iteration 60, Training loss = 0.31595877419083807
Iteration 70, Training loss = 0.30665855728468655
Iteration 80, Training loss = 0.2994011079680544
Iteration 90, Training loss = 0.293461926750496
Iteration 100, Training loss = 0.28882111894016405
Iteration 110, Training loss = 0.28486326737351914
Iteration 120, Training loss = 0.28153850058810764
Iteration 130, Training loss = 0.27878786722698745
Iteration 140, Training loss = 0.27656445439372745
Iteration 150, Training loss = 0.2747882264903036
Iteration 160, Training loss = 0.27323925662921067
Iteration 170, Training loss = 0.27194824250139854
Iteration 180, Training loss = 0.2708982561351889
Iteration 190, Training loss = 0.2699919592489919
Model training time: 87.94713521003723
Device: cuda
Iteration 0, Training loss = 1.0019854361295122
Iteration 10, Training loss = 0.9497217040950969
Iteration 20, Training loss = 0.6802716399509162
Iteration 30, Training loss = 0.39131079682715003
Iteration 40, Training loss = 0.34868027363649
Iteration 50, Training loss = 0.3280640879677514
Iteration 60, Training loss = 0.31631879205420865
Iteration 70, Training loss = 0.308455539384708
Iteration 80, Training loss = 0.3027083175955904
Iteration 90, Training loss = 0.29812647580883983
Iteration 100, Training loss = 0.2941940785103791
Iteration 110, Training loss = 0.2909137191697414
Iteration 120, Training loss = 0.28811560111169954
Iteration 130, Training loss = 0.28543288946223894
Iteration 140, Training loss = 0.28348000051755884
Iteration 150, Training loss = 0.28112241430952245
Iteration 160, Training loss = 0.2792387301278172
Iteration 170, Training loss = 0.2774818747441936
Iteration 180, Training loss = 0.27598317353546475
Iteration 190, Training loss = 0.2745592415188473
Model training time: 88.359299659729
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7712894608300477
Iteration 10, Training loss = 0.26007399193888425
Iteration 20, Training loss = 0.2530384965367236
Iteration 30, Training loss = 0.24927990387484755
Iteration 40, Training loss = 0.2461437310202647
Iteration 50, Training loss = 0.24360626058305723
Iteration 60, Training loss = 0.24175177098727688
Iteration 70, Training loss = 0.23890928736466183
Iteration 80, Training loss = 0.23697499937172664
Iteration 90, Training loss = 0.2348231261384112
Iteration 100, Training loss = 0.2332420106841923
Iteration 110, Training loss = 0.2309777494023845
Iteration 120, Training loss = 0.22914746704964603
Iteration 130, Training loss = 0.22754504049048008
Iteration 140, Training loss = 0.2267366457396789
Iteration 150, Training loss = 0.22548880152963552
Iteration 160, Training loss = 0.2238351104839662
Iteration 170, Training loss = 0.22395485053267375
Iteration 180, Training loss = 0.22345571026891542
Iteration 190, Training loss = 0.22225136646310872
Model training time: 105.32042169570923
Device: cuda
Iteration 0, Training loss = 0.7799621899826474
Iteration 10, Training loss = 0.2666379646837856
Iteration 20, Training loss = 0.25526028793963623
Iteration 30, Training loss = 0.24780805336794035
Iteration 40, Training loss = 0.242311425795711
Iteration 50, Training loss = 0.23737792023085508
Iteration 60, Training loss = 0.23344226850796554
Iteration 70, Training loss = 0.2306462805041678
Iteration 80, Training loss = 0.2283380005968685
Iteration 90, Training loss = 0.22518304550510632
Iteration 100, Training loss = 0.2235733015557467
Iteration 110, Training loss = 0.22173763993793769
Iteration 120, Training loss = 0.21996297650309798
Iteration 130, Training loss = 0.21796622691951134
Iteration 140, Training loss = 0.21703047516415253
Iteration 150, Training loss = 0.21672336795067382
Iteration 160, Training loss = 0.2157300602256819
Iteration 170, Training loss = 0.21399221367250054
Iteration 180, Training loss = 0.2133883469474229
Iteration 190, Training loss = 0.2120455693155888
Model training time: 104.78088974952698
Device: cuda
Iteration 0, Training loss = 0.7323498504358111
Iteration 10, Training loss = 0.26551377517114827
Iteration 20, Training loss = 0.25472974677098864
Iteration 30, Training loss = 0.2494885854188524
Iteration 40, Training loss = 0.2459097648756556
Iteration 50, Training loss = 0.24384758890252425
Iteration 60, Training loss = 0.24236047257991739
Iteration 70, Training loss = 0.24063572486001122
Iteration 80, Training loss = 0.23952296767699516
Iteration 90, Training loss = 0.23893066374787983
Iteration 100, Training loss = 0.2376364004363015
Iteration 110, Training loss = 0.23654192879110503
Iteration 120, Training loss = 0.2352252216698471
Iteration 130, Training loss = 0.23384518302598242
Iteration 140, Training loss = 0.23330513797429803
Iteration 150, Training loss = 0.23216128981611342
Iteration 160, Training loss = 0.23205843710365365
Iteration 170, Training loss = 0.23070079913547772
Iteration 180, Training loss = 0.23001604806857306
Iteration 190, Training loss = 0.228935049553616
Model training time: 106.07688546180725
Device: cuda
Iteration 0, Training loss = 0.7696238021945838
Iteration 10, Training loss = 0.2591031319833841
Iteration 20, Training loss = 0.25158886483376597
Iteration 30, Training loss = 0.24730166757691283
Iteration 40, Training loss = 0.2436301370149086
Iteration 50, Training loss = 0.24069686632684587
Iteration 60, Training loss = 0.2383875785526294
Iteration 70, Training loss = 0.23686159862155776
Iteration 80, Training loss = 0.2351106759386259
Iteration 90, Training loss = 0.23419859314441105
Iteration 100, Training loss = 0.23340114532602324
Iteration 110, Training loss = 0.23153714379228058
Iteration 120, Training loss = 0.23028825093556837
Iteration 130, Training loss = 0.22914281117641896
Iteration 140, Training loss = 0.22826796849472472
Iteration 150, Training loss = 0.226443039862209
Iteration 160, Training loss = 0.22531279345148988
Iteration 170, Training loss = 0.22442175653598498
Iteration 180, Training loss = 0.22346512221034445
Iteration 190, Training loss = 0.2224147886344509
Model training time: 105.83982753753662
Device: cuda
Iteration 0, Training loss = 1.0459259451157243
Iteration 10, Training loss = 0.2647147120019426
Iteration 20, Training loss = 0.2476535618629929
Iteration 30, Training loss = 0.24172588868955144
Iteration 40, Training loss = 0.23757893826641124
Iteration 50, Training loss = 0.23389423544773466
Iteration 60, Training loss = 0.23149316026389455
Iteration 70, Training loss = 0.22926939949934477
Iteration 80, Training loss = 0.22723652275400935
Iteration 90, Training loss = 0.2252529767989102
Iteration 100, Training loss = 0.22400554972215947
Iteration 110, Training loss = 0.2227597441388966
Iteration 120, Training loss = 0.22145728721817814
Iteration 130, Training loss = 0.21988988732671044
Iteration 140, Training loss = 0.21927612353359816
Iteration 150, Training loss = 0.2189748954145152
Iteration 160, Training loss = 0.21838617093915225
Iteration 170, Training loss = 0.21766477536419113
Iteration 180, Training loss = 0.21731115130766251
Iteration 190, Training loss = 0.21626975810376264
Model training time: 105.84290361404419
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1157512852412448
Iteration 10, Training loss = 0.9270874718632595
Iteration 20, Training loss = 0.6395913862215116
Iteration 30, Training loss = 0.43078598491937714
Iteration 40, Training loss = 0.38327569880727996
Iteration 50, Training loss = 0.3550701764283688
Iteration 60, Training loss = 0.33600928704761707
Iteration 70, Training loss = 0.32349852997511985
Iteration 80, Training loss = 0.31446327256449197
Iteration 90, Training loss = 0.30700556625393344
Iteration 100, Training loss = 0.30047696011623515
Iteration 110, Training loss = 0.29507339918945374
Iteration 120, Training loss = 0.29034160090199973
Iteration 130, Training loss = 0.2863487547881378
Iteration 140, Training loss = 0.2829432473957683
Iteration 150, Training loss = 0.2798170370318122
Iteration 160, Training loss = 0.27721490770506224
Iteration 170, Training loss = 0.2746047813616711
Iteration 180, Training loss = 0.2725374972668745
Iteration 190, Training loss = 0.2706717910835876
Model training time: 86.88422799110413
Device: cuda
Iteration 0, Training loss = 0.9922272156716547
Iteration 10, Training loss = 0.7968849005046826
Iteration 20, Training loss = 0.4207419714468732
Iteration 30, Training loss = 0.36430010968057064
Iteration 40, Training loss = 0.3387770681454829
Iteration 50, Training loss = 0.32275306792438174
Iteration 60, Training loss = 0.31270058469319173
Iteration 70, Training loss = 0.3055542358727201
Iteration 80, Training loss = 0.2995131227749312
Iteration 90, Training loss = 0.29497981309602106
Iteration 100, Training loss = 0.2909499479335965
Iteration 110, Training loss = 0.2871327760649651
Iteration 120, Training loss = 0.2838428360714462
Iteration 130, Training loss = 0.28127933241264585
Iteration 140, Training loss = 0.27858068714635426
Iteration 150, Training loss = 0.27664254069977756
Iteration 160, Training loss = 0.2746325948480832
Iteration 170, Training loss = 0.2729766408734691
Iteration 180, Training loss = 0.2713326653758781
Iteration 190, Training loss = 0.26995397470981675
Model training time: 87.29564118385315
Device: cuda
Iteration 0, Training loss = 1.027203750307277
Iteration 10, Training loss = 0.9817858321759083
Iteration 20, Training loss = 0.8558444361276835
Iteration 30, Training loss = 0.5075885457218993
Iteration 40, Training loss = 0.39143043543611256
Iteration 50, Training loss = 0.3533082633633302
Iteration 60, Training loss = 0.32863313378202424
Iteration 70, Training loss = 0.31267301478484066
Iteration 80, Training loss = 0.30264029505755075
Iteration 90, Training loss = 0.29636627078489297
Iteration 100, Training loss = 0.2920530684129955
Iteration 110, Training loss = 0.28888305310908585
Iteration 120, Training loss = 0.28614162139540433
Iteration 130, Training loss = 0.28393100891217193
Iteration 140, Training loss = 0.2816997448266563
Iteration 150, Training loss = 0.27960764198196425
Iteration 160, Training loss = 0.27768289346527536
Iteration 170, Training loss = 0.27572766000508686
Iteration 180, Training loss = 0.2741817528377434
Iteration 190, Training loss = 0.2724552152737001
Model training time: 88.22392416000366
Device: cuda
Iteration 0, Training loss = 1.003360453370697
Iteration 10, Training loss = 0.901411152708617
Iteration 20, Training loss = 0.5136292088623485
Iteration 30, Training loss = 0.3787580413549922
Iteration 40, Training loss = 0.35113559542426764
Iteration 50, Training loss = 0.3339699922404624
Iteration 60, Training loss = 0.32213496972443695
Iteration 70, Training loss = 0.31351984101043195
Iteration 80, Training loss = 0.3072153549037026
Iteration 90, Training loss = 0.30237697496036064
Iteration 100, Training loss = 0.29832724180550607
Iteration 110, Training loss = 0.2947893548423095
Iteration 120, Training loss = 0.2914268385447544
Iteration 130, Training loss = 0.28805288432513254
Iteration 140, Training loss = 0.284541082981135
Iteration 150, Training loss = 0.28163232412595146
Iteration 160, Training loss = 0.2789385610375075
Iteration 170, Training loss = 0.2765759543648788
Iteration 180, Training loss = 0.2746315967228453
Iteration 190, Training loss = 0.27273802522380475
Model training time: 86.98007893562317
Device: cuda
Iteration 0, Training loss = 0.9976108550448106
Iteration 10, Training loss = 0.8383323164165164
Iteration 20, Training loss = 0.5076400252289114
Iteration 30, Training loss = 0.3945538395470049
Iteration 40, Training loss = 0.3564032831614878
Iteration 50, Training loss = 0.3324412595092817
Iteration 60, Training loss = 0.31615098823790977
Iteration 70, Training loss = 0.3042418353494085
Iteration 80, Training loss = 0.29531569787939294
Iteration 90, Training loss = 0.288258278792043
Iteration 100, Training loss = 0.2823046131767603
Iteration 110, Training loss = 0.2777160735164947
Iteration 120, Training loss = 0.27410236405113997
Iteration 130, Training loss = 0.2709458429213009
Iteration 140, Training loss = 0.2683979210243098
Iteration 150, Training loss = 0.26636151379520034
Iteration 160, Training loss = 0.2646085317934396
Iteration 170, Training loss = 0.2633279679798618
Iteration 180, Training loss = 0.26213690855023936
Iteration 190, Training loss = 0.26114323737041134
Model training time: 87.39713859558105
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6729584532196816
Iteration 10, Training loss = 0.26163703013779754
Iteration 20, Training loss = 0.25287295986174385
Iteration 30, Training loss = 0.24852156583174667
Iteration 40, Training loss = 0.24547272587767813
Iteration 50, Training loss = 0.2425986126642539
Iteration 60, Training loss = 0.2406645482879574
Iteration 70, Training loss = 0.23852895748672992
Iteration 80, Training loss = 0.23540003033079765
Iteration 90, Training loss = 0.23395781826958528
Iteration 100, Training loss = 0.23200211046177885
Iteration 110, Training loss = 0.23020540011852764
Iteration 120, Training loss = 0.22809602018021788
Iteration 130, Training loss = 0.22706039798317462
Iteration 140, Training loss = 0.22602139526107698
Iteration 150, Training loss = 0.22499870970736982
Iteration 160, Training loss = 0.22356497938350095
Iteration 170, Training loss = 0.22264817682687943
Iteration 180, Training loss = 0.22097639223565202
Iteration 190, Training loss = 0.22073576624615718
Model training time: 103.3882999420166
Device: cuda
Iteration 0, Training loss = 0.6872002641889142
Iteration 10, Training loss = 0.2611538598958863
Iteration 20, Training loss = 0.2528077011463429
Iteration 30, Training loss = 0.24798139007919925
Iteration 40, Training loss = 0.24410450561355448
Iteration 50, Training loss = 0.24104338610143408
Iteration 60, Training loss = 0.2377574726145435
Iteration 70, Training loss = 0.23525628223834835
Iteration 80, Training loss = 0.23307958773853704
Iteration 90, Training loss = 0.23111430453655218
Iteration 100, Training loss = 0.22919982794410668
Iteration 110, Training loss = 0.22702791103439238
Iteration 120, Training loss = 0.22601300815332312
Iteration 130, Training loss = 0.22503481107170875
Iteration 140, Training loss = 0.2230023990856533
Iteration 150, Training loss = 0.22111669416397306
Iteration 160, Training loss = 0.22022637652715818
Iteration 170, Training loss = 0.2194112652312901
Iteration 180, Training loss = 0.2176192730538926
Iteration 190, Training loss = 0.2166312673460629
Model training time: 104.50520348548889
Device: cuda
Iteration 0, Training loss = 0.6933928297485336
Iteration 10, Training loss = 0.25800794079791545
Iteration 20, Training loss = 0.24952366244562024
Iteration 30, Training loss = 0.24314158323241203
Iteration 40, Training loss = 0.23779685820563365
Iteration 50, Training loss = 0.23081089691518294
Iteration 60, Training loss = 0.2256469477608475
Iteration 70, Training loss = 0.22163043181991463
Iteration 80, Training loss = 0.21817361923345066
Iteration 90, Training loss = 0.2160558653296339
Iteration 100, Training loss = 0.21345417573146036
Iteration 110, Training loss = 0.21209275758583956
Iteration 120, Training loss = 0.2113284125978231
Iteration 130, Training loss = 0.20962730288216913
Iteration 140, Training loss = 0.20950957908144008
Iteration 150, Training loss = 0.20826361753208875
Iteration 160, Training loss = 0.2077507774997277
Iteration 170, Training loss = 0.20721621235007234
Iteration 180, Training loss = 0.20651459120232027
Iteration 190, Training loss = 0.2061749492697508
Model training time: 102.55532336235046
Device: cuda
Iteration 0, Training loss = 0.6377738579638645
Iteration 10, Training loss = 0.25786405181192024
Iteration 20, Training loss = 0.2510467644681653
Iteration 30, Training loss = 0.24518860357003985
Iteration 40, Training loss = 0.2404336530092842
Iteration 50, Training loss = 0.23568194121411
Iteration 60, Training loss = 0.23147229583298035
Iteration 70, Training loss = 0.22764217493645214
Iteration 80, Training loss = 0.2253474691300935
Iteration 90, Training loss = 0.22357385939316368
Iteration 100, Training loss = 0.22116237563096872
Iteration 110, Training loss = 0.21896791034186436
Iteration 120, Training loss = 0.2174374327135721
Iteration 130, Training loss = 0.2155453930484902
Iteration 140, Training loss = 0.2132262596386974
Iteration 150, Training loss = 0.21143307809246653
Iteration 160, Training loss = 0.21098291203740435
Iteration 170, Training loss = 0.2094658285776293
Iteration 180, Training loss = 0.20786503675341894
Iteration 190, Training loss = 0.20698285460941152
Model training time: 103.598219871521
Device: cuda
Iteration 0, Training loss = 0.6171147742513883
Iteration 10, Training loss = 0.26533509610495903
Iteration 20, Training loss = 0.25454729890166705
Iteration 30, Training loss = 0.24709780250854121
Iteration 40, Training loss = 0.24130987769249854
Iteration 50, Training loss = 0.2365329049293123
Iteration 60, Training loss = 0.2322814575515994
Iteration 70, Training loss = 0.22984646482632176
Iteration 80, Training loss = 0.22590497872154014
Iteration 90, Training loss = 0.22439569269124301
Iteration 100, Training loss = 0.221203514426658
Iteration 110, Training loss = 0.21871974724220883
Iteration 120, Training loss = 0.21599481553380484
Iteration 130, Training loss = 0.21390335350218465
Iteration 140, Training loss = 0.2124912176308274
Iteration 150, Training loss = 0.21078442567485875
Iteration 160, Training loss = 0.2095913499160771
Iteration 170, Training loss = 0.20882824461988328
Iteration 180, Training loss = 0.20802101523550023
Iteration 190, Training loss = 0.20721506386923155
Model training time: 103.64031457901001
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.008004707949502
Iteration 10, Training loss = 0.9199774557539684
Iteration 20, Training loss = 0.584679793647646
Iteration 30, Training loss = 0.40168545187674193
Iteration 40, Training loss = 0.358868700955018
Iteration 50, Training loss = 0.3346154535365162
Iteration 60, Training loss = 0.31958730531446006
Iteration 70, Training loss = 0.3094010062281214
Iteration 80, Training loss = 0.3023432303247094
Iteration 90, Training loss = 0.2971433054736971
Iteration 100, Training loss = 0.2929429539973164
Iteration 110, Training loss = 0.2892007274467489
Iteration 120, Training loss = 0.2862614142295812
Iteration 130, Training loss = 0.2838454831202151
Iteration 140, Training loss = 0.2816996340060349
Iteration 150, Training loss = 0.27956473686648914
Iteration 160, Training loss = 0.2775458131734165
Iteration 170, Training loss = 0.27594274062365653
Iteration 180, Training loss = 0.2743565308845649
Iteration 190, Training loss = 0.27303770669796856
Model training time: 90.77583312988281
Device: cuda
Iteration 0, Training loss = 1.1633117210489785
Iteration 10, Training loss = 0.9601508376817727
Iteration 20, Training loss = 0.7067050999504024
Iteration 30, Training loss = 0.39247824617866744
Iteration 40, Training loss = 0.34908663999226136
Iteration 50, Training loss = 0.32806783156879876
Iteration 60, Training loss = 0.31581137841680146
Iteration 70, Training loss = 0.30764453508158285
Iteration 80, Training loss = 0.30204810079448735
Iteration 90, Training loss = 0.2971747415117721
Iteration 100, Training loss = 0.29332925352216055
Iteration 110, Training loss = 0.29023758345452694
Iteration 120, Training loss = 0.28726417504557683
Iteration 130, Training loss = 0.28474071760803965
Iteration 140, Training loss = 0.28250109476486074
Iteration 150, Training loss = 0.2806343787843609
Iteration 160, Training loss = 0.2786096607585219
Iteration 170, Training loss = 0.2770700445150636
Iteration 180, Training loss = 0.275684592131408
Iteration 190, Training loss = 0.27444129195470207
Model training time: 92.0755124092102
Device: cuda
Iteration 0, Training loss = 0.9835537717504016
Iteration 10, Training loss = 0.8509638963975283
Iteration 20, Training loss = 0.45226834003630906
Iteration 30, Training loss = 0.3715911823309074
Iteration 40, Training loss = 0.34433321900142594
Iteration 50, Training loss = 0.3279059507115124
Iteration 60, Training loss = 0.3172369812757282
Iteration 70, Training loss = 0.3097276443538885
Iteration 80, Training loss = 0.30442461205452465
Iteration 90, Training loss = 0.30017928447186515
Iteration 100, Training loss = 0.2966459108286562
Iteration 110, Training loss = 0.29315797169185437
Iteration 120, Training loss = 0.2901067757036438
Iteration 130, Training loss = 0.2872050882612533
Iteration 140, Training loss = 0.2844009143604782
Iteration 150, Training loss = 0.2821215420203694
Iteration 160, Training loss = 0.27977752106458165
Iteration 170, Training loss = 0.2775883770414761
Iteration 180, Training loss = 0.2757066390849199
Iteration 190, Training loss = 0.27410527161766773
Model training time: 87.93014240264893
Device: cuda
Iteration 0, Training loss = 1.02216608241453
Iteration 10, Training loss = 0.95281108225229
Iteration 20, Training loss = 0.6662907641534367
Iteration 30, Training loss = 0.3887871860303255
Iteration 40, Training loss = 0.3442703576772034
Iteration 50, Training loss = 0.3222428872712299
Iteration 60, Training loss = 0.3098802843985777
Iteration 70, Training loss = 0.30161869677446657
Iteration 80, Training loss = 0.29570749764653154
Iteration 90, Training loss = 0.2910530582503314
Iteration 100, Training loss = 0.2872129502834766
Iteration 110, Training loss = 0.2841699838313583
Iteration 120, Training loss = 0.281537742378781
Iteration 130, Training loss = 0.2793466469349642
Iteration 140, Training loss = 0.2771905688952303
Iteration 150, Training loss = 0.2753419005661553
Iteration 160, Training loss = 0.27388214359632707
Iteration 170, Training loss = 0.2722087208064359
Iteration 180, Training loss = 0.27070367453534144
Iteration 190, Training loss = 0.26941905928365256
Model training time: 88.37598180770874
Device: cuda
Iteration 0, Training loss = 1.1585596467911765
Iteration 10, Training loss = 0.928266871062087
Iteration 20, Training loss = 0.514587604970678
Iteration 30, Training loss = 0.3457331657409668
Iteration 40, Training loss = 0.32595567957252336
Iteration 50, Training loss = 0.31513647657546234
Iteration 60, Training loss = 0.30836359009182773
Iteration 70, Training loss = 0.30353502555779627
Iteration 80, Training loss = 0.29977796477497926
Iteration 90, Training loss = 0.29667627667110713
Iteration 100, Training loss = 0.2941470359233332
Iteration 110, Training loss = 0.29191311934092434
Iteration 120, Training loss = 0.28977839827321056
Iteration 130, Training loss = 0.2879152731478359
Iteration 140, Training loss = 0.28641158475858536
Iteration 150, Training loss = 0.2847615372362495
Iteration 160, Training loss = 0.28327303318437597
Iteration 170, Training loss = 0.28194607904329716
Iteration 180, Training loss = 0.2805525536579024
Iteration 190, Training loss = 0.2792606643123719
Model training time: 88.14439249038696
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6576380713799964
Iteration 10, Training loss = 0.26076043393580156
Iteration 20, Training loss = 0.2516335000534323
Iteration 30, Training loss = 0.24677751992646488
Iteration 40, Training loss = 0.24224912500590737
Iteration 50, Training loss = 0.2388228395768863
Iteration 60, Training loss = 0.23482592357886328
Iteration 70, Training loss = 0.23195719834390044
Iteration 80, Training loss = 0.22973536400543865
Iteration 90, Training loss = 0.22638030302401893
Iteration 100, Training loss = 0.2235213506777408
Iteration 110, Training loss = 0.2209158153622693
Iteration 120, Training loss = 0.21919179052006246
Iteration 130, Training loss = 0.21704029815032466
Iteration 140, Training loss = 0.21441320268568057
Iteration 150, Training loss = 0.212386393906057
Iteration 160, Training loss = 0.21053843874908243
Iteration 170, Training loss = 0.20896529096018604
Iteration 180, Training loss = 0.20795735909849333
Iteration 190, Training loss = 0.2059224721666687
Iteration 200, Training loss = 0.20517974166474678
Iteration 210, Training loss = 0.20434635120825098
Iteration 220, Training loss = 0.20312303692535397
Iteration 230, Training loss = 0.20235256724438425
Iteration 240, Training loss = 0.2012498147478981
Iteration 250, Training loss = 0.201104897275098
Iteration 260, Training loss = 0.20064440376822076
Iteration 270, Training loss = 0.1990382194284521
Iteration 280, Training loss = 0.19926080191575585
Iteration 290, Training loss = 0.19966001006758818
Model training time: 157.77526378631592
Device: cuda
Iteration 0, Training loss = 0.9784599799509487
Iteration 10, Training loss = 0.26390036843157855
Iteration 20, Training loss = 0.2496518003269489
Iteration 30, Training loss = 0.2428701652561204
Iteration 40, Training loss = 0.23826374048615195
Iteration 50, Training loss = 0.23228116519298161
Iteration 60, Training loss = 0.22740723429017726
Iteration 70, Training loss = 0.2223458296053346
Iteration 80, Training loss = 0.21810906180169334
Iteration 90, Training loss = 0.21483691658184256
Iteration 100, Training loss = 0.2126025351770132
Iteration 110, Training loss = 0.2102055089146837
Iteration 120, Training loss = 0.2083964177991374
Iteration 130, Training loss = 0.20690513430185814
Iteration 140, Training loss = 0.20494185197764678
Iteration 150, Training loss = 0.2035818554767685
Iteration 160, Training loss = 0.2025183358164301
Iteration 170, Training loss = 0.20148459952730532
Iteration 180, Training loss = 0.20020135032964675
Iteration 190, Training loss = 0.1993079313119878
Iteration 200, Training loss = 0.1995870001866656
Iteration 210, Training loss = 0.19844548588226263
Iteration 220, Training loss = 0.1978358135312868
Iteration 230, Training loss = 0.19704396009769914
Iteration 240, Training loss = 0.19665257218711024
Iteration 250, Training loss = 0.19540464749997233
Iteration 260, Training loss = 0.19540590376166975
Iteration 270, Training loss = 0.19467684925143713
Iteration 280, Training loss = 0.19367723985965257
Iteration 290, Training loss = 0.19340390501855073
Model training time: 155.4630732536316
Device: cuda
Iteration 0, Training loss = 0.6476623983309575
Iteration 10, Training loss = 0.25816432826816316
Iteration 20, Training loss = 0.24979108609890532
Iteration 30, Training loss = 0.24484672329724558
Iteration 40, Training loss = 0.23969650564413095
Iteration 50, Training loss = 0.23503647268395736
Iteration 60, Training loss = 0.23049754708933196
Iteration 70, Training loss = 0.2264991627676868
Iteration 80, Training loss = 0.22369493540493685
Iteration 90, Training loss = 0.2204328024214174
Iteration 100, Training loss = 0.21838599353234936
Iteration 110, Training loss = 0.21661534441801883
Iteration 120, Training loss = 0.213612560696954
Iteration 130, Training loss = 0.21119857311573503
Iteration 140, Training loss = 0.20925739963175888
Iteration 150, Training loss = 0.2079046839987972
Iteration 160, Training loss = 0.20665049791949425
Iteration 170, Training loss = 0.20548592865034107
Iteration 180, Training loss = 0.20427737082862105
Iteration 190, Training loss = 0.2025917660327714
Iteration 200, Training loss = 0.20222131337871274
Iteration 210, Training loss = 0.20184300479098036
Iteration 220, Training loss = 0.20062595035781583
Iteration 230, Training loss = 0.20087873260997977
Iteration 240, Training loss = 0.1991796825468973
Iteration 250, Training loss = 0.19938810323940062
Iteration 260, Training loss = 0.1989098691875363
Iteration 270, Training loss = 0.19766681733345004
Iteration 280, Training loss = 0.19741081137922717
Iteration 290, Training loss = 0.19718292393872727
Model training time: 154.467383146286
Device: cuda
Iteration 0, Training loss = 0.826750058094468
Iteration 10, Training loss = 0.26023722961364587
Iteration 20, Training loss = 0.2522940460421127
Iteration 30, Training loss = 0.24585473182054177
Iteration 40, Training loss = 0.23943621831208683
Iteration 50, Training loss = 0.2329447438836675
Iteration 60, Training loss = 0.22666573380153923
Iteration 70, Training loss = 0.22006912760382413
Iteration 80, Training loss = 0.21581396268405578
Iteration 90, Training loss = 0.21085092099721725
Iteration 100, Training loss = 0.2066602922488933
Iteration 110, Training loss = 0.2043168133093139
Iteration 120, Training loss = 0.20294508314522358
Iteration 130, Training loss = 0.200572759508365
Iteration 140, Training loss = 0.19976736160477773
Iteration 150, Training loss = 0.19822991065699022
Iteration 160, Training loss = 0.19771195249068246
Iteration 170, Training loss = 0.19711711114276986
Iteration 180, Training loss = 0.19591320446775842
Iteration 190, Training loss = 0.19568756940722754
Iteration 200, Training loss = 0.1952301903783409
Iteration 210, Training loss = 0.19442848726818407
Iteration 220, Training loss = 0.19366071567155835
Iteration 230, Training loss = 0.1939256224808335
Iteration 240, Training loss = 0.19340813806681886
Iteration 250, Training loss = 0.19248174668512968
Iteration 260, Training loss = 0.19254661770875747
Iteration 270, Training loss = 0.19122756704615912
Iteration 280, Training loss = 0.19110421893202653
Iteration 290, Training loss = 0.19057025248974058
Model training time: 156.21468710899353
Device: cuda
Iteration 0, Training loss = 0.6286429765005088
Iteration 10, Training loss = 0.253406282793956
Iteration 20, Training loss = 0.2445110103913716
Iteration 30, Training loss = 0.2378786402432018
Iteration 40, Training loss = 0.23220544220027278
Iteration 50, Training loss = 0.22789044440983283
Iteration 60, Training loss = 0.22510265061952012
Iteration 70, Training loss = 0.22192439050853396
Iteration 80, Training loss = 0.21968167946283812
Iteration 90, Training loss = 0.21679391473820364
Iteration 100, Training loss = 0.2149309281611558
Iteration 110, Training loss = 0.2117439007102433
Iteration 120, Training loss = 0.2097777856410127
Iteration 130, Training loss = 0.20753276691778808
Iteration 140, Training loss = 0.20572267844371012
Iteration 150, Training loss = 0.20450942647659173
Iteration 160, Training loss = 0.20316487855613954
Iteration 170, Training loss = 0.20254961099953686
Iteration 180, Training loss = 0.2018817148507363
Iteration 190, Training loss = 0.2001850012211765
Iteration 200, Training loss = 0.19958873701983157
Iteration 210, Training loss = 0.19867942368580124
Iteration 220, Training loss = 0.1987338491126931
Iteration 230, Training loss = 0.19801632627014965
Iteration 240, Training loss = 0.19695107470160536
Iteration 250, Training loss = 0.19699965724067595
Iteration 260, Training loss = 0.1961306012156656
Iteration 270, Training loss = 0.1955345595756108
Iteration 280, Training loss = 0.19466409869221452
Iteration 290, Training loss = 0.19445505483892292
Model training time: 155.73816418647766
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.033525950827841
Iteration 10, Training loss = 0.68935607442267
Iteration 20, Training loss = 0.3787466792737023
Iteration 30, Training loss = 0.3442644794397146
Iteration 40, Training loss = 0.32626902584446543
Iteration 50, Training loss = 0.3154906015707852
Iteration 60, Training loss = 0.3088134538752115
Iteration 70, Training loss = 0.30398793787341427
Iteration 80, Training loss = 0.3002529823318232
Iteration 90, Training loss = 0.2972065464528075
Iteration 100, Training loss = 0.2944317751408489
Iteration 110, Training loss = 0.2920868430133305
Iteration 120, Training loss = 0.28986179938688694
Iteration 130, Training loss = 0.2876107473711125
Iteration 140, Training loss = 0.2858480871553571
Iteration 150, Training loss = 0.2836807512161807
Iteration 160, Training loss = 0.2819871759587858
Iteration 170, Training loss = 0.2803497417462363
Iteration 180, Training loss = 0.27876424435841834
Iteration 190, Training loss = 0.27737459442950335
Iteration 200, Training loss = 0.27610692998831843
Iteration 210, Training loss = 0.27513238904912013
Iteration 220, Training loss = 0.2736201353966468
Iteration 230, Training loss = 0.2725544581979008
Iteration 240, Training loss = 0.27143849706678647
Iteration 250, Training loss = 0.2706221050558021
Iteration 260, Training loss = 0.26950087089804126
Iteration 270, Training loss = 0.2687429517291071
Iteration 280, Training loss = 0.2677163321876641
Iteration 290, Training loss = 0.26683369300555954
Model training time: 133.12000393867493
Device: cuda
Iteration 0, Training loss = 1.0161861193526456
Iteration 10, Training loss = 0.9330885198156712
Iteration 20, Training loss = 0.5545828360477891
Iteration 30, Training loss = 0.37904652794394594
Iteration 40, Training loss = 0.3445864419382941
Iteration 50, Training loss = 0.3248121508421967
Iteration 60, Training loss = 0.3132837938627377
Iteration 70, Training loss = 0.3058715778387199
Iteration 80, Training loss = 0.30066030378708253
Iteration 90, Training loss = 0.296350625251165
Iteration 100, Training loss = 0.2919042849367525
Iteration 110, Training loss = 0.28850655142244935
Iteration 120, Training loss = 0.28585872624168673
Iteration 130, Training loss = 0.28360192261221334
Iteration 140, Training loss = 0.28160011964978665
Iteration 150, Training loss = 0.2798663566551976
Iteration 160, Training loss = 0.27802056811984455
Iteration 170, Training loss = 0.27652566281054847
Iteration 180, Training loss = 0.275048399880781
Iteration 190, Training loss = 0.27393465905876485
Iteration 200, Training loss = 0.27276639947013764
Iteration 210, Training loss = 0.2717855475644511
Iteration 220, Training loss = 0.2707045646181407
Iteration 230, Training loss = 0.2697583820499462
Iteration 240, Training loss = 0.2690198030909095
Iteration 250, Training loss = 0.26821066365187163
Iteration 260, Training loss = 0.26736337011431954
Iteration 270, Training loss = 0.2666405679851987
Iteration 280, Training loss = 0.26598577495420817
Iteration 290, Training loss = 0.26540443319241014
Model training time: 132.83865809440613
Device: cuda
Iteration 0, Training loss = 1.0184057183906472
Iteration 10, Training loss = 0.9535579519035163
Iteration 20, Training loss = 0.6505185126464534
Iteration 30, Training loss = 0.3803498224325965
Iteration 40, Training loss = 0.3463092047279164
Iteration 50, Training loss = 0.3274206020497237
Iteration 60, Training loss = 0.3155654037222447
Iteration 70, Training loss = 0.3075989241064605
Iteration 80, Training loss = 0.3018474100071928
Iteration 90, Training loss = 0.29740228644655636
Iteration 100, Training loss = 0.29388196813280876
Iteration 110, Training loss = 0.2907873280436594
Iteration 120, Training loss = 0.288182121140039
Iteration 130, Training loss = 0.28568156527188443
Iteration 140, Training loss = 0.2835817140037731
Iteration 150, Training loss = 0.28155577444351904
Iteration 160, Training loss = 0.27990220672931276
Iteration 170, Training loss = 0.2783543115378003
Iteration 180, Training loss = 0.27692293855598415
Iteration 190, Training loss = 0.2755303363029374
Iteration 200, Training loss = 0.274265092380399
Iteration 210, Training loss = 0.2731595236366078
Iteration 220, Training loss = 0.27221128227274877
Iteration 230, Training loss = 0.2711705926616313
Iteration 240, Training loss = 0.2701967693504496
Iteration 250, Training loss = 0.26931378575488385
Iteration 260, Training loss = 0.2685587031103797
Iteration 270, Training loss = 0.2677764503271759
Iteration 280, Training loss = 0.2672520072422651
Iteration 290, Training loss = 0.26640257699365477
Model training time: 131.63426542282104
Device: cuda
Iteration 0, Training loss = 1.0575574440182554
Iteration 10, Training loss = 0.9522585737503181
Iteration 20, Training loss = 0.7015586089496174
Iteration 30, Training loss = 0.39270814228577416
Iteration 40, Training loss = 0.3495809879632031
Iteration 50, Training loss = 0.3298076594171743
Iteration 60, Training loss = 0.3172428286284858
Iteration 70, Training loss = 0.30858512076641686
Iteration 80, Training loss = 0.30191854438034155
Iteration 90, Training loss = 0.29636003686787143
Iteration 100, Training loss = 0.29175624714997433
Iteration 110, Training loss = 0.28796554033894806
Iteration 120, Training loss = 0.2846178213635022
Iteration 130, Training loss = 0.2818381378097915
Iteration 140, Training loss = 0.27936687423299644
Iteration 150, Training loss = 0.2771527811740559
Iteration 160, Training loss = 0.27516387659181407
Iteration 170, Training loss = 0.2733571111037425
Iteration 180, Training loss = 0.2717449920698459
Iteration 190, Training loss = 0.27046774659693673
Iteration 200, Training loss = 0.26915010146922985
Iteration 210, Training loss = 0.26813777231275315
Iteration 220, Training loss = 0.26712366819526034
Iteration 230, Training loss = 0.2662000764729613
Iteration 240, Training loss = 0.265466152196502
Iteration 250, Training loss = 0.26469052270884663
Iteration 260, Training loss = 0.2639503397462443
Iteration 270, Training loss = 0.2633061594088487
Iteration 280, Training loss = 0.2626386385311803
Iteration 290, Training loss = 0.2621334161582351
Model training time: 131.49463629722595
Device: cuda
Iteration 0, Training loss = 1.0328959771420707
Iteration 10, Training loss = 0.9270016994083765
Iteration 20, Training loss = 0.5397298183770214
Iteration 30, Training loss = 0.36229553881843213
Iteration 40, Training loss = 0.3340887323418772
Iteration 50, Training loss = 0.3197625762180901
Iteration 60, Training loss = 0.31143081745642437
Iteration 70, Training loss = 0.30608205266350985
Iteration 80, Training loss = 0.302207550038437
Iteration 90, Training loss = 0.29900094585326337
Iteration 100, Training loss = 0.2961483436296119
Iteration 110, Training loss = 0.29350626445206257
Iteration 120, Training loss = 0.29090050330101436
Iteration 130, Training loss = 0.2886350232268938
Iteration 140, Training loss = 0.2864472552340198
Iteration 150, Training loss = 0.284436609839556
Iteration 160, Training loss = 0.28257745585559935
Iteration 170, Training loss = 0.28068858020675097
Iteration 180, Training loss = 0.2789429824519677
Iteration 190, Training loss = 0.2773479524341392
Iteration 200, Training loss = 0.2758540160611236
Iteration 210, Training loss = 0.2745583972219405
Iteration 220, Training loss = 0.2730476761881722
Iteration 230, Training loss = 0.2717747873926567
Iteration 240, Training loss = 0.27052434331784814
Iteration 250, Training loss = 0.2693971481724455
Iteration 260, Training loss = 0.26841262071314503
Iteration 270, Training loss = 0.2673468720393377
Iteration 280, Training loss = 0.2663372492566524
Iteration 290, Training loss = 0.2654218967219242
Model training time: 133.60808396339417
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9506040491867296
Iteration 10, Training loss = 0.26569268801959894
Iteration 20, Training loss = 0.2530362028763889
Iteration 30, Training loss = 0.24767222906717665
Iteration 40, Training loss = 0.24484938590564104
Iteration 50, Training loss = 0.24117800661928718
Iteration 60, Training loss = 0.2381754635718198
Iteration 70, Training loss = 0.23603049416735444
Iteration 80, Training loss = 0.234032774880781
Iteration 90, Training loss = 0.23280090809806495
Iteration 100, Training loss = 0.23055809524280108
Iteration 110, Training loss = 0.22935587554701015
Iteration 120, Training loss = 0.227616818788626
Iteration 130, Training loss = 0.22533940039484898
Iteration 140, Training loss = 0.22265972225920053
Iteration 150, Training loss = 0.22111627193433897
Iteration 160, Training loss = 0.22017865278025228
Iteration 170, Training loss = 0.21781751190562515
Iteration 180, Training loss = 0.21733998595657994
Iteration 190, Training loss = 0.21661179988999055
Iteration 200, Training loss = 0.21505340939211787
Iteration 210, Training loss = 0.21441215358800808
Iteration 220, Training loss = 0.21324644127711834
Iteration 230, Training loss = 0.21334094570422
Iteration 240, Training loss = 0.21219907162517093
Iteration 250, Training loss = 0.21219241684054635
Iteration 260, Training loss = 0.21245466390980935
Iteration 270, Training loss = 0.21145502409331735
Iteration 280, Training loss = 0.21068535558428372
Iteration 290, Training loss = 0.21105395682318448
Model training time: 162.22280192375183
Device: cuda
Iteration 0, Training loss = 0.8149985080769795
Iteration 10, Training loss = 0.2707364225070067
Iteration 20, Training loss = 0.25469858365327336
Iteration 30, Training loss = 0.24933575734171393
Iteration 40, Training loss = 0.2460248445050191
Iteration 50, Training loss = 0.24307566412135995
Iteration 60, Training loss = 0.24133514331486844
Iteration 70, Training loss = 0.2386593803420627
Iteration 80, Training loss = 0.23679568670852422
Iteration 90, Training loss = 0.23519240588527038
Iteration 100, Training loss = 0.2338306485488109
Iteration 110, Training loss = 0.23257775123052968
Iteration 120, Training loss = 0.23132863566820616
Iteration 130, Training loss = 0.22967574947466285
Iteration 140, Training loss = 0.22901283617674872
Iteration 150, Training loss = 0.2282138846038906
Iteration 160, Training loss = 0.2265690786822656
Iteration 170, Training loss = 0.22586408966462201
Iteration 180, Training loss = 0.22462982660921665
Iteration 190, Training loss = 0.2230135165425536
Iteration 200, Training loss = 0.2219992826803256
Iteration 210, Training loss = 0.22092307380628354
Iteration 220, Training loss = 0.2201809249457522
Iteration 230, Training loss = 0.21975805895452638
Iteration 240, Training loss = 0.21840780129155582
Iteration 250, Training loss = 0.21800933414777024
Iteration 260, Training loss = 0.2173267266742398
Iteration 270, Training loss = 0.21632944207106317
Iteration 280, Training loss = 0.21599149667899198
Iteration 290, Training loss = 0.21519668607749315
Model training time: 155.6657633781433
Device: cuda
Iteration 0, Training loss = 0.7707067140294622
Iteration 10, Training loss = 0.26795156431760975
Iteration 20, Training loss = 0.2594239944298966
Iteration 30, Training loss = 0.25377950463759696
Iteration 40, Training loss = 0.24792953245071175
Iteration 50, Training loss = 0.24380570105288277
Iteration 60, Training loss = 0.23889021636787397
Iteration 70, Training loss = 0.2325644573111078
Iteration 80, Training loss = 0.22720782812369072
Iteration 90, Training loss = 0.22221705182196153
Iteration 100, Training loss = 0.21910292924027755
Iteration 110, Training loss = 0.2165421480380594
Iteration 120, Training loss = 0.21478526822807714
Iteration 130, Training loss = 0.21341661048137536
Iteration 140, Training loss = 0.21265017862722602
Iteration 150, Training loss = 0.21158750006166843
Iteration 160, Training loss = 0.21158909372823
Iteration 170, Training loss = 0.21000663431014044
Iteration 180, Training loss = 0.2090462190403488
Iteration 190, Training loss = 0.20932994001786298
Iteration 200, Training loss = 0.20863915420617665
Iteration 210, Training loss = 0.20785540284602463
Iteration 220, Training loss = 0.20820673013931326
Iteration 230, Training loss = 0.2073628208881862
Iteration 240, Training loss = 0.20686155131884984
Iteration 250, Training loss = 0.20612403026107726
Iteration 260, Training loss = 0.2062475194581773
Iteration 270, Training loss = 0.2057748268703427
Iteration 280, Training loss = 0.20551067339955462
Iteration 290, Training loss = 0.20555090028386716
Model training time: 156.0202775001526
Device: cuda
Iteration 0, Training loss = 0.9166576052332617
Iteration 10, Training loss = 0.26165215303043476
Iteration 20, Training loss = 0.25325490086909935
Iteration 30, Training loss = 0.2490065439489217
Iteration 40, Training loss = 0.24487920193854024
Iteration 50, Training loss = 0.2411669788579964
Iteration 60, Training loss = 0.23700700873468458
Iteration 70, Training loss = 0.23454331060010064
Iteration 80, Training loss = 0.2328533419649191
Iteration 90, Training loss = 0.22878028996306818
Iteration 100, Training loss = 0.22664341445083191
Iteration 110, Training loss = 0.22514668577880606
Iteration 120, Training loss = 0.2234761451098157
Iteration 130, Training loss = 0.22161095965536398
Iteration 140, Training loss = 0.2202317346311366
Iteration 150, Training loss = 0.2184973684552218
Iteration 160, Training loss = 0.2178743841356573
Iteration 170, Training loss = 0.21675370906008357
Iteration 180, Training loss = 0.2156689256772435
Iteration 190, Training loss = 0.21537387736376204
Iteration 200, Training loss = 0.2142609231913638
Iteration 210, Training loss = 0.2129286285895701
Iteration 220, Training loss = 0.21260734055795624
Iteration 230, Training loss = 0.21223202766286836
Iteration 240, Training loss = 0.2116154415699818
Iteration 250, Training loss = 0.21053134309033217
Iteration 260, Training loss = 0.2101633787227312
Iteration 270, Training loss = 0.20982833758682085
Iteration 280, Training loss = 0.20910490831905934
Iteration 290, Training loss = 0.2091198201419799
Model training time: 155.59543466567993
Device: cuda
Iteration 0, Training loss = 0.7188162236828493
Iteration 10, Training loss = 0.2596447361547491
Iteration 20, Training loss = 0.24850443440329364
Iteration 30, Training loss = 0.2437791773287205
Iteration 40, Training loss = 0.23959744800125715
Iteration 50, Training loss = 0.236719931333752
Iteration 60, Training loss = 0.23356697172575944
Iteration 70, Training loss = 0.2311578078362324
Iteration 80, Training loss = 0.22843559427281557
Iteration 90, Training loss = 0.22545434800641878
Iteration 100, Training loss = 0.22388411221290616
Iteration 110, Training loss = 0.22207747666801148
Iteration 120, Training loss = 0.22102651828065623
Iteration 130, Training loss = 0.21936620339823404
Iteration 140, Training loss = 0.21791394669156675
Iteration 150, Training loss = 0.2173111858545435
Iteration 160, Training loss = 0.21575546578546992
Iteration 170, Training loss = 0.2150086177824196
Iteration 180, Training loss = 0.2140461053455713
Iteration 190, Training loss = 0.21322094511971346
Iteration 200, Training loss = 0.21225983981430963
Iteration 210, Training loss = 0.21206037923874058
Iteration 220, Training loss = 0.21181192341064425
Iteration 230, Training loss = 0.21050448692234608
Iteration 240, Training loss = 0.20973064821691548
Iteration 250, Training loss = 0.20955322239069904
Iteration 260, Training loss = 0.20888015162425239
Iteration 270, Training loss = 0.20832413013744874
Iteration 280, Training loss = 0.2074845773021332
Iteration 290, Training loss = 0.20747807925932343
Model training time: 156.77700638771057
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0708286200828183
Iteration 10, Training loss = 0.9863304154491886
Iteration 20, Training loss = 0.9006180075554235
Iteration 30, Training loss = 0.5274435940940502
Iteration 40, Training loss = 0.37566665435530083
Iteration 50, Training loss = 0.34224550606407783
Iteration 60, Training loss = 0.3233365955385977
Iteration 70, Training loss = 0.31126083879219707
Iteration 80, Training loss = 0.30293992477453363
Iteration 90, Training loss = 0.29678768428272545
Iteration 100, Training loss = 0.29219671910669265
Iteration 110, Training loss = 0.2882645948638928
Iteration 120, Training loss = 0.28510457470327544
Iteration 130, Training loss = 0.2821797361962732
Iteration 140, Training loss = 0.27959402316851995
Iteration 150, Training loss = 0.27725508722496955
Iteration 160, Training loss = 0.2751487874342512
Iteration 170, Training loss = 0.27321526855591133
Iteration 180, Training loss = 0.271442796683196
Iteration 190, Training loss = 0.26980949063350157
Iteration 200, Training loss = 0.2686385003259049
Iteration 210, Training loss = 0.2670868365297017
Iteration 220, Training loss = 0.26591348013058125
Iteration 230, Training loss = 0.26482914104949473
Iteration 240, Training loss = 0.26385053586368123
Iteration 250, Training loss = 0.2629327693047593
Iteration 260, Training loss = 0.2623400578306893
Iteration 270, Training loss = 0.26166479237215284
Iteration 280, Training loss = 0.26103279487056247
Iteration 290, Training loss = 0.26021359381029163
Model training time: 135.80185770988464
Device: cuda
Iteration 0, Training loss = 1.006224851414886
Iteration 10, Training loss = 0.9505325436014818
Iteration 20, Training loss = 0.6550094089121276
Iteration 30, Training loss = 0.38759739655487185
Iteration 40, Training loss = 0.34701047675372904
Iteration 50, Training loss = 0.3288515243525944
Iteration 60, Training loss = 0.31762604704059066
Iteration 70, Training loss = 0.30956521706055784
Iteration 80, Training loss = 0.3033225558896619
Iteration 90, Training loss = 0.2981867289157093
Iteration 100, Training loss = 0.2937395251818344
Iteration 110, Training loss = 0.2898659954528543
Iteration 120, Training loss = 0.2865386693118271
Iteration 130, Training loss = 0.28350978792074516
Iteration 140, Training loss = 0.2808877499571147
Iteration 150, Training loss = 0.2786155587860516
Iteration 160, Training loss = 0.27646425112035605
Iteration 170, Training loss = 0.27457625014802156
Iteration 180, Training loss = 0.272929201493396
Iteration 190, Training loss = 0.27140456587797795
Iteration 200, Training loss = 0.2702124886773978
Iteration 210, Training loss = 0.26900706523195017
Iteration 220, Training loss = 0.2678519887511436
Iteration 230, Training loss = 0.26699779557907555
Iteration 240, Training loss = 0.2660847830858993
Iteration 250, Training loss = 0.2651929199226543
Iteration 260, Training loss = 0.26443429128887
Iteration 270, Training loss = 0.2637109432829494
Iteration 280, Training loss = 0.2631848194336487
Iteration 290, Training loss = 0.2625193286317313
Model training time: 129.55044960975647
Device: cuda
Iteration 0, Training loss = 1.0061323858490867
Iteration 10, Training loss = 0.9367198345158926
Iteration 20, Training loss = 0.5681353166663329
Iteration 30, Training loss = 0.3510558884996767
Iteration 40, Training loss = 0.32281558628737494
Iteration 50, Training loss = 0.30942577222862777
Iteration 60, Training loss = 0.30219217222048644
Iteration 70, Training loss = 0.29778551086603006
Iteration 80, Training loss = 0.29430733861054403
Iteration 90, Training loss = 0.29125211218295316
Iteration 100, Training loss = 0.2885313838648161
Iteration 110, Training loss = 0.2862842424296871
Iteration 120, Training loss = 0.2836299069568551
Iteration 130, Training loss = 0.28119466985974995
Iteration 140, Training loss = 0.27888032711014044
Iteration 150, Training loss = 0.27682177489736176
Iteration 160, Training loss = 0.274748262351708
Iteration 170, Training loss = 0.27284584721194055
Iteration 180, Training loss = 0.2712339737585613
Iteration 190, Training loss = 0.26986797253170547
Iteration 200, Training loss = 0.2685206825089512
Iteration 210, Training loss = 0.26731060351499925
Iteration 220, Training loss = 0.26627079841227563
Iteration 230, Training loss = 0.26540561563382714
Iteration 240, Training loss = 0.2644728652717992
Iteration 250, Training loss = 0.26371965696245936
Iteration 260, Training loss = 0.2630303839794371
Iteration 270, Training loss = 0.2623893107416122
Iteration 280, Training loss = 0.2617359756791996
Iteration 290, Training loss = 0.26109796744426284
Model training time: 131.53276419639587
Device: cuda
Iteration 0, Training loss = 1.0525385341257507
Iteration 10, Training loss = 0.9638938085507538
Iteration 20, Training loss = 0.8112585275427192
Iteration 30, Training loss = 0.4516845563061301
Iteration 40, Training loss = 0.3649335992105359
Iteration 50, Training loss = 0.33335345611878225
Iteration 60, Training loss = 0.31606817436708956
Iteration 70, Training loss = 0.30633570082467637
Iteration 80, Training loss = 0.30005261132416944
Iteration 90, Training loss = 0.29566494965380097
Iteration 100, Training loss = 0.29220362307084385
Iteration 110, Training loss = 0.28905072561115674
Iteration 120, Training loss = 0.2864882555986432
Iteration 130, Training loss = 0.28423543719994243
Iteration 140, Training loss = 0.28215284021368325
Iteration 150, Training loss = 0.2803583061976525
Iteration 160, Training loss = 0.27866769473792274
Iteration 170, Training loss = 0.27714544043991235
Iteration 180, Training loss = 0.27562615874315866
Iteration 190, Training loss = 0.2741731585298843
Iteration 200, Training loss = 0.27283714924539837
Iteration 210, Training loss = 0.27153126733066385
Iteration 220, Training loss = 0.2702241011739643
Iteration 230, Training loss = 0.26893377224700504
Iteration 240, Training loss = 0.26775250674087836
Iteration 250, Training loss = 0.26645157745978443
Iteration 260, Training loss = 0.26514859901646437
Iteration 270, Training loss = 0.2638809849974896
Iteration 280, Training loss = 0.2628187002122547
Iteration 290, Training loss = 0.2616946652675945
Model training time: 131.6413950920105
Device: cuda
Iteration 0, Training loss = 1.0107773351900224
Iteration 10, Training loss = 0.9349321647215698
Iteration 20, Training loss = 0.5858051914712707
Iteration 30, Training loss = 0.3654097019724061
Iteration 40, Training loss = 0.3355235121152014
Iteration 50, Training loss = 0.3206917513658002
Iteration 60, Training loss = 0.3122935430045278
Iteration 70, Training loss = 0.3068644339286386
Iteration 80, Training loss = 0.3028547619142486
Iteration 90, Training loss = 0.2995607567179867
Iteration 100, Training loss = 0.2967218731960719
Iteration 110, Training loss = 0.294183642483941
Iteration 120, Training loss = 0.29189248381241184
Iteration 130, Training loss = 0.28978724826406915
Iteration 140, Training loss = 0.28787618498464473
Iteration 150, Training loss = 0.28590630548701734
Iteration 160, Training loss = 0.2841994504446556
Iteration 170, Training loss = 0.2820162062530945
Iteration 180, Training loss = 0.2801794928586512
Iteration 190, Training loss = 0.2784619531966295
Iteration 200, Training loss = 0.2769661577323736
Iteration 210, Training loss = 0.27507581361558187
Iteration 220, Training loss = 0.2737982477857472
Iteration 230, Training loss = 0.27254631729304934
Iteration 240, Training loss = 0.2713055646376229
Iteration 250, Training loss = 0.27020680269375263
Iteration 260, Training loss = 0.26920823915241415
Iteration 270, Training loss = 0.26836195517755307
Iteration 280, Training loss = 0.26747354300417564
Iteration 290, Training loss = 0.26668892207287126
Model training time: 132.0521080493927
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7143992247650756
Iteration 10, Training loss = 0.2659194699824578
Iteration 20, Training loss = 0.2556274358643169
Iteration 30, Training loss = 0.2489446232379493
Iteration 40, Training loss = 0.24342819078674327
Iteration 50, Training loss = 0.237513099794815
Iteration 60, Training loss = 0.23258229162877755
Iteration 70, Training loss = 0.22869742962985004
Iteration 80, Training loss = 0.22564006917918278
Iteration 90, Training loss = 0.2231872000124928
Iteration 100, Training loss = 0.22171166957723604
Iteration 110, Training loss = 0.21993276691537791
Iteration 120, Training loss = 0.21783948736603553
Iteration 130, Training loss = 0.21624199306416453
Iteration 140, Training loss = 0.21479272833387153
Iteration 150, Training loss = 0.2140445318298536
Iteration 160, Training loss = 0.21331523980304923
Iteration 170, Training loss = 0.212802421878888
Iteration 180, Training loss = 0.21192443234615985
Iteration 190, Training loss = 0.21112928237342085
Iteration 200, Training loss = 0.21032226163235473
Iteration 210, Training loss = 0.2095134391496747
Iteration 220, Training loss = 0.2092118293732477
Iteration 230, Training loss = 0.2082101067586759
Iteration 240, Training loss = 0.20783097580183504
Iteration 250, Training loss = 0.2068780540301494
Iteration 260, Training loss = 0.2064482522699937
Iteration 270, Training loss = 0.2064372470318261
Iteration 280, Training loss = 0.20575916705783862
Iteration 290, Training loss = 0.20560799691528442
Model training time: 155.63482928276062
Device: cuda
Iteration 0, Training loss = 0.6656495578785497
Iteration 10, Training loss = 0.26286897826353517
Iteration 20, Training loss = 0.2531249066205925
Iteration 30, Training loss = 0.24681415495442421
Iteration 40, Training loss = 0.24363238325671768
Iteration 50, Training loss = 0.23785276006519362
Iteration 60, Training loss = 0.2328934467354785
Iteration 70, Training loss = 0.2285482987454815
Iteration 80, Training loss = 0.2248409434106679
Iteration 90, Training loss = 0.22212402596528535
Iteration 100, Training loss = 0.21942511760149394
Iteration 110, Training loss = 0.21667607550759582
Iteration 120, Training loss = 0.21396597676863105
Iteration 130, Training loss = 0.21240132701635073
Iteration 140, Training loss = 0.21056834367480462
Iteration 150, Training loss = 0.20976362371091115
Iteration 160, Training loss = 0.20837386228486932
Iteration 170, Training loss = 0.20778398437664525
Iteration 180, Training loss = 0.206797349891348
Iteration 190, Training loss = 0.20588950194941594
Iteration 200, Training loss = 0.20524571606884856
Iteration 210, Training loss = 0.2047316379682781
Iteration 220, Training loss = 0.20430632075407604
Iteration 230, Training loss = 0.20360364758614766
Iteration 240, Training loss = 0.20315836708244053
Iteration 250, Training loss = 0.20255736733356053
Iteration 260, Training loss = 0.20166890433206974
Iteration 270, Training loss = 0.2012803706613349
Iteration 280, Training loss = 0.2005350740477623
Iteration 290, Training loss = 0.2000954362944887
Model training time: 155.76890444755554
Device: cuda
Iteration 0, Training loss = 0.6751986990252072
Iteration 10, Training loss = 0.26106206329480786
Iteration 20, Training loss = 0.2521934208548098
Iteration 30, Training loss = 0.24932001083728475
Iteration 40, Training loss = 0.24632675743709176
Iteration 50, Training loss = 0.24402650806502627
Iteration 60, Training loss = 0.2406206403026858
Iteration 70, Training loss = 0.23839147623456997
Iteration 80, Training loss = 0.2366514884242134
Iteration 90, Training loss = 0.2360885971322764
Iteration 100, Training loss = 0.23538848957989464
Iteration 110, Training loss = 0.23412792908690744
Iteration 120, Training loss = 0.2329536016498293
Iteration 130, Training loss = 0.23239630033187658
Iteration 140, Training loss = 0.23103871939762452
Iteration 150, Training loss = 0.23025847744133512
Iteration 160, Training loss = 0.2289475152408817
Iteration 170, Training loss = 0.2277590362738466
Iteration 180, Training loss = 0.22593758016211357
Iteration 190, Training loss = 0.22539615885469585
Iteration 200, Training loss = 0.22387392597935968
Iteration 210, Training loss = 0.2230091598922202
Iteration 220, Training loss = 0.22125771609258998
Iteration 230, Training loss = 0.22037210251423406
Iteration 240, Training loss = 0.21923252537882645
Iteration 250, Training loss = 0.21832031197394933
Iteration 260, Training loss = 0.2170850423324772
Iteration 270, Training loss = 0.21644058805545363
Iteration 280, Training loss = 0.21553099554113267
Iteration 290, Training loss = 0.21488442517293857
Model training time: 166.20082449913025
Device: cuda
Iteration 0, Training loss = 0.690299666425795
Iteration 10, Training loss = 0.25909070111360155
Iteration 20, Training loss = 0.24926197283493115
Iteration 30, Training loss = 0.2425773528654841
Iteration 40, Training loss = 0.23632116432664757
Iteration 50, Training loss = 0.23154838101771497
Iteration 60, Training loss = 0.22828290187562061
Iteration 70, Training loss = 0.2253205768276935
Iteration 80, Training loss = 0.22301565870173617
Iteration 90, Training loss = 0.2210042043149327
Iteration 100, Training loss = 0.21965897993407874
Iteration 110, Training loss = 0.2176345100955582
Iteration 120, Training loss = 0.21643059113520688
Iteration 130, Training loss = 0.21480810989515256
Iteration 140, Training loss = 0.2144177343705376
Iteration 150, Training loss = 0.21242926315772043
Iteration 160, Training loss = 0.21153692598060025
Iteration 170, Training loss = 0.21134514768425258
Iteration 180, Training loss = 0.21023837213582622
Iteration 190, Training loss = 0.209655883688182
Iteration 200, Training loss = 0.20892363315849558
Iteration 210, Training loss = 0.20839186388881964
Iteration 220, Training loss = 0.2081318240315083
Iteration 230, Training loss = 0.2073224669651604
Iteration 240, Training loss = 0.20727144776151196
Iteration 250, Training loss = 0.20686158866195356
Iteration 260, Training loss = 0.20620400383707685
Iteration 270, Training loss = 0.2053585410226344
Iteration 280, Training loss = 0.20536521295441842
Iteration 290, Training loss = 0.20418910013813948
Model training time: 157.78220915794373
Device: cuda
Iteration 0, Training loss = 0.6999859930022866
Iteration 10, Training loss = 0.2609271236287191
Iteration 20, Training loss = 0.2500294949182875
Iteration 30, Training loss = 0.24519059002471605
Iteration 40, Training loss = 0.23995126114558366
Iteration 50, Training loss = 0.23652288240107727
Iteration 60, Training loss = 0.2334283359222493
Iteration 70, Training loss = 0.230498123742622
Iteration 80, Training loss = 0.2278027385653653
Iteration 90, Training loss = 0.22548786955761563
Iteration 100, Training loss = 0.22305110123991678
Iteration 110, Training loss = 0.2207219530681721
Iteration 120, Training loss = 0.21925766418798495
Iteration 130, Training loss = 0.2170699968681497
Iteration 140, Training loss = 0.21527773398031044
Iteration 150, Training loss = 0.2138213136799399
Iteration 160, Training loss = 0.21192677653694558
Iteration 170, Training loss = 0.21126720517621203
Iteration 180, Training loss = 0.20994538861635811
Iteration 190, Training loss = 0.20976676559188465
Iteration 200, Training loss = 0.20884802872563102
Iteration 210, Training loss = 0.2081119024688626
Iteration 220, Training loss = 0.20768318186372015
Iteration 230, Training loss = 0.20709200244009063
Iteration 240, Training loss = 0.20691723249329494
Iteration 250, Training loss = 0.20667308627584943
Iteration 260, Training loss = 0.20646745614148226
Iteration 270, Training loss = 0.20567950713685004
Iteration 280, Training loss = 0.2056140336625224
Iteration 290, Training loss = 0.20539535402566123
Model training time: 156.54748344421387
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0234600987474798
Iteration 10, Training loss = 0.9623454381998169
Iteration 20, Training loss = 0.6774874673051349
Iteration 30, Training loss = 0.37043737703099955
Iteration 40, Training loss = 0.3403947697388635
Iteration 50, Training loss = 0.3243163951444857
Iteration 60, Training loss = 0.3147943199366403
Iteration 70, Training loss = 0.30876180852513047
Iteration 80, Training loss = 0.3042971150494084
Iteration 90, Training loss = 0.30104080930388294
Iteration 100, Training loss = 0.2983494041798767
Iteration 110, Training loss = 0.29576775916912945
Iteration 120, Training loss = 0.2935931215707673
Iteration 130, Training loss = 0.2913413501680619
Iteration 140, Training loss = 0.2894443900800213
Iteration 150, Training loss = 0.28724252201670886
Iteration 160, Training loss = 0.2850792000282186
Iteration 170, Training loss = 0.2830714719419618
Iteration 180, Training loss = 0.28132591289231335
Iteration 190, Training loss = 0.2797862120856673
Iteration 200, Training loss = 0.2782987877004949
Iteration 210, Training loss = 0.2769494580587521
Iteration 220, Training loss = 0.2757540170022131
Iteration 230, Training loss = 0.2745240662544605
Iteration 240, Training loss = 0.27348194951439597
Iteration 250, Training loss = 0.2723622340483469
Iteration 260, Training loss = 0.27153095702210006
Iteration 270, Training loss = 0.2707961770834415
Iteration 280, Training loss = 0.2699790856740088
Iteration 290, Training loss = 0.2691895919908334
Model training time: 131.13280153274536
Device: cuda
Iteration 0, Training loss = 1.0548180746541473
Iteration 10, Training loss = 0.8926849964167246
Iteration 20, Training loss = 0.4968607104935889
Iteration 30, Training loss = 0.3790772313716625
Iteration 40, Training loss = 0.3475838299055942
Iteration 50, Training loss = 0.32996892128094635
Iteration 60, Training loss = 0.319569200584011
Iteration 70, Training loss = 0.31318261586219864
Iteration 80, Training loss = 0.30819101209501953
Iteration 90, Training loss = 0.30454217485596424
Iteration 100, Training loss = 0.30135626288144407
Iteration 110, Training loss = 0.2985332715922637
Iteration 120, Training loss = 0.2959500671551534
Iteration 130, Training loss = 0.29371108464698237
Iteration 140, Training loss = 0.29156642513615744
Iteration 150, Training loss = 0.28942254791825506
Iteration 160, Training loss = 0.2875251647526935
Iteration 170, Training loss = 0.28566453320855956
Iteration 180, Training loss = 0.28376899361971214
Iteration 190, Training loss = 0.2818787345802524
Iteration 200, Training loss = 0.28012109778767347
Iteration 210, Training loss = 0.27850692262805404
Iteration 220, Training loss = 0.27696369651515607
Iteration 230, Training loss = 0.27548031508922577
Iteration 240, Training loss = 0.2739884670435949
Iteration 250, Training loss = 0.2726096921661287
Iteration 260, Training loss = 0.27131256125596764
Iteration 270, Training loss = 0.2701887326725459
Iteration 280, Training loss = 0.2689996860813142
Iteration 290, Training loss = 0.26795288402650314
Model training time: 129.79035091400146
Device: cuda
Iteration 0, Training loss = 0.9842345269338271
Iteration 10, Training loss = 0.8499285407707131
Iteration 20, Training loss = 0.41593245876903395
Iteration 30, Training loss = 0.35061100514979976
Iteration 40, Training loss = 0.3291803296051072
Iteration 50, Training loss = 0.31601815769949493
Iteration 60, Training loss = 0.30730131026763313
Iteration 70, Training loss = 0.3012602389327839
Iteration 80, Training loss = 0.29688721925886147
Iteration 90, Training loss = 0.2933620777314858
Iteration 100, Training loss = 0.2905677847618341
Iteration 110, Training loss = 0.2879875819164962
Iteration 120, Training loss = 0.28565697880691826
Iteration 130, Training loss = 0.2835443750673287
Iteration 140, Training loss = 0.28171314824074867
Iteration 150, Training loss = 0.279988112131057
Iteration 160, Training loss = 0.2783247901546176
Iteration 170, Training loss = 0.27664000652820664
Iteration 180, Training loss = 0.2751366138710814
Iteration 190, Training loss = 0.2736717305139393
Iteration 200, Training loss = 0.27223717354544713
Iteration 210, Training loss = 0.27107123079874323
Iteration 220, Training loss = 0.2697674711629496
Iteration 230, Training loss = 0.2685191846987093
Iteration 240, Training loss = 0.2673502370318258
Iteration 250, Training loss = 0.2664207598740194
Iteration 260, Training loss = 0.2654064710795446
Iteration 270, Training loss = 0.26476682951678376
Iteration 280, Training loss = 0.2636789319445954
Iteration 290, Training loss = 0.26295926237871226
Model training time: 134.72945618629456
Device: cuda
Iteration 0, Training loss = 1.0197730690745983
Iteration 10, Training loss = 0.910770595939627
Iteration 20, Training loss = 0.5097948021230628
Iteration 30, Training loss = 0.36885973551371487
Iteration 40, Training loss = 0.3419485898377243
Iteration 50, Training loss = 0.3255170229055691
Iteration 60, Training loss = 0.3152115235858333
Iteration 70, Training loss = 0.3081832292870806
Iteration 80, Training loss = 0.30296823743180556
Iteration 90, Training loss = 0.2989048639596519
Iteration 100, Training loss = 0.2954113892399081
Iteration 110, Training loss = 0.2925299864451764
Iteration 120, Training loss = 0.2902041932536384
Iteration 130, Training loss = 0.2880453655888613
Iteration 140, Training loss = 0.2861316710277562
Iteration 150, Training loss = 0.2843736602088152
Iteration 160, Training loss = 0.2825925544661991
Iteration 170, Training loss = 0.2812304456355208
Iteration 180, Training loss = 0.2795702249497247
Iteration 190, Training loss = 0.27823848098732945
Iteration 200, Training loss = 0.27687025616806876
Iteration 210, Training loss = 0.27551816952286273
Iteration 220, Training loss = 0.2744982186298971
Iteration 230, Training loss = 0.2731701309181587
Iteration 240, Training loss = 0.27229061674047034
Iteration 250, Training loss = 0.270971649567671
Iteration 260, Training loss = 0.26984665313731093
Iteration 270, Training loss = 0.26881013796202496
Iteration 280, Training loss = 0.2678803442357239
Iteration 290, Training loss = 0.26689019077626613
Model training time: 129.47484159469604
Device: cuda
Iteration 0, Training loss = 1.0011930196539254
Iteration 10, Training loss = 0.9437137408493218
Iteration 20, Training loss = 0.5951716225170338
Iteration 30, Training loss = 0.36530698747453044
Iteration 40, Training loss = 0.3325440728310811
Iteration 50, Training loss = 0.3160131368740996
Iteration 60, Training loss = 0.30697545957623035
Iteration 70, Training loss = 0.30098437862448196
Iteration 80, Training loss = 0.2961931737369833
Iteration 90, Training loss = 0.2924898697230198
Iteration 100, Training loss = 0.2893442810122961
Iteration 110, Training loss = 0.2868361020275813
Iteration 120, Training loss = 0.28402054470402277
Iteration 130, Training loss = 0.2819144124882273
Iteration 140, Training loss = 0.27984485037390316
Iteration 150, Training loss = 0.2779789643123133
Iteration 160, Training loss = 0.2761124154376926
Iteration 170, Training loss = 0.27450407828603474
Iteration 180, Training loss = 0.27302212666801334
Iteration 190, Training loss = 0.27153124474440016
Iteration 200, Training loss = 0.2703678719609182
Iteration 210, Training loss = 0.26940444687304715
Iteration 220, Training loss = 0.2681424042106252
Iteration 230, Training loss = 0.2671865285813953
Iteration 240, Training loss = 0.266249666256564
Iteration 250, Training loss = 0.265498635302733
Iteration 260, Training loss = 0.26462788878067356
Iteration 270, Training loss = 0.2638986428013437
Iteration 280, Training loss = 0.2632080460684351
Iteration 290, Training loss = 0.2624969231844237
Model training time: 132.09120774269104
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5094490584536269
Iteration 10, Training loss = 0.2542627117174878
Iteration 20, Training loss = 0.24449176871675557
Iteration 30, Training loss = 0.23797737915276326
Iteration 40, Training loss = 0.2306384410862484
Iteration 50, Training loss = 0.2275321529163431
Iteration 60, Training loss = 0.22346770813007621
Iteration 70, Training loss = 0.2206792480359643
Iteration 80, Training loss = 0.21799775762325627
Iteration 90, Training loss = 0.21591097138118512
Model training time: 51.05231237411499
Device: cuda
Iteration 0, Training loss = 0.5922032945561639
Iteration 10, Training loss = 0.24716452408139988
Iteration 20, Training loss = 0.23562792482950498
Iteration 30, Training loss = 0.2261833193876553
Iteration 40, Training loss = 0.2184931043147608
Iteration 50, Training loss = 0.21427082719583487
Iteration 60, Training loss = 0.21146730340782724
Iteration 70, Training loss = 0.20893744246686918
Iteration 80, Training loss = 0.20643902920890086
Iteration 90, Training loss = 0.20469212713346932
Model training time: 52.87101745605469
Device: cuda
Iteration 0, Training loss = 0.4793132585777786
Iteration 10, Training loss = 0.24941477486209776
Iteration 20, Training loss = 0.23911511585946232
Iteration 30, Training loss = 0.22980156831317028
Iteration 40, Training loss = 0.2221462588946698
Iteration 50, Training loss = 0.2164365348133279
Iteration 60, Training loss = 0.21242407098701155
Iteration 70, Training loss = 0.21032310145530517
Iteration 80, Training loss = 0.20680464976455917
Iteration 90, Training loss = 0.20459001039585537
Model training time: 51.553755044937134
Device: cuda
Iteration 0, Training loss = 0.49862241326463713
Iteration 10, Training loss = 0.24857767038137515
Iteration 20, Training loss = 0.2350520437028593
Iteration 30, Training loss = 0.22668666526963865
Iteration 40, Training loss = 0.22185576124427972
Iteration 50, Training loss = 0.21859785238817586
Iteration 60, Training loss = 0.21441134436114648
Iteration 70, Training loss = 0.21069723041164384
Iteration 80, Training loss = 0.20771216565053052
Iteration 90, Training loss = 0.20652244263497738
Model training time: 51.78600454330444
Device: cuda
Iteration 0, Training loss = 0.5428454553749025
Iteration 10, Training loss = 0.2512762684907232
Iteration 20, Training loss = 0.2413850965803674
Iteration 30, Training loss = 0.23123557579683335
Iteration 40, Training loss = 0.2223731392484889
Iteration 50, Training loss = 0.21657691156315745
Iteration 60, Training loss = 0.21117218987921538
Iteration 70, Training loss = 0.20933905666915037
Iteration 80, Training loss = 0.20583639870291762
Iteration 90, Training loss = 0.20360272736728335
Model training time: 51.822890281677246
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.009860439320742
Iteration 10, Training loss = 0.4319629708877776
Iteration 20, Training loss = 0.340817176094211
Iteration 30, Training loss = 0.3121756242168729
Iteration 40, Training loss = 0.30061747752508877
Iteration 50, Training loss = 0.2941345826116082
Iteration 60, Training loss = 0.2896682855292036
Iteration 70, Training loss = 0.28591866939177524
Iteration 80, Training loss = 0.28254515246531
Iteration 90, Training loss = 0.2797368021442873
Model training time: 43.15851807594299
Device: cuda
Iteration 0, Training loss = 1.0049180827908597
Iteration 10, Training loss = 0.502596201641219
Iteration 20, Training loss = 0.33834838042752796
Iteration 30, Training loss = 0.30901951808256906
Iteration 40, Training loss = 0.2948598614183523
Iteration 50, Training loss = 0.28622302672764866
Iteration 60, Training loss = 0.2799228094427984
Iteration 70, Training loss = 0.2753810727964302
Iteration 80, Training loss = 0.27223779009452165
Iteration 90, Training loss = 0.2697441600551761
Model training time: 43.70035481452942
Device: cuda
Iteration 0, Training loss = 1.0006158467355133
Iteration 10, Training loss = 0.43315223819118437
Iteration 20, Training loss = 0.3379388824466066
Iteration 30, Training loss = 0.3124617590381793
Iteration 40, Training loss = 0.29804098712546484
Iteration 50, Training loss = 0.290404574941059
Iteration 60, Training loss = 0.28519358377696236
Iteration 70, Training loss = 0.2810669679185669
Iteration 80, Training loss = 0.27775936898466463
Iteration 90, Training loss = 0.27466454205371565
Model training time: 43.289124727249146
Device: cuda
Iteration 0, Training loss = 0.9886519487487202
Iteration 10, Training loss = 0.44145527474816715
Iteration 20, Training loss = 0.3290590208934814
Iteration 30, Training loss = 0.30542028570867913
Iteration 40, Training loss = 0.294137341166524
Iteration 50, Training loss = 0.2867751755812555
Iteration 60, Training loss = 0.28145379214107846
Iteration 70, Training loss = 0.27744026065377864
Iteration 80, Training loss = 0.27355954859216336
Iteration 90, Training loss = 0.27024655877533604
Model training time: 43.27260661125183
Device: cuda
Iteration 0, Training loss = 1.0051785559544553
Iteration 10, Training loss = 0.7869492332669782
Iteration 20, Training loss = 0.34306964581295596
Iteration 30, Training loss = 0.310066692549149
Iteration 40, Training loss = 0.29888862835076474
Iteration 50, Training loss = 0.2918764326640249
Iteration 60, Training loss = 0.28620350010891515
Iteration 70, Training loss = 0.2811720353927797
Iteration 80, Training loss = 0.27667300178518306
Iteration 90, Training loss = 0.2728602668996585
Model training time: 43.47152900695801
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6536643289155879
Iteration 10, Training loss = 0.26199572726398634
Iteration 20, Training loss = 0.25513123533266796
Iteration 30, Training loss = 0.2512758339757492
Iteration 40, Training loss = 0.24646756094463224
Iteration 50, Training loss = 0.24324617969643406
Iteration 60, Training loss = 0.23984078160029346
Iteration 70, Training loss = 0.2370213906463353
Iteration 80, Training loss = 0.2328376442049664
Iteration 90, Training loss = 0.23070099619702045
Model training time: 52.00505352020264
Device: cuda
Iteration 0, Training loss = 0.6140286280947217
Iteration 10, Training loss = 0.2564567546867574
Iteration 20, Training loss = 0.24740967356552512
Iteration 30, Training loss = 0.24061900048942889
Iteration 40, Training loss = 0.2362682697144461
Iteration 50, Training loss = 0.2320800042433831
Iteration 60, Training loss = 0.22920878734845515
Iteration 70, Training loss = 0.22645851170540432
Iteration 80, Training loss = 0.22340663293372054
Iteration 90, Training loss = 0.22049985681694298
Model training time: 52.29748058319092
Device: cuda
Iteration 0, Training loss = 0.7030936986857408
Iteration 10, Training loss = 0.25461469504071205
Iteration 20, Training loss = 0.24627708493941633
Iteration 30, Training loss = 0.24164841775996634
Iteration 40, Training loss = 0.23762222137708064
Iteration 50, Training loss = 0.23263038728232824
Iteration 60, Training loss = 0.22970359969983378
Iteration 70, Training loss = 0.22334580630352652
Iteration 80, Training loss = 0.21854622165963378
Iteration 90, Training loss = 0.216814479675622
Model training time: 51.34424686431885
Device: cuda
Iteration 0, Training loss = 0.5876587557518453
Iteration 10, Training loss = 0.2559817147240512
Iteration 20, Training loss = 0.24812746033541516
Iteration 30, Training loss = 0.24436578871659737
Iteration 40, Training loss = 0.23919723788163563
Iteration 50, Training loss = 0.23467338178983324
Iteration 60, Training loss = 0.23094616099723025
Iteration 70, Training loss = 0.22878520113143158
Iteration 80, Training loss = 0.22520793242871617
Iteration 90, Training loss = 0.22375029355251472
Model training time: 51.10304522514343
Device: cuda
Iteration 0, Training loss = 0.9016590514569826
Iteration 10, Training loss = 0.2569874509375263
Iteration 20, Training loss = 0.24540764345059093
Iteration 30, Training loss = 0.23881290472376432
Iteration 40, Training loss = 0.2350648509442229
Iteration 50, Training loss = 0.2324954548773985
Iteration 60, Training loss = 0.23000894119787157
Iteration 70, Training loss = 0.22762784007350412
Iteration 80, Training loss = 0.225586931575225
Iteration 90, Training loss = 0.2235281356130039
Model training time: 53.597362756729126
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9927121393617071
Iteration 10, Training loss = 0.6537185112515027
Iteration 20, Training loss = 0.35387161976633
Iteration 30, Training loss = 0.319582596818125
Iteration 40, Training loss = 0.3067441289888167
Iteration 50, Training loss = 0.29958341562935575
Iteration 60, Training loss = 0.29440799712701804
Iteration 70, Training loss = 0.2905125129092981
Iteration 80, Training loss = 0.28709431171200755
Iteration 90, Training loss = 0.284018040424975
Model training time: 48.11499094963074
Device: cuda
Iteration 0, Training loss = 1.0210022616040042
Iteration 10, Training loss = 0.653405676020837
Iteration 20, Training loss = 0.33969637408816494
Iteration 30, Training loss = 0.3064926644430899
Iteration 40, Training loss = 0.2934657903004789
Iteration 50, Training loss = 0.28615542524663357
Iteration 60, Training loss = 0.2814981241058784
Iteration 70, Training loss = 0.27756130473449214
Iteration 80, Training loss = 0.27425746670213796
Iteration 90, Training loss = 0.2717328581589186
Model training time: 44.4758095741272
Device: cuda
Iteration 0, Training loss = 1.0098424652009552
Iteration 10, Training loss = 0.9086120438922115
Iteration 20, Training loss = 0.3773198009944424
Iteration 30, Training loss = 0.32305432318486543
Iteration 40, Training loss = 0.3060266032237695
Iteration 50, Training loss = 0.29725001438478005
Iteration 60, Training loss = 0.2908954961680904
Iteration 70, Training loss = 0.28493682594550435
Iteration 80, Training loss = 0.2804518276351993
Iteration 90, Training loss = 0.27679570661474373
Model training time: 43.299617528915405
Device: cuda
Iteration 0, Training loss = 0.9912497525353697
Iteration 10, Training loss = 0.5161261357565481
Iteration 20, Training loss = 0.33747633925433884
Iteration 30, Training loss = 0.30508169023090065
Iteration 40, Training loss = 0.29456962455632324
Iteration 50, Training loss = 0.2885344008087535
Iteration 60, Training loss = 0.2841144056932112
Iteration 70, Training loss = 0.2802537484658255
Iteration 80, Training loss = 0.2769695092488721
Iteration 90, Training loss = 0.274150694343859
Model training time: 43.66382646560669
Device: cuda
Iteration 0, Training loss = 1.0166182875921883
Iteration 10, Training loss = 0.7557626785146699
Iteration 20, Training loss = 0.33847062340227224
Iteration 30, Training loss = 0.3104610944631313
Iteration 40, Training loss = 0.30111029246240206
Iteration 50, Training loss = 0.29496796423745214
Iteration 60, Training loss = 0.2895370368951747
Iteration 70, Training loss = 0.2850490579016272
Iteration 80, Training loss = 0.2811144143342972
Iteration 90, Training loss = 0.2773392586348709
Model training time: 43.92455577850342
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6641854660779454
Iteration 10, Training loss = 0.25438457008532406
Iteration 20, Training loss = 0.24674182773069378
Iteration 30, Training loss = 0.2413883194525796
Iteration 40, Training loss = 0.23649257735392085
Iteration 50, Training loss = 0.2323556173475545
Iteration 60, Training loss = 0.2278469581490712
Iteration 70, Training loss = 0.225539096791216
Iteration 80, Training loss = 0.22197233815170084
Iteration 90, Training loss = 0.21867012649269427
Model training time: 53.507866621017456
Device: cuda
Iteration 0, Training loss = 0.5009883175678461
Iteration 10, Training loss = 0.25303583942155283
Iteration 20, Training loss = 0.24696448200876142
Iteration 30, Training loss = 0.24301070712741293
Iteration 40, Training loss = 0.23819876705763127
Iteration 50, Training loss = 0.2354839312286989
Iteration 60, Training loss = 0.23219767754974147
Iteration 70, Training loss = 0.22965673716247226
Iteration 80, Training loss = 0.22674840518226058
Iteration 90, Training loss = 0.2249912447036756
Model training time: 52.171387910842896
Device: cuda
Iteration 0, Training loss = 0.5259460226314697
Iteration 10, Training loss = 0.2521765706902843
Iteration 20, Training loss = 0.24435112116628352
Iteration 30, Training loss = 0.23957564093659633
Iteration 40, Training loss = 0.2341893506179999
Iteration 50, Training loss = 0.22701121558216525
Iteration 60, Training loss = 0.22320509057934002
Iteration 70, Training loss = 0.2200661141416351
Iteration 80, Training loss = 0.2190346798022925
Iteration 90, Training loss = 0.2164987245028879
Model training time: 51.50901007652283
Device: cuda
Iteration 0, Training loss = 0.5176023243955781
Iteration 10, Training loss = 0.25405413031433743
Iteration 20, Training loss = 0.24172549707404636
Iteration 30, Training loss = 0.23273370275161168
Iteration 40, Training loss = 0.22829766685174682
Iteration 50, Training loss = 0.22502714366658835
Iteration 60, Training loss = 0.22143358776918623
Iteration 70, Training loss = 0.2180288173168104
Iteration 80, Training loss = 0.21525826261854633
Iteration 90, Training loss = 0.2122837464543867
Model training time: 51.3460488319397
Device: cuda
Iteration 0, Training loss = 0.5822128559176338
Iteration 10, Training loss = 0.2519116010922785
Iteration 20, Training loss = 0.24124054513312426
Iteration 30, Training loss = 0.23493692064184254
Iteration 40, Training loss = 0.22941096762046398
Iteration 50, Training loss = 0.22356718231993783
Iteration 60, Training loss = 0.2184381984336613
Iteration 70, Training loss = 0.21309700210468244
Iteration 80, Training loss = 0.2097577715414344
Iteration 90, Training loss = 0.2074248217425104
Model training time: 51.25105428695679
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9833581152464518
Iteration 10, Training loss = 0.39365529191407395
Iteration 20, Training loss = 0.32274669314989457
Iteration 30, Training loss = 0.3041813645971889
Iteration 40, Training loss = 0.2947218194784321
Iteration 50, Training loss = 0.28819090200105535
Iteration 60, Training loss = 0.2834218252838091
Iteration 70, Training loss = 0.27960666305144244
Iteration 80, Training loss = 0.27651945503369946
Iteration 90, Training loss = 0.27384959669111714
Model training time: 43.32482981681824
Device: cuda
Iteration 0, Training loss = 1.0116200120916667
Iteration 10, Training loss = 0.4356352560961795
Iteration 20, Training loss = 0.3334649286749288
Iteration 30, Training loss = 0.3116734360342453
Iteration 40, Training loss = 0.3006987529502365
Iteration 50, Training loss = 0.29339411057369474
Iteration 60, Training loss = 0.28773136114020614
Iteration 70, Training loss = 0.2827075493249131
Iteration 80, Training loss = 0.2783643118766549
Iteration 90, Training loss = 0.27467646395683865
Model training time: 42.9428174495697
Device: cuda
Iteration 0, Training loss = 0.9963102496565109
Iteration 10, Training loss = 0.3777635309294985
Iteration 20, Training loss = 0.3212524053909011
Iteration 30, Training loss = 0.30655614613187804
Iteration 40, Training loss = 0.29894688739903613
Iteration 50, Training loss = 0.2929930887376714
Iteration 60, Training loss = 0.2875825708989081
Iteration 70, Training loss = 0.28193030221338133
Iteration 80, Training loss = 0.27745292366416924
Iteration 90, Training loss = 0.273864029647363
Model training time: 43.413877964019775
Device: cuda
Iteration 0, Training loss = 1.0014684338835191
Iteration 10, Training loss = 0.4159330438109922
Iteration 20, Training loss = 0.32713518214139176
Iteration 30, Training loss = 0.3044573925886547
Iteration 40, Training loss = 0.2947866248990664
Iteration 50, Training loss = 0.28861668723980394
Iteration 60, Training loss = 0.2838687534843172
Iteration 70, Training loss = 0.2800333294070373
Iteration 80, Training loss = 0.27671202373201564
Iteration 90, Training loss = 0.27371851928514085
Model training time: 43.10016345977783
Device: cuda
Iteration 0, Training loss = 0.9970922452774232
Iteration 10, Training loss = 0.5642403072291945
Iteration 20, Training loss = 0.3477740259998936
Iteration 30, Training loss = 0.32035694565380457
Iteration 40, Training loss = 0.30740323045784856
Iteration 50, Training loss = 0.29907277913777647
Iteration 60, Training loss = 0.2926081442984484
Iteration 70, Training loss = 0.28639839596811856
Iteration 80, Training loss = 0.28060528677471036
Iteration 90, Training loss = 0.27573636432176063
Model training time: 42.99030542373657
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5350477533825373
Iteration 10, Training loss = 0.2527450579906347
Iteration 20, Training loss = 0.2442909423397182
Iteration 30, Training loss = 0.23554725121463183
Iteration 40, Training loss = 0.22795563261532034
Iteration 50, Training loss = 0.22119343603710862
Iteration 60, Training loss = 0.2158977784595247
Iteration 70, Training loss = 0.21299621562042767
Iteration 80, Training loss = 0.20983423801369008
Iteration 90, Training loss = 0.20773911305655868
Iteration 100, Training loss = 0.20700378709677922
Iteration 110, Training loss = 0.20593512346900406
Iteration 120, Training loss = 0.20454119224344847
Iteration 130, Training loss = 0.20293944002496705
Iteration 140, Training loss = 0.2033370908791736
Iteration 150, Training loss = 0.20183798340634052
Iteration 160, Training loss = 0.2003983089172667
Iteration 170, Training loss = 0.19995216854800613
Iteration 180, Training loss = 0.19825248499615142
Iteration 190, Training loss = 0.19749554161875357
Model training time: 104.20148420333862
Device: cuda
Iteration 0, Training loss = 0.5676976341673018
Iteration 10, Training loss = 0.25181568468093296
Iteration 20, Training loss = 0.24609834628662244
Iteration 30, Training loss = 0.24136991609456176
Iteration 40, Training loss = 0.23566402875554476
Iteration 50, Training loss = 0.2280073421522434
Iteration 60, Training loss = 0.22184179228962767
Iteration 70, Training loss = 0.2190762949841363
Iteration 80, Training loss = 0.2155091292055698
Iteration 90, Training loss = 0.21438140867697414
Iteration 100, Training loss = 0.21230403082242313
Iteration 110, Training loss = 0.20960990011186922
Iteration 120, Training loss = 0.2082210235189439
Iteration 130, Training loss = 0.20711755739033366
Iteration 140, Training loss = 0.20607919165679675
Iteration 150, Training loss = 0.20555263349998373
Iteration 160, Training loss = 0.20428207297482445
Iteration 170, Training loss = 0.20322443774876525
Iteration 180, Training loss = 0.20274491200435538
Iteration 190, Training loss = 0.20113628130343
Model training time: 104.77591061592102
Device: cuda
Iteration 0, Training loss = 0.4974205544259011
Iteration 10, Training loss = 0.25224418389739484
Iteration 20, Training loss = 0.23638094624292474
Iteration 30, Training loss = 0.22723911877622327
Iteration 40, Training loss = 0.2221726755686014
Iteration 50, Training loss = 0.21831009098606016
Iteration 60, Training loss = 0.21455301685353456
Iteration 70, Training loss = 0.21216721446693088
Iteration 80, Training loss = 0.21006526582430118
Iteration 90, Training loss = 0.20823502964531826
Iteration 100, Training loss = 0.2074937047504484
Iteration 110, Training loss = 0.20640809591363474
Iteration 120, Training loss = 0.20582449876367323
Iteration 130, Training loss = 0.2047786715442707
Iteration 140, Training loss = 0.20232518415077258
Iteration 150, Training loss = 0.20199600109248703
Iteration 160, Training loss = 0.2011631637198296
Iteration 170, Training loss = 0.20025773148884496
Iteration 180, Training loss = 0.199486394221139
Iteration 190, Training loss = 0.1984928160427774
Model training time: 103.61898899078369
Device: cuda
Iteration 0, Training loss = 0.6103702324462861
Iteration 10, Training loss = 0.2503813073291617
Iteration 20, Training loss = 0.23729425951946734
Iteration 30, Training loss = 0.22879902924665815
Iteration 40, Training loss = 0.22246697652390449
Iteration 50, Training loss = 0.21839069953914417
Iteration 60, Training loss = 0.2144023825574585
Iteration 70, Training loss = 0.2112150592481397
Iteration 80, Training loss = 0.20953735233832502
Iteration 90, Training loss = 0.20704739429868452
Iteration 100, Training loss = 0.20553699197116834
Iteration 110, Training loss = 0.20331428348837696
Iteration 120, Training loss = 0.20294334049411028
Iteration 130, Training loss = 0.20294126671610385
Iteration 140, Training loss = 0.20097085863315742
Iteration 150, Training loss = 0.2003603823681432
Iteration 160, Training loss = 0.19891590376995666
Iteration 170, Training loss = 0.1977466597756231
Iteration 180, Training loss = 0.1978303104837495
Iteration 190, Training loss = 0.1967871322343915
Model training time: 104.99813914299011
Device: cuda
Iteration 0, Training loss = 0.6189017834516184
Iteration 10, Training loss = 0.25015253941675075
Iteration 20, Training loss = 0.23895035882550347
Iteration 30, Training loss = 0.23236763986959585
Iteration 40, Training loss = 0.22721515027648312
Iteration 50, Training loss = 0.22294129567450818
Iteration 60, Training loss = 0.22054107649563012
Iteration 70, Training loss = 0.21740929649795515
Iteration 80, Training loss = 0.2150008846841049
Iteration 90, Training loss = 0.21390186491947774
Iteration 100, Training loss = 0.21105878183180715
Iteration 110, Training loss = 0.2114576390667054
Iteration 120, Training loss = 0.2083645189410982
Iteration 130, Training loss = 0.20852153559754316
Iteration 140, Training loss = 0.20578514508560264
Iteration 150, Training loss = 0.2051469070011565
Iteration 160, Training loss = 0.20423513069099433
Iteration 170, Training loss = 0.20416042968161746
Iteration 180, Training loss = 0.20285439962119803
Iteration 190, Training loss = 0.20176554047310902
Model training time: 102.68161177635193
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9775568214369167
Iteration 10, Training loss = 0.4280824935465113
Iteration 20, Training loss = 0.3515051088812276
Iteration 30, Training loss = 0.3216320549929402
Iteration 40, Training loss = 0.30621247015189895
Iteration 50, Training loss = 0.2965972235935941
Iteration 60, Training loss = 0.29010961540313956
Iteration 70, Training loss = 0.2851854874673536
Iteration 80, Training loss = 0.28139699789860056
Iteration 90, Training loss = 0.2782271564836652
Iteration 100, Training loss = 0.27525853242986714
Iteration 110, Training loss = 0.27264911331434805
Iteration 120, Training loss = 0.2705398240187555
Iteration 130, Training loss = 0.26856785402892575
Iteration 140, Training loss = 0.2668543062364507
Iteration 150, Training loss = 0.2653157925345996
Iteration 160, Training loss = 0.2638962824375664
Iteration 170, Training loss = 0.2625995620580043
Iteration 180, Training loss = 0.2612649989258002
Iteration 190, Training loss = 0.2602306600055741
Model training time: 86.90207600593567
Device: cuda
Iteration 0, Training loss = 1.0330150601505945
Iteration 10, Training loss = 0.800678199053388
Iteration 20, Training loss = 0.3447228453277676
Iteration 30, Training loss = 0.31444833295974545
Iteration 40, Training loss = 0.30423173398573233
Iteration 50, Training loss = 0.2979199753185739
Iteration 60, Training loss = 0.2931611016753511
Iteration 70, Training loss = 0.289173530229789
Iteration 80, Training loss = 0.28567822291978046
Iteration 90, Training loss = 0.28269899392965053
Iteration 100, Training loss = 0.2797143886510743
Iteration 110, Training loss = 0.27702234669184855
Iteration 120, Training loss = 0.2744039490133452
Iteration 130, Training loss = 0.27198785698587036
Iteration 140, Training loss = 0.2695066090537907
Iteration 150, Training loss = 0.2670859423608093
Iteration 160, Training loss = 0.26498035942238124
Iteration 170, Training loss = 0.26296036876287066
Iteration 180, Training loss = 0.2612283920224296
Iteration 190, Training loss = 0.25962118275878504
Model training time: 88.02127766609192
Device: cuda
Iteration 0, Training loss = 0.9916889930897128
Iteration 10, Training loss = 0.5108593827875706
Iteration 20, Training loss = 0.33578887630533655
Iteration 30, Training loss = 0.3124661940278499
Iteration 40, Training loss = 0.3029609374142732
Iteration 50, Training loss = 0.29758707848862354
Iteration 60, Training loss = 0.2931682091323573
Iteration 70, Training loss = 0.2889753274854101
Iteration 80, Training loss = 0.2855196372576545
Iteration 90, Training loss = 0.2821646538578858
Iteration 100, Training loss = 0.27907482579747356
Iteration 110, Training loss = 0.2762425987288969
Iteration 120, Training loss = 0.27345771694298804
Iteration 130, Training loss = 0.2709232092841774
Iteration 140, Training loss = 0.2687518959977725
Iteration 150, Training loss = 0.26640128501246685
Iteration 160, Training loss = 0.26440246082102703
Iteration 170, Training loss = 0.2625972886928346
Iteration 180, Training loss = 0.26111941690811524
Iteration 190, Training loss = 0.2596444646505409
Model training time: 87.26159882545471
Device: cuda
Iteration 0, Training loss = 0.9920585121571585
Iteration 10, Training loss = 0.49808195475083
Iteration 20, Training loss = 0.3453348773299349
Iteration 30, Training loss = 0.3155500481838753
Iteration 40, Training loss = 0.3024408810436004
Iteration 50, Training loss = 0.29460457863949113
Iteration 60, Training loss = 0.2886423082530643
Iteration 70, Training loss = 0.2836398915286214
Iteration 80, Training loss = 0.27951763039928373
Iteration 90, Training loss = 0.27594919524383316
Iteration 100, Training loss = 0.27258172389525764
Iteration 110, Training loss = 0.26961104880305814
Iteration 120, Training loss = 0.26695709662922357
Iteration 130, Training loss = 0.26464429001614775
Iteration 140, Training loss = 0.26249654351149576
Iteration 150, Training loss = 0.26076409772723985
Iteration 160, Training loss = 0.25901496653911854
Iteration 170, Training loss = 0.2577119150692845
Iteration 180, Training loss = 0.2565236921099716
Iteration 190, Training loss = 0.2553461059639298
Model training time: 87.10716223716736
Device: cuda
Iteration 0, Training loss = 1.054907250057987
Iteration 10, Training loss = 0.7682636282778825
Iteration 20, Training loss = 0.3548981971659903
Iteration 30, Training loss = 0.31811563890725014
Iteration 40, Training loss = 0.30267067746085635
Iteration 50, Training loss = 0.293736562331277
Iteration 60, Training loss = 0.28686987264104385
Iteration 70, Training loss = 0.2813400941524321
Iteration 80, Training loss = 0.27640994252145434
Iteration 90, Training loss = 0.27269614118063423
Iteration 100, Training loss = 0.2685627144091643
Iteration 110, Training loss = 0.2657221796726199
Iteration 120, Training loss = 0.26336119197687863
Iteration 130, Training loss = 0.26138812826707347
Iteration 140, Training loss = 0.260179253543116
Iteration 150, Training loss = 0.2587260423142454
Iteration 160, Training loss = 0.2574087674515299
Iteration 170, Training loss = 0.25659424633004185
Iteration 180, Training loss = 0.25571423606635874
Iteration 190, Training loss = 0.25484684410056246
Model training time: 88.61882376670837
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7012461356764556
Iteration 10, Training loss = 0.25896264601850627
Iteration 20, Training loss = 0.2511005597076318
Iteration 30, Training loss = 0.24811578877540824
Iteration 40, Training loss = 0.24521438822547115
Iteration 50, Training loss = 0.24306985716049087
Iteration 60, Training loss = 0.24020708740190502
Iteration 70, Training loss = 0.2367135375629903
Iteration 80, Training loss = 0.23465438174494244
Iteration 90, Training loss = 0.23216143090810096
Iteration 100, Training loss = 0.2300893153192489
Iteration 110, Training loss = 0.22814810078491887
Iteration 120, Training loss = 0.22679451214415686
Iteration 130, Training loss = 0.22522423616621742
Iteration 140, Training loss = 0.22324789994673636
Iteration 150, Training loss = 0.22204258510297206
Iteration 160, Training loss = 0.22023262550770226
Iteration 170, Training loss = 0.2184424397262765
Iteration 180, Training loss = 0.21853533805715547
Iteration 190, Training loss = 0.21785382222898358
Model training time: 104.90968132019043
Device: cuda
Iteration 0, Training loss = 0.5588591916004046
Iteration 10, Training loss = 0.2540680695929481
Iteration 20, Training loss = 0.24686053736230074
Iteration 30, Training loss = 0.2428598346382596
Iteration 40, Training loss = 0.23896903737693953
Iteration 50, Training loss = 0.23415575751958112
Iteration 60, Training loss = 0.23048978957367866
Iteration 70, Training loss = 0.2271648060629645
Iteration 80, Training loss = 0.2240844842398282
Iteration 90, Training loss = 0.2216367592570568
Iteration 100, Training loss = 0.21967578154354928
Iteration 110, Training loss = 0.2193377081746772
Iteration 120, Training loss = 0.21721101240153462
Iteration 130, Training loss = 0.21595146391567827
Iteration 140, Training loss = 0.21533603634297416
Iteration 150, Training loss = 0.21455986236761038
Iteration 160, Training loss = 0.21430469557787257
Iteration 170, Training loss = 0.21361173787684187
Iteration 180, Training loss = 0.21347539828634723
Iteration 190, Training loss = 0.21382367322289048
Model training time: 110.04505228996277
Device: cuda
Iteration 0, Training loss = 0.5757591279886537
Iteration 10, Training loss = 0.25127682843883736
Iteration 20, Training loss = 0.2429652687423743
Iteration 30, Training loss = 0.23710473423900386
Iteration 40, Training loss = 0.23245804517847862
Iteration 50, Training loss = 0.22800258958382988
Iteration 60, Training loss = 0.22460777108663507
Iteration 70, Training loss = 0.22195640599612174
Iteration 80, Training loss = 0.219582600018447
Iteration 90, Training loss = 0.2178839468385925
Iteration 100, Training loss = 0.21539060845429903
Iteration 110, Training loss = 0.21579772930312677
Iteration 120, Training loss = 0.21460614897328775
Iteration 130, Training loss = 0.21329486649708945
Iteration 140, Training loss = 0.21259989023894144
Iteration 150, Training loss = 0.21219176652933724
Iteration 160, Training loss = 0.21144202192816838
Iteration 170, Training loss = 0.21055303073824175
Iteration 180, Training loss = 0.21089662214601299
Iteration 190, Training loss = 0.21065660972896846
Model training time: 103.04711675643921
Device: cuda
Iteration 0, Training loss = 0.6198374955186544
Iteration 10, Training loss = 0.25694394688918
Iteration 20, Training loss = 0.2443899732561146
Iteration 30, Training loss = 0.23610211243135878
Iteration 40, Training loss = 0.23050529260828767
Iteration 50, Training loss = 0.22693206374386898
Iteration 60, Training loss = 0.22358151651540045
Iteration 70, Training loss = 0.21998895932954102
Iteration 80, Training loss = 0.21888366672627574
Iteration 90, Training loss = 0.21662157411364608
Iteration 100, Training loss = 0.21462618830201124
Iteration 110, Training loss = 0.21393380386272295
Iteration 120, Training loss = 0.21259046154723618
Iteration 130, Training loss = 0.21162629115776346
Iteration 140, Training loss = 0.21128581142725003
Iteration 150, Training loss = 0.21080797370387913
Iteration 160, Training loss = 0.20950197109154292
Iteration 170, Training loss = 0.2088428821784532
Iteration 180, Training loss = 0.20907927537224194
Iteration 190, Training loss = 0.20750649332026014
Model training time: 104.01051020622253
Device: cuda
Iteration 0, Training loss = 0.6127032511315104
Iteration 10, Training loss = 0.25560838030087457
Iteration 20, Training loss = 0.24772891169526676
Iteration 30, Training loss = 0.2427884321344101
Iteration 40, Training loss = 0.23782539831307553
Iteration 50, Training loss = 0.23299076597569351
Iteration 60, Training loss = 0.22925377483806655
Iteration 70, Training loss = 0.22713190416374737
Iteration 80, Training loss = 0.2252928823521293
Iteration 90, Training loss = 0.22393641789549776
Iteration 100, Training loss = 0.22183806873118328
Iteration 110, Training loss = 0.22036412418718487
Iteration 120, Training loss = 0.21800460025992868
Iteration 130, Training loss = 0.21753722263306163
Iteration 140, Training loss = 0.2164127215039499
Iteration 150, Training loss = 0.2154558708949903
Iteration 160, Training loss = 0.21479392473836212
Iteration 170, Training loss = 0.21408692938580062
Iteration 180, Training loss = 0.21312340952401876
Iteration 190, Training loss = 0.2125030188183086
Model training time: 102.97622752189636
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0826507517414001
Iteration 10, Training loss = 0.8889903595026123
Iteration 20, Training loss = 0.3684351162818096
Iteration 30, Training loss = 0.3229845139687344
Iteration 40, Training loss = 0.3015727135589567
Iteration 50, Training loss = 0.28998763065505545
Iteration 60, Training loss = 0.2827112276747498
Iteration 70, Training loss = 0.2780652900945765
Iteration 80, Training loss = 0.2747372232612051
Iteration 90, Training loss = 0.2721114923893395
Iteration 100, Training loss = 0.2702744139705674
Iteration 110, Training loss = 0.26811055029961156
Iteration 120, Training loss = 0.2664404939003199
Iteration 130, Training loss = 0.2651894600931149
Iteration 140, Training loss = 0.2638364223713736
Iteration 150, Training loss = 0.262582649817334
Iteration 160, Training loss = 0.2615482003290486
Iteration 170, Training loss = 0.26074327413019777
Iteration 180, Training loss = 0.25966081529242363
Iteration 190, Training loss = 0.2590039139365457
Model training time: 85.46456813812256
Device: cuda
Iteration 0, Training loss = 1.0025216509152843
Iteration 10, Training loss = 0.8298962520340741
Iteration 20, Training loss = 0.3658854882740224
Iteration 30, Training loss = 0.3215293163934285
Iteration 40, Training loss = 0.3025851814171015
Iteration 50, Training loss = 0.2911214689040877
Iteration 60, Training loss = 0.28449848021851903
Iteration 70, Training loss = 0.2800902121725152
Iteration 80, Training loss = 0.27628991147616005
Iteration 90, Training loss = 0.273026804748661
Iteration 100, Training loss = 0.27072523181613073
Iteration 110, Training loss = 0.2683459956189911
Iteration 120, Training loss = 0.2662499266119615
Iteration 130, Training loss = 0.2644088821390928
Iteration 140, Training loss = 0.2633218945479855
Iteration 150, Training loss = 0.2622230426379035
Iteration 160, Training loss = 0.26123166688634464
Iteration 170, Training loss = 0.26037024396311576
Iteration 180, Training loss = 0.2597908016573863
Iteration 190, Training loss = 0.2588673835263991
Model training time: 86.6089837551117
Device: cuda
Iteration 0, Training loss = 1.0061561141262332
Iteration 10, Training loss = 0.46729947738439637
Iteration 20, Training loss = 0.3449648294336282
Iteration 30, Training loss = 0.313358479059926
Iteration 40, Training loss = 0.2990225961941495
Iteration 50, Training loss = 0.2898927135595975
Iteration 60, Training loss = 0.2832227082106738
Iteration 70, Training loss = 0.2778952634796392
Iteration 80, Training loss = 0.27350164233700125
Iteration 90, Training loss = 0.2698815843455728
Iteration 100, Training loss = 0.26679267867786255
Iteration 110, Training loss = 0.2643772766138393
Iteration 120, Training loss = 0.26300381363953573
Iteration 130, Training loss = 0.26124157641110063
Iteration 140, Training loss = 0.26034566328903663
Iteration 150, Training loss = 0.259350936471983
Iteration 160, Training loss = 0.2585018825949537
Iteration 170, Training loss = 0.2578884927831031
Iteration 180, Training loss = 0.2571168854183204
Iteration 190, Training loss = 0.2563213535897957
Model training time: 86.97107887268066
Device: cuda
Iteration 0, Training loss = 1.0296920243533414
Iteration 10, Training loss = 0.5517402280113022
Iteration 20, Training loss = 0.3555630443856733
Iteration 30, Training loss = 0.3187655720309542
Iteration 40, Training loss = 0.3005464202194468
Iteration 50, Training loss = 0.29052508310313374
Iteration 60, Training loss = 0.28365000905626914
Iteration 70, Training loss = 0.27853542320953445
Iteration 80, Training loss = 0.2743769146267496
Iteration 90, Training loss = 0.27081167393028016
Iteration 100, Training loss = 0.26771691668841796
Iteration 110, Training loss = 0.26557536872405985
Iteration 120, Training loss = 0.2637625570266934
Iteration 130, Training loss = 0.2621383124164173
Iteration 140, Training loss = 0.2608917613872316
Iteration 150, Training loss = 0.25985434438501087
Iteration 160, Training loss = 0.2588177997501653
Iteration 170, Training loss = 0.25773681971733853
Iteration 180, Training loss = 0.2568082200052086
Iteration 190, Training loss = 0.25581435321606966
Model training time: 86.88453006744385
Device: cuda
Iteration 0, Training loss = 0.9945561451427007
Iteration 10, Training loss = 0.5923818107739488
Iteration 20, Training loss = 0.3732739555994477
Iteration 30, Training loss = 0.32407117692740145
Iteration 40, Training loss = 0.3037674244646587
Iteration 50, Training loss = 0.2933293339631748
Iteration 60, Training loss = 0.28675450193391294
Iteration 70, Training loss = 0.2824255666959372
Iteration 80, Training loss = 0.27862461595897525
Iteration 90, Training loss = 0.2756805775132364
Iteration 100, Training loss = 0.27289884187479574
Iteration 110, Training loss = 0.2703633479521436
Iteration 120, Training loss = 0.2678757393619916
Iteration 130, Training loss = 0.265796611568108
Iteration 140, Training loss = 0.2639017919795565
Iteration 150, Training loss = 0.2622338640740362
Iteration 160, Training loss = 0.26070372720607543
Iteration 170, Training loss = 0.259441935554255
Iteration 180, Training loss = 0.25843185707746347
Iteration 190, Training loss = 0.25735721322223004
Model training time: 87.96866345405579
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.5592986787274733
Iteration 10, Training loss = 0.2567132331304631
Iteration 20, Training loss = 0.2507793029486123
Iteration 30, Training loss = 0.24516522132094778
Iteration 40, Training loss = 0.24083046538453126
Iteration 50, Training loss = 0.23737912337334047
Iteration 60, Training loss = 0.23532441561504946
Iteration 70, Training loss = 0.23261776530425138
Iteration 80, Training loss = 0.22967323074545756
Iteration 90, Training loss = 0.22679904058178746
Iteration 100, Training loss = 0.2223752790895126
Iteration 110, Training loss = 0.22004021726747114
Iteration 120, Training loss = 0.21812035520414463
Iteration 130, Training loss = 0.21685735222863228
Iteration 140, Training loss = 0.21507052467463958
Iteration 150, Training loss = 0.21349229512073226
Iteration 160, Training loss = 0.21188611845704602
Iteration 170, Training loss = 0.21063361189736293
Iteration 180, Training loss = 0.2096764701114151
Iteration 190, Training loss = 0.20960646765825247
Model training time: 105.96874713897705
Device: cuda
Iteration 0, Training loss = 0.5843875088681609
Iteration 10, Training loss = 0.2545203601549093
Iteration 20, Training loss = 0.24160211998743814
Iteration 30, Training loss = 0.2337497650999711
Iteration 40, Training loss = 0.2283180958025392
Iteration 50, Training loss = 0.22272687757130685
Iteration 60, Training loss = 0.218384856543082
Iteration 70, Training loss = 0.21655336397612066
Iteration 80, Training loss = 0.2139951220508349
Iteration 90, Training loss = 0.2126015666015356
Iteration 100, Training loss = 0.21106403229023007
Iteration 110, Training loss = 0.20985864569178217
Iteration 120, Training loss = 0.209230286274421
Iteration 130, Training loss = 0.20837498789080408
Iteration 140, Training loss = 0.20647677313761909
Iteration 150, Training loss = 0.20648591536081443
Iteration 160, Training loss = 0.20541609405172362
Iteration 170, Training loss = 0.20411886992812445
Iteration 180, Training loss = 0.20359647795377575
Iteration 190, Training loss = 0.20377066886598202
Model training time: 104.7844545841217
Device: cuda
Iteration 0, Training loss = 0.5375356917555916
Iteration 10, Training loss = 0.25234322175022766
Iteration 20, Training loss = 0.24390693738623623
Iteration 30, Training loss = 0.23841431266098276
Iteration 40, Training loss = 0.23381353447497902
Iteration 50, Training loss = 0.22921411099575334
Iteration 60, Training loss = 0.2243689044263692
Iteration 70, Training loss = 0.2203952443873911
Iteration 80, Training loss = 0.21882214145531956
Iteration 90, Training loss = 0.2165401228060878
Iteration 100, Training loss = 0.2149384091574401
Iteration 110, Training loss = 0.21398222638569214
Iteration 120, Training loss = 0.2115877357832456
Iteration 130, Training loss = 0.21041564861668802
Iteration 140, Training loss = 0.21025652070383183
Iteration 150, Training loss = 0.20910911365766213
Iteration 160, Training loss = 0.20858986621618847
Iteration 170, Training loss = 0.2064356521204944
Iteration 180, Training loss = 0.20563163564114248
Iteration 190, Training loss = 0.20448401083236167
Model training time: 103.80651307106018
Device: cuda
Iteration 0, Training loss = 0.5095749375281842
Iteration 10, Training loss = 0.25574866426120946
Iteration 20, Training loss = 0.2444494701110133
Iteration 30, Training loss = 0.2389574208123199
Iteration 40, Training loss = 0.23281531896415114
Iteration 50, Training loss = 0.22716220222034697
Iteration 60, Training loss = 0.21992799236504565
Iteration 70, Training loss = 0.21630842036614983
Iteration 80, Training loss = 0.212455237198612
Iteration 90, Training loss = 0.21036654296748286
Iteration 100, Training loss = 0.20750409739033362
Iteration 110, Training loss = 0.20692531696856745
Iteration 120, Training loss = 0.2072008401786877
Iteration 130, Training loss = 0.2047645813237524
Iteration 140, Training loss = 0.20557466512839095
Iteration 150, Training loss = 0.20481420854065954
Iteration 160, Training loss = 0.20391642386161385
Iteration 170, Training loss = 0.20345873939500306
Iteration 180, Training loss = 0.20306372233907766
Iteration 190, Training loss = 0.203159177400731
Model training time: 102.2492368221283
Device: cuda
Iteration 0, Training loss = 0.5386723711834116
Iteration 10, Training loss = 0.25459696870234055
Iteration 20, Training loss = 0.24712327555651814
Iteration 30, Training loss = 0.2430923172153226
Iteration 40, Training loss = 0.23720389633432717
Iteration 50, Training loss = 0.2311181261071137
Iteration 60, Training loss = 0.22596063535236563
Iteration 70, Training loss = 0.221494543593891
Iteration 80, Training loss = 0.22013583948912402
Iteration 90, Training loss = 0.21885644308952096
Iteration 100, Training loss = 0.21681636520469738
Iteration 110, Training loss = 0.2146628506517728
Iteration 120, Training loss = 0.21338997946344046
Iteration 130, Training loss = 0.21315514869067917
Iteration 140, Training loss = 0.21209647497455375
Iteration 150, Training loss = 0.21088230627031937
Iteration 160, Training loss = 0.20973126205996798
Iteration 170, Training loss = 0.20855179589611567
Iteration 180, Training loss = 0.2086007673536894
Iteration 190, Training loss = 0.20832070198640695
Model training time: 103.1540756225586
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0280133493010126
Iteration 10, Training loss = 0.4805976690738022
Iteration 20, Training loss = 0.3478462759757157
Iteration 30, Training loss = 0.3135022378862626
Iteration 40, Training loss = 0.2972409602516211
Iteration 50, Training loss = 0.28839192773975414
Iteration 60, Training loss = 0.2822794249285797
Iteration 70, Training loss = 0.2779349582066836
Iteration 80, Training loss = 0.27440310726009903
Iteration 90, Training loss = 0.2715326071904012
Iteration 100, Training loss = 0.2692739932141639
Iteration 110, Training loss = 0.2675431681386495
Iteration 120, Training loss = 0.2662776309013655
Iteration 130, Training loss = 0.2651797331707241
Iteration 140, Training loss = 0.2642225664983939
Iteration 150, Training loss = 0.26342271120438565
Iteration 160, Training loss = 0.2626183817375082
Iteration 170, Training loss = 0.261828944519992
Iteration 180, Training loss = 0.26098367918201565
Iteration 190, Training loss = 0.26035189238933903
Model training time: 87.24243807792664
Device: cuda
Iteration 0, Training loss = 1.0027649855065288
Iteration 10, Training loss = 0.6780553517128018
Iteration 20, Training loss = 0.3403497254408301
Iteration 30, Training loss = 0.3123768214993269
Iteration 40, Training loss = 0.30126452097809053
Iteration 50, Training loss = 0.29384922809880815
Iteration 60, Training loss = 0.2876055470462573
Iteration 70, Training loss = 0.28297648349772353
Iteration 80, Training loss = 0.27940301635002684
Iteration 90, Training loss = 0.2764952882547067
Iteration 100, Training loss = 0.27401884814801
Iteration 110, Training loss = 0.27194813214982105
Iteration 120, Training loss = 0.27017668955767704
Iteration 130, Training loss = 0.26875202895365385
Iteration 140, Training loss = 0.267481610124827
Iteration 150, Training loss = 0.26631973703678236
Iteration 160, Training loss = 0.2654248638930967
Iteration 170, Training loss = 0.26459735695083264
Iteration 180, Training loss = 0.2634684230419683
Iteration 190, Training loss = 0.26267887904482373
Model training time: 87.1957437992096
Device: cuda
Iteration 0, Training loss = 0.9867487041049662
Iteration 10, Training loss = 0.4962572173619097
Iteration 20, Training loss = 0.33615398215756864
Iteration 30, Training loss = 0.3103798932010267
Iteration 40, Training loss = 0.29840115181758964
Iteration 50, Training loss = 0.2910464864052814
Iteration 60, Training loss = 0.28542611962657866
Iteration 70, Training loss = 0.28092707291281543
Iteration 80, Training loss = 0.27696113833214986
Iteration 90, Training loss = 0.2735166512880718
Iteration 100, Training loss = 0.27055284176157113
Iteration 110, Training loss = 0.2680030665552068
Iteration 120, Training loss = 0.26611470504151996
Iteration 130, Training loss = 0.26438009692919745
Iteration 140, Training loss = 0.26306580615029207
Iteration 150, Training loss = 0.2618866606680879
Iteration 160, Training loss = 0.26093518889123535
Iteration 170, Training loss = 0.2599428380877862
Iteration 180, Training loss = 0.2590581074753916
Iteration 190, Training loss = 0.25845271293028793
Model training time: 86.33152866363525
Device: cuda
Iteration 0, Training loss = 0.9925221832554797
Iteration 10, Training loss = 0.4265078132651909
Iteration 20, Training loss = 0.32516181396803034
Iteration 30, Training loss = 0.3067201176437281
Iteration 40, Training loss = 0.2977838246535158
Iteration 50, Training loss = 0.2923907696948213
Iteration 60, Training loss = 0.2880864561902986
Iteration 70, Training loss = 0.2846147240867915
Iteration 80, Training loss = 0.281311059871251
Iteration 90, Training loss = 0.27815178823240155
Iteration 100, Training loss = 0.2755167234066612
Iteration 110, Training loss = 0.27309558948289975
Iteration 120, Training loss = 0.27094274914437866
Iteration 130, Training loss = 0.26897269306979515
Iteration 140, Training loss = 0.26729306809550046
Iteration 150, Training loss = 0.2658610496191944
Iteration 160, Training loss = 0.2645733232900825
Iteration 170, Training loss = 0.2636213808097216
Iteration 180, Training loss = 0.2623940090542844
Iteration 190, Training loss = 0.2615455156124533
Model training time: 86.49043250083923
Device: cuda
Iteration 0, Training loss = 1.0143267145024084
Iteration 10, Training loss = 0.48895466450340236
Iteration 20, Training loss = 0.33675126238899716
Iteration 30, Training loss = 0.3101433954104673
Iteration 40, Training loss = 0.29951897860655774
Iteration 50, Training loss = 0.29256222813816396
Iteration 60, Training loss = 0.28745286141411736
Iteration 70, Training loss = 0.28367259568077025
Iteration 80, Training loss = 0.2801842282708851
Iteration 90, Training loss = 0.2768782256374059
Iteration 100, Training loss = 0.273709936937899
Iteration 110, Training loss = 0.27073967169979296
Iteration 120, Training loss = 0.2682652567438871
Iteration 130, Training loss = 0.2659938676377474
Iteration 140, Training loss = 0.2642212775118703
Iteration 150, Training loss = 0.26228485588372186
Iteration 160, Training loss = 0.26055067007319405
Iteration 170, Training loss = 0.2591268384860734
Iteration 180, Training loss = 0.2579041601093283
Iteration 190, Training loss = 0.25698324911675213
Model training time: 88.29947590827942
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.514386587692808
Iteration 10, Training loss = 0.25334968361958465
Iteration 20, Training loss = 0.2437763970354279
Iteration 30, Training loss = 0.23204510954837823
Iteration 40, Training loss = 0.22501548913647995
Iteration 50, Training loss = 0.21920324538182692
Iteration 60, Training loss = 0.21592724847338968
Iteration 70, Training loss = 0.21283295968903757
Iteration 80, Training loss = 0.20976293321527523
Iteration 90, Training loss = 0.20848295272082162
Iteration 100, Training loss = 0.20657523097267452
Iteration 110, Training loss = 0.20580477732939523
Iteration 120, Training loss = 0.20321324589429987
Iteration 130, Training loss = 0.2023520342753095
Iteration 140, Training loss = 0.20105625020642257
Iteration 150, Training loss = 0.19932242403523973
Iteration 160, Training loss = 0.19819956596661134
Iteration 170, Training loss = 0.19710335601797693
Iteration 180, Training loss = 0.19620861826559244
Iteration 190, Training loss = 0.19548027921855882
Iteration 200, Training loss = 0.1950358137957409
Iteration 210, Training loss = 0.1949255241500552
Iteration 220, Training loss = 0.1939660397518345
Iteration 230, Training loss = 0.19303660771243508
Iteration 240, Training loss = 0.19173116900802525
Iteration 250, Training loss = 0.1925236868692368
Iteration 260, Training loss = 0.19216982859748327
Iteration 270, Training loss = 0.19108073444942297
Iteration 280, Training loss = 0.19008558336419862
Iteration 290, Training loss = 0.19025439985511378
Model training time: 163.71606254577637
Device: cuda
Iteration 0, Training loss = 0.47002179520182114
Iteration 10, Training loss = 0.24930799516147908
Iteration 20, Training loss = 0.23947855000345816
Iteration 30, Training loss = 0.23107546890835495
Iteration 40, Training loss = 0.22456141615390202
Iteration 50, Training loss = 0.2162329509637835
Iteration 60, Training loss = 0.21233182253437816
Iteration 70, Training loss = 0.21096533762412845
Iteration 80, Training loss = 0.20769119790369603
Iteration 90, Training loss = 0.20576945622846232
Iteration 100, Training loss = 0.20444864120343406
Iteration 110, Training loss = 0.20307351338517002
Iteration 120, Training loss = 0.2016976289902126
Iteration 130, Training loss = 0.20142619275729245
Iteration 140, Training loss = 0.20084191736095466
Iteration 150, Training loss = 0.19977901466461417
Iteration 160, Training loss = 0.19919219126893303
Iteration 170, Training loss = 0.19844973421053505
Iteration 180, Training loss = 0.19914650222507574
Iteration 190, Training loss = 0.19840011384888365
Iteration 200, Training loss = 0.19699078273325799
Iteration 210, Training loss = 0.19700808298140404
Iteration 220, Training loss = 0.19672671034390932
Iteration 230, Training loss = 0.19621886646704292
Iteration 240, Training loss = 0.19621073922760263
Iteration 250, Training loss = 0.19645063990397835
Iteration 260, Training loss = 0.19553973444581899
Iteration 270, Training loss = 0.19466265971089103
Iteration 280, Training loss = 0.1945478756332513
Iteration 290, Training loss = 0.19512381181842478
Model training time: 155.50565838813782
Device: cuda
Iteration 0, Training loss = 0.5428263360846418
Iteration 10, Training loss = 0.25043647896868265
Iteration 20, Training loss = 0.2429954855551731
Iteration 30, Training loss = 0.23733953317090617
Iteration 40, Training loss = 0.23101212237634614
Iteration 50, Training loss = 0.22635104871618833
Iteration 60, Training loss = 0.22226210828807394
Iteration 70, Training loss = 0.2197808558549777
Iteration 80, Training loss = 0.21670118295395924
Iteration 90, Training loss = 0.21524543681387173
Iteration 100, Training loss = 0.21303822009059764
Iteration 110, Training loss = 0.21111586929882986
Iteration 120, Training loss = 0.21018701527475156
Iteration 130, Training loss = 0.2077505336691046
Iteration 140, Training loss = 0.20594297915011572
Iteration 150, Training loss = 0.20475488458252702
Iteration 160, Training loss = 0.20409697424124285
Iteration 170, Training loss = 0.20193559264191416
Iteration 180, Training loss = 0.20069216920590574
Iteration 190, Training loss = 0.19975764803209548
Iteration 200, Training loss = 0.1977131211764876
Iteration 210, Training loss = 0.1977336864476487
Iteration 220, Training loss = 0.19689265495170694
Iteration 230, Training loss = 0.19662838428310564
Iteration 240, Training loss = 0.19562975792597628
Iteration 250, Training loss = 0.19570438619748154
Iteration 260, Training loss = 0.1949487905814053
Iteration 270, Training loss = 0.19475864600074494
Iteration 280, Training loss = 0.1939021033431081
Iteration 290, Training loss = 0.19383801626307623
Model training time: 154.23657870292664
Device: cuda
Iteration 0, Training loss = 0.49694698787413843
Iteration 10, Training loss = 0.2536072834027304
Iteration 20, Training loss = 0.2429331999732132
Iteration 30, Training loss = 0.2370165629407107
Iteration 40, Training loss = 0.22972141332545523
Iteration 50, Training loss = 0.2223798226989066
Iteration 60, Training loss = 0.21572050672322152
Iteration 70, Training loss = 0.21143860120966707
Iteration 80, Training loss = 0.2078589137832997
Iteration 90, Training loss = 0.20418805255583933
Iteration 100, Training loss = 0.20220929644911687
Iteration 110, Training loss = 0.2014432725368054
Iteration 120, Training loss = 0.2002600094091517
Iteration 130, Training loss = 0.1989841223457607
Iteration 140, Training loss = 0.19808882514587614
Iteration 150, Training loss = 0.19644649879731796
Iteration 160, Training loss = 0.19692765023856995
Iteration 170, Training loss = 0.19550234183091805
Iteration 180, Training loss = 0.1944699415679994
Iteration 190, Training loss = 0.19425910001408678
Iteration 200, Training loss = 0.19373781057232517
Iteration 210, Training loss = 0.19380803058965732
Iteration 220, Training loss = 0.19307746044551777
Iteration 230, Training loss = 0.19253078622621717
Iteration 240, Training loss = 0.19124761626088302
Iteration 250, Training loss = 0.19128647212110478
Iteration 260, Training loss = 0.191065057318378
Iteration 270, Training loss = 0.19020299226514653
Iteration 280, Training loss = 0.19053789511431216
Iteration 290, Training loss = 0.19046078859569085
Model training time: 156.07581615447998
Device: cuda
Iteration 0, Training loss = 0.4791768286692894
Iteration 10, Training loss = 0.2520622651726368
Iteration 20, Training loss = 0.24327308370688924
Iteration 30, Training loss = 0.23459212169881016
Iteration 40, Training loss = 0.22581520314005904
Iteration 50, Training loss = 0.22192390345163265
Iteration 60, Training loss = 0.21757499170974437
Iteration 70, Training loss = 0.21411882986745304
Iteration 80, Training loss = 0.21178486718645106
Iteration 90, Training loss = 0.20877313152965854
Iteration 100, Training loss = 0.20569318828802133
Iteration 110, Training loss = 0.20472751423427904
Iteration 120, Training loss = 0.20344964581706335
Iteration 130, Training loss = 0.2013380149194461
Iteration 140, Training loss = 0.20102888327678237
Iteration 150, Training loss = 0.19918986540693637
Iteration 160, Training loss = 0.1986822359281937
Iteration 170, Training loss = 0.19819846678949152
Iteration 180, Training loss = 0.19813617749126136
Iteration 190, Training loss = 0.19578347615807454
Iteration 200, Training loss = 0.19673397180062807
Iteration 210, Training loss = 0.19601595707544403
Iteration 220, Training loss = 0.19493886252941867
Iteration 230, Training loss = 0.1947895049796266
Iteration 240, Training loss = 0.19475899165897623
Iteration 250, Training loss = 0.19531316863999817
Iteration 260, Training loss = 0.1945851731675589
Iteration 270, Training loss = 0.19290950115923441
Iteration 280, Training loss = 0.1927680909705797
Iteration 290, Training loss = 0.19291444683839853
Model training time: 155.69077682495117
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0049442612085735
Iteration 10, Training loss = 0.49616687180199287
Iteration 20, Training loss = 0.34797055588146675
Iteration 30, Training loss = 0.315250950434739
Iteration 40, Training loss = 0.30119240083720433
Iteration 50, Training loss = 0.293091965374733
Iteration 60, Training loss = 0.28705936551238376
Iteration 70, Training loss = 0.2825814964515822
Iteration 80, Training loss = 0.27877011916827926
Iteration 90, Training loss = 0.27551321656449945
Iteration 100, Training loss = 0.2728320403119265
Iteration 110, Training loss = 0.2707104920691497
Iteration 120, Training loss = 0.26901859624117397
Iteration 130, Training loss = 0.2674549778531019
Iteration 140, Training loss = 0.266083908734108
Iteration 150, Training loss = 0.26506280787198944
Iteration 160, Training loss = 0.26389683983299983
Iteration 170, Training loss = 0.2629400595986526
Iteration 180, Training loss = 0.2619267421488179
Iteration 190, Training loss = 0.26100349015856195
Iteration 200, Training loss = 0.2600420899779398
Iteration 210, Training loss = 0.25947863484121697
Iteration 220, Training loss = 0.2587449740591407
Iteration 230, Training loss = 0.2584617591799027
Iteration 240, Training loss = 0.2578277029908887
Iteration 250, Training loss = 0.25728052719236866
Iteration 260, Training loss = 0.2568633456062751
Iteration 270, Training loss = 0.2564692598819444
Iteration 280, Training loss = 0.256089098390742
Iteration 290, Training loss = 0.25567797593970853
Model training time: 132.45794224739075
Device: cuda
Iteration 0, Training loss = 1.0210425132844985
Iteration 10, Training loss = 0.6366665024878615
Iteration 20, Training loss = 0.36265010328976927
Iteration 30, Training loss = 0.3242631591405476
Iteration 40, Training loss = 0.30916660755295444
Iteration 50, Training loss = 0.3014107924180227
Iteration 60, Training loss = 0.2955456982765879
Iteration 70, Training loss = 0.29087023272857826
Iteration 80, Training loss = 0.2870505849941302
Iteration 90, Training loss = 0.2836200357854511
Iteration 100, Training loss = 0.28040656230928823
Iteration 110, Training loss = 0.27710534906416195
Iteration 120, Training loss = 0.27402366994548655
Iteration 130, Training loss = 0.2712801359323266
Iteration 140, Training loss = 0.2685113013259724
Iteration 150, Training loss = 0.2662584455841679
Iteration 160, Training loss = 0.26425872182081167
Iteration 170, Training loss = 0.2624865427937981
Iteration 180, Training loss = 0.2610936583748164
Iteration 190, Training loss = 0.259788071073718
Iteration 200, Training loss = 0.2587776692092563
Iteration 210, Training loss = 0.2577586361177753
Iteration 220, Training loss = 0.25689702417890903
Iteration 230, Training loss = 0.25596428739209154
Iteration 240, Training loss = 0.2553024867472868
Iteration 250, Training loss = 0.25472656913444436
Iteration 260, Training loss = 0.2538423388835593
Iteration 270, Training loss = 0.2531782557182104
Iteration 280, Training loss = 0.25260892732307927
Iteration 290, Training loss = 0.2519223827318476
Model training time: 131.02917432785034
Device: cuda
Iteration 0, Training loss = 0.9967351573864427
Iteration 10, Training loss = 0.5893511248991506
Iteration 20, Training loss = 0.34062853154273065
Iteration 30, Training loss = 0.3136642428958387
Iteration 40, Training loss = 0.3017322119262259
Iteration 50, Training loss = 0.29542358511873945
Iteration 60, Training loss = 0.2910455959616793
Iteration 70, Training loss = 0.2874571667131731
Iteration 80, Training loss = 0.2838278093508312
Iteration 90, Training loss = 0.28057014498381583
Iteration 100, Training loss = 0.27766690352710627
Iteration 110, Training loss = 0.2750185751705712
Iteration 120, Training loss = 0.2726807942293747
Iteration 130, Training loss = 0.2701661772214183
Iteration 140, Training loss = 0.26807164088721425
Iteration 150, Training loss = 0.2658300434778158
Iteration 160, Training loss = 0.264215281661645
Iteration 170, Training loss = 0.2625106052105421
Iteration 180, Training loss = 0.2609523350765284
Iteration 190, Training loss = 0.2595002733838472
Iteration 200, Training loss = 0.25823569047032485
Iteration 210, Training loss = 0.2572333495207041
Iteration 220, Training loss = 0.2562469753276638
Iteration 230, Training loss = 0.2552830320726584
Iteration 240, Training loss = 0.25433159101817565
Iteration 250, Training loss = 0.25365671131209655
Iteration 260, Training loss = 0.25263183630696223
Iteration 270, Training loss = 0.2517602196094055
Iteration 280, Training loss = 0.2509893832895138
Iteration 290, Training loss = 0.2502990108046491
Model training time: 132.2633867263794
Device: cuda
Iteration 0, Training loss = 0.9886346085596893
Iteration 10, Training loss = 0.408833526778452
Iteration 20, Training loss = 0.3351983967221389
Iteration 30, Training loss = 0.3129302828638086
Iteration 40, Training loss = 0.30231007444728664
Iteration 50, Training loss = 0.29520599123712893
Iteration 60, Training loss = 0.2896433983169514
Iteration 70, Training loss = 0.28470336784750727
Iteration 80, Training loss = 0.2806720725885315
Iteration 90, Training loss = 0.27710847103783354
Iteration 100, Training loss = 0.2740082332895974
Iteration 110, Training loss = 0.2713438992819255
Iteration 120, Training loss = 0.26908893937348743
Iteration 130, Training loss = 0.2672564954864488
Iteration 140, Training loss = 0.26570426642028816
Iteration 150, Training loss = 0.264185037292522
Iteration 160, Training loss = 0.26280229300549185
Iteration 170, Training loss = 0.26167980507557387
Iteration 180, Training loss = 0.26068391306348343
Iteration 190, Training loss = 0.2596518554641317
Iteration 200, Training loss = 0.25879981772879423
Iteration 210, Training loss = 0.2578545078887778
Iteration 220, Training loss = 0.25707277478808066
Iteration 230, Training loss = 0.2561956866132434
Iteration 240, Training loss = 0.2552818505512889
Iteration 250, Training loss = 0.2545080891749472
Iteration 260, Training loss = 0.2536502506484708
Iteration 270, Training loss = 0.2527867028846002
Iteration 280, Training loss = 0.2518588914637416
Iteration 290, Training loss = 0.2510166449097975
Model training time: 131.58949518203735
Device: cuda
Iteration 0, Training loss = 1.0148600993664443
Iteration 10, Training loss = 0.47807036939169534
Iteration 20, Training loss = 0.3442243150554904
Iteration 30, Training loss = 0.3169082187964033
Iteration 40, Training loss = 0.303474201236741
Iteration 50, Training loss = 0.29517797597167567
Iteration 60, Training loss = 0.28859189188797885
Iteration 70, Training loss = 0.2832590846331299
Iteration 80, Training loss = 0.2786790569996141
Iteration 90, Training loss = 0.2745965589694769
Iteration 100, Training loss = 0.27094221907074745
Iteration 110, Training loss = 0.2679668286991177
Iteration 120, Training loss = 0.2655395081415592
Iteration 130, Training loss = 0.26346770329423447
Iteration 140, Training loss = 0.26152187847412817
Iteration 150, Training loss = 0.26011515231960913
Iteration 160, Training loss = 0.25877020213996527
Iteration 170, Training loss = 0.2576467703495707
Iteration 180, Training loss = 0.2566564724606983
Iteration 190, Training loss = 0.2556104004094445
Iteration 200, Training loss = 0.2549541156928418
Iteration 210, Training loss = 0.2540370667799766
Iteration 220, Training loss = 0.2533235104876338
Iteration 230, Training loss = 0.2527197513900715
Iteration 240, Training loss = 0.25210305379103803
Iteration 250, Training loss = 0.251429156488209
Iteration 260, Training loss = 0.2506563678095185
Iteration 270, Training loss = 0.2500118993348994
Iteration 280, Training loss = 0.24935360752569272
Iteration 290, Training loss = 0.24895993548898662
Model training time: 130.38255906105042
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6050060970153993
Iteration 10, Training loss = 0.25607847415145313
Iteration 20, Training loss = 0.2476306893382465
Iteration 30, Training loss = 0.24131283488746993
Iteration 40, Training loss = 0.23516470282259633
Iteration 50, Training loss = 0.22997847486278336
Iteration 60, Training loss = 0.2273336396378986
Iteration 70, Training loss = 0.2241402666285165
Iteration 80, Training loss = 0.22158641464665496
Iteration 90, Training loss = 0.21926922024595247
Iteration 100, Training loss = 0.21720491052034693
Iteration 110, Training loss = 0.21566169742341482
Iteration 120, Training loss = 0.21551321244845956
Iteration 130, Training loss = 0.21323339147696194
Iteration 140, Training loss = 0.2137443671503598
Iteration 150, Training loss = 0.21285948722869086
Iteration 160, Training loss = 0.21301834306907422
Iteration 170, Training loss = 0.21195155810609856
Iteration 180, Training loss = 0.2109175106785632
Iteration 190, Training loss = 0.21123341467348772
Iteration 200, Training loss = 0.21100327333764649
Iteration 210, Training loss = 0.2095296048890736
Iteration 220, Training loss = 0.20981473383689908
Iteration 230, Training loss = 0.21065109345800362
Iteration 240, Training loss = 0.20921760094836894
Iteration 250, Training loss = 0.21009867589641137
Iteration 260, Training loss = 0.20878957201147194
Iteration 270, Training loss = 0.2087352665154755
Iteration 280, Training loss = 0.20854306171397896
Iteration 290, Training loss = 0.20829516716896765
Model training time: 155.55315828323364
Device: cuda
Iteration 0, Training loss = 0.6336885215150819
Iteration 10, Training loss = 0.2606312334898309
Iteration 20, Training loss = 0.24781818337936956
Iteration 30, Training loss = 0.23974889018922393
Iteration 40, Training loss = 0.23324039262735238
Iteration 50, Training loss = 0.22747774423972747
Iteration 60, Training loss = 0.22285759988405515
Iteration 70, Training loss = 0.2192897807274546
Iteration 80, Training loss = 0.21654104031893012
Iteration 90, Training loss = 0.21485178805797497
Iteration 100, Training loss = 0.21356573991351208
Iteration 110, Training loss = 0.21384021418832116
Iteration 120, Training loss = 0.21285381776982013
Iteration 130, Training loss = 0.21245484415793534
Iteration 140, Training loss = 0.21178529197778598
Iteration 150, Training loss = 0.21227454605171814
Iteration 160, Training loss = 0.21113039211304654
Iteration 170, Training loss = 0.21110409188826204
Iteration 180, Training loss = 0.21065719064875318
Iteration 190, Training loss = 0.2097768486899123
Iteration 200, Training loss = 0.2094528402402528
Iteration 210, Training loss = 0.20963932321233264
Iteration 220, Training loss = 0.20897696956487025
Iteration 230, Training loss = 0.20889768805399933
Iteration 240, Training loss = 0.20968459059482047
Iteration 250, Training loss = 0.20792700951021462
Iteration 260, Training loss = 0.2078273315388556
Iteration 270, Training loss = 0.2078337528659414
Iteration 280, Training loss = 0.20852258511013905
Iteration 290, Training loss = 0.2073673044238916
Model training time: 156.54917120933533
Device: cuda
Iteration 0, Training loss = 0.9877431387618437
Iteration 10, Training loss = 0.2523378398422756
Iteration 20, Training loss = 0.2456393793910814
Iteration 30, Training loss = 0.24093771862998137
Iteration 40, Training loss = 0.23914936527645905
Iteration 50, Training loss = 0.23518645009319084
Iteration 60, Training loss = 0.23175469864403364
Iteration 70, Training loss = 0.2287689052684544
Iteration 80, Training loss = 0.2255600543092873
Iteration 90, Training loss = 0.2233075124299555
Iteration 100, Training loss = 0.22111012753252543
Iteration 110, Training loss = 0.2193988624951164
Iteration 120, Training loss = 0.21726079022660671
Iteration 130, Training loss = 0.2163276055480464
Iteration 140, Training loss = 0.2156523490284026
Iteration 150, Training loss = 0.2130508837865138
Iteration 160, Training loss = 0.2138316047086987
Iteration 170, Training loss = 0.21246965082484065
Iteration 180, Training loss = 0.2123497904032541
Iteration 190, Training loss = 0.2111340844349769
Iteration 200, Training loss = 0.21087772788854256
Iteration 210, Training loss = 0.2102818463367498
Iteration 220, Training loss = 0.21013436195853258
Iteration 230, Training loss = 0.2097460906005512
Iteration 240, Training loss = 0.20902941517585993
Iteration 250, Training loss = 0.20857556000099345
Iteration 260, Training loss = 0.20856734610570835
Iteration 270, Training loss = 0.20834625522283606
Iteration 280, Training loss = 0.2079321142132721
Iteration 290, Training loss = 0.20769721896502644
Model training time: 161.1037130355835
Device: cuda
Iteration 0, Training loss = 0.6274234542474331
Iteration 10, Training loss = 0.25587661463325306
Iteration 20, Training loss = 0.24543871043236434
Iteration 30, Training loss = 0.23873090964108346
Iteration 40, Training loss = 0.2349070407919243
Iteration 50, Training loss = 0.23286553698684342
Iteration 60, Training loss = 0.22980891046346533
Iteration 70, Training loss = 0.22767823310365978
Iteration 80, Training loss = 0.2263336391988736
Iteration 90, Training loss = 0.22382274518109696
Iteration 100, Training loss = 0.22102126286119295
Iteration 110, Training loss = 0.2188828669253908
Iteration 120, Training loss = 0.21773446316111464
Iteration 130, Training loss = 0.21641772991772426
Iteration 140, Training loss = 0.21597906123603228
Iteration 150, Training loss = 0.21478798903398594
Iteration 160, Training loss = 0.21379768002336308
Iteration 170, Training loss = 0.21523519572925914
Iteration 180, Training loss = 0.21369601258462337
Iteration 190, Training loss = 0.21335516676090244
Iteration 200, Training loss = 0.2131136873191263
Iteration 210, Training loss = 0.21242639168050617
Iteration 220, Training loss = 0.2121210905382766
Iteration 230, Training loss = 0.21220120747932222
Iteration 240, Training loss = 0.21158399683655607
Iteration 250, Training loss = 0.21102739847998064
Iteration 260, Training loss = 0.21115019880505798
Iteration 270, Training loss = 0.21064449348836486
Iteration 280, Training loss = 0.2100873619642731
Iteration 290, Training loss = 0.21006430638615792
Model training time: 155.14859056472778
Device: cuda
Iteration 0, Training loss = 0.5842486808288473
Iteration 10, Training loss = 0.25358252921852015
Iteration 20, Training loss = 0.24493208746752784
Iteration 30, Training loss = 0.24151910249748185
Iteration 40, Training loss = 0.23931557428966713
Iteration 50, Training loss = 0.23524437943974072
Iteration 60, Training loss = 0.23332993420107023
Iteration 70, Training loss = 0.23139526559711945
Iteration 80, Training loss = 0.2296070968033182
Iteration 90, Training loss = 0.2278879421783417
Iteration 100, Training loss = 0.22695789166859218
Iteration 110, Training loss = 0.22588973129777007
Iteration 120, Training loss = 0.22397849039506104
Iteration 130, Training loss = 0.22260519274066204
Iteration 140, Training loss = 0.22077280946851643
Iteration 150, Training loss = 0.22005196071746275
Iteration 160, Training loss = 0.21994939501471727
Iteration 170, Training loss = 0.21860027263622017
Iteration 180, Training loss = 0.21755216721688864
Iteration 190, Training loss = 0.21659915003988703
Iteration 200, Training loss = 0.2149332021306704
Iteration 210, Training loss = 0.21467058292024072
Iteration 220, Training loss = 0.21366117445449853
Iteration 230, Training loss = 0.21344644603371332
Iteration 240, Training loss = 0.21269265607396282
Iteration 250, Training loss = 0.21198520928476972
Iteration 260, Training loss = 0.21152791041727506
Iteration 270, Training loss = 0.21061768151873828
Iteration 280, Training loss = 0.210761309662252
Iteration 290, Training loss = 0.21080826331137456
Model training time: 155.9595227241516
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0118079017351673
Iteration 10, Training loss = 0.7196778191781217
Iteration 20, Training loss = 0.3607334908431725
Iteration 30, Training loss = 0.3176986332306273
Iteration 40, Training loss = 0.2994609224918968
Iteration 50, Training loss = 0.2893407452698193
Iteration 60, Training loss = 0.2824753022980459
Iteration 70, Training loss = 0.27730017794030054
Iteration 80, Training loss = 0.27330960104237456
Iteration 90, Training loss = 0.27053297419308464
Iteration 100, Training loss = 0.26838770005446083
Iteration 110, Training loss = 0.26653177269506684
Iteration 120, Training loss = 0.2650695955313147
Iteration 130, Training loss = 0.26364383216920256
Iteration 140, Training loss = 0.26249200008250323
Iteration 150, Training loss = 0.2613397620564223
Iteration 160, Training loss = 0.26022163637830326
Iteration 170, Training loss = 0.25927956853305456
Iteration 180, Training loss = 0.2583847466865261
Iteration 190, Training loss = 0.257783409541802
Iteration 200, Training loss = 0.2570105507681214
Iteration 210, Training loss = 0.25640506113340433
Iteration 220, Training loss = 0.2556415853827975
Iteration 230, Training loss = 0.25491856568950716
Iteration 240, Training loss = 0.25435452462685887
Iteration 250, Training loss = 0.25381295557965955
Iteration 260, Training loss = 0.2534784271176589
Iteration 270, Training loss = 0.25302551134735274
Iteration 280, Training loss = 0.25249569394037163
Iteration 290, Training loss = 0.25216334701017373
Model training time: 131.4902937412262
Device: cuda
Iteration 0, Training loss = 1.0329325716518605
Iteration 10, Training loss = 0.5834788237732201
Iteration 20, Training loss = 0.36531060371214197
Iteration 30, Training loss = 0.32282366230181864
Iteration 40, Training loss = 0.3056025090864149
Iteration 50, Training loss = 0.29503719979495746
Iteration 60, Training loss = 0.28784187446567105
Iteration 70, Training loss = 0.2823320554510733
Iteration 80, Training loss = 0.27787086995404225
Iteration 90, Training loss = 0.2740918727496923
Iteration 100, Training loss = 0.27070025719034757
Iteration 110, Training loss = 0.26800209355989324
Iteration 120, Training loss = 0.2656201523058928
Iteration 130, Training loss = 0.2638410405226539
Iteration 140, Training loss = 0.26245041061688856
Iteration 150, Training loss = 0.2610479709816325
Iteration 160, Training loss = 0.259930654638616
Iteration 170, Training loss = 0.2590546880377407
Iteration 180, Training loss = 0.2579748960838768
Iteration 190, Training loss = 0.25734844693765224
Iteration 200, Training loss = 0.2564525566002936
Iteration 210, Training loss = 0.25575869470004886
Iteration 220, Training loss = 0.25496242624723303
Iteration 230, Training loss = 0.2542751672842312
Iteration 240, Training loss = 0.2536596696075169
Iteration 250, Training loss = 0.25290242411178193
Iteration 260, Training loss = 0.25214891372018516
Iteration 270, Training loss = 0.2517558989672049
Iteration 280, Training loss = 0.2508928998041961
Iteration 290, Training loss = 0.25042746075345584
Model training time: 128.6265516281128
Device: cuda
Iteration 0, Training loss = 0.9938663936989071
Iteration 10, Training loss = 0.6796846567285552
Iteration 20, Training loss = 0.3632719652416054
Iteration 30, Training loss = 0.3212339521248173
Iteration 40, Training loss = 0.3036648600066834
Iteration 50, Training loss = 0.29470821830825134
Iteration 60, Training loss = 0.28909664281776976
Iteration 70, Training loss = 0.28482956322897723
Iteration 80, Training loss = 0.2814596164024482
Iteration 90, Training loss = 0.2784618478962931
Iteration 100, Training loss = 0.27561201604385355
Iteration 110, Training loss = 0.2731818377286124
Iteration 120, Training loss = 0.2707164355217689
Iteration 130, Training loss = 0.2686723296961253
Iteration 140, Training loss = 0.266896121402341
Iteration 150, Training loss = 0.26532550267965394
Iteration 160, Training loss = 0.2640244646886359
Iteration 170, Training loss = 0.2628591295521138
Iteration 180, Training loss = 0.261639587130154
Iteration 190, Training loss = 0.26054928265962995
Iteration 200, Training loss = 0.2594000646443113
Iteration 210, Training loss = 0.2586711236697421
Iteration 220, Training loss = 0.2578465192247245
Iteration 230, Training loss = 0.25705684573540677
Iteration 240, Training loss = 0.25636936192001614
Iteration 250, Training loss = 0.25583058702094214
Iteration 260, Training loss = 0.25521437025495936
Iteration 270, Training loss = 0.25459131883506914
Iteration 280, Training loss = 0.25417125892552567
Iteration 290, Training loss = 0.25352210273682063
Model training time: 131.98444986343384
Device: cuda
Iteration 0, Training loss = 1.0595490435278156
Iteration 10, Training loss = 0.46504366448370077
Iteration 20, Training loss = 0.34227517714439815
Iteration 30, Training loss = 0.30963330730326816
Iteration 40, Training loss = 0.29686047438054053
Iteration 50, Training loss = 0.28900355221500695
Iteration 60, Training loss = 0.28270409837112587
Iteration 70, Training loss = 0.27741034408314175
Iteration 80, Training loss = 0.273141802547919
Iteration 90, Training loss = 0.26956614105089527
Iteration 100, Training loss = 0.26687220171923787
Iteration 110, Training loss = 0.2647926630249324
Iteration 120, Training loss = 0.2630703988469253
Iteration 130, Training loss = 0.26181470332148576
Iteration 140, Training loss = 0.26064422116874203
Iteration 150, Training loss = 0.25956380369083065
Iteration 160, Training loss = 0.25874023640992855
Iteration 170, Training loss = 0.257988070532427
Iteration 180, Training loss = 0.2570842247068449
Iteration 190, Training loss = 0.25646840133332166
Iteration 200, Training loss = 0.2557922678370453
Iteration 210, Training loss = 0.2552019476024637
Iteration 220, Training loss = 0.25463688972498544
Iteration 230, Training loss = 0.25400938726871414
Iteration 240, Training loss = 0.25355796886666637
Iteration 250, Training loss = 0.2529809265498965
Iteration 260, Training loss = 0.25266515588067634
Iteration 270, Training loss = 0.25196538214316955
Iteration 280, Training loss = 0.25151050040277384
Iteration 290, Training loss = 0.2510039165038751
Model training time: 133.37605690956116
Device: cuda
Iteration 0, Training loss = 1.05530472341519
Iteration 10, Training loss = 0.9295606018555944
Iteration 20, Training loss = 0.4047267710830628
Iteration 30, Training loss = 0.33231974919901636
Iteration 40, Training loss = 0.3063357141383046
Iteration 50, Training loss = 0.2938158135437215
Iteration 60, Training loss = 0.28608699525384
Iteration 70, Training loss = 0.28063641968419994
Iteration 80, Training loss = 0.2762859731407489
Iteration 90, Training loss = 0.272646541005907
Iteration 100, Training loss = 0.269594458353505
Iteration 110, Training loss = 0.26689175693160394
Iteration 120, Training loss = 0.2648028108419864
Iteration 130, Training loss = 0.26295193405618966
Iteration 140, Training loss = 0.26131912984729677
Iteration 150, Training loss = 0.260079299422211
Iteration 160, Training loss = 0.2587936455451547
Iteration 170, Training loss = 0.2577720102527095
Iteration 180, Training loss = 0.25706503859588076
Iteration 190, Training loss = 0.2562372327464256
Iteration 200, Training loss = 0.2553732151630138
Iteration 210, Training loss = 0.2547274439733196
Iteration 220, Training loss = 0.2540709315900653
Iteration 230, Training loss = 0.2532568933311444
Iteration 240, Training loss = 0.2525893721738537
Iteration 250, Training loss = 0.2518486041061526
Iteration 260, Training loss = 0.25131114288154294
Iteration 270, Training loss = 0.2506089651231038
Iteration 280, Training loss = 0.24998492359682087
Iteration 290, Training loss = 0.249317448523085
Model training time: 130.7138693332672
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6586150343712537
Iteration 10, Training loss = 0.25596505569488026
Iteration 20, Training loss = 0.24470332916149504
Iteration 30, Training loss = 0.2329305558675138
Iteration 40, Training loss = 0.22809857161620917
Iteration 50, Training loss = 0.22168179455305703
Iteration 60, Training loss = 0.21770516885827876
Iteration 70, Training loss = 0.21647871117903592
Iteration 80, Training loss = 0.21453234906923974
Iteration 90, Training loss = 0.2128878563065217
Iteration 100, Training loss = 0.21101475189514368
Iteration 110, Training loss = 0.2096143136337652
Iteration 120, Training loss = 0.2088239368583475
Iteration 130, Training loss = 0.20805234429550518
Iteration 140, Training loss = 0.20703873943907297
Iteration 150, Training loss = 0.20613039824344923
Iteration 160, Training loss = 0.20516786541221504
Iteration 170, Training loss = 0.2041858553561691
Iteration 180, Training loss = 0.20382482890427545
Iteration 190, Training loss = 0.20318689380409355
Iteration 200, Training loss = 0.20329486876365346
Iteration 210, Training loss = 0.20351549790897033
Iteration 220, Training loss = 0.20250094184070175
Iteration 230, Training loss = 0.2021511958925545
Iteration 240, Training loss = 0.20146167146596725
Iteration 250, Training loss = 0.19991850725783275
Iteration 260, Training loss = 0.20094261048745013
Iteration 270, Training loss = 0.20011059467209452
Iteration 280, Training loss = 0.20057835192678916
Iteration 290, Training loss = 0.19959695525377194
Model training time: 155.91581678390503
Device: cuda
Iteration 0, Training loss = 0.5710608610481962
Iteration 10, Training loss = 0.2557996156429263
Iteration 20, Training loss = 0.2463356171335493
Iteration 30, Training loss = 0.23800377309610998
Iteration 40, Training loss = 0.23188998209217848
Iteration 50, Training loss = 0.22723354489404987
Iteration 60, Training loss = 0.22369159372031833
Iteration 70, Training loss = 0.22110163980585323
Iteration 80, Training loss = 0.2194425392147271
Iteration 90, Training loss = 0.21738884441067463
Iteration 100, Training loss = 0.21606571598037103
Iteration 110, Training loss = 0.21480103584849805
Iteration 120, Training loss = 0.21356838806272996
Iteration 130, Training loss = 0.21277938039914748
Iteration 140, Training loss = 0.21236310137963468
Iteration 150, Training loss = 0.21137555600222896
Iteration 160, Training loss = 0.21045499964285705
Iteration 170, Training loss = 0.20921071294072754
Iteration 180, Training loss = 0.20852801581668795
Iteration 190, Training loss = 0.20727788766092883
Iteration 200, Training loss = 0.2073040305756339
Iteration 210, Training loss = 0.20767381335394433
Iteration 220, Training loss = 0.20601290330002153
Iteration 230, Training loss = 0.20525261302440276
Iteration 240, Training loss = 0.20533144216055443
Iteration 250, Training loss = 0.20511090007698565
Iteration 260, Training loss = 0.20462623454143292
Iteration 270, Training loss = 0.20463080374077502
Iteration 280, Training loss = 0.20480904479220186
Iteration 290, Training loss = 0.20438163451507652
Model training time: 154.18122673034668
Device: cuda
Iteration 0, Training loss = 0.5433210995923232
Iteration 10, Training loss = 0.257700087777711
Iteration 20, Training loss = 0.2444899074065772
Iteration 30, Training loss = 0.2348582234242638
Iteration 40, Training loss = 0.22831477441308573
Iteration 50, Training loss = 0.22314905248961206
Iteration 60, Training loss = 0.21911410265771297
Iteration 70, Training loss = 0.21668789628667634
Iteration 80, Training loss = 0.2112946196235987
Iteration 90, Training loss = 0.20789299840355613
Iteration 100, Training loss = 0.2040726527319116
Iteration 110, Training loss = 0.20387271874681223
Iteration 120, Training loss = 0.20123561137055945
Iteration 130, Training loss = 0.19973647132624148
Iteration 140, Training loss = 0.19887689894322333
Iteration 150, Training loss = 0.19772579678792065
Iteration 160, Training loss = 0.1956411905261997
Iteration 170, Training loss = 0.1960608003972229
Iteration 180, Training loss = 0.19508975167864748
Iteration 190, Training loss = 0.19470548947267324
Iteration 200, Training loss = 0.19372865289774416
Iteration 210, Training loss = 0.1933740927109418
Iteration 220, Training loss = 0.19297565577538192
Iteration 230, Training loss = 0.19228808781172693
Iteration 240, Training loss = 0.1927126376882881
Iteration 250, Training loss = 0.19233882274776337
Iteration 260, Training loss = 0.19180096086953513
Iteration 270, Training loss = 0.19187513467366413
Iteration 280, Training loss = 0.19199898767738308
Iteration 290, Training loss = 0.19136623420344426
Model training time: 156.9077444076538
Device: cuda
Iteration 0, Training loss = 0.5155369652385573
Iteration 10, Training loss = 0.24730688611329613
Iteration 20, Training loss = 0.2364605834603887
Iteration 30, Training loss = 0.2299828910005006
Iteration 40, Training loss = 0.2241967190282541
Iteration 50, Training loss = 0.21908690036245176
Iteration 60, Training loss = 0.21464410209554738
Iteration 70, Training loss = 0.21121930235378966
Iteration 80, Training loss = 0.20901543969969483
Iteration 90, Training loss = 0.2072685979685541
Iteration 100, Training loss = 0.20507881653294435
Iteration 110, Training loss = 0.20437821155959698
Iteration 120, Training loss = 0.2024614670230459
Iteration 130, Training loss = 0.2016144662951153
Iteration 140, Training loss = 0.20010892231441293
Iteration 150, Training loss = 0.19973948116236104
Iteration 160, Training loss = 0.19945616427511337
Iteration 170, Training loss = 0.1989878218792397
Iteration 180, Training loss = 0.19823306320871048
Iteration 190, Training loss = 0.1967794880649801
Iteration 200, Training loss = 0.19703592074985365
Iteration 210, Training loss = 0.19646289702008768
Iteration 220, Training loss = 0.19588456859419767
Iteration 230, Training loss = 0.19611109169899119
Iteration 240, Training loss = 0.1959737777691921
Iteration 250, Training loss = 0.1954005413641364
Iteration 260, Training loss = 0.19542779690489065
Iteration 270, Training loss = 0.19458112379955467
Iteration 280, Training loss = 0.19480637199364909
Iteration 290, Training loss = 0.19466123376357353
Model training time: 155.91482067108154
Device: cuda
Iteration 0, Training loss = 0.5474195704766105
Iteration 10, Training loss = 0.25088175173316685
Iteration 20, Training loss = 0.24441983783504864
Iteration 30, Training loss = 0.23841607996395656
Iteration 40, Training loss = 0.2335942950253048
Iteration 50, Training loss = 0.2306724712252617
Iteration 60, Training loss = 0.2286972426978497
Iteration 70, Training loss = 0.22724199998609668
Iteration 80, Training loss = 0.22561670761311892
Iteration 90, Training loss = 0.2241774160892998
Iteration 100, Training loss = 0.22191563693323954
Iteration 110, Training loss = 0.2211211491691864
Iteration 120, Training loss = 0.21988989843944082
Iteration 130, Training loss = 0.21812403869542313
Iteration 140, Training loss = 0.21566007848260765
Iteration 150, Training loss = 0.2151707930549005
Iteration 160, Training loss = 0.2134869693988172
Iteration 170, Training loss = 0.21259191766669905
Iteration 180, Training loss = 0.2114979250866189
Iteration 190, Training loss = 0.20963117182615595
Iteration 200, Training loss = 0.20818134739129077
Iteration 210, Training loss = 0.20809741921668767
Iteration 220, Training loss = 0.2069474705213063
Iteration 230, Training loss = 0.2047741541742701
Iteration 240, Training loss = 0.20475969795525506
Iteration 250, Training loss = 0.2022500190496012
Iteration 260, Training loss = 0.20198486360200382
Iteration 270, Training loss = 0.20205041462190215
Iteration 280, Training loss = 0.20114977315032165
Iteration 290, Training loss = 0.20053584655390524
Model training time: 159.62339806556702
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 32, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0109179260944339
Iteration 10, Training loss = 0.5501367399608829
Iteration 20, Training loss = 0.35666309138331515
Iteration 30, Training loss = 0.323276892611103
Iteration 40, Training loss = 0.30896534409924226
Iteration 50, Training loss = 0.3005200856281828
Iteration 60, Training loss = 0.29424028652415724
Iteration 70, Training loss = 0.2889992726412004
Iteration 80, Training loss = 0.28427333489385126
Iteration 90, Training loss = 0.28015459853858116
Iteration 100, Training loss = 0.2768521245217208
Iteration 110, Training loss = 0.2739754608735334
Iteration 120, Training loss = 0.27142315572601255
Iteration 130, Training loss = 0.2693945434188006
Iteration 140, Training loss = 0.2677227773999042
Iteration 150, Training loss = 0.2664622284165306
Iteration 160, Training loss = 0.26531522421585735
Iteration 170, Training loss = 0.2643453502626165
Iteration 180, Training loss = 0.2634099015636825
Iteration 190, Training loss = 0.2625761024527631
Iteration 200, Training loss = 0.26181886105214136
Iteration 210, Training loss = 0.2610398478824059
Iteration 220, Training loss = 0.2602910239781652
Iteration 230, Training loss = 0.2596587593561223
Iteration 240, Training loss = 0.2590843903816352
Iteration 250, Training loss = 0.2584487315601067
Iteration 260, Training loss = 0.2577567354530168
Iteration 270, Training loss = 0.25712033077064206
Iteration 280, Training loss = 0.25673276756058017
Iteration 290, Training loss = 0.2560732932377958
Model training time: 135.10490775108337
Device: cuda
Iteration 0, Training loss = 0.9810992598389309
Iteration 10, Training loss = 0.4487451814097007
Iteration 20, Training loss = 0.33131661336445056
Iteration 30, Training loss = 0.30892990345527704
Iteration 40, Training loss = 0.2993991402570907
Iteration 50, Training loss = 0.2932461310746306
Iteration 60, Training loss = 0.2890350192640942
Iteration 70, Training loss = 0.2854654579164041
Iteration 80, Training loss = 0.2825282142829087
Iteration 90, Training loss = 0.2799905841633425
Iteration 100, Training loss = 0.2777580029125941
Iteration 110, Training loss = 0.2759275287389755
Iteration 120, Training loss = 0.27414885076714485
Iteration 130, Training loss = 0.2722269921737202
Iteration 140, Training loss = 0.2702689502234898
Iteration 150, Training loss = 0.2686728258362405
Iteration 160, Training loss = 0.26714818334752655
Iteration 170, Training loss = 0.2655903024116382
Iteration 180, Training loss = 0.2643391256543106
Iteration 190, Training loss = 0.2631563539041734
Iteration 200, Training loss = 0.26215764340779973
Iteration 210, Training loss = 0.2609795584246408
Iteration 220, Training loss = 0.26003740379726625
Iteration 230, Training loss = 0.2590234272957714
Iteration 240, Training loss = 0.2581785211731966
Iteration 250, Training loss = 0.25745320509550934
Iteration 260, Training loss = 0.25665592881185667
Iteration 270, Training loss = 0.2559875114290968
Iteration 280, Training loss = 0.2552571321767699
Iteration 290, Training loss = 0.2543280390034865
Model training time: 131.07941555976868
Device: cuda
Iteration 0, Training loss = 1.0001749209428237
Iteration 10, Training loss = 0.4260328519214441
Iteration 20, Training loss = 0.34420922992088027
Iteration 30, Training loss = 0.31765695240970965
Iteration 40, Training loss = 0.3059534353292017
Iteration 50, Training loss = 0.29932565308800624
Iteration 60, Training loss = 0.2943236571319455
Iteration 70, Training loss = 0.29007225231093875
Iteration 80, Training loss = 0.2863577899069821
Iteration 90, Training loss = 0.28290193771478916
Iteration 100, Training loss = 0.27969427704450295
Iteration 110, Training loss = 0.27678903093060914
Iteration 120, Training loss = 0.27406076607057606
Iteration 130, Training loss = 0.2715628033652721
Iteration 140, Training loss = 0.2695571481459943
Iteration 150, Training loss = 0.26778974122487315
Iteration 160, Training loss = 0.26597524127645583
Iteration 170, Training loss = 0.264591121554519
Iteration 180, Training loss = 0.2632086002978228
Iteration 190, Training loss = 0.26198102373259985
Iteration 200, Training loss = 0.260991775311223
Iteration 210, Training loss = 0.2600983792406306
Iteration 220, Training loss = 0.2591555590130226
Iteration 230, Training loss = 0.25829686340819547
Iteration 240, Training loss = 0.2578455032758216
Iteration 250, Training loss = 0.2570233659651897
Iteration 260, Training loss = 0.2564696912218526
Iteration 270, Training loss = 0.25583912246380247
Iteration 280, Training loss = 0.25549577566238063
Iteration 290, Training loss = 0.254877555150241
Model training time: 133.00395917892456
Device: cuda
Iteration 0, Training loss = 1.004594570210713
Iteration 10, Training loss = 0.5006186360308391
Iteration 20, Training loss = 0.34128291432274455
Iteration 30, Training loss = 0.3103753810536486
Iteration 40, Training loss = 0.2970411840095647
Iteration 50, Training loss = 0.29016143541792117
Iteration 60, Training loss = 0.2854767365439752
Iteration 70, Training loss = 0.2815597069609829
Iteration 80, Training loss = 0.2779597232330221
Iteration 90, Training loss = 0.27469044088523553
Iteration 100, Training loss = 0.2716989757506668
Iteration 110, Training loss = 0.26895551520600447
Iteration 120, Training loss = 0.26683353158232664
Iteration 130, Training loss = 0.2649923688562961
Iteration 140, Training loss = 0.26333534300832423
Iteration 150, Training loss = 0.26198955331241247
Iteration 160, Training loss = 0.26055714334688235
Iteration 170, Training loss = 0.2595916183318122
Iteration 180, Training loss = 0.25852004000263124
Iteration 190, Training loss = 0.25750639567435796
Iteration 200, Training loss = 0.25658167678565147
Iteration 210, Training loss = 0.2557570874131765
Iteration 220, Training loss = 0.2551059670223162
Iteration 230, Training loss = 0.25433596523059193
Iteration 240, Training loss = 0.25354512910793825
Iteration 250, Training loss = 0.25294425742605986
Iteration 260, Training loss = 0.2523869159146602
Iteration 270, Training loss = 0.25180438544249417
Iteration 280, Training loss = 0.2511197003844864
Iteration 290, Training loss = 0.25068373215039763
Model training time: 138.66913604736328
Device: cuda
Iteration 0, Training loss = 1.097026935140388
Iteration 10, Training loss = 0.7856854430916235
Iteration 20, Training loss = 0.35641406797612263
Iteration 30, Training loss = 0.32088575764732846
Iteration 40, Training loss = 0.30583848988244955
Iteration 50, Training loss = 0.29763061751681436
Iteration 60, Training loss = 0.29124316776094655
Iteration 70, Training loss = 0.28574280585128514
Iteration 80, Training loss = 0.28046099237249783
Iteration 90, Training loss = 0.2761449916310807
Iteration 100, Training loss = 0.27243877638843966
Iteration 110, Training loss = 0.26940616241884
Iteration 120, Training loss = 0.26688171932180627
Iteration 130, Training loss = 0.2648030031138702
Iteration 140, Training loss = 0.2630998509379045
Iteration 150, Training loss = 0.2616223056618007
Iteration 160, Training loss = 0.2602275092939488
Iteration 170, Training loss = 0.25929031273931913
Iteration 180, Training loss = 0.2581756372573012
Iteration 190, Training loss = 0.2570730322750948
Iteration 200, Training loss = 0.2560989005405447
Iteration 210, Training loss = 0.25536557371766455
Iteration 220, Training loss = 0.2546022906719051
Iteration 230, Training loss = 0.2541077111737203
Iteration 240, Training loss = 0.25326245523466034
Iteration 250, Training loss = 0.2527988527971376
Iteration 260, Training loss = 0.25208955858867915
Iteration 270, Training loss = 0.2514260354165448
Iteration 280, Training loss = 0.2509770504138083
Iteration 290, Training loss = 0.2505818646103504
Model training time: 134.70342111587524
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7983103459296019
Iteration 10, Training loss = 0.2626835746681633
Iteration 20, Training loss = 0.25306656546350836
Iteration 30, Training loss = 0.24810438124453965
Iteration 40, Training loss = 0.24361539023797868
Iteration 50, Training loss = 0.2398099039775738
Iteration 60, Training loss = 0.23555396810389947
Iteration 70, Training loss = 0.23209033535730436
Iteration 80, Training loss = 0.22957721348977894
Iteration 90, Training loss = 0.2260965636530936
Model training time: 30.03602385520935
Device: cuda
Iteration 0, Training loss = 1.0101257546800346
Iteration 10, Training loss = 0.2643989209031713
Iteration 20, Training loss = 0.2565394088913853
Iteration 30, Training loss = 0.2542448492657735
Iteration 40, Training loss = 0.2510333850907819
Iteration 50, Training loss = 0.2487488251881323
Iteration 60, Training loss = 0.2459225911953023
Iteration 70, Training loss = 0.2418465335731921
Iteration 80, Training loss = 0.23795062551895776
Iteration 90, Training loss = 0.23515575652681112
Model training time: 29.57921051979065
Device: cuda
Iteration 0, Training loss = 0.8600879809706683
Iteration 10, Training loss = 0.26084795813773565
Iteration 20, Training loss = 0.24812079274999924
Iteration 30, Training loss = 0.24022894086325225
Iteration 40, Training loss = 0.2334349913081685
Iteration 50, Training loss = 0.22828006103706822
Iteration 60, Training loss = 0.22435893733864246
Iteration 70, Training loss = 0.2213843061728178
Iteration 80, Training loss = 0.2180788473446588
Iteration 90, Training loss = 0.21677942026928426
Model training time: 30.305227994918823
Device: cuda
Iteration 0, Training loss = 0.848491493774497
Iteration 10, Training loss = 0.25969635882383385
Iteration 20, Training loss = 0.2504899389501931
Iteration 30, Training loss = 0.2457726636658544
Iteration 40, Training loss = 0.24271706527701897
Iteration 50, Training loss = 0.24021932126387305
Iteration 60, Training loss = 0.23739460747743
Iteration 70, Training loss = 0.23483647294522483
Iteration 80, Training loss = 0.23198877562935225
Iteration 90, Training loss = 0.2294276103305356
Model training time: 29.815380096435547
Device: cuda
Iteration 0, Training loss = 0.7641107572449578
Iteration 10, Training loss = 0.26154386579702443
Iteration 20, Training loss = 0.25044948265748324
Iteration 30, Training loss = 0.24494743185198825
Iteration 40, Training loss = 0.24183504585770593
Iteration 50, Training loss = 0.23947910172639839
Iteration 60, Training loss = 0.2371236931010721
Iteration 70, Training loss = 0.23499240902598928
Iteration 80, Training loss = 0.23130286060237654
Iteration 90, Training loss = 0.2294581389585555
Model training time: 29.748136520385742
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0002540853864328
Iteration 10, Training loss = 0.9750381771493073
Iteration 20, Training loss = 0.9420322692336668
Iteration 30, Training loss = 0.8761308368853324
Iteration 40, Training loss = 0.7482333481311798
Iteration 50, Training loss = 0.5706128074207167
Iteration 60, Training loss = 0.43743060594019684
Iteration 70, Training loss = 0.3833087460286375
Iteration 80, Training loss = 0.3593870466889966
Iteration 90, Training loss = 0.3450024526044366
Model training time: 26.634990692138672
Device: cuda
Iteration 0, Training loss = 1.0047889006310615
Iteration 10, Training loss = 0.9427754101545914
Iteration 20, Training loss = 0.8314755749011385
Iteration 30, Training loss = 0.613296450768116
Iteration 40, Training loss = 0.4348485474857155
Iteration 50, Training loss = 0.38310626422725436
Iteration 60, Training loss = 0.36229682163051935
Iteration 70, Training loss = 0.34621890976233183
Iteration 80, Training loss = 0.33448955265508185
Iteration 90, Training loss = 0.32523423973200977
Model training time: 26.053373098373413
Device: cuda
Iteration 0, Training loss = 1.2895792195762412
Iteration 10, Training loss = 0.9935471018155416
Iteration 20, Training loss = 0.9715985367839463
Iteration 30, Training loss = 0.9255349480587504
Iteration 40, Training loss = 0.8048895813416743
Iteration 50, Training loss = 0.5920840357092844
Iteration 60, Training loss = 0.44697131036560317
Iteration 70, Training loss = 0.4022917692787981
Iteration 80, Training loss = 0.3810726448821561
Iteration 90, Training loss = 0.3642021303879466
Model training time: 25.541940450668335
Device: cuda
Iteration 0, Training loss = 0.9970306241569887
Iteration 10, Training loss = 0.9704845920565047
Iteration 20, Training loss = 0.9062282162300055
Iteration 30, Training loss = 0.7263669827997973
Iteration 40, Training loss = 0.49259634755083903
Iteration 50, Training loss = 0.3944879269974243
Iteration 60, Training loss = 0.36821311280347296
Iteration 70, Training loss = 0.3522901296759573
Iteration 80, Training loss = 0.33962009652801184
Iteration 90, Training loss = 0.3295269945393438
Model training time: 26.43193793296814
Device: cuda
Iteration 0, Training loss = 1.0725953455132562
Iteration 10, Training loss = 0.9769019930834931
Iteration 20, Training loss = 0.925796850004058
Iteration 30, Training loss = 0.7983999312787816
Iteration 40, Training loss = 0.5524109767542945
Iteration 50, Training loss = 0.40005845500939136
Iteration 60, Training loss = 0.3619772106553045
Iteration 70, Training loss = 0.3438522741558471
Iteration 80, Training loss = 0.3322844281432709
Iteration 90, Training loss = 0.3236646997755852
Model training time: 26.505224227905273
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9283415336827725
Iteration 10, Training loss = 0.27838757540580733
Iteration 20, Training loss = 0.26239718399617984
Iteration 30, Training loss = 0.2558451971570075
Iteration 40, Training loss = 0.2515663942997006
Iteration 50, Training loss = 0.24700578521703176
Iteration 60, Training loss = 0.24417365306385475
Iteration 70, Training loss = 0.23997208530056305
Iteration 80, Training loss = 0.2372857989464405
Iteration 90, Training loss = 0.23484122994297368
Model training time: 30.478538036346436
Device: cuda
Iteration 0, Training loss = 0.8886546026105466
Iteration 10, Training loss = 0.26816085963577463
Iteration 20, Training loss = 0.25787362722670976
Iteration 30, Training loss = 0.2550844548693026
Iteration 40, Training loss = 0.25381673487344225
Iteration 50, Training loss = 0.24948546481161302
Iteration 60, Training loss = 0.24696984473201963
Iteration 70, Training loss = 0.24504259818993904
Iteration 80, Training loss = 0.24388150590053503
Iteration 90, Training loss = 0.24284066054700076
Model training time: 29.423675775527954
Device: cuda
Iteration 0, Training loss = 0.9717720185500988
Iteration 10, Training loss = 0.27987954927527386
Iteration 20, Training loss = 0.2572610772030365
Iteration 30, Training loss = 0.25052524653177904
Iteration 40, Training loss = 0.24690465272768683
Iteration 50, Training loss = 0.2432719090782502
Iteration 60, Training loss = 0.24111891748464626
Iteration 70, Training loss = 0.23884933852199194
Iteration 80, Training loss = 0.23729968387723546
Iteration 90, Training loss = 0.23687965134923586
Model training time: 29.962926387786865
Device: cuda
Iteration 0, Training loss = 1.3459169435616276
Iteration 10, Training loss = 0.2831524407518083
Iteration 20, Training loss = 0.25796718690274417
Iteration 30, Training loss = 0.2510077260830552
Iteration 40, Training loss = 0.24674818837556287
Iteration 50, Training loss = 0.24454801317286376
Iteration 60, Training loss = 0.24084945213823503
Iteration 70, Training loss = 0.23865629721810852
Iteration 80, Training loss = 0.23695722470680872
Iteration 90, Training loss = 0.23495104228672775
Model training time: 30.766942262649536
Device: cuda
Iteration 0, Training loss = 0.9029887671343946
Iteration 10, Training loss = 0.2685486793806012
Iteration 20, Training loss = 0.2538482836911068
Iteration 30, Training loss = 0.2495409656981915
Iteration 40, Training loss = 0.24585075150941305
Iteration 50, Training loss = 0.24290800684892036
Iteration 60, Training loss = 0.24211357004832532
Iteration 70, Training loss = 0.24003834597730406
Iteration 80, Training loss = 0.23796051499492304
Iteration 90, Training loss = 0.23731253857629886
Model training time: 30.096646785736084
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.3092339583065198
Iteration 10, Training loss = 0.9815658552635119
Iteration 20, Training loss = 0.946070512999659
Iteration 30, Training loss = 0.8532666175261788
Iteration 40, Training loss = 0.661817941688685
Iteration 50, Training loss = 0.48123271756126107
Iteration 60, Training loss = 0.411623563023581
Iteration 70, Training loss = 0.38473813761259623
Iteration 80, Training loss = 0.3672668306147995
Iteration 90, Training loss = 0.3525244466204574
Model training time: 25.901984930038452
Device: cuda
Iteration 0, Training loss = 0.995748029526881
Iteration 10, Training loss = 0.9589313853190141
Iteration 20, Training loss = 0.9013024355478333
Iteration 30, Training loss = 0.7803143457226132
Iteration 40, Training loss = 0.6216336641334681
Iteration 50, Training loss = 0.5064156412502417
Iteration 60, Training loss = 0.4465235697762402
Iteration 70, Training loss = 0.4137170029003263
Iteration 80, Training loss = 0.39080761984926493
Iteration 90, Training loss = 0.372367601679719
Model training time: 25.44464373588562
Device: cuda
Iteration 0, Training loss = 1.048523972575791
Iteration 10, Training loss = 0.9333600517056414
Iteration 20, Training loss = 0.8076939001175516
Iteration 30, Training loss = 0.594328507853015
Iteration 40, Training loss = 0.4352975069468724
Iteration 50, Training loss = 0.382381040117015
Iteration 60, Training loss = 0.36062201533628546
Iteration 70, Training loss = 0.34597639336390196
Iteration 80, Training loss = 0.3346272121017106
Iteration 90, Training loss = 0.3257661332254824
Model training time: 26.26533079147339
Device: cuda
Iteration 0, Training loss = 1.0856159816041662
Iteration 10, Training loss = 0.9938697872530435
Iteration 20, Training loss = 0.9753488944348506
Iteration 30, Training loss = 0.9306416004752192
Iteration 40, Training loss = 0.8098519238584859
Iteration 50, Training loss = 0.5930834805619889
Iteration 60, Training loss = 0.4334384082740056
Iteration 70, Training loss = 0.38195404554334816
Iteration 80, Training loss = 0.35952125460917245
Iteration 90, Training loss = 0.34517933262719047
Model training time: 25.59102463722229
Device: cuda
Iteration 0, Training loss = 1.4288894744310978
Iteration 10, Training loss = 0.9925397959884238
Iteration 20, Training loss = 0.9808240342831266
Iteration 30, Training loss = 0.9610782999923264
Iteration 40, Training loss = 0.9137014481756423
Iteration 50, Training loss = 0.8024842269754641
Iteration 60, Training loss = 0.5972167590390081
Iteration 70, Training loss = 0.43148773620669967
Iteration 80, Training loss = 0.37267550293374174
Iteration 90, Training loss = 0.3497994680335556
Model training time: 25.683094024658203
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9104002318232531
Iteration 10, Training loss = 0.2635874748589912
Iteration 20, Training loss = 0.2541854202675359
Iteration 30, Training loss = 0.24741938908174993
Iteration 40, Training loss = 0.24404189674462673
Iteration 50, Training loss = 0.24154502300968492
Iteration 60, Training loss = 0.23895882286023404
Iteration 70, Training loss = 0.23691668505397973
Iteration 80, Training loss = 0.23487798898404347
Iteration 90, Training loss = 0.23409130964158237
Model training time: 30.47470498085022
Device: cuda
Iteration 0, Training loss = 0.8526798542858898
Iteration 10, Training loss = 0.2687644910265282
Iteration 20, Training loss = 0.2576977857860966
Iteration 30, Training loss = 0.25447672184394754
Iteration 40, Training loss = 0.25159744705986864
Iteration 50, Training loss = 0.2496490537904311
Iteration 60, Training loss = 0.24840471508422335
Iteration 70, Training loss = 0.24593014655193846
Iteration 80, Training loss = 0.2454201041356377
Iteration 90, Training loss = 0.24449177698236732
Model training time: 30.64403772354126
Device: cuda
Iteration 0, Training loss = 0.8461206821715774
Iteration 10, Training loss = 0.2707955719192247
Iteration 20, Training loss = 0.261515100437086
Iteration 30, Training loss = 0.2562905962844402
Iteration 40, Training loss = 0.253210275069527
Iteration 50, Training loss = 0.2502198838741307
Iteration 60, Training loss = 0.24875730466871446
Iteration 70, Training loss = 0.24518315080139372
Iteration 80, Training loss = 0.2434460959860668
Iteration 90, Training loss = 0.24209684299098122
Model training time: 29.91795253753662
Device: cuda
Iteration 0, Training loss = 0.8306904600726234
Iteration 10, Training loss = 0.2652152175992583
Iteration 20, Training loss = 0.25521106704853586
Iteration 30, Training loss = 0.25043851338722856
Iteration 40, Training loss = 0.2464497927306355
Iteration 50, Training loss = 0.24316396521053452
Iteration 60, Training loss = 0.24010937574549
Iteration 70, Training loss = 0.23754504801714477
Iteration 80, Training loss = 0.2343280484566942
Iteration 90, Training loss = 0.23227420265691867
Model training time: 29.553864002227783
Device: cuda
Iteration 0, Training loss = 0.9260817466726625
Iteration 10, Training loss = 0.2694429449989024
Iteration 20, Training loss = 0.25929988247616853
Iteration 30, Training loss = 0.2547265712333762
Iteration 40, Training loss = 0.2510740348106421
Iteration 50, Training loss = 0.24708052091120522
Iteration 60, Training loss = 0.24497179924577905
Iteration 70, Training loss = 0.2422267143133182
Iteration 80, Training loss = 0.2402730674487381
Iteration 90, Training loss = 0.23695435763701148
Model training time: 29.912473440170288
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0008694224311534
Iteration 10, Training loss = 0.9636445817164176
Iteration 20, Training loss = 0.88084794976861
Iteration 30, Training loss = 0.6876702471343792
Iteration 40, Training loss = 0.47286253346913104
Iteration 50, Training loss = 0.38407411847425543
Iteration 60, Training loss = 0.35652482099291205
Iteration 70, Training loss = 0.3387091262401014
Iteration 80, Training loss = 0.32675139390040137
Iteration 90, Training loss = 0.31700628139258585
Model training time: 25.898895502090454
Device: cuda
Iteration 0, Training loss = 1.003301829819518
Iteration 10, Training loss = 0.9811249401258386
Iteration 20, Training loss = 0.9417163614489606
Iteration 30, Training loss = 0.8409995479860167
Iteration 40, Training loss = 0.6374041900254678
Iteration 50, Training loss = 0.4573471022832797
Iteration 60, Training loss = 0.39381852638030396
Iteration 70, Training loss = 0.36918343877158877
Iteration 80, Training loss = 0.35373694820392537
Iteration 90, Training loss = 0.3425013733226896
Model training time: 25.88351011276245
Device: cuda
Iteration 0, Training loss = 0.997311498519879
Iteration 10, Training loss = 0.9863811305179688
Iteration 20, Training loss = 0.9691012291516659
Iteration 30, Training loss = 0.9145828974995636
Iteration 40, Training loss = 0.762169837663715
Iteration 50, Training loss = 0.5086027681107682
Iteration 60, Training loss = 0.397860668203681
Iteration 70, Training loss = 0.3712577352633223
Iteration 80, Training loss = 0.35663788301357324
Iteration 90, Training loss = 0.3451101708671321
Model training time: 26.080639123916626
Device: cuda
Iteration 0, Training loss = 0.9963346161128243
Iteration 10, Training loss = 0.970196048706626
Iteration 20, Training loss = 0.9259703573973282
Iteration 30, Training loss = 0.793252081974693
Iteration 40, Training loss = 0.546002494396219
Iteration 50, Training loss = 0.40797763163052897
Iteration 60, Training loss = 0.374007078256584
Iteration 70, Training loss = 0.35662882541112856
Iteration 80, Training loss = 0.3427208559092692
Iteration 90, Training loss = 0.33303012637700435
Model training time: 26.611724138259888
Device: cuda
Iteration 0, Training loss = 1.0060009625222948
Iteration 10, Training loss = 0.9797286178178833
Iteration 20, Training loss = 0.9411494746300333
Iteration 30, Training loss = 0.8263943445855293
Iteration 40, Training loss = 0.5873357429020647
Iteration 50, Training loss = 0.4109039230335162
Iteration 60, Training loss = 0.3647482698954246
Iteration 70, Training loss = 0.34710254581366184
Iteration 80, Training loss = 0.3344619624712617
Iteration 90, Training loss = 0.3244999708328846
Model training time: 26.1989164352417
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8262309637910502
Iteration 10, Training loss = 0.26127766702630095
Iteration 20, Training loss = 0.25368440669515857
Iteration 30, Training loss = 0.248450872114891
Iteration 40, Training loss = 0.24611239642768667
Iteration 50, Training loss = 0.2418873266177477
Iteration 60, Training loss = 0.23900521215465334
Iteration 70, Training loss = 0.23738819169969375
Iteration 80, Training loss = 0.2347287248082207
Iteration 90, Training loss = 0.23196515127800513
Iteration 100, Training loss = 0.22938236025508474
Iteration 110, Training loss = 0.2262858523838762
Iteration 120, Training loss = 0.22419788134126847
Iteration 130, Training loss = 0.22218114321214566
Iteration 140, Training loss = 0.2204590264869773
Iteration 150, Training loss = 0.21838526467338276
Iteration 160, Training loss = 0.21737656601961108
Iteration 170, Training loss = 0.21538421058136484
Iteration 180, Training loss = 0.2143771107789975
Iteration 190, Training loss = 0.21269346125747846
Model training time: 59.60275411605835
Device: cuda
Iteration 0, Training loss = 0.9227017617743948
Iteration 10, Training loss = 0.26473197099811213
Iteration 20, Training loss = 0.25409753656617684
Iteration 30, Training loss = 0.24955941304780435
Iteration 40, Training loss = 0.24606762538497576
Iteration 50, Training loss = 0.2415620569265695
Iteration 60, Training loss = 0.23997558494984816
Iteration 70, Training loss = 0.23761146245659262
Iteration 80, Training loss = 0.23534066449616842
Iteration 90, Training loss = 0.23337706516762285
Iteration 100, Training loss = 0.2312372046104376
Iteration 110, Training loss = 0.2298111986854802
Iteration 120, Training loss = 0.22694354695110505
Iteration 130, Training loss = 0.22410941779037605
Iteration 140, Training loss = 0.2222665838789249
Iteration 150, Training loss = 0.2201172526979792
Iteration 160, Training loss = 0.2180776032416717
Iteration 170, Training loss = 0.2157244499106914
Iteration 180, Training loss = 0.2151215176867402
Iteration 190, Training loss = 0.21293871021933025
Model training time: 59.649171590805054
Device: cuda
Iteration 0, Training loss = 0.7492140283907093
Iteration 10, Training loss = 0.2658870818750294
Iteration 20, Training loss = 0.2550321525493682
Iteration 30, Training loss = 0.24985493899543504
Iteration 40, Training loss = 0.2463076464507891
Iteration 50, Training loss = 0.24233353926219803
Iteration 60, Training loss = 0.24040614878353866
Iteration 70, Training loss = 0.23808363429158205
Iteration 80, Training loss = 0.23583978414535522
Iteration 90, Training loss = 0.2328087547119113
Iteration 100, Training loss = 0.2311992834587604
Iteration 110, Training loss = 0.22995729118153668
Iteration 120, Training loss = 0.22754150049553978
Iteration 130, Training loss = 0.2260192493741639
Iteration 140, Training loss = 0.22359761563332184
Iteration 150, Training loss = 0.22259488100734887
Iteration 160, Training loss = 0.22208644669269018
Iteration 170, Training loss = 0.21961821180178923
Iteration 180, Training loss = 0.21860188321358914
Iteration 190, Training loss = 0.21756149356923818
Model training time: 60.09309530258179
Device: cuda
Iteration 0, Training loss = 0.8317653335234969
Iteration 10, Training loss = 0.2616350006870026
Iteration 20, Training loss = 0.2518666591045361
Iteration 30, Training loss = 0.24738588921977703
Iteration 40, Training loss = 0.24320022085150658
Iteration 50, Training loss = 0.2405567839021844
Iteration 60, Training loss = 0.23813605042183456
Iteration 70, Training loss = 0.23563721894785977
Iteration 80, Training loss = 0.23345903832699366
Iteration 90, Training loss = 0.2311129193159117
Iteration 100, Training loss = 0.2292950186824453
Iteration 110, Training loss = 0.22619411262913028
Iteration 120, Training loss = 0.22511102172775546
Iteration 130, Training loss = 0.2231729685684333
Iteration 140, Training loss = 0.22055245262845127
Iteration 150, Training loss = 0.21900919864431095
Iteration 160, Training loss = 0.21749404346309423
Iteration 170, Training loss = 0.21595649680365686
Iteration 180, Training loss = 0.21319445096640197
Iteration 190, Training loss = 0.21208361053956304
Model training time: 59.444079637527466
Device: cuda
Iteration 0, Training loss = 0.8642353268637173
Iteration 10, Training loss = 0.26445750341467233
Iteration 20, Training loss = 0.25269111451031506
Iteration 30, Training loss = 0.24723517617597673
Iteration 40, Training loss = 0.2436531019455569
Iteration 50, Training loss = 0.2416805293465006
Iteration 60, Training loss = 0.23827079219230707
Iteration 70, Training loss = 0.2356310012763825
Iteration 80, Training loss = 0.23289607449070268
Iteration 90, Training loss = 0.23132011794669616
Iteration 100, Training loss = 0.22847350924343302
Iteration 110, Training loss = 0.22568062185377313
Iteration 120, Training loss = 0.2231783391772837
Iteration 130, Training loss = 0.22232562958618293
Iteration 140, Training loss = 0.22121836396231168
Iteration 150, Training loss = 0.21751234691212143
Iteration 160, Training loss = 0.2158153562442116
Iteration 170, Training loss = 0.21474704650289195
Iteration 180, Training loss = 0.21278318838364835
Iteration 190, Training loss = 0.21276490152746008
Model training time: 63.963369846343994
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.227468516515649
Iteration 10, Training loss = 0.9865609303764675
Iteration 20, Training loss = 0.961303734836947
Iteration 30, Training loss = 0.8901923491759001
Iteration 40, Training loss = 0.7100870710064248
Iteration 50, Training loss = 0.4836473498079512
Iteration 60, Training loss = 0.391039100246153
Iteration 70, Training loss = 0.363957551582424
Iteration 80, Training loss = 0.34608844793649113
Iteration 90, Training loss = 0.3335332633648518
Iteration 100, Training loss = 0.3244670968916681
Iteration 110, Training loss = 0.31772470279880194
Iteration 120, Training loss = 0.3121459988724206
Iteration 130, Training loss = 0.30705722554582326
Iteration 140, Training loss = 0.3032452608076271
Iteration 150, Training loss = 0.30069775476259886
Iteration 160, Training loss = 0.29812033442051517
Iteration 170, Training loss = 0.2950916788428302
Iteration 180, Training loss = 0.2925803599078298
Iteration 190, Training loss = 0.291210664715168
Model training time: 55.253966331481934
Device: cuda
Iteration 0, Training loss = 1.0423793504779466
Iteration 10, Training loss = 0.9769070920161003
Iteration 20, Training loss = 0.9308914833022777
Iteration 30, Training loss = 0.7974557204235003
Iteration 40, Training loss = 0.5290056822668527
Iteration 50, Training loss = 0.38736531498351534
Iteration 60, Training loss = 0.3597258419374337
Iteration 70, Training loss = 0.3469215976299295
Iteration 80, Training loss = 0.33682997596724595
Iteration 90, Training loss = 0.32777835097577834
Iteration 100, Training loss = 0.3212124664282453
Iteration 110, Training loss = 0.3152473819788527
Iteration 120, Training loss = 0.3110557870156523
Iteration 130, Training loss = 0.3081443963419412
Iteration 140, Training loss = 0.30580469814763556
Iteration 150, Training loss = 0.30125097570499937
Iteration 160, Training loss = 0.2989963552946054
Iteration 170, Training loss = 0.29682047744304085
Iteration 180, Training loss = 0.29353514256108787
Iteration 190, Training loss = 0.29279454113636616
Model training time: 52.32182049751282
Device: cuda
Iteration 0, Training loss = 1.0117600246328087
Iteration 10, Training loss = 0.9724604916457392
Iteration 20, Training loss = 0.9151442828385726
Iteration 30, Training loss = 0.7664995035111616
Iteration 40, Training loss = 0.5121831289955959
Iteration 50, Training loss = 0.38749699668895793
Iteration 60, Training loss = 0.35992771365504334
Iteration 70, Training loss = 0.3450131368809852
Iteration 80, Training loss = 0.3352646061763671
Iteration 90, Training loss = 0.3255620161836274
Iteration 100, Training loss = 0.31854144775349164
Iteration 110, Training loss = 0.31355818256663814
Iteration 120, Training loss = 0.3085524534113741
Iteration 130, Training loss = 0.3050618767018479
Iteration 140, Training loss = 0.30107538466868194
Iteration 150, Training loss = 0.29854665937775
Iteration 160, Training loss = 0.29595012559694944
Iteration 170, Training loss = 0.2937650494097512
Iteration 180, Training loss = 0.29142000141063173
Iteration 190, Training loss = 0.28978379754628536
Model training time: 51.46889042854309
Device: cuda
Iteration 0, Training loss = 1.0098174669316426
Iteration 10, Training loss = 0.9747442833467382
Iteration 20, Training loss = 0.9102470209633095
Iteration 30, Training loss = 0.7462318667755035
Iteration 40, Training loss = 0.49815966937565
Iteration 50, Training loss = 0.3940040338154576
Iteration 60, Training loss = 0.36670023951553493
Iteration 70, Training loss = 0.35075578719809436
Iteration 80, Training loss = 0.33873329516770184
Iteration 90, Training loss = 0.3303716366993632
Iteration 100, Training loss = 0.3229925617359687
Iteration 110, Training loss = 0.3176038519339861
Iteration 120, Training loss = 0.312986193359762
Iteration 130, Training loss = 0.30912720422813855
Iteration 140, Training loss = 0.3058406751656878
Iteration 150, Training loss = 0.3037637655717739
Iteration 160, Training loss = 0.30011298219506866
Iteration 170, Training loss = 0.297840663128429
Iteration 180, Training loss = 0.29624647513967783
Iteration 190, Training loss = 0.29441268007824384
Model training time: 51.072190046310425
Device: cuda
Iteration 0, Training loss = 1.0362818264154996
Iteration 10, Training loss = 0.9632786402955723
Iteration 20, Training loss = 0.8739497752293296
Iteration 30, Training loss = 0.6687555095711768
Iteration 40, Training loss = 0.45589127992662254
Iteration 50, Training loss = 0.3882221192985341
Iteration 60, Training loss = 0.3620264248715507
Iteration 70, Training loss = 0.34381143482410964
Iteration 80, Training loss = 0.330646069801372
Iteration 90, Training loss = 0.32256021728550177
Iteration 100, Training loss = 0.3158160160129197
Iteration 110, Training loss = 0.3112121396450605
Iteration 120, Training loss = 0.30742555072054195
Iteration 130, Training loss = 0.3043146462040247
Iteration 140, Training loss = 0.3017940769592921
Iteration 150, Training loss = 0.30014030181843304
Iteration 160, Training loss = 0.29831550763425047
Iteration 170, Training loss = 0.2973821444643868
Iteration 180, Training loss = 0.2955182281813184
Iteration 190, Training loss = 0.2937579136251827
Model training time: 51.723074197769165
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9118173031703286
Iteration 10, Training loss = 0.31535744407902594
Iteration 20, Training loss = 0.2669300815621436
Iteration 30, Training loss = 0.2540265991204027
Iteration 40, Training loss = 0.249126515670675
Iteration 50, Training loss = 0.2469262678698065
Iteration 60, Training loss = 0.243771991112094
Iteration 70, Training loss = 0.24190406090971353
Iteration 80, Training loss = 0.24074708594792132
Iteration 90, Training loss = 0.23888612855315786
Iteration 100, Training loss = 0.23795752083333793
Iteration 110, Training loss = 0.23663743729320702
Iteration 120, Training loss = 0.23591143101597753
Iteration 130, Training loss = 0.23497019463403213
Iteration 140, Training loss = 0.23295773112255594
Iteration 150, Training loss = 0.23265157690370716
Iteration 160, Training loss = 0.23171170009075157
Iteration 170, Training loss = 0.23110568908533613
Iteration 180, Training loss = 0.2299901886622687
Iteration 190, Training loss = 0.2296039320996418
Model training time: 59.894017934799194
Device: cuda
Iteration 0, Training loss = 0.917176409907963
Iteration 10, Training loss = 0.27743957346476217
Iteration 20, Training loss = 0.25874667427534065
Iteration 30, Training loss = 0.25170951369016065
Iteration 40, Training loss = 0.24773495755909722
Iteration 50, Training loss = 0.24643601081221575
Iteration 60, Training loss = 0.24406404667286483
Iteration 70, Training loss = 0.24194325859419966
Iteration 80, Training loss = 0.24027769879442482
Iteration 90, Training loss = 0.23785327979619952
Iteration 100, Training loss = 0.23639999114084934
Iteration 110, Training loss = 0.2351675594702435
Iteration 120, Training loss = 0.2338677713116586
Iteration 130, Training loss = 0.2315144666655052
Iteration 140, Training loss = 0.2318000517749556
Iteration 150, Training loss = 0.22971921144188315
Iteration 160, Training loss = 0.22787472893650404
Iteration 170, Training loss = 0.22697552142367847
Iteration 180, Training loss = 0.2248044395864298
Iteration 190, Training loss = 0.22415018189644467
Model training time: 60.04401206970215
Device: cuda
Iteration 0, Training loss = 1.050077357729852
Iteration 10, Training loss = 0.2767726434889623
Iteration 20, Training loss = 0.2617689061927911
Iteration 30, Training loss = 0.25523896439783816
Iteration 40, Training loss = 0.2508295504871198
Iteration 50, Training loss = 0.24855191818469966
Iteration 60, Training loss = 0.24633553872505823
Iteration 70, Training loss = 0.24404073265439646
Iteration 80, Training loss = 0.2427365199667244
Iteration 90, Training loss = 0.24091165635176903
Iteration 100, Training loss = 0.23902292297658137
Iteration 110, Training loss = 0.23740257067236922
Iteration 120, Training loss = 0.23606859107956218
Iteration 130, Training loss = 0.23414127798616022
Iteration 140, Training loss = 0.23298508540731697
Iteration 150, Training loss = 0.2308972528303303
Iteration 160, Training loss = 0.2303069891129139
Iteration 170, Training loss = 0.2285087701779057
Iteration 180, Training loss = 0.2285967905452286
Iteration 190, Training loss = 0.22633146584609856
Model training time: 59.914490938186646
Device: cuda
Iteration 0, Training loss = 0.9111144115095553
Iteration 10, Training loss = 0.2688106431981216
Iteration 20, Training loss = 0.25626285795284354
Iteration 30, Training loss = 0.2511629805256779
Iteration 40, Training loss = 0.24736112820497458
Iteration 50, Training loss = 0.24497895083133725
Iteration 60, Training loss = 0.24287934788471258
Iteration 70, Training loss = 0.24023705015435887
Iteration 80, Training loss = 0.23917668935469383
Iteration 90, Training loss = 0.23781007276353053
Iteration 100, Training loss = 0.23606143441897084
Iteration 110, Training loss = 0.23451805877800724
Iteration 120, Training loss = 0.23328168834177193
Iteration 130, Training loss = 0.23169420000867566
Iteration 140, Training loss = 0.23019123541704123
Iteration 150, Training loss = 0.22878740270788542
Iteration 160, Training loss = 0.2278216646777259
Iteration 170, Training loss = 0.22474693535750614
Iteration 180, Training loss = 0.2247182304300548
Iteration 190, Training loss = 0.2227903888853276
Model training time: 59.374234437942505
Device: cuda
Iteration 0, Training loss = 0.9404193574679647
Iteration 10, Training loss = 0.2760586533425511
Iteration 20, Training loss = 0.25502523322756165
Iteration 30, Training loss = 0.2508943283903426
Iteration 40, Training loss = 0.24823453874835646
Iteration 50, Training loss = 0.24523072513405253
Iteration 60, Training loss = 0.24372400674555036
Iteration 70, Training loss = 0.24139325237936443
Iteration 80, Training loss = 0.24079616357018982
Iteration 90, Training loss = 0.23948319659428896
Iteration 100, Training loss = 0.2382449466681135
Iteration 110, Training loss = 0.2373412191003993
Iteration 120, Training loss = 0.23652293029182775
Iteration 130, Training loss = 0.23585237933385775
Iteration 140, Training loss = 0.2347407420548264
Iteration 150, Training loss = 0.2344196932255358
Iteration 160, Training loss = 0.2340584584120391
Iteration 170, Training loss = 0.23378906035480868
Iteration 180, Training loss = 0.23228646256929433
Iteration 190, Training loss = 0.22965317608221716
Model training time: 59.27610731124878
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0828912623838527
Iteration 10, Training loss = 0.9793446570778814
Iteration 20, Training loss = 0.9386017313901929
Iteration 30, Training loss = 0.8368722069378637
Iteration 40, Training loss = 0.6231860511256877
Iteration 50, Training loss = 0.44569823714558054
Iteration 60, Training loss = 0.3908362269833468
Iteration 70, Training loss = 0.366179005679301
Iteration 80, Training loss = 0.34844095817798576
Iteration 90, Training loss = 0.33606841231601825
Iteration 100, Training loss = 0.32630293294427476
Iteration 110, Training loss = 0.31895929056665173
Iteration 120, Training loss = 0.31386539995094426
Iteration 130, Training loss = 0.3097674749827615
Iteration 140, Training loss = 0.30548166019329126
Iteration 150, Training loss = 0.3027998833840596
Iteration 160, Training loss = 0.30080005864878206
Iteration 170, Training loss = 0.29883892031539466
Iteration 180, Training loss = 0.2979376440606831
Iteration 190, Training loss = 0.2956780834114494
Model training time: 51.23174333572388
Device: cuda
Iteration 0, Training loss = 1.0412340368625623
Iteration 10, Training loss = 0.9981191500949399
Iteration 20, Training loss = 0.9842926013296929
Iteration 30, Training loss = 0.9646333464677783
Iteration 40, Training loss = 0.9131916145195708
Iteration 50, Training loss = 0.7937829602455747
Iteration 60, Training loss = 0.590348086927248
Iteration 70, Training loss = 0.43228199077401186
Iteration 80, Training loss = 0.3795375938432804
Iteration 90, Training loss = 0.35487183467777456
Iteration 100, Training loss = 0.33776991062118233
Iteration 110, Training loss = 0.3259834094323974
Iteration 120, Training loss = 0.31754396495899717
Iteration 130, Training loss = 0.31135850403331905
Iteration 140, Training loss = 0.306170008248753
Iteration 150, Training loss = 0.30217039289969755
Iteration 160, Training loss = 0.299393607078543
Iteration 170, Training loss = 0.29685320348843286
Iteration 180, Training loss = 0.2951170165757626
Iteration 190, Training loss = 0.2921403334048635
Model training time: 51.58410310745239
Device: cuda
Iteration 0, Training loss = 1.1649724550293263
Iteration 10, Training loss = 0.9719701285523493
Iteration 20, Training loss = 0.9211536654527637
Iteration 30, Training loss = 0.8031945109079426
Iteration 40, Training loss = 0.5997588984344316
Iteration 50, Training loss = 0.4436585182585002
Iteration 60, Training loss = 0.38765964113571794
Iteration 70, Training loss = 0.36461423891753963
Iteration 80, Training loss = 0.3479745494570709
Iteration 90, Training loss = 0.3360033842964449
Iteration 100, Training loss = 0.32602294278893496
Iteration 110, Training loss = 0.3188590642046813
Iteration 120, Training loss = 0.313295382808372
Iteration 130, Training loss = 0.30863638546155847
Iteration 140, Training loss = 0.3048718154142444
Iteration 150, Training loss = 0.3016496903942403
Iteration 160, Training loss = 0.3001893312458831
Iteration 170, Training loss = 0.29802573043943026
Iteration 180, Training loss = 0.295120257758288
Iteration 190, Training loss = 0.2945549595327193
Model training time: 52.42877721786499
Device: cuda
Iteration 0, Training loss = 1.124640489977915
Iteration 10, Training loss = 0.9796764836219198
Iteration 20, Training loss = 0.9490633059814932
Iteration 30, Training loss = 0.883117043165769
Iteration 40, Training loss = 0.7424729285032853
Iteration 50, Training loss = 0.5662702941088285
Iteration 60, Training loss = 0.45818212801131647
Iteration 70, Training loss = 0.41425934707484957
Iteration 80, Training loss = 0.3911305488163722
Iteration 90, Training loss = 0.37362034957190066
Iteration 100, Training loss = 0.35865209593577085
Iteration 110, Training loss = 0.34692813232901015
Iteration 120, Training loss = 0.3366531560530409
Iteration 130, Training loss = 0.3293554429414768
Iteration 140, Training loss = 0.32315060215583746
Iteration 150, Training loss = 0.3170369858471092
Iteration 160, Training loss = 0.31305871313608785
Iteration 170, Training loss = 0.30990770272010765
Iteration 180, Training loss = 0.3061503094175588
Iteration 190, Training loss = 0.30352105229085197
Model training time: 54.1164276599884
Device: cuda
Iteration 0, Training loss = 1.2414819678822577
Iteration 10, Training loss = 0.9988277280964137
Iteration 20, Training loss = 0.9813201649177478
Iteration 30, Training loss = 0.9526275923286659
Iteration 40, Training loss = 0.8735623780080086
Iteration 50, Training loss = 0.6924042503039042
Iteration 60, Training loss = 0.4786116950897779
Iteration 70, Training loss = 0.38671663093970016
Iteration 80, Training loss = 0.35586901844123714
Iteration 90, Training loss = 0.33794310372232816
Iteration 100, Training loss = 0.32611740016994845
Iteration 110, Training loss = 0.3164888086958208
Iteration 120, Training loss = 0.31037665529239583
Iteration 130, Training loss = 0.30617771855586967
Iteration 140, Training loss = 0.3022642890036394
Iteration 150, Training loss = 0.3006044885386591
Iteration 160, Training loss = 0.29708806090596795
Iteration 170, Training loss = 0.2948851588267635
Iteration 180, Training loss = 0.2932855633001972
Iteration 190, Training loss = 0.2914680490891139
Model training time: 51.46868443489075
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9300235298232756
Iteration 10, Training loss = 0.26502892446978654
Iteration 20, Training loss = 0.2549098667819143
Iteration 30, Training loss = 0.2500380743025006
Iteration 40, Training loss = 0.24528913322278267
Iteration 50, Training loss = 0.24205614935948652
Iteration 60, Training loss = 0.2396965553075219
Iteration 70, Training loss = 0.23654955488760115
Iteration 80, Training loss = 0.2339286193035651
Iteration 90, Training loss = 0.23070306212141894
Iteration 100, Training loss = 0.2278994756260356
Iteration 110, Training loss = 0.22509019141611847
Iteration 120, Training loss = 0.22331145513748776
Iteration 130, Training loss = 0.22062692906401585
Iteration 140, Training loss = 0.21886762692300593
Iteration 150, Training loss = 0.21759393667253318
Iteration 160, Training loss = 0.2162150026159586
Iteration 170, Training loss = 0.2146371277994004
Iteration 180, Training loss = 0.21363469983500558
Iteration 190, Training loss = 0.2143285640195948
Model training time: 59.4907751083374
Device: cuda
Iteration 0, Training loss = 0.8640602369239365
Iteration 10, Training loss = 0.2663200289946823
Iteration 20, Training loss = 0.25560808394122236
Iteration 30, Training loss = 0.2520201303748693
Iteration 40, Training loss = 0.2483728172770445
Iteration 50, Training loss = 0.24717067221656513
Iteration 60, Training loss = 0.24506905930485703
Iteration 70, Training loss = 0.24330687465299156
Iteration 80, Training loss = 0.24183089437260144
Iteration 90, Training loss = 0.24088096780621487
Iteration 100, Training loss = 0.24069285774288546
Iteration 110, Training loss = 0.23928001832558912
Iteration 120, Training loss = 0.23863746740535838
Iteration 130, Training loss = 0.23713805119772463
Iteration 140, Training loss = 0.2353892738836399
Iteration 150, Training loss = 0.23421767375607422
Iteration 160, Training loss = 0.23352087548245554
Iteration 170, Training loss = 0.23198760448446595
Iteration 180, Training loss = 0.23053713517200544
Iteration 190, Training loss = 0.22956103979101503
Model training time: 59.68277645111084
Device: cuda
Iteration 0, Training loss = 0.8749049042446025
Iteration 10, Training loss = 0.2663765153375225
Iteration 20, Training loss = 0.2565828837490312
Iteration 30, Training loss = 0.2534979917288978
Iteration 40, Training loss = 0.24953588177472497
Iteration 50, Training loss = 0.24596177588626383
Iteration 60, Training loss = 0.24307280282179514
Iteration 70, Training loss = 0.24004687538037553
Iteration 80, Training loss = 0.23642653093677787
Iteration 90, Training loss = 0.23391360886718915
Iteration 100, Training loss = 0.2315343925414454
Iteration 110, Training loss = 0.23006619653408078
Iteration 120, Training loss = 0.22810733069976172
Iteration 130, Training loss = 0.22596513004838556
Iteration 140, Training loss = 0.22449002861256762
Iteration 150, Training loss = 0.22296256985929278
Iteration 160, Training loss = 0.2218341570041606
Iteration 170, Training loss = 0.22062265437438291
Iteration 180, Training loss = 0.21932633509094587
Iteration 190, Training loss = 0.21750904889642328
Model training time: 59.300103425979614
Device: cuda
Iteration 0, Training loss = 0.880976905281417
Iteration 10, Training loss = 0.2607074237028182
Iteration 20, Training loss = 0.25581031533830983
Iteration 30, Training loss = 0.25076771462741104
Iteration 40, Training loss = 0.24790896223363093
Iteration 50, Training loss = 0.24484823086267507
Iteration 60, Training loss = 0.2417647994082907
Iteration 70, Training loss = 0.23967782512379154
Iteration 80, Training loss = 0.23928425540238762
Iteration 90, Training loss = 0.2365492176318514
Iteration 100, Training loss = 0.23525419885265655
Iteration 110, Training loss = 0.2328881068146171
Iteration 120, Training loss = 0.23159173991225193
Iteration 130, Training loss = 0.2291798952984925
Iteration 140, Training loss = 0.2286721936242592
Iteration 150, Training loss = 0.2255909982078893
Iteration 160, Training loss = 0.22409862387871396
Iteration 170, Training loss = 0.22272175006964356
Iteration 180, Training loss = 0.22179332559091458
Iteration 190, Training loss = 0.21991569322088492
Model training time: 59.41188931465149
Device: cuda
Iteration 0, Training loss = 1.0375970285295864
Iteration 10, Training loss = 0.27146900297650967
Iteration 20, Training loss = 0.2553026859023145
Iteration 30, Training loss = 0.25241762169317344
Iteration 40, Training loss = 0.2482915897516237
Iteration 50, Training loss = 0.24539474516675092
Iteration 60, Training loss = 0.24111652331075806
Iteration 70, Training loss = 0.23772462896534785
Iteration 80, Training loss = 0.23532024788971684
Iteration 90, Training loss = 0.2326874235186024
Iteration 100, Training loss = 0.23093700556507432
Iteration 110, Training loss = 0.22952532537893397
Iteration 120, Training loss = 0.2290370785743718
Iteration 130, Training loss = 0.22711660147865037
Iteration 140, Training loss = 0.226211485043528
Iteration 150, Training loss = 0.22407831553963647
Iteration 160, Training loss = 0.22384338811543827
Iteration 170, Training loss = 0.22337160655409818
Iteration 180, Training loss = 0.22187016360857637
Iteration 190, Training loss = 0.22030989962499498
Model training time: 60.06349515914917
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9976941496853667
Iteration 10, Training loss = 0.9394564238435404
Iteration 20, Training loss = 0.8115803163120712
Iteration 30, Training loss = 0.5560500459106648
Iteration 40, Training loss = 0.41017229156793605
Iteration 50, Training loss = 0.37584034177128245
Iteration 60, Training loss = 0.3600767846556677
Iteration 70, Training loss = 0.347064720403745
Iteration 80, Training loss = 0.3378641161221813
Iteration 90, Training loss = 0.32976097437206675
Iteration 100, Training loss = 0.32344346129951845
Iteration 110, Training loss = 0.3184482990471637
Iteration 120, Training loss = 0.3143232967974483
Iteration 130, Training loss = 0.31115029413918943
Iteration 140, Training loss = 0.3082003499862652
Iteration 150, Training loss = 0.3053904991362982
Iteration 160, Training loss = 0.3033199305983557
Iteration 170, Training loss = 0.3013203703406928
Iteration 180, Training loss = 0.29916554847777177
Iteration 190, Training loss = 0.29690073646497034
Model training time: 53.00312852859497
Device: cuda
Iteration 0, Training loss = 1.0825986292051233
Iteration 10, Training loss = 0.9721468879980741
Iteration 20, Training loss = 0.9084471619647482
Iteration 30, Training loss = 0.7293660423317969
Iteration 40, Training loss = 0.46461123188480663
Iteration 50, Training loss = 0.36794519590006936
Iteration 60, Training loss = 0.34662352631921356
Iteration 70, Training loss = 0.3351875495363549
Iteration 80, Training loss = 0.3265766803049235
Iteration 90, Training loss = 0.3204099950223153
Iteration 100, Training loss = 0.31649683696636255
Iteration 110, Training loss = 0.3125177188483989
Iteration 120, Training loss = 0.3101149130846567
Iteration 130, Training loss = 0.30751191936253347
Iteration 140, Training loss = 0.3055822532534023
Iteration 150, Training loss = 0.30354639729439925
Iteration 160, Training loss = 0.30212938080087376
Iteration 170, Training loss = 0.30013622890635966
Iteration 180, Training loss = 0.29853779235899736
Iteration 190, Training loss = 0.29650540376343015
Model training time: 51.265395641326904
Device: cuda
Iteration 0, Training loss = 1.040721931607251
Iteration 10, Training loss = 0.9392325204351674
Iteration 20, Training loss = 0.8265219268879452
Iteration 30, Training loss = 0.6159253974179715
Iteration 40, Training loss = 0.44400195439080686
Iteration 50, Training loss = 0.3811840001799634
Iteration 60, Training loss = 0.35751076719323216
Iteration 70, Training loss = 0.34337025340916455
Iteration 80, Training loss = 0.33225607303317617
Iteration 90, Training loss = 0.32387822355337187
Iteration 100, Training loss = 0.3196575774086846
Iteration 110, Training loss = 0.31284191938126144
Iteration 120, Training loss = 0.3084563881446774
Iteration 130, Training loss = 0.3051857865518994
Iteration 140, Training loss = 0.30166152443574823
Iteration 150, Training loss = 0.29889813547836985
Iteration 160, Training loss = 0.29702636981068026
Iteration 170, Training loss = 0.295090823141849
Iteration 180, Training loss = 0.2929737025844878
Iteration 190, Training loss = 0.29167456032285366
Model training time: 51.694916009902954
Device: cuda
Iteration 0, Training loss = 1.0496234905316635
Iteration 10, Training loss = 0.9851415451022162
Iteration 20, Training loss = 0.941719573189095
Iteration 30, Training loss = 0.8206677356203973
Iteration 40, Training loss = 0.5801905275542955
Iteration 50, Training loss = 0.4281276374479423
Iteration 60, Training loss = 0.385958687118862
Iteration 70, Training loss = 0.3641863299020823
Iteration 80, Training loss = 0.3486549672153261
Iteration 90, Training loss = 0.33696117543655896
Iteration 100, Training loss = 0.3280589111905167
Iteration 110, Training loss = 0.3208310460122887
Iteration 120, Training loss = 0.3158974173852211
Iteration 130, Training loss = 0.3115724119299276
Iteration 140, Training loss = 0.3084561808241738
Iteration 150, Training loss = 0.305945478775651
Iteration 160, Training loss = 0.3034323525889484
Iteration 170, Training loss = 0.3017760759606454
Iteration 180, Training loss = 0.299845798076063
Iteration 190, Training loss = 0.2980400921353971
Model training time: 51.89544463157654
Device: cuda
Iteration 0, Training loss = 0.9968238291533097
Iteration 10, Training loss = 0.9720591850902723
Iteration 20, Training loss = 0.9172684855507192
Iteration 30, Training loss = 0.7806150330149609
Iteration 40, Training loss = 0.558410070080688
Iteration 50, Training loss = 0.4242920393241201
Iteration 60, Training loss = 0.3820377695531661
Iteration 70, Training loss = 0.36013630054135254
Iteration 80, Training loss = 0.34431689020228273
Iteration 90, Training loss = 0.33292261946604446
Iteration 100, Training loss = 0.3242487815267222
Iteration 110, Training loss = 0.317625285634672
Iteration 120, Training loss = 0.3130283804907315
Iteration 130, Training loss = 0.30876548141960936
Iteration 140, Training loss = 0.30614778053933295
Iteration 150, Training loss = 0.3028071822175657
Iteration 160, Training loss = 0.3000757834617642
Iteration 170, Training loss = 0.2972506691076329
Iteration 180, Training loss = 0.2949376785812747
Iteration 190, Training loss = 0.2924083015984959
Model training time: 52.10563015937805
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8943288686194858
Iteration 10, Training loss = 0.26271384936455944
Iteration 20, Training loss = 0.2529584521040824
Iteration 30, Training loss = 0.24823900772897517
Iteration 40, Training loss = 0.2431184446466142
Iteration 50, Training loss = 0.2397383449662135
Iteration 60, Training loss = 0.2372827250600437
Iteration 70, Training loss = 0.23583828668663467
Iteration 80, Training loss = 0.2330015405004727
Iteration 90, Training loss = 0.23158870266687467
Iteration 100, Training loss = 0.23049614902423776
Iteration 110, Training loss = 0.22884451471953002
Iteration 120, Training loss = 0.2277001528348324
Iteration 130, Training loss = 0.22683344079532486
Iteration 140, Training loss = 0.22540486100071294
Iteration 150, Training loss = 0.22398710312042835
Iteration 160, Training loss = 0.2221033162828805
Iteration 170, Training loss = 0.22141343088829574
Iteration 180, Training loss = 0.22104198906732642
Iteration 190, Training loss = 0.21882503231366476
Iteration 200, Training loss = 0.21798707040899618
Iteration 210, Training loss = 0.2189227399834688
Iteration 220, Training loss = 0.21645703461867025
Iteration 230, Training loss = 0.21578114627352082
Iteration 240, Training loss = 0.21345176339005503
Iteration 250, Training loss = 0.2127674703940677
Iteration 260, Training loss = 0.21117632155833038
Iteration 270, Training loss = 0.21115962664286295
Iteration 280, Training loss = 0.20949444811845172
Iteration 290, Training loss = 0.20921490060678427
Model training time: 90.08528184890747
Device: cuda
Iteration 0, Training loss = 0.8755519159750086
Iteration 10, Training loss = 0.2597705366242911
Iteration 20, Training loss = 0.25116731107666873
Iteration 30, Training loss = 0.24703901678611692
Iteration 40, Training loss = 0.24389917158706176
Iteration 50, Training loss = 0.24070370884765174
Iteration 60, Training loss = 0.23916270558672828
Iteration 70, Training loss = 0.2364419821091896
Iteration 80, Training loss = 0.2340518063393192
Iteration 90, Training loss = 0.2329033376010144
Iteration 100, Training loss = 0.23108167042910766
Iteration 110, Training loss = 0.22875414716736706
Iteration 120, Training loss = 0.22716929328038496
Iteration 130, Training loss = 0.22518160208555812
Iteration 140, Training loss = 0.22386804881735126
Iteration 150, Training loss = 0.22252610228631808
Iteration 160, Training loss = 0.22160829783637742
Iteration 170, Training loss = 0.21951177453505244
Iteration 180, Training loss = 0.21642393748397412
Iteration 190, Training loss = 0.21525287851331315
Iteration 200, Training loss = 0.21321084667087178
Iteration 210, Training loss = 0.2126485315569933
Iteration 220, Training loss = 0.2111766417533303
Iteration 230, Training loss = 0.21143335964224766
Iteration 240, Training loss = 0.20955734206858465
Iteration 250, Training loss = 0.20940325862687567
Iteration 260, Training loss = 0.20821512465315742
Iteration 270, Training loss = 0.20787195159473282
Iteration 280, Training loss = 0.20728928001894467
Iteration 290, Training loss = 0.20655922267747961
Model training time: 89.08789372444153
Device: cuda
Iteration 0, Training loss = 0.8214471844803308
Iteration 10, Training loss = 0.2662197415523483
Iteration 20, Training loss = 0.2548524012096262
Iteration 30, Training loss = 0.24772701268466774
Iteration 40, Training loss = 0.24529967068330102
Iteration 50, Training loss = 0.24015366196056495
Iteration 60, Training loss = 0.23513890478921973
Iteration 70, Training loss = 0.23158062627350073
Iteration 80, Training loss = 0.22803931719295067
Iteration 90, Training loss = 0.22481519682107917
Iteration 100, Training loss = 0.22295496994314562
Iteration 110, Training loss = 0.21866069708901328
Iteration 120, Training loss = 0.2165240244804949
Iteration 130, Training loss = 0.21435685794134646
Iteration 140, Training loss = 0.21327045988201518
Iteration 150, Training loss = 0.21120576787254083
Iteration 160, Training loss = 0.21029523472135195
Iteration 170, Training loss = 0.21048569178955567
Iteration 180, Training loss = 0.2085256727349355
Iteration 190, Training loss = 0.2078926581259511
Iteration 200, Training loss = 0.20731840041524546
Iteration 210, Training loss = 0.2071074237642081
Iteration 220, Training loss = 0.20605551343465198
Iteration 230, Training loss = 0.20619303527949512
Iteration 240, Training loss = 0.20648919006332683
Iteration 250, Training loss = 0.2038960637173791
Iteration 260, Training loss = 0.20348470407911545
Iteration 270, Training loss = 0.20293952179127844
Iteration 280, Training loss = 0.2017845128469421
Iteration 290, Training loss = 0.20201116330597713
Model training time: 89.54647493362427
Device: cuda
Iteration 0, Training loss = 0.9044346486888646
Iteration 10, Training loss = 0.26107787952762873
Iteration 20, Training loss = 0.25181645558076204
Iteration 30, Training loss = 0.2466717917058203
Iteration 40, Training loss = 0.24116369347641434
Iteration 50, Training loss = 0.23869865222109687
Iteration 60, Training loss = 0.23491067457314274
Iteration 70, Training loss = 0.2325811976036012
Iteration 80, Training loss = 0.22791864845343834
Iteration 90, Training loss = 0.22540736903890896
Iteration 100, Training loss = 0.22223945070004117
Iteration 110, Training loss = 0.21992235645147915
Iteration 120, Training loss = 0.2180820245673691
Iteration 130, Training loss = 0.2163866515752774
Iteration 140, Training loss = 0.2149553140580366
Iteration 150, Training loss = 0.21398167193367862
Iteration 160, Training loss = 0.213228696246366
Iteration 170, Training loss = 0.21252433714515345
Iteration 180, Training loss = 0.2106198617009725
Iteration 190, Training loss = 0.20956005757125679
Iteration 200, Training loss = 0.20867088181529067
Iteration 210, Training loss = 0.2075073135935742
Iteration 220, Training loss = 0.2068099282717935
Iteration 230, Training loss = 0.20691612967546436
Iteration 240, Training loss = 0.20551005718500717
Iteration 250, Training loss = 0.20512493134696702
Iteration 260, Training loss = 0.2039225840050241
Iteration 270, Training loss = 0.20316997706746134
Iteration 280, Training loss = 0.20273861116257266
Iteration 290, Training loss = 0.20270956728769385
Model training time: 90.16140961647034
Device: cuda
Iteration 0, Training loss = 0.8654578749972265
Iteration 10, Training loss = 0.26084884302915584
Iteration 20, Training loss = 0.25088682947095464
Iteration 30, Training loss = 0.24558930869263726
Iteration 40, Training loss = 0.241786222131068
Iteration 50, Training loss = 0.2383375604443504
Iteration 60, Training loss = 0.2353947002818619
Iteration 70, Training loss = 0.2326116031350721
Iteration 80, Training loss = 0.2307537949603537
Iteration 90, Training loss = 0.22842832744697442
Iteration 100, Training loss = 0.22627910471336854
Iteration 110, Training loss = 0.2231517450677024
Iteration 120, Training loss = 0.22043222952004216
Iteration 130, Training loss = 0.218504523921416
Iteration 140, Training loss = 0.2157332117214871
Iteration 150, Training loss = 0.2138133700127187
Iteration 160, Training loss = 0.21233722117644002
Iteration 170, Training loss = 0.21134621257655287
Iteration 180, Training loss = 0.20902352114230538
Iteration 190, Training loss = 0.2081942018751361
Iteration 200, Training loss = 0.20667963532577965
Iteration 210, Training loss = 0.20590278740234422
Iteration 220, Training loss = 0.20515358609997708
Iteration 230, Training loss = 0.20517684051380064
Iteration 240, Training loss = 0.2039218598589805
Iteration 250, Training loss = 0.20348643666304253
Iteration 260, Training loss = 0.20276540600591236
Iteration 270, Training loss = 0.20122526028162038
Iteration 280, Training loss = 0.20136910932507493
Iteration 290, Training loss = 0.20062891375903347
Model training time: 95.91235136985779
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0069467408069666
Iteration 10, Training loss = 0.9440295316170955
Iteration 20, Training loss = 0.830155577060681
Iteration 30, Training loss = 0.5930653977509283
Iteration 40, Training loss = 0.4245920544805158
Iteration 50, Training loss = 0.3821994369300667
Iteration 60, Training loss = 0.3636316544047876
Iteration 70, Training loss = 0.34974077703872164
Iteration 80, Training loss = 0.33779970279350374
Iteration 90, Training loss = 0.3292078561540963
Iteration 100, Training loss = 0.3223772080624161
Iteration 110, Training loss = 0.3182123212710671
Iteration 120, Training loss = 0.31260481052064665
Iteration 130, Training loss = 0.3077921262685803
Iteration 140, Training loss = 0.3047998274725992
Iteration 150, Training loss = 0.3021158337737051
Iteration 160, Training loss = 0.2989445016291982
Iteration 170, Training loss = 0.29762903309386707
Iteration 180, Training loss = 0.29571367029982487
Iteration 190, Training loss = 0.293346528413791
Iteration 200, Training loss = 0.2919127134885189
Iteration 210, Training loss = 0.29130903354301546
Iteration 220, Training loss = 0.28917757626892865
Iteration 230, Training loss = 0.28771393152250757
Iteration 240, Training loss = 0.2864221963617537
Iteration 250, Training loss = 0.2851953415767006
Iteration 260, Training loss = 0.28390263629276397
Iteration 270, Training loss = 0.2825684888639312
Iteration 280, Training loss = 0.2816936062729877
Iteration 290, Training loss = 0.28159782808759937
Model training time: 77.12949180603027
Device: cuda
Iteration 0, Training loss = 1.012252640609004
Iteration 10, Training loss = 0.9922984979578838
Iteration 20, Training loss = 0.9710150856326744
Iteration 30, Training loss = 0.9244679255761962
Iteration 40, Training loss = 0.808601144719239
Iteration 50, Training loss = 0.6065249980071893
Iteration 60, Training loss = 0.4406209898311735
Iteration 70, Training loss = 0.3830703674019247
Iteration 80, Training loss = 0.35779773044413415
Iteration 90, Training loss = 0.3413294162151318
Iteration 100, Training loss = 0.3295622093666003
Iteration 110, Training loss = 0.3208295565440459
Iteration 120, Training loss = 0.31462246532313487
Iteration 130, Training loss = 0.31051742325082493
Iteration 140, Training loss = 0.3064029133622197
Iteration 150, Training loss = 0.30396716671001506
Iteration 160, Training loss = 0.3012696845088028
Iteration 170, Training loss = 0.29751327137152356
Iteration 180, Training loss = 0.2946281600833515
Iteration 190, Training loss = 0.29305978925619725
Iteration 200, Training loss = 0.2898209567591188
Iteration 210, Training loss = 0.2880105434552483
Iteration 220, Training loss = 0.28638194962112223
Iteration 230, Training loss = 0.28462666208329407
Iteration 240, Training loss = 0.2825785011485003
Iteration 250, Training loss = 0.2811023530680776
Iteration 260, Training loss = 0.2804735086966252
Iteration 270, Training loss = 0.2787842082516583
Iteration 280, Training loss = 0.2767918701261138
Iteration 290, Training loss = 0.27572853985615975
Model training time: 77.19328665733337
Device: cuda
Iteration 0, Training loss = 1.0056391888194613
Iteration 10, Training loss = 0.8802164099066729
Iteration 20, Training loss = 0.6802155175358777
Iteration 30, Training loss = 0.4865044767154012
Iteration 40, Training loss = 0.4046317075473675
Iteration 50, Training loss = 0.3762215407574234
Iteration 60, Training loss = 0.35898901018255575
Iteration 70, Training loss = 0.34709956478957393
Iteration 80, Training loss = 0.33719310222040627
Iteration 90, Training loss = 0.3279035527637039
Iteration 100, Training loss = 0.32066505888234015
Iteration 110, Training loss = 0.3148583933207148
Iteration 120, Training loss = 0.3095159369390368
Iteration 130, Training loss = 0.30533400710654146
Iteration 140, Training loss = 0.3015396052584556
Iteration 150, Training loss = 0.29937393495426085
Iteration 160, Training loss = 0.29559223497835335
Iteration 170, Training loss = 0.2932825981850785
Iteration 180, Training loss = 0.2898487463089579
Iteration 190, Training loss = 0.2875074630486216
Iteration 200, Training loss = 0.285237240942492
Iteration 210, Training loss = 0.2834908690141595
Iteration 220, Training loss = 0.282193841493648
Iteration 230, Training loss = 0.2813890725374222
Iteration 240, Training loss = 0.2792741487855497
Iteration 250, Training loss = 0.2779689223869987
Iteration 260, Training loss = 0.27656335149698213
Iteration 270, Training loss = 0.275242969105785
Iteration 280, Training loss = 0.275229852952531
Iteration 290, Training loss = 0.27353334462873025
Model training time: 78.21497440338135
Device: cuda
Iteration 0, Training loss = 1.016901079007393
Iteration 10, Training loss = 0.9654665917590044
Iteration 20, Training loss = 0.8942346999034789
Iteration 30, Training loss = 0.7129787930831817
Iteration 40, Training loss = 0.49028692758025755
Iteration 50, Training loss = 0.40701802391648867
Iteration 60, Training loss = 0.3821018811873192
Iteration 70, Training loss = 0.36455672014738627
Iteration 80, Training loss = 0.3514081907733051
Iteration 90, Training loss = 0.3409189078686894
Iteration 100, Training loss = 0.33128774079723633
Iteration 110, Training loss = 0.3242100333246056
Iteration 120, Training loss = 0.31815222668762944
Iteration 130, Training loss = 0.31288479815646647
Iteration 140, Training loss = 0.3088809193836318
Iteration 150, Training loss = 0.30536361700958675
Iteration 160, Training loss = 0.30286523354226264
Iteration 170, Training loss = 0.3004136161815717
Iteration 180, Training loss = 0.2974914382189368
Iteration 190, Training loss = 0.295669356406023
Iteration 200, Training loss = 0.29390065094843004
Iteration 210, Training loss = 0.291703300487592
Iteration 220, Training loss = 0.2902062001147708
Iteration 230, Training loss = 0.28783848082673724
Iteration 240, Training loss = 0.2863999779385645
Iteration 250, Training loss = 0.2857901895247796
Iteration 260, Training loss = 0.28407399226789887
Iteration 270, Training loss = 0.282034070847403
Iteration 280, Training loss = 0.2810020580671836
Iteration 290, Training loss = 0.2796996875085693
Model training time: 79.87171792984009
Device: cuda
Iteration 0, Training loss = 1.0006695954120102
Iteration 10, Training loss = 0.9218796347650353
Iteration 20, Training loss = 0.7568335538900993
Iteration 30, Training loss = 0.5009391742627978
Iteration 40, Training loss = 0.3893497530101002
Iteration 50, Training loss = 0.3624821824296085
Iteration 60, Training loss = 0.34734982489675714
Iteration 70, Training loss = 0.3346754158464607
Iteration 80, Training loss = 0.32601282583630603
Iteration 90, Training loss = 0.3178137231276231
Iteration 100, Training loss = 0.3126843015208912
Iteration 110, Training loss = 0.30792585123276367
Iteration 120, Training loss = 0.3046652797051674
Iteration 130, Training loss = 0.30085661762578475
Iteration 140, Training loss = 0.2980461325406452
Iteration 150, Training loss = 0.2955064428385329
Iteration 160, Training loss = 0.2929706400719242
Iteration 170, Training loss = 0.2909360542389506
Iteration 180, Training loss = 0.28856491747397733
Iteration 190, Training loss = 0.2868550279074245
Iteration 200, Training loss = 0.28511735293024404
Iteration 210, Training loss = 0.2835858078034604
Iteration 220, Training loss = 0.28240604021986904
Iteration 230, Training loss = 0.28052070704491244
Iteration 240, Training loss = 0.27972127975905
Iteration 250, Training loss = 0.277644789773197
Iteration 260, Training loss = 0.27713199866854626
Iteration 270, Training loss = 0.27508411261338545
Iteration 280, Training loss = 0.27426915381841616
Iteration 290, Training loss = 0.2731839340666066
Model training time: 77.4591794013977
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9708381223217877
Iteration 10, Training loss = 0.2751046024298898
Iteration 20, Training loss = 0.25847728244492396
Iteration 30, Training loss = 0.2513960831695133
Iteration 40, Training loss = 0.2460588577432909
Iteration 50, Training loss = 0.2420455319222045
Iteration 60, Training loss = 0.2405405591291506
Iteration 70, Training loss = 0.23678730989711871
Iteration 80, Training loss = 0.23515616926018168
Iteration 90, Training loss = 0.23234085979812963
Iteration 100, Training loss = 0.23061875984576588
Iteration 110, Training loss = 0.229294305190372
Iteration 120, Training loss = 0.2271180062190346
Iteration 130, Training loss = 0.22551747868602404
Iteration 140, Training loss = 0.22401978161887845
Iteration 150, Training loss = 0.22295774253094253
Iteration 160, Training loss = 0.22155404616381236
Iteration 170, Training loss = 0.2203600299027231
Iteration 180, Training loss = 0.21938909852994237
Iteration 190, Training loss = 0.21971268325612164
Iteration 200, Training loss = 0.21765194545333513
Iteration 210, Training loss = 0.2169232749276691
Iteration 220, Training loss = 0.2173426790513854
Iteration 230, Training loss = 0.21436055040589852
Iteration 240, Training loss = 0.21440462778875793
Iteration 250, Training loss = 0.2127333952176974
Iteration 260, Training loss = 0.2134719000418405
Iteration 270, Training loss = 0.21265290490383112
Iteration 280, Training loss = 0.21186580976858232
Iteration 290, Training loss = 0.21135687043413448
Model training time: 90.57223701477051
Device: cuda
Iteration 0, Training loss = 0.9874855897564819
Iteration 10, Training loss = 0.2866813045912895
Iteration 20, Training loss = 0.25934573939601
Iteration 30, Training loss = 0.2540685438735474
Iteration 40, Training loss = 0.2514743304626953
Iteration 50, Training loss = 0.2484248275126236
Iteration 60, Training loss = 0.2470185805634024
Iteration 70, Training loss = 0.24531660016608123
Iteration 80, Training loss = 0.2439319522700448
Iteration 90, Training loss = 0.24198419888670317
Iteration 100, Training loss = 0.24190514106393438
Iteration 110, Training loss = 0.23984383197798245
Iteration 120, Training loss = 0.23893473704080076
Iteration 130, Training loss = 0.23772332410593539
Iteration 140, Training loss = 0.23686309374760892
Iteration 150, Training loss = 0.23638853689898615
Iteration 160, Training loss = 0.23545808917370395
Iteration 170, Training loss = 0.23291723707303907
Iteration 180, Training loss = 0.23263661671375882
Iteration 190, Training loss = 0.22948366500761197
Iteration 200, Training loss = 0.22948914888688332
Iteration 210, Training loss = 0.22678359255986513
Iteration 220, Training loss = 0.22564212268821282
Iteration 230, Training loss = 0.22363909477916893
Iteration 240, Training loss = 0.2222471573718504
Iteration 250, Training loss = 0.22127275356060064
Iteration 260, Training loss = 0.22121879490821258
Iteration 270, Training loss = 0.21953289534734644
Iteration 280, Training loss = 0.21901031468369533
Iteration 290, Training loss = 0.2180419532933097
Model training time: 91.7355477809906
Device: cuda
Iteration 0, Training loss = 0.88700330214224
Iteration 10, Training loss = 0.30543117592300195
Iteration 20, Training loss = 0.2653243615143541
Iteration 30, Training loss = 0.25462437475073163
Iteration 40, Training loss = 0.2489976171710065
Iteration 50, Training loss = 0.2471054566367237
Iteration 60, Training loss = 0.24313750996249886
Iteration 70, Training loss = 0.24036207863530099
Iteration 80, Training loss = 0.23856602979886934
Iteration 90, Training loss = 0.23790354274465267
Iteration 100, Training loss = 0.23512636114289795
Iteration 110, Training loss = 0.2340152365501952
Iteration 120, Training loss = 0.2301574513892045
Iteration 130, Training loss = 0.22878319672916247
Iteration 140, Training loss = 0.22591950794780888
Iteration 150, Training loss = 0.2238256893872063
Iteration 160, Training loss = 0.22287538867209844
Iteration 170, Training loss = 0.22081512021989638
Iteration 180, Training loss = 0.21830241821238383
Iteration 190, Training loss = 0.2167817065537264
Iteration 200, Training loss = 0.216116681587005
Iteration 210, Training loss = 0.214606077198821
Iteration 220, Training loss = 0.21375979767473424
Iteration 230, Training loss = 0.21123326630983952
Iteration 240, Training loss = 0.2105067680783318
Iteration 250, Training loss = 0.21015720494127504
Iteration 260, Training loss = 0.21005106915310384
Iteration 270, Training loss = 0.20924530149513973
Iteration 280, Training loss = 0.20917926142037202
Iteration 290, Training loss = 0.2078091768178963
Model training time: 89.40237784385681
Device: cuda
Iteration 0, Training loss = 1.0952212870984837
Iteration 10, Training loss = 0.2815483907858531
Iteration 20, Training loss = 0.2570172594725222
Iteration 30, Training loss = 0.25258289482714474
Iteration 40, Training loss = 0.2494383496074861
Iteration 50, Training loss = 0.24711422255073767
Iteration 60, Training loss = 0.24544252268070185
Iteration 70, Training loss = 0.24377268426804152
Iteration 80, Training loss = 0.24206025729288802
Iteration 90, Training loss = 0.24155432129827675
Iteration 100, Training loss = 0.23954097905020783
Iteration 110, Training loss = 0.23844926681927436
Iteration 120, Training loss = 0.237590658923854
Iteration 130, Training loss = 0.23632978871104798
Iteration 140, Training loss = 0.235643343217131
Iteration 150, Training loss = 0.2346826774486597
Iteration 160, Training loss = 0.2335891287755851
Iteration 170, Training loss = 0.23223318429528803
Iteration 180, Training loss = 0.23083140585877468
Iteration 190, Training loss = 0.2302211060544143
Iteration 200, Training loss = 0.22833167736369056
Iteration 210, Training loss = 0.22713851784738365
Iteration 220, Training loss = 0.22508698442707892
Iteration 230, Training loss = 0.22326206520271763
Iteration 240, Training loss = 0.2226640825038371
Iteration 250, Training loss = 0.22204138680500685
Iteration 260, Training loss = 0.22128538049505528
Iteration 270, Training loss = 0.21961685590410002
Iteration 280, Training loss = 0.2184808662404185
Iteration 290, Training loss = 0.21809138803954287
Model training time: 90.35023856163025
Device: cuda
Iteration 0, Training loss = 1.0271116233102366
Iteration 10, Training loss = 0.28182707536191753
Iteration 20, Training loss = 0.2552862486977508
Iteration 30, Training loss = 0.24868603320657343
Iteration 40, Training loss = 0.24429061890080356
Iteration 50, Training loss = 0.2402755914678896
Iteration 60, Training loss = 0.23900743393938323
Iteration 70, Training loss = 0.23564617954878417
Iteration 80, Training loss = 0.2344377181596226
Iteration 90, Training loss = 0.23190251557867309
Iteration 100, Training loss = 0.2297987058614763
Iteration 110, Training loss = 0.228059909697892
Iteration 120, Training loss = 0.2258937548899996
Iteration 130, Training loss = 0.22415503704749443
Iteration 140, Training loss = 0.2234690924341552
Iteration 150, Training loss = 0.22141784415152913
Iteration 160, Training loss = 0.2210611343599748
Iteration 170, Training loss = 0.2202925843677083
Iteration 180, Training loss = 0.21978934505135542
Iteration 190, Training loss = 0.21801804492007132
Iteration 200, Training loss = 0.2181648721801486
Iteration 210, Training loss = 0.21748044500172425
Iteration 220, Training loss = 0.21665765114740473
Iteration 230, Training loss = 0.21600028584544786
Iteration 240, Training loss = 0.21635391192447737
Iteration 250, Training loss = 0.2156201170190521
Iteration 260, Training loss = 0.21521333399890125
Iteration 270, Training loss = 0.21422097900351464
Iteration 280, Training loss = 0.2141316709815016
Iteration 290, Training loss = 0.21381792454903828
Model training time: 88.98253726959229
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.035099102103192
Iteration 10, Training loss = 1.001668486042299
Iteration 20, Training loss = 0.9964365610753857
Iteration 30, Training loss = 0.9914939766344817
Iteration 40, Training loss = 0.9827344279358352
Iteration 50, Training loss = 0.9657763069954471
Iteration 60, Training loss = 0.9165765254393868
Iteration 70, Training loss = 0.7918628791104192
Iteration 80, Training loss = 0.5754645144305943
Iteration 90, Training loss = 0.44058977578573183
Iteration 100, Training loss = 0.39950417061358834
Iteration 110, Training loss = 0.377347268704055
Iteration 120, Training loss = 0.3605335000632466
Iteration 130, Training loss = 0.3464279136651956
Iteration 140, Training loss = 0.3352985487899918
Iteration 150, Training loss = 0.3275498976742012
Iteration 160, Training loss = 0.31987045518154106
Iteration 170, Training loss = 0.3144134103100081
Iteration 180, Training loss = 0.31116457852188517
Iteration 190, Training loss = 0.30645418289490944
Iteration 200, Training loss = 0.30269912209199823
Iteration 210, Training loss = 0.2992213371295284
Iteration 220, Training loss = 0.296634831506273
Iteration 230, Training loss = 0.29450111890184705
Iteration 240, Training loss = 0.2926486588762578
Iteration 250, Training loss = 0.2905162641607621
Iteration 260, Training loss = 0.2887504972193552
Iteration 270, Training loss = 0.28678407314894855
Iteration 280, Training loss = 0.2857357676046482
Iteration 290, Training loss = 0.284394180285182
Model training time: 78.26370143890381
Device: cuda
Iteration 0, Training loss = 0.9972312712439017
Iteration 10, Training loss = 0.979760045590608
Iteration 20, Training loss = 0.9414757003530788
Iteration 30, Training loss = 0.8503018754403948
Iteration 40, Training loss = 0.6614361560862997
Iteration 50, Training loss = 0.47638436360059727
Iteration 60, Training loss = 0.40669495212859
Iteration 70, Training loss = 0.38062827793008464
Iteration 80, Training loss = 0.3638053265026802
Iteration 90, Training loss = 0.3509973634556296
Iteration 100, Training loss = 0.33921544239428886
Iteration 110, Training loss = 0.3316458708422195
Iteration 120, Training loss = 0.3224571455216062
Iteration 130, Training loss = 0.31553023412895664
Iteration 140, Training loss = 0.31064090391863947
Iteration 150, Training loss = 0.3070299212195447
Iteration 160, Training loss = 0.303627701341242
Iteration 170, Training loss = 0.30092068484439943
Iteration 180, Training loss = 0.2986507355303004
Iteration 190, Training loss = 0.29707277235057616
Iteration 200, Training loss = 0.29541942635596086
Iteration 210, Training loss = 0.29389655716033375
Iteration 220, Training loss = 0.29207263750154616
Iteration 230, Training loss = 0.2907584528416251
Iteration 240, Training loss = 0.289583472813961
Iteration 250, Training loss = 0.28922178823014966
Iteration 260, Training loss = 0.2875624088849422
Iteration 270, Training loss = 0.28721033533414203
Iteration 280, Training loss = 0.285830104984523
Iteration 290, Training loss = 0.28465901595958765
Model training time: 78.64602637290955
Device: cuda
Iteration 0, Training loss = 0.9948638596972406
Iteration 10, Training loss = 0.9764419794082642
Iteration 20, Training loss = 0.9391406299411387
Iteration 30, Training loss = 0.8428061617457349
Iteration 40, Training loss = 0.6414505966043703
Iteration 50, Training loss = 0.4525791745686877
Iteration 60, Training loss = 0.3850747065411674
Iteration 70, Training loss = 0.3603670207630609
Iteration 80, Training loss = 0.34482127046527494
Iteration 90, Training loss = 0.3327423254216927
Iteration 100, Training loss = 0.3229199700453431
Iteration 110, Training loss = 0.3159108297260487
Iteration 120, Training loss = 0.31060337055708476
Iteration 130, Training loss = 0.3052552409362102
Iteration 140, Training loss = 0.301806360843101
Iteration 150, Training loss = 0.2983037556571085
Iteration 160, Training loss = 0.29518573245276575
Iteration 170, Training loss = 0.29366682289879104
Iteration 180, Training loss = 0.2899717386218085
Iteration 190, Training loss = 0.28843863981501494
Iteration 200, Training loss = 0.28573954796445544
Iteration 210, Training loss = 0.2841859133779139
Iteration 220, Training loss = 0.2817057418650475
Iteration 230, Training loss = 0.280773915314444
Iteration 240, Training loss = 0.2785741711296321
Iteration 250, Training loss = 0.2768206720190924
Iteration 260, Training loss = 0.27609658644395174
Iteration 270, Training loss = 0.2742615510948038
Iteration 280, Training loss = 0.2727352716928519
Iteration 290, Training loss = 0.27185416541957624
Model training time: 77.6838321685791
Device: cuda
Iteration 0, Training loss = 1.3305776896684065
Iteration 10, Training loss = 0.9142120255940203
Iteration 20, Training loss = 0.7500478727518073
Iteration 30, Training loss = 0.5146410424928158
Iteration 40, Training loss = 0.4094018283961476
Iteration 50, Training loss = 0.37973338387150696
Iteration 60, Training loss = 0.3630411564439967
Iteration 70, Training loss = 0.3483197892057723
Iteration 80, Training loss = 0.33790106358735456
Iteration 90, Training loss = 0.32920789034758213
Iteration 100, Training loss = 0.32270593300533756
Iteration 110, Training loss = 0.31642911766750226
Iteration 120, Training loss = 0.31155021287104934
Iteration 130, Training loss = 0.3076329067853338
Iteration 140, Training loss = 0.3039928760361556
Iteration 150, Training loss = 0.3014867548349399
Iteration 160, Training loss = 0.3005188987882817
Iteration 170, Training loss = 0.29710269280245916
Iteration 180, Training loss = 0.29626912130537814
Iteration 190, Training loss = 0.2932155086222478
Iteration 200, Training loss = 0.2922720552066674
Iteration 210, Training loss = 0.29135734436736593
Iteration 220, Training loss = 0.2903336688228276
Iteration 230, Training loss = 0.28802779964778735
Iteration 240, Training loss = 0.2870927311228093
Iteration 250, Training loss = 0.28757463427989377
Iteration 260, Training loss = 0.2850688038672802
Iteration 270, Training loss = 0.28432434250191213
Iteration 280, Training loss = 0.2830593007198278
Iteration 290, Training loss = 0.2831029934439682
Model training time: 79.55858254432678
Device: cuda
Iteration 0, Training loss = 1.0056694806485935
Iteration 10, Training loss = 0.9991781170241498
Iteration 20, Training loss = 0.9965678300834508
Iteration 30, Training loss = 0.9886395824704193
Iteration 40, Training loss = 0.9769122021785681
Iteration 50, Training loss = 0.9417875125500315
Iteration 60, Training loss = 0.856738950751254
Iteration 70, Training loss = 0.6766383832203593
Iteration 80, Training loss = 0.4922058007855346
Iteration 90, Training loss = 0.40891257777882084
Iteration 100, Training loss = 0.37727081156583225
Iteration 110, Training loss = 0.35756409448989923
Iteration 120, Training loss = 0.3435087385384933
Iteration 130, Training loss = 0.33288186147881016
Iteration 140, Training loss = 0.32425097581269086
Iteration 150, Training loss = 0.3177863422079363
Iteration 160, Training loss = 0.3131718245393412
Iteration 170, Training loss = 0.30892020348765425
Iteration 180, Training loss = 0.30548082943555815
Iteration 190, Training loss = 0.3019213324079767
Iteration 200, Training loss = 0.2991663265775367
Iteration 210, Training loss = 0.2971024698249384
Iteration 220, Training loss = 0.29442710109091036
Iteration 230, Training loss = 0.29241649129828395
Iteration 240, Training loss = 0.29067928160446277
Iteration 250, Training loss = 0.2887416651139513
Iteration 260, Training loss = 0.2869772560858496
Iteration 270, Training loss = 0.28522407634246755
Iteration 280, Training loss = 0.2833160976496872
Iteration 290, Training loss = 0.2816508279186516
Model training time: 77.31112217903137
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8525597232839336
Iteration 10, Training loss = 0.2644355967640877
Iteration 20, Training loss = 0.2568526263686194
Iteration 30, Training loss = 0.2535747980006075
Iteration 40, Training loss = 0.24953612202895437
Iteration 50, Training loss = 0.24667356883126182
Iteration 60, Training loss = 0.24463181068067966
Iteration 70, Training loss = 0.24085551223173235
Iteration 80, Training loss = 0.23918260381993464
Iteration 90, Training loss = 0.23811003033089753
Iteration 100, Training loss = 0.2363670654774864
Iteration 110, Training loss = 0.23465569763655825
Iteration 120, Training loss = 0.23335431768122503
Iteration 130, Training loss = 0.2325311109855555
Iteration 140, Training loss = 0.2309540957932311
Iteration 150, Training loss = 0.22973066687152005
Iteration 160, Training loss = 0.2286443032576266
Iteration 170, Training loss = 0.22955108286389983
Iteration 180, Training loss = 0.2273024174110325
Iteration 190, Training loss = 0.22567329555749893
Iteration 200, Training loss = 0.22455842206299592
Iteration 210, Training loss = 0.224480033568714
Iteration 220, Training loss = 0.22291908143223196
Iteration 230, Training loss = 0.22289079964016947
Iteration 240, Training loss = 0.22213733009094203
Iteration 250, Training loss = 0.21948207832045025
Iteration 260, Training loss = 0.21908733821001605
Iteration 270, Training loss = 0.21836082198193685
Iteration 280, Training loss = 0.21708620523197064
Iteration 290, Training loss = 0.2160645717943924
Model training time: 91.2536792755127
Device: cuda
Iteration 0, Training loss = 0.861777629253369
Iteration 10, Training loss = 0.26907719167822225
Iteration 20, Training loss = 0.25695930871266676
Iteration 30, Training loss = 0.2517050507923831
Iteration 40, Training loss = 0.24845964166421244
Iteration 50, Training loss = 0.24573674528063208
Iteration 60, Training loss = 0.24237058812005507
Iteration 70, Training loss = 0.24006555733329432
Iteration 80, Training loss = 0.2376301415660531
Iteration 90, Training loss = 0.234554393639887
Iteration 100, Training loss = 0.23221409108471755
Iteration 110, Training loss = 0.22985241468545894
Iteration 120, Training loss = 0.22673998557139133
Iteration 130, Training loss = 0.22496035022004215
Iteration 140, Training loss = 0.2237463057545072
Iteration 150, Training loss = 0.22269152630354472
Iteration 160, Training loss = 0.2208862194908414
Iteration 170, Training loss = 0.2196923062781205
Iteration 180, Training loss = 0.21840995625741239
Iteration 190, Training loss = 0.217733688292584
Iteration 200, Training loss = 0.21733341866357314
Iteration 210, Training loss = 0.21514601271221603
Iteration 220, Training loss = 0.21409791707992554
Iteration 230, Training loss = 0.2127656479532592
Iteration 240, Training loss = 0.21239024742645918
Iteration 250, Training loss = 0.21155061623180546
Iteration 260, Training loss = 0.21173933155582722
Iteration 270, Training loss = 0.2102326479367012
Iteration 280, Training loss = 0.20927156144438158
Iteration 290, Training loss = 0.2096311004769399
Model training time: 90.82402086257935
Device: cuda
Iteration 0, Training loss = 0.8240854159933357
Iteration 10, Training loss = 0.2606713924071063
Iteration 20, Training loss = 0.2517660997340069
Iteration 30, Training loss = 0.24765254258389635
Iteration 40, Training loss = 0.24501964914194052
Iteration 50, Training loss = 0.2415206741883559
Iteration 60, Training loss = 0.23832767715920572
Iteration 70, Training loss = 0.23780736550328813
Iteration 80, Training loss = 0.2354682697982028
Iteration 90, Training loss = 0.233721605407587
Iteration 100, Training loss = 0.23263385740743167
Iteration 110, Training loss = 0.2325018589263377
Iteration 120, Training loss = 0.2306730126341184
Iteration 130, Training loss = 0.22957223822961106
Iteration 140, Training loss = 0.22876645617438976
Iteration 150, Training loss = 0.22649568308522736
Iteration 160, Training loss = 0.22575523290369245
Iteration 170, Training loss = 0.2243221329462125
Iteration 180, Training loss = 0.22312550977376347
Iteration 190, Training loss = 0.22111495566252926
Iteration 200, Training loss = 0.22033234296933465
Iteration 210, Training loss = 0.2194232981706011
Iteration 220, Training loss = 0.21847172229042375
Iteration 230, Training loss = 0.21806696237285356
Iteration 240, Training loss = 0.21753798756334516
Iteration 250, Training loss = 0.21504764769964171
Iteration 260, Training loss = 0.21471985739066404
Iteration 270, Training loss = 0.21325340809453514
Iteration 280, Training loss = 0.21261135852279295
Iteration 290, Training loss = 0.2124110035562285
Model training time: 88.27659749984741
Device: cuda
Iteration 0, Training loss = 0.8072527727643073
Iteration 10, Training loss = 0.262286896183007
Iteration 20, Training loss = 0.2537051141621986
Iteration 30, Training loss = 0.24918075535752346
Iteration 40, Training loss = 0.24548016783695867
Iteration 50, Training loss = 0.24186856105275775
Iteration 60, Training loss = 0.23910350287738052
Iteration 70, Training loss = 0.23604225496882977
Iteration 80, Training loss = 0.2336782433344546
Iteration 90, Training loss = 0.23020267803311925
Iteration 100, Training loss = 0.2285987172221792
Iteration 110, Training loss = 0.22659380888737343
Iteration 120, Training loss = 0.22464259728285427
Iteration 130, Training loss = 0.22379049694768471
Iteration 140, Training loss = 0.22252218875188182
Iteration 150, Training loss = 0.2214721340848052
Iteration 160, Training loss = 0.2199203928095707
Iteration 170, Training loss = 0.21877837429443994
Iteration 180, Training loss = 0.21874252310841555
Iteration 190, Training loss = 0.21684581044935375
Iteration 200, Training loss = 0.2155305848317446
Iteration 210, Training loss = 0.21427875303703806
Iteration 220, Training loss = 0.21390937200347007
Iteration 230, Training loss = 0.2130009661190176
Iteration 240, Training loss = 0.21241553502094343
Iteration 250, Training loss = 0.21062536911975935
Iteration 260, Training loss = 0.2110956150336542
Iteration 270, Training loss = 0.2091596465251872
Iteration 280, Training loss = 0.2088280318007953
Iteration 290, Training loss = 0.20790972335687005
Model training time: 89.23303961753845
Device: cuda
Iteration 0, Training loss = 0.8510168190451636
Iteration 10, Training loss = 0.26634641686355437
Iteration 20, Training loss = 0.2552593478905982
Iteration 30, Training loss = 0.2508736351046009
Iteration 40, Training loss = 0.2482969088182933
Iteration 50, Training loss = 0.24559336526382372
Iteration 60, Training loss = 0.24260238655235455
Iteration 70, Training loss = 0.23961595053546095
Iteration 80, Training loss = 0.23734476910409144
Iteration 90, Training loss = 0.23314120577297348
Iteration 100, Training loss = 0.23048118656672142
Iteration 110, Training loss = 0.22824079968071215
Iteration 120, Training loss = 0.22620057508997296
Iteration 130, Training loss = 0.22436356346532343
Iteration 140, Training loss = 0.22187087180965764
Iteration 150, Training loss = 0.2208222580920671
Iteration 160, Training loss = 0.21837650646190138
Iteration 170, Training loss = 0.2173748482108692
Iteration 180, Training loss = 0.21543910498348412
Iteration 190, Training loss = 0.21430976728454304
Iteration 200, Training loss = 0.21292063565069927
Iteration 210, Training loss = 0.21188951290891941
Iteration 220, Training loss = 0.21071960741051154
Iteration 230, Training loss = 0.20987675547743764
Iteration 240, Training loss = 0.20900053300575358
Iteration 250, Training loss = 0.20818650072827433
Iteration 260, Training loss = 0.20791688053936197
Iteration 270, Training loss = 0.20704264607694414
Iteration 280, Training loss = 0.20609190174634906
Iteration 290, Training loss = 0.20615758415725496
Model training time: 89.21625590324402
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9738380537516829
Iteration 10, Training loss = 0.9256652884437266
Iteration 20, Training loss = 0.8059469727790298
Iteration 30, Training loss = 0.5869344864490528
Iteration 40, Training loss = 0.4399988940660504
Iteration 50, Training loss = 0.3928223073770459
Iteration 60, Training loss = 0.3714544708314149
Iteration 70, Training loss = 0.35469591272050055
Iteration 80, Training loss = 0.34029103390836485
Iteration 90, Training loss = 0.330558690831857
Iteration 100, Training loss = 0.3231155809716902
Iteration 110, Training loss = 0.3170738333233313
Iteration 120, Training loss = 0.31214884808961896
Iteration 130, Training loss = 0.3079878494935335
Iteration 140, Training loss = 0.3044695483457639
Iteration 150, Training loss = 0.3024198353866448
Iteration 160, Training loss = 0.2990270221722875
Iteration 170, Training loss = 0.2986214138603441
Iteration 180, Training loss = 0.29513855505248776
Iteration 190, Training loss = 0.29511531485595566
Iteration 200, Training loss = 0.2914742188177247
Iteration 210, Training loss = 0.28957121239768135
Iteration 220, Training loss = 0.2882602471804273
Iteration 230, Training loss = 0.28712969226537693
Iteration 240, Training loss = 0.2865050667581927
Iteration 250, Training loss = 0.28583089870531203
Iteration 260, Training loss = 0.284400202297934
Iteration 270, Training loss = 0.2830374473390948
Iteration 280, Training loss = 0.28226215028820406
Iteration 290, Training loss = 0.28267260702047947
Model training time: 81.1491265296936
Device: cuda
Iteration 0, Training loss = 1.0473872198574785
Iteration 10, Training loss = 0.9864652470690041
Iteration 20, Training loss = 0.9518757638148063
Iteration 30, Training loss = 0.8656044262618834
Iteration 40, Training loss = 0.6597356973350912
Iteration 50, Training loss = 0.4396526983394715
Iteration 60, Training loss = 0.37091712164130186
Iteration 70, Training loss = 0.35025242258961076
Iteration 80, Training loss = 0.33704279888655253
Iteration 90, Training loss = 0.3270998296242405
Iteration 100, Training loss = 0.32008871767256
Iteration 110, Training loss = 0.3147805484308713
Iteration 120, Training loss = 0.31100737040745463
Iteration 130, Training loss = 0.30772640250155314
Iteration 140, Training loss = 0.3054929059628703
Iteration 150, Training loss = 0.30256930591115627
Iteration 160, Training loss = 0.30049518776544626
Iteration 170, Training loss = 0.2988002607643892
Iteration 180, Training loss = 0.29775687346711827
Iteration 190, Training loss = 0.29642536883480886
Iteration 200, Training loss = 0.29468880766543787
Iteration 210, Training loss = 0.2941164082447112
Iteration 220, Training loss = 0.29318942889067284
Iteration 230, Training loss = 0.29091218229077287
Iteration 240, Training loss = 0.290111711203764
Iteration 250, Training loss = 0.28831155875310804
Iteration 260, Training loss = 0.2867826233451493
Iteration 270, Training loss = 0.28560902545417566
Iteration 280, Training loss = 0.28455252232758893
Iteration 290, Training loss = 0.28333866992146495
Model training time: 81.25797271728516
Device: cuda
Iteration 0, Training loss = 1.029507737228836
Iteration 10, Training loss = 1.006072819808831
Iteration 20, Training loss = 0.9952836333265627
Iteration 30, Training loss = 0.9779365863201123
Iteration 40, Training loss = 0.9425280174771369
Iteration 50, Training loss = 0.8499248059475479
Iteration 60, Training loss = 0.6565330292003743
Iteration 70, Training loss = 0.46027829012145166
Iteration 80, Training loss = 0.3911467622155729
Iteration 90, Training loss = 0.37045735372725314
Iteration 100, Training loss = 0.356179338650427
Iteration 110, Training loss = 0.34527079120350346
Iteration 120, Training loss = 0.3355824603982594
Iteration 130, Training loss = 0.32740869124730426
Iteration 140, Training loss = 0.3214887633991702
Iteration 150, Training loss = 0.31625475745270215
Iteration 160, Training loss = 0.311304691405112
Iteration 170, Training loss = 0.30717900676139887
Iteration 180, Training loss = 0.30309075678604236
Iteration 190, Training loss = 0.2998506294356452
Iteration 200, Training loss = 0.2977297543255603
Iteration 210, Training loss = 0.2942746191640983
Iteration 220, Training loss = 0.29178488276143005
Iteration 230, Training loss = 0.28968078855443113
Iteration 240, Training loss = 0.2876730572414283
Iteration 250, Training loss = 0.28550284589834257
Iteration 260, Training loss = 0.2839585773754811
Iteration 270, Training loss = 0.2835142690202464
Iteration 280, Training loss = 0.28045947813757377
Iteration 290, Training loss = 0.27933045336301776
Model training time: 76.30604982376099
Device: cuda
Iteration 0, Training loss = 1.0182890926582226
Iteration 10, Training loss = 1.0036233919830138
Iteration 20, Training loss = 0.9891641177417
Iteration 30, Training loss = 0.9625969470123162
Iteration 40, Training loss = 0.8902856746445531
Iteration 50, Training loss = 0.6792055974547989
Iteration 60, Training loss = 0.4322811649473393
Iteration 70, Training loss = 0.36052137476522567
Iteration 80, Training loss = 0.3437154757371847
Iteration 90, Training loss = 0.33202669873905644
Iteration 100, Training loss = 0.3232288457057326
Iteration 110, Training loss = 0.3161074779747765
Iteration 120, Training loss = 0.3110726368456071
Iteration 130, Training loss = 0.3061976576772865
Iteration 140, Training loss = 0.30327144139198864
Iteration 150, Training loss = 0.29905268866658785
Iteration 160, Training loss = 0.2959125597335866
Iteration 170, Training loss = 0.29432388297889545
Iteration 180, Training loss = 0.2915155690721268
Iteration 190, Training loss = 0.2900673727626386
Iteration 200, Training loss = 0.28821892222920475
Iteration 210, Training loss = 0.2862326812700949
Iteration 220, Training loss = 0.28499282180255164
Iteration 230, Training loss = 0.28356152909677385
Iteration 240, Training loss = 0.28265247483184375
Iteration 250, Training loss = 0.281382200366633
Iteration 260, Training loss = 0.27959202957038143
Iteration 270, Training loss = 0.2785174871124507
Iteration 280, Training loss = 0.27746424330893343
Iteration 290, Training loss = 0.27657411063926807
Model training time: 79.14053726196289
Device: cuda
Iteration 0, Training loss = 1.0072219011864225
Iteration 10, Training loss = 0.9430251579353774
Iteration 20, Training loss = 0.8252049225252032
Iteration 30, Training loss = 0.5901215679691609
Iteration 40, Training loss = 0.4321255804835886
Iteration 50, Training loss = 0.38952551681350395
Iteration 60, Training loss = 0.36686689395835437
Iteration 70, Training loss = 0.3492634069660436
Iteration 80, Training loss = 0.3373011424057726
Iteration 90, Training loss = 0.327264039484775
Iteration 100, Training loss = 0.31984917934678025
Iteration 110, Training loss = 0.3144778609995681
Iteration 120, Training loss = 0.3102128038515791
Iteration 130, Training loss = 0.3068554948493478
Iteration 140, Training loss = 0.3033239498086598
Iteration 150, Training loss = 0.3010658483574356
Iteration 160, Training loss = 0.29848845137490165
Iteration 170, Training loss = 0.2966341528555621
Iteration 180, Training loss = 0.2944038610383508
Iteration 190, Training loss = 0.2924351230767614
Iteration 200, Training loss = 0.29032722612222034
Iteration 210, Training loss = 0.289112982686591
Iteration 220, Training loss = 0.28764843307255544
Iteration 230, Training loss = 0.2860427472902381
Iteration 240, Training loss = 0.2849415902066346
Iteration 250, Training loss = 0.28342758522229494
Iteration 260, Training loss = 0.2823908549647976
Iteration 270, Training loss = 0.2811589261183992
Iteration 280, Training loss = 0.27931539822315826
Iteration 290, Training loss = 0.2788980697520113
Model training time: 78.17671060562134
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6152362843642488
Iteration 10, Training loss = 0.2563331085774634
Iteration 20, Training loss = 0.24904263253517198
Iteration 30, Training loss = 0.24399359232705572
Iteration 40, Training loss = 0.23954613780341863
Iteration 50, Training loss = 0.2343596619900298
Iteration 60, Training loss = 0.23193650696300655
Iteration 70, Training loss = 0.22845864814260733
Iteration 80, Training loss = 0.22515044580911092
Iteration 90, Training loss = 0.22179109106461206
Model training time: 29.66094207763672
Device: cuda
Iteration 0, Training loss = 0.7336448757832753
Iteration 10, Training loss = 0.25805328073708905
Iteration 20, Training loss = 0.24570083287027147
Iteration 30, Training loss = 0.24092183470869985
Iteration 40, Training loss = 0.23266426131921114
Iteration 50, Training loss = 0.22819267926008804
Iteration 60, Training loss = 0.22294099974459497
Iteration 70, Training loss = 0.22217218124348184
Iteration 80, Training loss = 0.21619554443491829
Iteration 90, Training loss = 0.21305538721130665
Model training time: 29.652058124542236
Device: cuda
Iteration 0, Training loss = 0.6266983621506299
Iteration 10, Training loss = 0.2508450492784597
Iteration 20, Training loss = 0.2431059055354284
Iteration 30, Training loss = 0.23631253061087235
Iteration 40, Training loss = 0.2318311151171076
Iteration 50, Training loss = 0.22481425811991024
Iteration 60, Training loss = 0.22154097266243275
Iteration 70, Training loss = 0.21650595639062964
Iteration 80, Training loss = 0.21470031027056743
Iteration 90, Training loss = 0.21104292920678133
Model training time: 30.08984875679016
Device: cuda
Iteration 0, Training loss = 0.6235440530350819
Iteration 10, Training loss = 0.25030202983658095
Iteration 20, Training loss = 0.23979300670865653
Iteration 30, Training loss = 0.23009547141295125
Iteration 40, Training loss = 0.2227929691977547
Iteration 50, Training loss = 0.21757249256982897
Iteration 60, Training loss = 0.21277487445352733
Iteration 70, Training loss = 0.2116277761001518
Iteration 80, Training loss = 0.20869415307390518
Iteration 90, Training loss = 0.20552899119790624
Model training time: 30.74226450920105
Device: cuda
Iteration 0, Training loss = 0.7047632291985019
Iteration 10, Training loss = 0.25348097517870477
Iteration 20, Training loss = 0.24554837307924235
Iteration 30, Training loss = 0.24097287100582307
Iteration 40, Training loss = 0.23488137169160705
Iteration 50, Training loss = 0.229253512793693
Iteration 60, Training loss = 0.2253703353341651
Iteration 70, Training loss = 0.22146851515424423
Iteration 80, Training loss = 0.21930743545149836
Iteration 90, Training loss = 0.2175997799253406
Model training time: 30.058078050613403
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0554278945001427
Iteration 10, Training loss = 0.8832456567437177
Iteration 20, Training loss = 0.4580257586091037
Iteration 30, Training loss = 0.3687765532069736
Iteration 40, Training loss = 0.3396617934323739
Iteration 50, Training loss = 0.3232355412941624
Iteration 60, Training loss = 0.3132452964782715
Iteration 70, Training loss = 0.3060156653324763
Iteration 80, Training loss = 0.30143460998500604
Iteration 90, Training loss = 0.2969766065550311
Model training time: 26.32849955558777
Device: cuda
Iteration 0, Training loss = 1.0404154197605335
Iteration 10, Training loss = 0.8000259137383982
Iteration 20, Training loss = 0.44181654497909084
Iteration 30, Training loss = 0.3708174471261997
Iteration 40, Training loss = 0.34187627468564086
Iteration 50, Training loss = 0.32410347015385466
Iteration 60, Training loss = 0.3119627391370598
Iteration 70, Training loss = 0.30426019869276866
Iteration 80, Training loss = 0.2983929217581588
Iteration 90, Training loss = 0.29417634190280656
Model training time: 26.182915925979614
Device: cuda
Iteration 0, Training loss = 1.1618375968242036
Iteration 10, Training loss = 0.7332671923913817
Iteration 20, Training loss = 0.3821274255928786
Iteration 30, Training loss = 0.3382735316808097
Iteration 40, Training loss = 0.3194858278485312
Iteration 50, Training loss = 0.3075931290353554
Iteration 60, Training loss = 0.29889352691634263
Iteration 70, Training loss = 0.2928405778059637
Iteration 80, Training loss = 0.28818426803114333
Iteration 90, Training loss = 0.2834915019535788
Model training time: 26.74698781967163
Device: cuda
Iteration 0, Training loss = 0.999419003005189
Iteration 10, Training loss = 0.8998616283066607
Iteration 20, Training loss = 0.45569986191348755
Iteration 30, Training loss = 0.3527230636221199
Iteration 40, Training loss = 0.3264973509858772
Iteration 50, Training loss = 0.3132738246433977
Iteration 60, Training loss = 0.3048598165241417
Iteration 70, Training loss = 0.2991971412430639
Iteration 80, Training loss = 0.2944672269665677
Iteration 90, Training loss = 0.2918486427425762
Model training time: 26.191073656082153
Device: cuda
Iteration 0, Training loss = 1.099558649143735
Iteration 10, Training loss = 0.959429521203617
Iteration 20, Training loss = 0.7428717594503781
Iteration 30, Training loss = 0.39965873560755727
Iteration 40, Training loss = 0.35001113060591876
Iteration 50, Training loss = 0.32819147415207206
Iteration 60, Training loss = 0.3171557129725166
Iteration 70, Training loss = 0.30818484291650244
Iteration 80, Training loss = 0.30333551760888905
Iteration 90, Training loss = 0.2993790018529708
Model training time: 25.785757780075073
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7321282321416237
Iteration 10, Training loss = 0.25912528431070025
Iteration 20, Training loss = 0.24700119807092463
Iteration 30, Training loss = 0.24248911969471668
Iteration 40, Training loss = 0.2368026246915117
Iteration 50, Training loss = 0.2317796844791099
Iteration 60, Training loss = 0.2276477148567421
Iteration 70, Training loss = 0.22577253865878938
Iteration 80, Training loss = 0.22311616976911894
Iteration 90, Training loss = 0.22252292949076435
Model training time: 29.831079959869385
Device: cuda
Iteration 0, Training loss = 0.6938146896984266
Iteration 10, Training loss = 0.25891927363359984
Iteration 20, Training loss = 0.25007959959155696
Iteration 30, Training loss = 0.24631733688467367
Iteration 40, Training loss = 0.24346166183263207
Iteration 50, Training loss = 0.2415821912927904
Iteration 60, Training loss = 0.23807361070516606
Iteration 70, Training loss = 0.23580141898226623
Iteration 80, Training loss = 0.23034345200240325
Iteration 90, Training loss = 0.22693925955589267
Model training time: 29.928887605667114
Device: cuda
Iteration 0, Training loss = 0.7471315975926348
Iteration 10, Training loss = 0.25539073074497465
Iteration 20, Training loss = 0.2479224281322553
Iteration 30, Training loss = 0.24025960968456406
Iteration 40, Training loss = 0.23557676403706776
Iteration 50, Training loss = 0.22935822901230504
Iteration 60, Training loss = 0.22429262191633095
Iteration 70, Training loss = 0.22088843947591413
Iteration 80, Training loss = 0.21798519531021948
Iteration 90, Training loss = 0.21668518579380525
Model training time: 30.336079120635986
Device: cuda
Iteration 0, Training loss = 0.7184263970252972
Iteration 10, Training loss = 0.2545292072969934
Iteration 20, Training loss = 0.24413793829184224
Iteration 30, Training loss = 0.23764515038273762
Iteration 40, Training loss = 0.23482881439624778
Iteration 50, Training loss = 0.2319374209728794
Iteration 60, Training loss = 0.22896541017984998
Iteration 70, Training loss = 0.22709342182258477
Iteration 80, Training loss = 0.22405299465149497
Iteration 90, Training loss = 0.21996651111161653
Model training time: 30.232757806777954
Device: cuda
Iteration 0, Training loss = 0.8294163842995962
Iteration 10, Training loss = 0.25588553654398893
Iteration 20, Training loss = 0.24512944248131507
Iteration 30, Training loss = 0.2387965089025129
Iteration 40, Training loss = 0.2335081648639435
Iteration 50, Training loss = 0.22913628499864955
Iteration 60, Training loss = 0.22759668615417203
Iteration 70, Training loss = 0.2265056576129895
Iteration 80, Training loss = 0.22355396116989246
Iteration 90, Training loss = 0.2213475149538782
Model training time: 31.176772356033325
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0187922554891466
Iteration 10, Training loss = 0.9682464625524438
Iteration 20, Training loss = 0.7869002564229827
Iteration 30, Training loss = 0.4497665865841695
Iteration 40, Training loss = 0.3743116362515279
Iteration 50, Training loss = 0.3403400591966035
Iteration 60, Training loss = 0.3238060017282836
Iteration 70, Training loss = 0.31322716831585057
Iteration 80, Training loss = 0.3058262416274075
Iteration 90, Training loss = 0.30156441903920567
Model training time: 27.498734712600708
Device: cuda
Iteration 0, Training loss = 1.0110680193140886
Iteration 10, Training loss = 0.8842159486623202
Iteration 20, Training loss = 0.5492741492923331
Iteration 30, Training loss = 0.3815037122239237
Iteration 40, Training loss = 0.3439393032000261
Iteration 50, Training loss = 0.32835740722031986
Iteration 60, Training loss = 0.31466752294325023
Iteration 70, Training loss = 0.30662164761536365
Iteration 80, Training loss = 0.2997517667098898
Iteration 90, Training loss = 0.29579180219899054
Model training time: 25.781660079956055
Device: cuda
Iteration 0, Training loss = 1.0284269316760815
Iteration 10, Training loss = 0.9301106230072353
Iteration 20, Training loss = 0.5837044848336114
Iteration 30, Training loss = 0.37681952212455766
Iteration 40, Training loss = 0.33918313379736914
Iteration 50, Training loss = 0.31793960446608815
Iteration 60, Training loss = 0.3049159712981487
Iteration 70, Training loss = 0.29618232441697145
Iteration 80, Training loss = 0.2893163304829943
Iteration 90, Training loss = 0.28632678991354604
Model training time: 25.676485061645508
Device: cuda
Iteration 0, Training loss = 1.0346709484063485
Iteration 10, Training loss = 0.8718550936611378
Iteration 20, Training loss = 0.4847411282972437
Iteration 30, Training loss = 0.37599694592077376
Iteration 40, Training loss = 0.3395245117697739
Iteration 50, Training loss = 0.3183636429949083
Iteration 60, Training loss = 0.306136614098641
Iteration 70, Training loss = 0.29746726983123356
Iteration 80, Training loss = 0.29072086675011594
Iteration 90, Training loss = 0.2861443200837011
Model training time: 25.736502647399902
Device: cuda
Iteration 0, Training loss = 1.2235733541313576
Iteration 10, Training loss = 0.9778767740668882
Iteration 20, Training loss = 0.8557039581347203
Iteration 30, Training loss = 0.4330880030053825
Iteration 40, Training loss = 0.35465339027741105
Iteration 50, Training loss = 0.3305468487278851
Iteration 60, Training loss = 0.3161045525960876
Iteration 70, Training loss = 0.3073748717273491
Iteration 80, Training loss = 0.2981524147849152
Iteration 90, Training loss = 0.29303052323163997
Model training time: 25.38843274116516
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7490785215499897
Iteration 10, Training loss = 0.2602811188657503
Iteration 20, Training loss = 0.25379403936114286
Iteration 30, Training loss = 0.24826515926687037
Iteration 40, Training loss = 0.24227600843434172
Iteration 50, Training loss = 0.23773410452016885
Iteration 60, Training loss = 0.23254945971395657
Iteration 70, Training loss = 0.23096195464404884
Iteration 80, Training loss = 0.2284148579130426
Iteration 90, Training loss = 0.22435545564993567
Model training time: 29.685785055160522
Device: cuda
Iteration 0, Training loss = 0.6059957280539084
Iteration 10, Training loss = 0.2586841538620456
Iteration 20, Training loss = 0.2495687139638956
Iteration 30, Training loss = 0.2423889568966368
Iteration 40, Training loss = 0.23803898926086473
Iteration 50, Training loss = 0.2316300226510435
Iteration 60, Training loss = 0.2257184310307825
Iteration 70, Training loss = 0.22391267387187425
Iteration 80, Training loss = 0.2208953434286486
Iteration 90, Training loss = 0.2173234236197195
Model training time: 29.669355630874634
Device: cuda
Iteration 0, Training loss = 0.6784232181771366
Iteration 10, Training loss = 0.25938627260606645
Iteration 20, Training loss = 0.25133916840029225
Iteration 30, Training loss = 0.24554518970170458
Iteration 40, Training loss = 0.24169569437342567
Iteration 50, Training loss = 0.23602982333316896
Iteration 60, Training loss = 0.22973804233442757
Iteration 70, Training loss = 0.22454453652031756
Iteration 80, Training loss = 0.2204712253909756
Iteration 90, Training loss = 0.21769015361433444
Model training time: 29.632184505462646
Device: cuda
Iteration 0, Training loss = 0.6691085527052626
Iteration 10, Training loss = 0.2577359503234066
Iteration 20, Training loss = 0.24683471837481438
Iteration 30, Training loss = 0.2392035580199698
Iteration 40, Training loss = 0.2355082662065248
Iteration 50, Training loss = 0.22993833146953352
Iteration 60, Training loss = 0.22525261587278855
Iteration 70, Training loss = 0.2207027589497359
Iteration 80, Training loss = 0.21706182729218892
Iteration 90, Training loss = 0.21436339433210483
Model training time: 30.921952486038208
Device: cuda
Iteration 0, Training loss = 0.7503042395276148
Iteration 10, Training loss = 0.26193207405183627
Iteration 20, Training loss = 0.2520854699510883
Iteration 30, Training loss = 0.24884049997525515
Iteration 40, Training loss = 0.24331100605392225
Iteration 50, Training loss = 0.2406258950918769
Iteration 60, Training loss = 0.23867871803074067
Iteration 70, Training loss = 0.23597060651450918
Iteration 80, Training loss = 0.23414213768238032
Iteration 90, Training loss = 0.2313591402725897
Model training time: 29.811963319778442
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1051424157216354
Iteration 10, Training loss = 0.9473518089971681
Iteration 20, Training loss = 0.6297295175600743
Iteration 30, Training loss = 0.3940167355796565
Iteration 40, Training loss = 0.35088314900651646
Iteration 50, Training loss = 0.3277423586246472
Iteration 60, Training loss = 0.3152300502367066
Iteration 70, Training loss = 0.3080331068395992
Iteration 80, Training loss = 0.30287584590451155
Iteration 90, Training loss = 0.2992889352754694
Model training time: 25.78584623336792
Device: cuda
Iteration 0, Training loss = 1.0095961514302498
Iteration 10, Training loss = 0.9715098360022485
Iteration 20, Training loss = 0.7882293256872518
Iteration 30, Training loss = 0.4096502107410615
Iteration 40, Training loss = 0.3596221344626468
Iteration 50, Training loss = 0.33726510304759666
Iteration 60, Training loss = 0.3215307108877938
Iteration 70, Training loss = 0.310896926815959
Iteration 80, Training loss = 0.3040514523208429
Iteration 90, Training loss = 0.3007861272148464
Model training time: 25.926625967025757
Device: cuda
Iteration 0, Training loss = 1.0253732005755107
Iteration 10, Training loss = 0.9615082867479555
Iteration 20, Training loss = 0.71947398925749
Iteration 30, Training loss = 0.4010678871674238
Iteration 40, Training loss = 0.3549979836612508
Iteration 50, Training loss = 0.3327628557664761
Iteration 60, Training loss = 0.31836065246863066
Iteration 70, Training loss = 0.3094535927841629
Iteration 80, Training loss = 0.3028394296981286
Iteration 90, Training loss = 0.29709683052727565
Model training time: 25.734824419021606
Device: cuda
Iteration 0, Training loss = 1.0272429876281444
Iteration 10, Training loss = 0.9358482188072758
Iteration 20, Training loss = 0.5992847398859291
Iteration 30, Training loss = 0.367949158889084
Iteration 40, Training loss = 0.33597449526406714
Iteration 50, Training loss = 0.3194999310417452
Iteration 60, Training loss = 0.3091235208698517
Iteration 70, Training loss = 0.3023636246073073
Iteration 80, Training loss = 0.2959881215783709
Iteration 90, Training loss = 0.2920752509996511
Model training time: 25.71908736228943
Device: cuda
Iteration 0, Training loss = 1.1015926340351934
Iteration 10, Training loss = 0.8328061930223364
Iteration 20, Training loss = 0.4561263548147275
Iteration 30, Training loss = 0.36767606523589813
Iteration 40, Training loss = 0.3409240896575117
Iteration 50, Training loss = 0.32451880820419476
Iteration 60, Training loss = 0.313772520005415
Iteration 70, Training loss = 0.30503283916176227
Iteration 80, Training loss = 0.2990935633075986
Iteration 90, Training loss = 0.2940350707962317
Model training time: 25.838625192642212
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6809331394624019
Iteration 10, Training loss = 0.2542211283808169
Iteration 20, Training loss = 0.24590913825420943
Iteration 30, Training loss = 0.23971860225937794
Iteration 40, Training loss = 0.2331904425209271
Iteration 50, Training loss = 0.22981130691686114
Iteration 60, Training loss = 0.22750165068728911
Iteration 70, Training loss = 0.22516424093701412
Iteration 80, Training loss = 0.222420958865092
Iteration 90, Training loss = 0.219763675029727
Iteration 100, Training loss = 0.21718758985328213
Iteration 110, Training loss = 0.21541254816279895
Iteration 120, Training loss = 0.21404026272360255
Iteration 130, Training loss = 0.21180643803543514
Iteration 140, Training loss = 0.2092782434993896
Iteration 150, Training loss = 0.20913880077249186
Iteration 160, Training loss = 0.2062589043148474
Iteration 170, Training loss = 0.20567856556263522
Iteration 180, Training loss = 0.20556964610077907
Iteration 190, Training loss = 0.20397699519919887
Model training time: 59.46873235702515
Device: cuda
Iteration 0, Training loss = 0.622704692148932
Iteration 10, Training loss = 0.25484634557927865
Iteration 20, Training loss = 0.24636351335163853
Iteration 30, Training loss = 0.2386515406522774
Iteration 40, Training loss = 0.23218457184839938
Iteration 50, Training loss = 0.22915612996200432
Iteration 60, Training loss = 0.22371491599054152
Iteration 70, Training loss = 0.2214715570787301
Iteration 80, Training loss = 0.21894665098420663
Iteration 90, Training loss = 0.2152702712278435
Iteration 100, Training loss = 0.21248892190807683
Iteration 110, Training loss = 0.21036016235604954
Iteration 120, Training loss = 0.20772544089434802
Iteration 130, Training loss = 0.20662510826967764
Iteration 140, Training loss = 0.2049454977546913
Iteration 150, Training loss = 0.20306778054882363
Iteration 160, Training loss = 0.20283591477335364
Iteration 170, Training loss = 0.20140159238075864
Iteration 180, Training loss = 0.2001511192912065
Iteration 190, Training loss = 0.19977161547412042
Model training time: 59.957329511642456
Device: cuda
Iteration 0, Training loss = 0.6749461772649185
Iteration 10, Training loss = 0.25578426112587327
Iteration 20, Training loss = 0.25054690731320406
Iteration 30, Training loss = 0.24314285548412856
Iteration 40, Training loss = 0.2363243350300236
Iteration 50, Training loss = 0.23097087414080394
Iteration 60, Training loss = 0.22482793087112732
Iteration 70, Training loss = 0.2214760884354656
Iteration 80, Training loss = 0.21704047655569758
Iteration 90, Training loss = 0.21421302639055942
Iteration 100, Training loss = 0.21166467382280146
Iteration 110, Training loss = 0.2097636391431237
Iteration 120, Training loss = 0.2078266280932703
Iteration 130, Training loss = 0.20584531988642643
Iteration 140, Training loss = 0.2044326561876541
Iteration 150, Training loss = 0.20410470743686104
Iteration 160, Training loss = 0.20266608328778962
Iteration 170, Training loss = 0.20008996132635265
Iteration 180, Training loss = 0.20037666326703657
Iteration 190, Training loss = 0.19922832313223163
Model training time: 59.61976933479309
Device: cuda
Iteration 0, Training loss = 0.628442131522773
Iteration 10, Training loss = 0.25027764764960836
Iteration 20, Training loss = 0.2440403852485804
Iteration 30, Training loss = 0.23920907400512464
Iteration 40, Training loss = 0.23548383787634292
Iteration 50, Training loss = 0.22899641913636296
Iteration 60, Training loss = 0.22549456642301763
Iteration 70, Training loss = 0.22206797378794582
Iteration 80, Training loss = 0.21678198096544846
Iteration 90, Training loss = 0.21389265380043915
Iteration 100, Training loss = 0.21028836190268613
Iteration 110, Training loss = 0.20661105312730954
Iteration 120, Training loss = 0.2055809959265345
Iteration 130, Training loss = 0.20234128242529534
Iteration 140, Training loss = 0.20167445646967866
Iteration 150, Training loss = 0.2009473209435813
Iteration 160, Training loss = 0.19949494676601484
Iteration 170, Training loss = 0.19932585803494937
Iteration 180, Training loss = 0.1994025990150977
Iteration 190, Training loss = 0.19708668422583797
Model training time: 59.94241905212402
Device: cuda
Iteration 0, Training loss = 0.6304339357620277
Iteration 10, Training loss = 0.25321123979373833
Iteration 20, Training loss = 0.24433331436293138
Iteration 30, Training loss = 0.23713820798385546
Iteration 40, Training loss = 0.2320820138146336
Iteration 50, Training loss = 0.22730694995554174
Iteration 60, Training loss = 0.2249258051171971
Iteration 70, Training loss = 0.22059150806803635
Iteration 80, Training loss = 0.21655274387718976
Iteration 90, Training loss = 0.21620336905625706
Iteration 100, Training loss = 0.21250486528671883
Iteration 110, Training loss = 0.21055201203494833
Iteration 120, Training loss = 0.20939211327816554
Iteration 130, Training loss = 0.20737045528232187
Iteration 140, Training loss = 0.2069957718829026
Iteration 150, Training loss = 0.20623136444944115
Iteration 160, Training loss = 0.20479903316152268
Iteration 170, Training loss = 0.20269897126633188
Iteration 180, Training loss = 0.20146398285880757
Iteration 190, Training loss = 0.20243732677997597
Model training time: 59.41170620918274
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.002237180724812
Iteration 10, Training loss = 0.9207740627625138
Iteration 20, Training loss = 0.5532151162624359
Iteration 30, Training loss = 0.3908646915845825
Iteration 40, Training loss = 0.3554850420082249
Iteration 50, Training loss = 0.33193610061481954
Iteration 60, Training loss = 0.31799788909833787
Iteration 70, Training loss = 0.30703223824213094
Iteration 80, Training loss = 0.2997646658604848
Iteration 90, Training loss = 0.2952363781163082
Iteration 100, Training loss = 0.29073819608072154
Iteration 110, Training loss = 0.28753623992636584
Iteration 120, Training loss = 0.28542754831521405
Iteration 130, Training loss = 0.28265294497427734
Iteration 140, Training loss = 0.2800769023417275
Iteration 150, Training loss = 0.2781995345212987
Iteration 160, Training loss = 0.27715083420420616
Iteration 170, Training loss = 0.2754594958634768
Iteration 180, Training loss = 0.2739877880051516
Iteration 190, Training loss = 0.27243495527384937
Model training time: 51.365193605422974
Device: cuda
Iteration 0, Training loss = 1.0151697541204627
Iteration 10, Training loss = 0.8807304613832114
Iteration 20, Training loss = 0.4366741969389616
Iteration 30, Training loss = 0.35282477405336166
Iteration 40, Training loss = 0.3266997444600875
Iteration 50, Training loss = 0.3134709125123738
Iteration 60, Training loss = 0.30522181410432436
Iteration 70, Training loss = 0.29810660116482474
Iteration 80, Training loss = 0.29337933857515813
Iteration 90, Training loss = 0.2888939340333432
Iteration 100, Training loss = 0.2854086474519997
Iteration 110, Training loss = 0.28211494805156323
Iteration 120, Training loss = 0.27947530459954545
Iteration 130, Training loss = 0.2772769016272204
Iteration 140, Training loss = 0.27519784853366264
Iteration 150, Training loss = 0.2735999060281809
Iteration 160, Training loss = 0.2716587046206285
Iteration 170, Training loss = 0.2700359082740286
Iteration 180, Training loss = 0.26863952452577833
Iteration 190, Training loss = 0.2673299925554778
Model training time: 53.54318070411682
Device: cuda
Iteration 0, Training loss = 1.0159679688693244
Iteration 10, Training loss = 0.8854170304853559
Iteration 20, Training loss = 0.47397735614131614
Iteration 30, Training loss = 0.37642313489591445
Iteration 40, Training loss = 0.3458922752291684
Iteration 50, Training loss = 0.3285539589714313
Iteration 60, Training loss = 0.31636691230218766
Iteration 70, Training loss = 0.30847372031874126
Iteration 80, Training loss = 0.30285111847131146
Iteration 90, Training loss = 0.2980103382597799
Iteration 100, Training loss = 0.2949316339648288
Iteration 110, Training loss = 0.29039724185558907
Iteration 120, Training loss = 0.28799933472261335
Iteration 130, Training loss = 0.2853839682496112
Iteration 140, Training loss = 0.2826300598141076
Iteration 150, Training loss = 0.28060329658685673
Iteration 160, Training loss = 0.278503279503129
Iteration 170, Training loss = 0.2762207035425205
Iteration 180, Training loss = 0.27483367423216504
Iteration 190, Training loss = 0.2728315244478304
Model training time: 51.61934733390808
Device: cuda
Iteration 0, Training loss = 1.0427207121814506
Iteration 10, Training loss = 0.9751341377479442
Iteration 20, Training loss = 0.8153472571557271
Iteration 30, Training loss = 0.42298047734055544
Iteration 40, Training loss = 0.3530107984220348
Iteration 50, Training loss = 0.32590361011489
Iteration 60, Training loss = 0.31224119000964695
Iteration 70, Training loss = 0.3053935730946813
Iteration 80, Training loss = 0.3007014262071554
Iteration 90, Training loss = 0.29700067459385177
Iteration 100, Training loss = 0.29423182443288215
Iteration 110, Training loss = 0.29169182173439845
Iteration 120, Training loss = 0.28983180098919475
Iteration 130, Training loss = 0.28693880734668264
Iteration 140, Training loss = 0.2854251690964768
Iteration 150, Training loss = 0.2831925791530793
Iteration 160, Training loss = 0.2819703512865564
Iteration 170, Training loss = 0.28155212421060183
Iteration 180, Training loss = 0.27863036743972613
Iteration 190, Training loss = 0.2778702076650472
Model training time: 51.17653679847717
Device: cuda
Iteration 0, Training loss = 1.064209308889177
Iteration 10, Training loss = 0.9091622946055039
Iteration 20, Training loss = 0.4754271332336509
Iteration 30, Training loss = 0.3630374805362904
Iteration 40, Training loss = 0.3382553140754285
Iteration 50, Training loss = 0.3232461432903861
Iteration 60, Training loss = 0.31440457845655617
Iteration 70, Training loss = 0.3063653739178238
Iteration 80, Training loss = 0.3014057223203678
Iteration 90, Training loss = 0.29660199625768524
Iteration 100, Training loss = 0.2931197374555224
Iteration 110, Training loss = 0.2907906980906132
Iteration 120, Training loss = 0.28788560128586305
Iteration 130, Training loss = 0.284908948853108
Iteration 140, Training loss = 0.2826703881871873
Iteration 150, Training loss = 0.2802765325503649
Iteration 160, Training loss = 0.2780603644424591
Iteration 170, Training loss = 0.27560510374785624
Iteration 180, Training loss = 0.2730524477319441
Iteration 190, Training loss = 0.2711160547203488
Model training time: 51.5258686542511
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.7150397407259919
Iteration 10, Training loss = 0.259357871496735
Iteration 20, Training loss = 0.25118057595358956
Iteration 30, Training loss = 0.24653669000391798
Iteration 40, Training loss = 0.24138195909883664
Iteration 50, Training loss = 0.23802508982483317
Iteration 60, Training loss = 0.23490752002611254
Iteration 70, Training loss = 0.23223851257620226
Iteration 80, Training loss = 0.23056308565220396
Iteration 90, Training loss = 0.22826028669225998
Iteration 100, Training loss = 0.22711274348163374
Iteration 110, Training loss = 0.2249898904763558
Iteration 120, Training loss = 0.22407558722340543
Iteration 130, Training loss = 0.22200000847595325
Iteration 140, Training loss = 0.22163311874376979
Iteration 150, Training loss = 0.2195411833084148
Iteration 160, Training loss = 0.21887310417953898
Iteration 170, Training loss = 0.21818482850628776
Iteration 180, Training loss = 0.21621365143337112
Iteration 190, Training loss = 0.21591154320372474
Model training time: 61.97509789466858
Device: cuda
Iteration 0, Training loss = 0.7708397379531953
Iteration 10, Training loss = 0.2632658553944118
Iteration 20, Training loss = 0.2547142601387512
Iteration 30, Training loss = 0.2510982259460118
Iteration 40, Training loss = 0.24779106129482747
Iteration 50, Training loss = 0.24487670457017593
Iteration 60, Training loss = 0.24293790397724668
Iteration 70, Training loss = 0.24235184317913608
Iteration 80, Training loss = 0.23992229202663265
Iteration 90, Training loss = 0.24096304253823514
Iteration 100, Training loss = 0.23848635937280702
Iteration 110, Training loss = 0.23813552895317908
Iteration 120, Training loss = 0.236716874771648
Iteration 130, Training loss = 0.23457093432473675
Iteration 140, Training loss = 0.2342817843824193
Iteration 150, Training loss = 0.2350102719189464
Iteration 160, Training loss = 0.23214509953623233
Iteration 170, Training loss = 0.23257834309541084
Iteration 180, Training loss = 0.2314077769788567
Iteration 190, Training loss = 0.23130211637215914
Model training time: 64.27507185935974
Device: cuda
Iteration 0, Training loss = 0.7174790498715092
Iteration 10, Training loss = 0.2547526967842222
Iteration 20, Training loss = 0.24584369393794434
Iteration 30, Training loss = 0.2405349756208595
Iteration 40, Training loss = 0.2349486651915859
Iteration 50, Training loss = 0.2316707050166844
Iteration 60, Training loss = 0.22940584747255713
Iteration 70, Training loss = 0.22709584632069593
Iteration 80, Training loss = 0.2249384260048037
Iteration 90, Training loss = 0.22386694055248574
Iteration 100, Training loss = 0.2216957744192962
Iteration 110, Training loss = 0.22158826052566657
Iteration 120, Training loss = 0.22004652469630404
Iteration 130, Training loss = 0.21929483846333866
Iteration 140, Training loss = 0.2180904489064562
Iteration 150, Training loss = 0.21733276103285776
Iteration 160, Training loss = 0.21740153057563708
Iteration 170, Training loss = 0.21631151038235513
Iteration 180, Training loss = 0.215385422218537
Iteration 190, Training loss = 0.21598356825430037
Model training time: 59.74356245994568
Device: cuda
Iteration 0, Training loss = 0.8798358833444291
Iteration 10, Training loss = 0.2713142179204646
Iteration 20, Training loss = 0.24402295049405903
Iteration 30, Training loss = 0.23580641655818277
Iteration 40, Training loss = 0.23023058322892673
Iteration 50, Training loss = 0.22800823081518717
Iteration 60, Training loss = 0.22464290401641873
Iteration 70, Training loss = 0.221954598195023
Iteration 80, Training loss = 0.21980975809448583
Iteration 90, Training loss = 0.2179797282397459
Iteration 100, Training loss = 0.2158666041450224
Iteration 110, Training loss = 0.21443553855597686
Iteration 120, Training loss = 0.213022338775333
Iteration 130, Training loss = 0.21182229351882198
Iteration 140, Training loss = 0.21132934561386202
Iteration 150, Training loss = 0.20985858574293662
Iteration 160, Training loss = 0.20903107811863295
Iteration 170, Training loss = 0.20947473179458995
Iteration 180, Training loss = 0.2069544615448961
Iteration 190, Training loss = 0.2084607089271292
Model training time: 59.24445080757141
Device: cuda
Iteration 0, Training loss = 0.9132152496328676
Iteration 10, Training loss = 0.2539699972755667
Iteration 20, Training loss = 0.24319281353466754
Iteration 30, Training loss = 0.23505307057341515
Iteration 40, Training loss = 0.23206646114155866
Iteration 50, Training loss = 0.2294665850734941
Iteration 60, Training loss = 0.2260438767824196
Iteration 70, Training loss = 0.2228520269267225
Iteration 80, Training loss = 0.2199270567600278
Iteration 90, Training loss = 0.21929255304273199
Iteration 100, Training loss = 0.2176721842536604
Iteration 110, Training loss = 0.21621947875921277
Iteration 120, Training loss = 0.21405964141378656
Iteration 130, Training loss = 0.212982182521463
Iteration 140, Training loss = 0.2124243877504183
Iteration 150, Training loss = 0.2115094268667525
Iteration 160, Training loss = 0.21152269084384476
Iteration 170, Training loss = 0.2105465423369753
Iteration 180, Training loss = 0.2099015137207681
Iteration 190, Training loss = 0.21151217236970934
Model training time: 60.03224730491638
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0470631825175263
Iteration 10, Training loss = 0.9858690156164952
Iteration 20, Training loss = 0.9159813647108953
Iteration 30, Training loss = 0.6333110983245038
Iteration 40, Training loss = 0.4103855085401719
Iteration 50, Training loss = 0.3583047096999947
Iteration 60, Training loss = 0.33262033019088894
Iteration 70, Training loss = 0.3171346787957178
Iteration 80, Training loss = 0.30740796612656635
Iteration 90, Training loss = 0.3005819420019786
Iteration 100, Training loss = 0.29608790792416834
Iteration 110, Training loss = 0.291681792497059
Iteration 120, Training loss = 0.28828192448270495
Iteration 130, Training loss = 0.28450768012643435
Iteration 140, Training loss = 0.28095788044342096
Iteration 150, Training loss = 0.27845135464328497
Iteration 160, Training loss = 0.2757515871654386
Iteration 170, Training loss = 0.2733084636465939
Iteration 180, Training loss = 0.2713294457554241
Iteration 190, Training loss = 0.27034789737727905
Model training time: 50.93185472488403
Device: cuda
Iteration 0, Training loss = 0.9956521161512476
Iteration 10, Training loss = 0.977117429608884
Iteration 20, Training loss = 0.8669104766154635
Iteration 30, Training loss = 0.4890412166354737
Iteration 40, Training loss = 0.37054412534847353
Iteration 50, Training loss = 0.3419063092717802
Iteration 60, Training loss = 0.32540921830900627
Iteration 70, Training loss = 0.31391254650509876
Iteration 80, Training loss = 0.30656050275201385
Iteration 90, Training loss = 0.30029994051381587
Iteration 100, Training loss = 0.2957377206300192
Iteration 110, Training loss = 0.29214978657194957
Iteration 120, Training loss = 0.2882594571669321
Iteration 130, Training loss = 0.2854780862152864
Iteration 140, Training loss = 0.28160268385053255
Iteration 150, Training loss = 0.2794455865011123
Iteration 160, Training loss = 0.27697564150400206
Iteration 170, Training loss = 0.27530510619210735
Iteration 180, Training loss = 0.2733864039902526
Iteration 190, Training loss = 0.27109441535484385
Model training time: 51.448405265808105
Device: cuda
Iteration 0, Training loss = 1.0369288207252245
Iteration 10, Training loss = 0.9823571535700185
Iteration 20, Training loss = 0.8841226139794225
Iteration 30, Training loss = 0.4939606610127693
Iteration 40, Training loss = 0.3616817597893701
Iteration 50, Training loss = 0.3314463907249884
Iteration 60, Training loss = 0.3178352406203459
Iteration 70, Training loss = 0.3082957831053918
Iteration 80, Training loss = 0.3022243528118456
Iteration 90, Training loss = 0.297916047167087
Iteration 100, Training loss = 0.29390131746945175
Iteration 110, Training loss = 0.29144206225584096
Iteration 120, Training loss = 0.28911513899979385
Iteration 130, Training loss = 0.2862908000099486
Iteration 140, Training loss = 0.2839992607129369
Iteration 150, Training loss = 0.28223760854362867
Iteration 160, Training loss = 0.2809301663712027
Iteration 170, Training loss = 0.2807933733515117
Iteration 180, Training loss = 0.2775330005420579
Iteration 190, Training loss = 0.27606496880739784
Model training time: 51.87552237510681
Device: cuda
Iteration 0, Training loss = 1.0163211903134406
Iteration 10, Training loss = 0.9809467150393315
Iteration 20, Training loss = 0.87166501617662
Iteration 30, Training loss = 0.5052124601631349
Iteration 40, Training loss = 0.39131834080829714
Iteration 50, Training loss = 0.35609346995319147
Iteration 60, Training loss = 0.33404625693093176
Iteration 70, Training loss = 0.31859658852867456
Iteration 80, Training loss = 0.30852231260946983
Iteration 90, Training loss = 0.3013647487702001
Iteration 100, Training loss = 0.29701817158051735
Iteration 110, Training loss = 0.2957368552108894
Iteration 120, Training loss = 0.28992884390164114
Iteration 130, Training loss = 0.2863015275142619
Iteration 140, Training loss = 0.28372194556798336
Iteration 150, Training loss = 0.28175047263143144
Iteration 160, Training loss = 0.2799820468765526
Iteration 170, Training loss = 0.2788393344856115
Iteration 180, Training loss = 0.27780437707037164
Iteration 190, Training loss = 0.27598411723035543
Model training time: 51.28184938430786
Device: cuda
Iteration 0, Training loss = 0.9982325823410697
Iteration 10, Training loss = 0.8213947130286176
Iteration 20, Training loss = 0.43486126199148706
Iteration 30, Training loss = 0.36403716121606783
Iteration 40, Training loss = 0.3338021389816118
Iteration 50, Training loss = 0.3152703555021885
Iteration 60, Training loss = 0.3042455979303461
Iteration 70, Training loss = 0.2964381403537188
Iteration 80, Training loss = 0.29118021998716437
Iteration 90, Training loss = 0.286865567671504
Iteration 100, Training loss = 0.2834657320869718
Iteration 110, Training loss = 0.2806813182030323
Iteration 120, Training loss = 0.2782979208778068
Iteration 130, Training loss = 0.2758481755204823
Iteration 140, Training loss = 0.2737185629669595
Iteration 150, Training loss = 0.2718101424223559
Iteration 160, Training loss = 0.27115959303390574
Iteration 170, Training loss = 0.26940795646053584
Iteration 180, Training loss = 0.2673529948013416
Iteration 190, Training loss = 0.26616738067157025
Model training time: 51.72200655937195
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6464271975168283
Iteration 10, Training loss = 0.26122093639799937
Iteration 20, Training loss = 0.2514894635277094
Iteration 30, Training loss = 0.24629209971658272
Iteration 40, Training loss = 0.24099220396671894
Iteration 50, Training loss = 0.2357979153017491
Iteration 60, Training loss = 0.23011321372887938
Iteration 70, Training loss = 0.22481172769397928
Iteration 80, Training loss = 0.22146453193708318
Iteration 90, Training loss = 0.21842146859220837
Iteration 100, Training loss = 0.21618432569618962
Iteration 110, Training loss = 0.21639256064154674
Iteration 120, Training loss = 0.21416228642498236
Iteration 130, Training loss = 0.21271297778340353
Iteration 140, Training loss = 0.21169906433509744
Iteration 150, Training loss = 0.21246344034654507
Iteration 160, Training loss = 0.21093885247834063
Iteration 170, Training loss = 0.21191306980911662
Iteration 180, Training loss = 0.21000258117050363
Iteration 190, Training loss = 0.2094082618897088
Model training time: 59.0273802280426
Device: cuda
Iteration 0, Training loss = 0.6368877092277371
Iteration 10, Training loss = 0.2572889166753649
Iteration 20, Training loss = 0.2504134800842995
Iteration 30, Training loss = 0.24446113605574132
Iteration 40, Training loss = 0.23946387853455428
Iteration 50, Training loss = 0.2334189816661503
Iteration 60, Training loss = 0.2301139491047836
Iteration 70, Training loss = 0.22505694739340584
Iteration 80, Training loss = 0.22217786286908073
Iteration 90, Training loss = 0.2201287807977718
Iteration 100, Training loss = 0.21776387926892957
Iteration 110, Training loss = 0.21678238149714354
Iteration 120, Training loss = 0.21614271715499353
Iteration 130, Training loss = 0.21329582640946199
Iteration 140, Training loss = 0.21185614589763724
Iteration 150, Training loss = 0.21154329619833812
Iteration 160, Training loss = 0.21107111172975548
Iteration 170, Training loss = 0.21101961173297126
Iteration 180, Training loss = 0.20900870341321695
Iteration 190, Training loss = 0.20932684122508274
Model training time: 61.33775997161865
Device: cuda
Iteration 0, Training loss = 0.7477698385859457
Iteration 10, Training loss = 0.254164035913449
Iteration 20, Training loss = 0.24472184198490088
Iteration 30, Training loss = 0.2363517302389882
Iteration 40, Training loss = 0.23033309781896896
Iteration 50, Training loss = 0.22576255783222723
Iteration 60, Training loss = 0.22211730811331007
Iteration 70, Training loss = 0.2211526823072618
Iteration 80, Training loss = 0.21792482555920376
Iteration 90, Training loss = 0.21631395219316804
Iteration 100, Training loss = 0.21493240283883136
Iteration 110, Training loss = 0.21298573893193462
Iteration 120, Training loss = 0.21267741108718125
Iteration 130, Training loss = 0.2113359047162936
Iteration 140, Training loss = 0.21030255709437357
Iteration 150, Training loss = 0.20957175473084197
Iteration 160, Training loss = 0.2087867633768038
Iteration 170, Training loss = 0.20876306601336614
Iteration 180, Training loss = 0.2085275020144412
Iteration 190, Training loss = 0.20806122477647762
Model training time: 59.474313259124756
Device: cuda
Iteration 0, Training loss = 0.6425951778312812
Iteration 10, Training loss = 0.25352928769473293
Iteration 20, Training loss = 0.24445871479701306
Iteration 30, Training loss = 0.2392097095720434
Iteration 40, Training loss = 0.23575467339172457
Iteration 50, Training loss = 0.23216387798676744
Iteration 60, Training loss = 0.23107091535404684
Iteration 70, Training loss = 0.22865122041984456
Iteration 80, Training loss = 0.22627281735916643
Iteration 90, Training loss = 0.22434022597932585
Iteration 100, Training loss = 0.22195435596117075
Iteration 110, Training loss = 0.21908827493156213
Iteration 120, Training loss = 0.2170548949768578
Iteration 130, Training loss = 0.2161464552588509
Iteration 140, Training loss = 0.21452322299929633
Iteration 150, Training loss = 0.2135863550691213
Iteration 160, Training loss = 0.21190571763377258
Iteration 170, Training loss = 0.21014472978990434
Iteration 180, Training loss = 0.209654931752866
Iteration 190, Training loss = 0.20653246526700864
Model training time: 59.34896922111511
Device: cuda
Iteration 0, Training loss = 0.6656298445186754
Iteration 10, Training loss = 0.2602517993553825
Iteration 20, Training loss = 0.252688177757793
Iteration 30, Training loss = 0.24758697491913026
Iteration 40, Training loss = 0.24355102492415387
Iteration 50, Training loss = 0.24082474144184646
Iteration 60, Training loss = 0.23644924703715503
Iteration 70, Training loss = 0.23262739275100727
Iteration 80, Training loss = 0.23025741680088826
Iteration 90, Training loss = 0.227416117158201
Iteration 100, Training loss = 0.22624371337573884
Iteration 110, Training loss = 0.22327696150483717
Iteration 120, Training loss = 0.22174056302666087
Iteration 130, Training loss = 0.21962946707355804
Iteration 140, Training loss = 0.21857194321742957
Iteration 150, Training loss = 0.21637406120985603
Iteration 160, Training loss = 0.21547480132700741
Iteration 170, Training loss = 0.21359244892418672
Iteration 180, Training loss = 0.21262339833709928
Iteration 190, Training loss = 0.21132338882068505
Model training time: 60.01043128967285
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0138188191658057
Iteration 10, Training loss = 0.9435812105303225
Iteration 20, Training loss = 0.6991304316094532
Iteration 30, Training loss = 0.4241002194835368
Iteration 40, Training loss = 0.3760063495612951
Iteration 50, Training loss = 0.3517450183486017
Iteration 60, Training loss = 0.33414463432514724
Iteration 70, Training loss = 0.3211972262116446
Iteration 80, Training loss = 0.31241976854881803
Iteration 90, Training loss = 0.30734186097619615
Iteration 100, Training loss = 0.3009553216217797
Iteration 110, Training loss = 0.297505757946899
Iteration 120, Training loss = 0.2936425738864475
Iteration 130, Training loss = 0.2915494406712804
Iteration 140, Training loss = 0.2881092933784936
Iteration 150, Training loss = 0.2859485894081673
Iteration 160, Training loss = 0.2834446967943855
Iteration 170, Training loss = 0.2821397449515292
Iteration 180, Training loss = 0.28045173270114954
Iteration 190, Training loss = 0.27841702132409324
Model training time: 51.70994210243225
Device: cuda
Iteration 0, Training loss = 1.0300014701442441
Iteration 10, Training loss = 0.9513863932683272
Iteration 20, Training loss = 0.636187625967938
Iteration 30, Training loss = 0.39846708254825663
Iteration 40, Training loss = 0.35988627813288554
Iteration 50, Training loss = 0.33697711046479173
Iteration 60, Training loss = 0.32314393449808665
Iteration 70, Training loss = 0.3114505654947769
Iteration 80, Training loss = 0.3052680610458632
Iteration 90, Training loss = 0.30137860177507725
Iteration 100, Training loss = 0.29765302047205433
Iteration 110, Training loss = 0.29584477683484267
Iteration 120, Training loss = 0.29146107030663515
Iteration 130, Training loss = 0.28984768454723314
Iteration 140, Training loss = 0.28877618793704085
Iteration 150, Training loss = 0.2854157801411578
Iteration 160, Training loss = 0.28401288296577437
Iteration 170, Training loss = 0.2824872557955664
Iteration 180, Training loss = 0.2797412517422063
Iteration 190, Training loss = 0.2777367759729929
Model training time: 52.331976652145386
Device: cuda
Iteration 0, Training loss = 1.037614605565002
Iteration 10, Training loss = 0.9119657459466354
Iteration 20, Training loss = 0.508373129123075
Iteration 30, Training loss = 0.35560060008136546
Iteration 40, Training loss = 0.33042513971455434
Iteration 50, Training loss = 0.3177029800731779
Iteration 60, Training loss = 0.31186504863598496
Iteration 70, Training loss = 0.30531904644868224
Iteration 80, Training loss = 0.3009279832459878
Iteration 90, Training loss = 0.2975087797152247
Iteration 100, Training loss = 0.2943878380788697
Iteration 110, Training loss = 0.29168128463381154
Iteration 120, Training loss = 0.2885012710511972
Iteration 130, Training loss = 0.28685089362704236
Iteration 140, Training loss = 0.28419898777912206
Iteration 150, Training loss = 0.2819939404366097
Iteration 160, Training loss = 0.2804345406483913
Iteration 170, Training loss = 0.2790249416577643
Iteration 180, Training loss = 0.2770906912819775
Iteration 190, Training loss = 0.27523724090505913
Model training time: 53.49190139770508
Device: cuda
Iteration 0, Training loss = 0.9997244851600721
Iteration 10, Training loss = 0.9319988530903047
Iteration 20, Training loss = 0.5745492007421411
Iteration 30, Training loss = 0.38106784534051225
Iteration 40, Training loss = 0.3521651203073741
Iteration 50, Training loss = 0.33633571359270437
Iteration 60, Training loss = 0.3270948171471628
Iteration 70, Training loss = 0.3178510438416891
Iteration 80, Training loss = 0.3104622375561995
Iteration 90, Training loss = 0.30528807179363454
Iteration 100, Training loss = 0.2990854918812784
Iteration 110, Training loss = 0.2946758555329364
Iteration 120, Training loss = 0.2905215220750818
Iteration 130, Training loss = 0.2869872345008712
Iteration 140, Training loss = 0.283979461028956
Iteration 150, Training loss = 0.2807810672959268
Iteration 160, Training loss = 0.27849803918945615
Iteration 170, Training loss = 0.2762535683054855
Iteration 180, Training loss = 0.2742349600446397
Iteration 190, Training loss = 0.2726536457953246
Model training time: 50.913516998291016
Device: cuda
Iteration 0, Training loss = 0.9846095267125374
Iteration 10, Training loss = 0.8934051037410607
Iteration 20, Training loss = 0.5014919690463854
Iteration 30, Training loss = 0.3488267984079278
Iteration 40, Training loss = 0.3230920607485057
Iteration 50, Training loss = 0.31349265474628135
Iteration 60, Training loss = 0.3044939375873925
Iteration 70, Training loss = 0.29978612058116616
Iteration 80, Training loss = 0.29799923250352706
Iteration 90, Training loss = 0.29415290273618006
Iteration 100, Training loss = 0.2916694194942281
Iteration 110, Training loss = 0.28977916342912663
Iteration 120, Training loss = 0.28759048803560977
Iteration 130, Training loss = 0.2866696855728177
Iteration 140, Training loss = 0.28436863980287513
Iteration 150, Training loss = 0.2824151230895001
Iteration 160, Training loss = 0.2808545295167085
Iteration 170, Training loss = 0.2791126692208691
Iteration 180, Training loss = 0.2773652400659478
Iteration 190, Training loss = 0.27658777111682337
Model training time: 51.271878242492676
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6396755244996812
Iteration 10, Training loss = 0.25199992324850984
Iteration 20, Training loss = 0.2421902054965784
Iteration 30, Training loss = 0.2350089198725235
Iteration 40, Training loss = 0.22869768119664582
Iteration 50, Training loss = 0.2252323121768265
Iteration 60, Training loss = 0.2218247970233217
Iteration 70, Training loss = 0.2180924498732539
Iteration 80, Training loss = 0.21533661162939624
Iteration 90, Training loss = 0.2127590007396136
Iteration 100, Training loss = 0.21079699723904835
Iteration 110, Training loss = 0.20876773806729754
Iteration 120, Training loss = 0.20698119465999557
Iteration 130, Training loss = 0.20530248811279517
Iteration 140, Training loss = 0.20419191270346804
Iteration 150, Training loss = 0.20347282178879936
Iteration 160, Training loss = 0.20297529602396316
Iteration 170, Training loss = 0.20224154539010375
Iteration 180, Training loss = 0.20078533250784528
Iteration 190, Training loss = 0.20212579000255335
Iteration 200, Training loss = 0.1995746890703837
Iteration 210, Training loss = 0.19976178437903308
Iteration 220, Training loss = 0.19844064548395682
Iteration 230, Training loss = 0.19979255333326865
Iteration 240, Training loss = 0.19860372307219942
Iteration 250, Training loss = 0.1980171427850562
Iteration 260, Training loss = 0.19709862012794052
Iteration 270, Training loss = 0.19662803978375767
Iteration 280, Training loss = 0.1964854945379179
Iteration 290, Training loss = 0.19680900698986606
Model training time: 89.63404822349548
Device: cuda
Iteration 0, Training loss = 0.6150540689339384
Iteration 10, Training loss = 0.2573151409554021
Iteration 20, Training loss = 0.2474950769385278
Iteration 30, Training loss = 0.2395907398943164
Iteration 40, Training loss = 0.23313090766685596
Iteration 50, Training loss = 0.22637332486357667
Iteration 60, Training loss = 0.22274117024192489
Iteration 70, Training loss = 0.21777106303235758
Iteration 80, Training loss = 0.21341198258065947
Iteration 90, Training loss = 0.20967711508274078
Iteration 100, Training loss = 0.207287451747247
Iteration 110, Training loss = 0.2051201713617873
Iteration 120, Training loss = 0.20347295863472897
Iteration 130, Training loss = 0.20226253022030355
Iteration 140, Training loss = 0.20259565165365376
Iteration 150, Training loss = 0.20016287882690845
Iteration 160, Training loss = 0.19919397505585124
Iteration 170, Training loss = 0.1982416945019206
Iteration 180, Training loss = 0.19757101328476614
Iteration 190, Training loss = 0.19792982237638482
Iteration 200, Training loss = 0.1973438055259018
Iteration 210, Training loss = 0.195431018329185
Iteration 220, Training loss = 0.19625739234945047
Iteration 230, Training loss = 0.1967542779762388
Iteration 240, Training loss = 0.1952406182381266
Iteration 250, Training loss = 0.19444964145836624
Iteration 260, Training loss = 0.19338106263230964
Iteration 270, Training loss = 0.19257785707424227
Iteration 280, Training loss = 0.1938254055769547
Iteration 290, Training loss = 0.19286389922462224
Model training time: 88.47669839859009
Device: cuda
Iteration 0, Training loss = 0.6556275001038676
Iteration 10, Training loss = 0.2528371831789109
Iteration 20, Training loss = 0.24554892028299508
Iteration 30, Training loss = 0.23970001270085717
Iteration 40, Training loss = 0.23290198152335945
Iteration 50, Training loss = 0.22949369006974685
Iteration 60, Training loss = 0.22346819872441498
Iteration 70, Training loss = 0.220609065739141
Iteration 80, Training loss = 0.2185243869893217
Iteration 90, Training loss = 0.21430666809496673
Iteration 100, Training loss = 0.21190609253834988
Iteration 110, Training loss = 0.2096271690539116
Iteration 120, Training loss = 0.20729884861604028
Iteration 130, Training loss = 0.2060771711566598
Iteration 140, Training loss = 0.204721851049414
Iteration 150, Training loss = 0.2031424449045877
Iteration 160, Training loss = 0.20265185347069864
Iteration 170, Training loss = 0.2006352542931043
Iteration 180, Training loss = 0.1991521428172715
Iteration 190, Training loss = 0.1989694067461479
Iteration 200, Training loss = 0.19833723927177668
Iteration 210, Training loss = 0.19690706487295132
Iteration 220, Training loss = 0.19520940973562895
Iteration 230, Training loss = 0.19499605757314803
Iteration 240, Training loss = 0.19583447104778842
Iteration 250, Training loss = 0.19380483898707634
Iteration 260, Training loss = 0.1946632587103452
Iteration 270, Training loss = 0.19320543434308923
Iteration 280, Training loss = 0.19317620567941435
Iteration 290, Training loss = 0.19211540543946667
Model training time: 89.14044499397278
Device: cuda
Iteration 0, Training loss = 0.6329449519587024
Iteration 10, Training loss = 0.25147365559126444
Iteration 20, Training loss = 0.24172921907066722
Iteration 30, Training loss = 0.23318676042240022
Iteration 40, Training loss = 0.2287856868787664
Iteration 50, Training loss = 0.22410958620229204
Iteration 60, Training loss = 0.219310127414655
Iteration 70, Training loss = 0.21639294184492405
Iteration 80, Training loss = 0.21249766460651362
Iteration 90, Training loss = 0.21138066416921247
Iteration 100, Training loss = 0.2095382163202129
Iteration 110, Training loss = 0.2080377869415974
Iteration 120, Training loss = 0.20799307363188785
Iteration 130, Training loss = 0.20558254874270895
Iteration 140, Training loss = 0.20366850491739125
Iteration 150, Training loss = 0.20393747796758938
Iteration 160, Training loss = 0.20214019751779122
Iteration 170, Training loss = 0.20188972883034442
Iteration 180, Training loss = 0.20173347233862116
Iteration 190, Training loss = 0.20074443634293507
Iteration 200, Training loss = 0.19960259883732034
Iteration 210, Training loss = 0.19928955085179656
Iteration 220, Training loss = 0.1985909816975467
Iteration 230, Training loss = 0.1987145302304323
Iteration 240, Training loss = 0.19851949339471578
Iteration 250, Training loss = 0.1975674856328158
Iteration 260, Training loss = 0.19687625391471789
Iteration 270, Training loss = 0.19773554305235544
Iteration 280, Training loss = 0.19584885847885253
Iteration 290, Training loss = 0.19546982037704347
Model training time: 89.64892745018005
Device: cuda
Iteration 0, Training loss = 0.6735164308605562
Iteration 10, Training loss = 0.2513528987045449
Iteration 20, Training loss = 0.2418529474792849
Iteration 30, Training loss = 0.23459057715060055
Iteration 40, Training loss = 0.22980462317017541
Iteration 50, Training loss = 0.22331439635315956
Iteration 60, Training loss = 0.21832786320488234
Iteration 70, Training loss = 0.21666169270945057
Iteration 80, Training loss = 0.2142979507357026
Iteration 90, Training loss = 0.21234230560381057
Iteration 100, Training loss = 0.2107436376925252
Iteration 110, Training loss = 0.209433120517915
Iteration 120, Training loss = 0.20775086148349559
Iteration 130, Training loss = 0.20530811632456986
Iteration 140, Training loss = 0.20635856004152897
Iteration 150, Training loss = 0.204395507628791
Iteration 160, Training loss = 0.20347616085467707
Iteration 170, Training loss = 0.20102354513418272
Iteration 180, Training loss = 0.2015592483938604
Iteration 190, Training loss = 0.20019135257040244
Iteration 200, Training loss = 0.1993145421507278
Iteration 210, Training loss = 0.19855400826332073
Iteration 220, Training loss = 0.19763692324432197
Iteration 230, Training loss = 0.1981019336856218
Iteration 240, Training loss = 0.19622100306593854
Iteration 250, Training loss = 0.19611191054905094
Iteration 260, Training loss = 0.19520197644973722
Iteration 270, Training loss = 0.19541912633871686
Iteration 280, Training loss = 0.19463946846660207
Iteration 290, Training loss = 0.19469659372372328
Model training time: 90.32862734794617
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0221372255380603
Iteration 10, Training loss = 0.929856553889703
Iteration 20, Training loss = 0.5886568942507684
Iteration 30, Training loss = 0.39658706271705996
Iteration 40, Training loss = 0.36230947286034554
Iteration 50, Training loss = 0.33998938264766176
Iteration 60, Training loss = 0.3245278482419857
Iteration 70, Training loss = 0.31406563491637
Iteration 80, Training loss = 0.30605843814386835
Iteration 90, Training loss = 0.3003816281395834
Iteration 100, Training loss = 0.29377181900008287
Iteration 110, Training loss = 0.29054991425811383
Iteration 120, Training loss = 0.28576524400912623
Iteration 130, Training loss = 0.2832435893551739
Iteration 140, Training loss = 0.2815241172981723
Iteration 150, Training loss = 0.27850086943826813
Iteration 160, Training loss = 0.2767650566023329
Iteration 170, Training loss = 0.2759873451674042
Iteration 180, Training loss = 0.27385524987886495
Iteration 190, Training loss = 0.27279744254074234
Iteration 200, Training loss = 0.2715075508407924
Iteration 210, Training loss = 0.27108661229339776
Iteration 220, Training loss = 0.27146268121286293
Iteration 230, Training loss = 0.26918728102520467
Iteration 240, Training loss = 0.2688676390239006
Iteration 250, Training loss = 0.2676957009927086
Iteration 260, Training loss = 0.2675139586562696
Iteration 270, Training loss = 0.2671736973711258
Iteration 280, Training loss = 0.2660443186400017
Iteration 290, Training loss = 0.26531542805225955
Model training time: 77.54705119132996
Device: cuda
Iteration 0, Training loss = 1.0177668438153566
Iteration 10, Training loss = 0.9696063808196984
Iteration 20, Training loss = 0.7987168020960214
Iteration 30, Training loss = 0.4025105276113547
Iteration 40, Training loss = 0.3534426583328109
Iteration 50, Training loss = 0.3322170264479043
Iteration 60, Training loss = 0.31948522301975657
Iteration 70, Training loss = 0.31139785832397027
Iteration 80, Training loss = 0.3058130807346768
Iteration 90, Training loss = 0.3016209828392895
Iteration 100, Training loss = 0.2978760317183923
Iteration 110, Training loss = 0.2947048730608346
Iteration 120, Training loss = 0.2922446168419244
Iteration 130, Training loss = 0.2893771420498401
Iteration 140, Training loss = 0.28712025281167836
Iteration 150, Training loss = 0.2846078623608115
Iteration 160, Training loss = 0.2829095830663967
Iteration 170, Training loss = 0.2809840249626533
Iteration 180, Training loss = 0.2790105898599118
Iteration 190, Training loss = 0.2771892154922232
Iteration 200, Training loss = 0.27579807731264455
Iteration 210, Training loss = 0.274212308767913
Iteration 220, Training loss = 0.2726967884362608
Iteration 230, Training loss = 0.2714978724286176
Iteration 240, Training loss = 0.26991359871510723
Iteration 250, Training loss = 0.2691641426532741
Iteration 260, Training loss = 0.2683013939627127
Iteration 270, Training loss = 0.2673283025694354
Iteration 280, Training loss = 0.26542556865347755
Iteration 290, Training loss = 0.26487648937005354
Model training time: 77.68013644218445
Device: cuda
Iteration 0, Training loss = 1.0389115922692893
Iteration 10, Training loss = 0.9587727756315959
Iteration 20, Training loss = 0.7693425399669702
Iteration 30, Training loss = 0.4170795891451951
Iteration 40, Training loss = 0.3541982394197713
Iteration 50, Training loss = 0.32965825800446497
Iteration 60, Training loss = 0.3166722052771112
Iteration 70, Training loss = 0.3067654021408247
Iteration 80, Training loss = 0.3002112694984473
Iteration 90, Training loss = 0.29479776884334674
Iteration 100, Training loss = 0.29078959173338426
Iteration 110, Training loss = 0.28689854415718485
Iteration 120, Training loss = 0.2836024583753756
Iteration 130, Training loss = 0.28066587707270746
Iteration 140, Training loss = 0.27813128565532574
Iteration 150, Training loss = 0.27676079418636174
Iteration 160, Training loss = 0.2741871247976874
Iteration 170, Training loss = 0.27223978868285237
Iteration 180, Training loss = 0.27094143758649414
Iteration 190, Training loss = 0.26998797552165205
Iteration 200, Training loss = 0.2690951005704161
Iteration 210, Training loss = 0.2676890730353945
Iteration 220, Training loss = 0.26601224176693655
Iteration 230, Training loss = 0.2655441602647017
Iteration 240, Training loss = 0.2645705451352009
Iteration 250, Training loss = 0.26419999425249974
Iteration 260, Training loss = 0.26320565419928466
Iteration 270, Training loss = 0.26224344457693144
Iteration 280, Training loss = 0.26174388253602426
Iteration 290, Training loss = 0.2612072422308622
Model training time: 79.07304430007935
Device: cuda
Iteration 0, Training loss = 1.063999186679361
Iteration 10, Training loss = 0.8543204459303243
Iteration 20, Training loss = 0.4241741563099018
Iteration 30, Training loss = 0.3471793024148342
Iteration 40, Training loss = 0.3241607502894701
Iteration 50, Training loss = 0.31056662760494985
Iteration 60, Training loss = 0.30047371976329507
Iteration 70, Training loss = 0.294003388515993
Iteration 80, Training loss = 0.2894136822166074
Iteration 90, Training loss = 0.28600876897141553
Iteration 100, Training loss = 0.28256551489449927
Iteration 110, Training loss = 0.27936669788210866
Iteration 120, Training loss = 0.27760013293672875
Iteration 130, Training loss = 0.27528513334511556
Iteration 140, Training loss = 0.27324813230026174
Iteration 150, Training loss = 0.27204153769546086
Iteration 160, Training loss = 0.27012385269581984
Iteration 170, Training loss = 0.2690517993076988
Iteration 180, Training loss = 0.26811876471923746
Iteration 190, Training loss = 0.26724958513381974
Iteration 200, Training loss = 0.266198375482778
Iteration 210, Training loss = 0.26611641369724043
Iteration 220, Training loss = 0.26493692887578035
Iteration 230, Training loss = 0.26407783802436746
Iteration 240, Training loss = 0.2633759140032501
Iteration 250, Training loss = 0.2633101080638775
Iteration 260, Training loss = 0.2625660913218047
Iteration 270, Training loss = 0.26223978097888007
Iteration 280, Training loss = 0.26101736911541024
Iteration 290, Training loss = 0.26052062933283726
Model training time: 80.54672837257385
Device: cuda
Iteration 0, Training loss = 0.9957718595790402
Iteration 10, Training loss = 0.9091469360434491
Iteration 20, Training loss = 0.4787141471957239
Iteration 30, Training loss = 0.36265132162306046
Iteration 40, Training loss = 0.33780994593809194
Iteration 50, Training loss = 0.32261206101680145
Iteration 60, Training loss = 0.3132133390304547
Iteration 70, Training loss = 0.30504493811280253
Iteration 80, Training loss = 0.2995844263097514
Iteration 90, Training loss = 0.2947048381474859
Iteration 100, Training loss = 0.2900783512111447
Iteration 110, Training loss = 0.28661296599441105
Iteration 120, Training loss = 0.282831487664278
Iteration 130, Training loss = 0.27884283517869773
Iteration 140, Training loss = 0.2759157171571888
Iteration 150, Training loss = 0.27365348290130137
Iteration 160, Training loss = 0.2715656126754871
Iteration 170, Training loss = 0.26974661032790725
Iteration 180, Training loss = 0.26858866236348083
Iteration 190, Training loss = 0.26662217505312197
Iteration 200, Training loss = 0.2656473194199484
Iteration 210, Training loss = 0.2653339816176373
Iteration 220, Training loss = 0.2641280642598148
Iteration 230, Training loss = 0.26345746989411434
Iteration 240, Training loss = 0.26261274261031176
Iteration 250, Training loss = 0.2616641390438817
Iteration 260, Training loss = 0.26125974873989677
Iteration 270, Training loss = 0.26044952747038597
Iteration 280, Training loss = 0.2601720682448811
Iteration 290, Training loss = 0.259319061278433
Model training time: 82.46080923080444
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.857794353639446
Iteration 10, Training loss = 0.26550251853782775
Iteration 20, Training loss = 0.2528931639980579
Iteration 30, Training loss = 0.24709810392148251
Iteration 40, Training loss = 0.24106331903865372
Iteration 50, Training loss = 0.2364291680463846
Iteration 60, Training loss = 0.23192538515381192
Iteration 70, Training loss = 0.23008686829592295
Iteration 80, Training loss = 0.22750931290756676
Iteration 90, Training loss = 0.22565697727427966
Iteration 100, Training loss = 0.22323689814926923
Iteration 110, Training loss = 0.2224555519827898
Iteration 120, Training loss = 0.21919513277817462
Iteration 130, Training loss = 0.2176060869138022
Iteration 140, Training loss = 0.21712074713142598
Iteration 150, Training loss = 0.21442313740650812
Iteration 160, Training loss = 0.21313708477118165
Iteration 170, Training loss = 0.2120554116037157
Iteration 180, Training loss = 0.21325002797847784
Iteration 190, Training loss = 0.21104618665820735
Iteration 200, Training loss = 0.209461592749697
Iteration 210, Training loss = 0.20949313278503465
Iteration 220, Training loss = 0.20996867627769278
Iteration 230, Training loss = 0.2092677039872621
Iteration 240, Training loss = 0.20818313808688796
Iteration 250, Training loss = 0.20841675438886678
Iteration 260, Training loss = 0.2086334164087899
Iteration 270, Training loss = 0.20851103115628883
Iteration 280, Training loss = 0.2072594134847899
Iteration 290, Training loss = 0.20708400617547082
Model training time: 88.97899270057678
Device: cuda
Iteration 0, Training loss = 0.6970506468112918
Iteration 10, Training loss = 0.2631372818768312
Iteration 20, Training loss = 0.2521240863463153
Iteration 30, Training loss = 0.24681709631197696
Iteration 40, Training loss = 0.242808082325447
Iteration 50, Training loss = 0.2383483933725795
Iteration 60, Training loss = 0.23461351979181963
Iteration 70, Training loss = 0.23068801828340632
Iteration 80, Training loss = 0.22853234190727778
Iteration 90, Training loss = 0.22672572800358712
Iteration 100, Training loss = 0.22520164724277414
Iteration 110, Training loss = 0.22262938349431263
Iteration 120, Training loss = 0.22108680535341807
Iteration 130, Training loss = 0.21925838036093734
Iteration 140, Training loss = 0.21708084041801629
Iteration 150, Training loss = 0.21595055350790854
Iteration 160, Training loss = 0.21530509013052723
Iteration 170, Training loss = 0.21443691723732558
Iteration 180, Training loss = 0.21385397207765763
Iteration 190, Training loss = 0.2119656495641971
Iteration 200, Training loss = 0.21196507954079172
Iteration 210, Training loss = 0.2108137024845478
Iteration 220, Training loss = 0.21016850311687027
Iteration 230, Training loss = 0.21013406185424272
Iteration 240, Training loss = 0.20819637784059497
Iteration 250, Training loss = 0.20790744673658684
Iteration 260, Training loss = 0.20777536331167543
Iteration 270, Training loss = 0.20837967421697534
Iteration 280, Training loss = 0.2089654237823786
Iteration 290, Training loss = 0.20777347424755926
Model training time: 88.36138868331909
Device: cuda
Iteration 0, Training loss = 1.1905860212690011
Iteration 10, Training loss = 0.2587945082625329
Iteration 20, Training loss = 0.2472155335660718
Iteration 30, Training loss = 0.2408598179474545
Iteration 40, Training loss = 0.23645816538212955
Iteration 50, Training loss = 0.232663889977955
Iteration 60, Training loss = 0.22906134551130056
Iteration 70, Training loss = 0.22656798398725075
Iteration 80, Training loss = 0.22522926452943093
Iteration 90, Training loss = 0.2233299454269202
Iteration 100, Training loss = 0.22210816441526735
Iteration 110, Training loss = 0.22154172459518276
Iteration 120, Training loss = 0.2202328029966009
Iteration 130, Training loss = 0.21896053695880272
Iteration 140, Training loss = 0.217237101552855
Iteration 150, Training loss = 0.2154752132685288
Iteration 160, Training loss = 0.21422556323417719
Iteration 170, Training loss = 0.21494067921010768
Iteration 180, Training loss = 0.2132690492387555
Iteration 190, Training loss = 0.21235946486681556
Iteration 200, Training loss = 0.2114688856302252
Iteration 210, Training loss = 0.20941759293206072
Iteration 220, Training loss = 0.20932039146981954
Iteration 230, Training loss = 0.21015766603143318
Iteration 240, Training loss = 0.20854157526136022
Iteration 250, Training loss = 0.20849316646367455
Iteration 260, Training loss = 0.20827137192954187
Iteration 270, Training loss = 0.20738165501667105
Iteration 280, Training loss = 0.20667930480074767
Iteration 290, Training loss = 0.20670109911673312
Model training time: 89.21669483184814
Device: cuda
Iteration 0, Training loss = 0.7266933798502033
Iteration 10, Training loss = 0.25669827607374834
Iteration 20, Training loss = 0.24898625928278706
Iteration 30, Training loss = 0.24207692499753933
Iteration 40, Training loss = 0.23473311805926658
Iteration 50, Training loss = 0.22876089418999815
Iteration 60, Training loss = 0.22582565158029685
Iteration 70, Training loss = 0.22231288412630845
Iteration 80, Training loss = 0.21985908626502262
Iteration 90, Training loss = 0.21857429976048676
Iteration 100, Training loss = 0.21643293846920492
Iteration 110, Training loss = 0.21457560263250186
Iteration 120, Training loss = 0.21645121740690176
Iteration 130, Training loss = 0.21274819437432405
Iteration 140, Training loss = 0.21295449368043798
Iteration 150, Training loss = 0.2109619471186025
Iteration 160, Training loss = 0.21004421947802898
Iteration 170, Training loss = 0.20849420576567812
Iteration 180, Training loss = 0.2094243750480062
Iteration 190, Training loss = 0.20803856309773266
Iteration 200, Training loss = 0.20734866600537646
Iteration 210, Training loss = 0.2066616642734279
Iteration 220, Training loss = 0.20648130731306213
Iteration 230, Training loss = 0.20623478862110545
Iteration 240, Training loss = 0.20556139981977029
Iteration 250, Training loss = 0.20542399955976412
Iteration 260, Training loss = 0.2043815874967022
Iteration 270, Training loss = 0.20327952662527848
Iteration 280, Training loss = 0.203717690087171
Iteration 290, Training loss = 0.2036481282633284
Model training time: 88.91020941734314
Device: cuda
Iteration 0, Training loss = 0.9734252938901745
Iteration 10, Training loss = 0.25948870502376326
Iteration 20, Training loss = 0.24945676492320168
Iteration 30, Training loss = 0.24355227215854441
Iteration 40, Training loss = 0.24053500175188128
Iteration 50, Training loss = 0.23825954912221375
Iteration 60, Training loss = 0.23627520046660289
Iteration 70, Training loss = 0.23399145951593556
Iteration 80, Training loss = 0.23409634490231962
Iteration 90, Training loss = 0.22930463025535364
Iteration 100, Training loss = 0.22901293448204005
Iteration 110, Training loss = 0.22689097720211832
Iteration 120, Training loss = 0.2266442128209676
Iteration 130, Training loss = 0.22336483538006816
Iteration 140, Training loss = 0.2236741657778261
Iteration 150, Training loss = 0.2215409506706224
Iteration 160, Training loss = 0.2202476112594927
Iteration 170, Training loss = 0.219727775108987
Iteration 180, Training loss = 0.2182980608465015
Iteration 190, Training loss = 0.21692162017891373
Iteration 200, Training loss = 0.2175445251778704
Iteration 210, Training loss = 0.21606596717655946
Iteration 220, Training loss = 0.2155027958937889
Iteration 230, Training loss = 0.21436952903938755
Iteration 240, Training loss = 0.21312564273099394
Iteration 250, Training loss = 0.21288083015000764
Iteration 260, Training loss = 0.21234240706848062
Iteration 270, Training loss = 0.21322014913466816
Iteration 280, Training loss = 0.21099573776917757
Iteration 290, Training loss = 0.21098711719547492
Model training time: 89.28106117248535
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0194728999322162
Iteration 10, Training loss = 0.8509206344252047
Iteration 20, Training loss = 0.4539652172350077
Iteration 30, Training loss = 0.37082383202181923
Iteration 40, Training loss = 0.34129920925783075
Iteration 50, Training loss = 0.3255316786432036
Iteration 60, Training loss = 0.31469632670787223
Iteration 70, Training loss = 0.30690271814088316
Iteration 80, Training loss = 0.30114457299167985
Iteration 90, Training loss = 0.2960520678960183
Iteration 100, Training loss = 0.2917934070894684
Iteration 110, Training loss = 0.2885483424876623
Iteration 120, Training loss = 0.2853717150319602
Iteration 130, Training loss = 0.2828473867713541
Iteration 140, Training loss = 0.28023895930840775
Iteration 150, Training loss = 0.2783735903420886
Iteration 160, Training loss = 0.2763183491241529
Iteration 170, Training loss = 0.27521405549009065
Iteration 180, Training loss = 0.273012964081937
Iteration 190, Training loss = 0.27182358798485445
Iteration 200, Training loss = 0.27022123995466507
Iteration 210, Training loss = 0.26989915419891836
Iteration 220, Training loss = 0.2686137567107804
Iteration 230, Training loss = 0.26714284602858596
Iteration 240, Training loss = 0.26602470766806946
Iteration 250, Training loss = 0.26516919907020486
Iteration 260, Training loss = 0.26427483043952843
Iteration 270, Training loss = 0.2636168957620427
Iteration 280, Training loss = 0.26292042333434745
Iteration 290, Training loss = 0.2629667703728169
Model training time: 77.31925058364868
Device: cuda
Iteration 0, Training loss = 1.028216916149941
Iteration 10, Training loss = 0.9687869465293516
Iteration 20, Training loss = 0.7944118782230045
Iteration 30, Training loss = 0.4409415948650111
Iteration 40, Training loss = 0.3655453015496765
Iteration 50, Training loss = 0.33607346337774524
Iteration 60, Training loss = 0.32119454882571086
Iteration 70, Training loss = 0.3106471910568827
Iteration 80, Training loss = 0.30317794743943327
Iteration 90, Training loss = 0.2984611099036995
Iteration 100, Training loss = 0.2948252214613744
Iteration 110, Training loss = 0.29108845636896463
Iteration 120, Training loss = 0.28807893125475315
Iteration 130, Training loss = 0.28441621104012366
Iteration 140, Training loss = 0.2816584919385864
Iteration 150, Training loss = 0.27984676844831824
Iteration 160, Training loss = 0.27806015841770865
Iteration 170, Training loss = 0.27622745758381445
Iteration 180, Training loss = 0.2744745001700765
Iteration 190, Training loss = 0.27277647343954603
Iteration 200, Training loss = 0.27167296719147965
Iteration 210, Training loss = 0.27141845528630243
Iteration 220, Training loss = 0.2706669449518268
Iteration 230, Training loss = 0.2699066868438813
Iteration 240, Training loss = 0.26721581903056824
Iteration 250, Training loss = 0.26640380476263986
Iteration 260, Training loss = 0.2656174220396701
Iteration 270, Training loss = 0.2641467310236272
Iteration 280, Training loss = 0.2639162301441322
Iteration 290, Training loss = 0.2632559632189608
Model training time: 77.84475493431091
Device: cuda
Iteration 0, Training loss = 1.0031035569842888
Iteration 10, Training loss = 0.9616105766687992
Iteration 20, Training loss = 0.7146876509062909
Iteration 30, Training loss = 0.3968799611652531
Iteration 40, Training loss = 0.35377305126996433
Iteration 50, Training loss = 0.33115817066551984
Iteration 60, Training loss = 0.31695560470295414
Iteration 70, Training loss = 0.3067609182590448
Iteration 80, Training loss = 0.2995382307664208
Iteration 90, Training loss = 0.29458533486594324
Iteration 100, Training loss = 0.2897255802931993
Iteration 110, Training loss = 0.28635712817383274
Iteration 120, Training loss = 0.2826369638748215
Iteration 130, Training loss = 0.280386569203386
Iteration 140, Training loss = 0.27841861571234783
Iteration 150, Training loss = 0.27570518030636554
Iteration 160, Training loss = 0.27324623074652493
Iteration 170, Training loss = 0.27238162884101774
Iteration 180, Training loss = 0.27026985052991026
Iteration 190, Training loss = 0.26950959609326536
Iteration 200, Training loss = 0.2679881137494304
Iteration 210, Training loss = 0.2674432309425395
Iteration 220, Training loss = 0.2663085935052466
Iteration 230, Training loss = 0.26546216129824735
Iteration 240, Training loss = 0.26546851973890684
Iteration 250, Training loss = 0.2637489300203208
Iteration 260, Training loss = 0.263260115387935
Iteration 270, Training loss = 0.26225416653830075
Iteration 280, Training loss = 0.2613311527647834
Iteration 290, Training loss = 0.2606697966511123
Model training time: 79.75946474075317
Device: cuda
Iteration 0, Training loss = 1.0759674961440229
Iteration 10, Training loss = 0.9333992868229963
Iteration 20, Training loss = 0.5777677575459227
Iteration 30, Training loss = 0.3605536292284583
Iteration 40, Training loss = 0.3345127156103291
Iteration 50, Training loss = 0.32119720364394394
Iteration 60, Training loss = 0.311237003132341
Iteration 70, Training loss = 0.3045283054959947
Iteration 80, Training loss = 0.2991944067720054
Iteration 90, Training loss = 0.29484975100427435
Iteration 100, Training loss = 0.2915526351347062
Iteration 110, Training loss = 0.28744766252916215
Iteration 120, Training loss = 0.28437410633345156
Iteration 130, Training loss = 0.28163108136054976
Iteration 140, Training loss = 0.2786276738208849
Iteration 150, Training loss = 0.2765974306685913
Iteration 160, Training loss = 0.2746258090083726
Iteration 170, Training loss = 0.2728567303738732
Iteration 180, Training loss = 0.2714893779029017
Iteration 190, Training loss = 0.2699735724695639
Iteration 200, Training loss = 0.26877273374421584
Iteration 210, Training loss = 0.26761797455629865
Iteration 220, Training loss = 0.26628183055183163
Iteration 230, Training loss = 0.265507979741419
Iteration 240, Training loss = 0.2639491595867751
Iteration 250, Training loss = 0.26376716727795807
Iteration 260, Training loss = 0.2625934642870069
Iteration 270, Training loss = 0.26198695387673265
Iteration 280, Training loss = 0.26085575212890977
Iteration 290, Training loss = 0.26048458900284654
Model training time: 77.17726516723633
Device: cuda
Iteration 0, Training loss = 1.0153693659006109
Iteration 10, Training loss = 0.738943216305424
Iteration 20, Training loss = 0.40825289797379777
Iteration 30, Training loss = 0.35942082272635567
Iteration 40, Training loss = 0.3339554198842118
Iteration 50, Training loss = 0.3187185160923695
Iteration 60, Training loss = 0.3093412962656666
Iteration 70, Training loss = 0.3028762578100398
Iteration 80, Training loss = 0.2973619653046995
Iteration 90, Training loss = 0.2928664690054557
Iteration 100, Training loss = 0.2888949820096942
Iteration 110, Training loss = 0.28543976906705015
Iteration 120, Training loss = 0.2826956229077445
Iteration 130, Training loss = 0.279732671012913
Iteration 140, Training loss = 0.27787814832827895
Iteration 150, Training loss = 0.27553063703044023
Iteration 160, Training loss = 0.2727253639683631
Iteration 170, Training loss = 0.27107425764706977
Iteration 180, Training loss = 0.26920205563450783
Iteration 190, Training loss = 0.2673699366297699
Iteration 200, Training loss = 0.26616752385229303
Iteration 210, Training loss = 0.26514265272352433
Iteration 220, Training loss = 0.26458204616814995
Iteration 230, Training loss = 0.2632131412409354
Iteration 240, Training loss = 0.26244643733697237
Iteration 250, Training loss = 0.26167007218956373
Iteration 260, Training loss = 0.26161439767206346
Iteration 270, Training loss = 0.2607269667916828
Iteration 280, Training loss = 0.25971866438642216
Iteration 290, Training loss = 0.2598102000600474
Model training time: 77.28767132759094
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.6468667684545839
Iteration 10, Training loss = 0.2624380766697552
Iteration 20, Training loss = 0.2510556378154363
Iteration 30, Training loss = 0.2445277387033338
Iteration 40, Training loss = 0.23866985126394005
Iteration 50, Training loss = 0.2322671780191758
Iteration 60, Training loss = 0.227655074806605
Iteration 70, Training loss = 0.2241777219058235
Iteration 80, Training loss = 0.22050979218765157
Iteration 90, Training loss = 0.22070602932270023
Iteration 100, Training loss = 0.21681264701528827
Iteration 110, Training loss = 0.21621774115424225
Iteration 120, Training loss = 0.21452093898242222
Iteration 130, Training loss = 0.2130775613269368
Iteration 140, Training loss = 0.2123748689961894
Iteration 150, Training loss = 0.21234022620795429
Iteration 160, Training loss = 0.21019378120916477
Iteration 170, Training loss = 0.20986972360507303
Iteration 180, Training loss = 0.2088316353622842
Iteration 190, Training loss = 0.20835719801089614
Iteration 200, Training loss = 0.2075355830039955
Iteration 210, Training loss = 0.20692402448343195
Iteration 220, Training loss = 0.20518372853956937
Iteration 230, Training loss = 0.2052086763407873
Iteration 240, Training loss = 0.20477282630216673
Iteration 250, Training loss = 0.20482227854106738
Iteration 260, Training loss = 0.20385789320520734
Iteration 270, Training loss = 0.20308841214231824
Iteration 280, Training loss = 0.20246028507821226
Iteration 290, Training loss = 0.20214959755899825
Model training time: 89.42648792266846
Device: cuda
Iteration 0, Training loss = 0.7663479507113424
Iteration 10, Training loss = 0.2571734512125812
Iteration 20, Training loss = 0.24937037248974261
Iteration 30, Training loss = 0.24383825700784076
Iteration 40, Training loss = 0.2403691498481709
Iteration 50, Training loss = 0.23798684991788174
Iteration 60, Training loss = 0.23471147768595368
Iteration 70, Training loss = 0.2315605507740652
Iteration 80, Training loss = 0.2275688669819763
Iteration 90, Training loss = 0.22543873684705742
Iteration 100, Training loss = 0.22322883352565304
Iteration 110, Training loss = 0.22065474550073275
Iteration 120, Training loss = 0.21883676625824205
Iteration 130, Training loss = 0.2179178104236506
Iteration 140, Training loss = 0.2159578689702467
Iteration 150, Training loss = 0.21586352250210328
Iteration 160, Training loss = 0.21388603286178792
Iteration 170, Training loss = 0.2132906252923219
Iteration 180, Training loss = 0.21301161825368944
Iteration 190, Training loss = 0.2121774285743778
Iteration 200, Training loss = 0.21100972402067
Iteration 210, Training loss = 0.21120082540212623
Iteration 220, Training loss = 0.21020559286293777
Iteration 230, Training loss = 0.21132953216632208
Iteration 240, Training loss = 0.2099477342793331
Iteration 250, Training loss = 0.2097447165760441
Iteration 260, Training loss = 0.20921618437421494
Iteration 270, Training loss = 0.20809766052713716
Iteration 280, Training loss = 0.20769364973485183
Iteration 290, Training loss = 0.2078004606464059
Model training time: 89.2553768157959
Device: cuda
Iteration 0, Training loss = 0.6685322202778093
Iteration 10, Training loss = 0.25600550316526116
Iteration 20, Training loss = 0.24829691230962816
Iteration 30, Training loss = 0.2405032220022114
Iteration 40, Training loss = 0.2364040906518554
Iteration 50, Training loss = 0.23129407611590078
Iteration 60, Training loss = 0.2285639378759596
Iteration 70, Training loss = 0.22430115646642187
Iteration 80, Training loss = 0.22031552380553765
Iteration 90, Training loss = 0.2162857093816794
Iteration 100, Training loss = 0.21526866557373517
Iteration 110, Training loss = 0.2132097312433708
Iteration 120, Training loss = 0.21119329002168444
Iteration 130, Training loss = 0.20852411027259873
Iteration 140, Training loss = 0.20736024496348007
Iteration 150, Training loss = 0.2054693660730325
Iteration 160, Training loss = 0.20519025033510827
Iteration 170, Training loss = 0.20297660667827164
Iteration 180, Training loss = 0.20223146050736523
Iteration 190, Training loss = 0.20058752869926214
Iteration 200, Training loss = 0.19923044561188002
Iteration 210, Training loss = 0.19822577700234842
Iteration 220, Training loss = 0.19755953841883203
Iteration 230, Training loss = 0.19684332062080862
Iteration 240, Training loss = 0.19684750637138523
Iteration 250, Training loss = 0.19631395449384975
Iteration 260, Training loss = 0.19526792328426804
Iteration 270, Training loss = 0.19521822672391284
Iteration 280, Training loss = 0.19496982775016683
Iteration 290, Training loss = 0.19464977301549222
Model training time: 90.35575485229492
Device: cuda
Iteration 0, Training loss = 0.7552273534778235
Iteration 10, Training loss = 0.26011044503266106
Iteration 20, Training loss = 0.2509176744427082
Iteration 30, Training loss = 0.24511320068352466
Iteration 40, Training loss = 0.23998902943255246
Iteration 50, Training loss = 0.23656028091619555
Iteration 60, Training loss = 0.23150735179295287
Iteration 70, Training loss = 0.22808215011289154
Iteration 80, Training loss = 0.22506313396680758
Iteration 90, Training loss = 0.22291485353368493
Iteration 100, Training loss = 0.22035014071470296
Iteration 110, Training loss = 0.21838470677966657
Iteration 120, Training loss = 0.2159972944913279
Iteration 130, Training loss = 0.2139260820720507
Iteration 140, Training loss = 0.21185830685395549
Iteration 150, Training loss = 0.20976271257164397
Iteration 160, Training loss = 0.20901563292108297
Iteration 170, Training loss = 0.2068231143237312
Iteration 180, Training loss = 0.2051390621829148
Iteration 190, Training loss = 0.20396041873716503
Iteration 200, Training loss = 0.20321175945553802
Iteration 210, Training loss = 0.2026781063652845
Iteration 220, Training loss = 0.20122280220190683
Iteration 230, Training loss = 0.20072884674089542
Iteration 240, Training loss = 0.20063543136137119
Iteration 250, Training loss = 0.2018108853827352
Iteration 260, Training loss = 0.20027178097606282
Iteration 270, Training loss = 0.1992591909956241
Iteration 280, Training loss = 0.19930736522599696
Iteration 290, Training loss = 0.19831124556381347
Model training time: 90.60259866714478
Device: cuda
Iteration 0, Training loss = 0.6695372686869856
Iteration 10, Training loss = 0.2600195282611294
Iteration 20, Training loss = 0.2471538672340665
Iteration 30, Training loss = 0.23915565935309957
Iteration 40, Training loss = 0.2324763085819097
Iteration 50, Training loss = 0.22815474083170223
Iteration 60, Training loss = 0.223447366649114
Iteration 70, Training loss = 0.22214492334835773
Iteration 80, Training loss = 0.2194909928501516
Iteration 90, Training loss = 0.21531339263282537
Iteration 100, Training loss = 0.2127440408376104
Iteration 110, Training loss = 0.211249065068033
Iteration 120, Training loss = 0.20958818840807764
Iteration 130, Training loss = 0.20753656234142284
Iteration 140, Training loss = 0.20709407297165497
Iteration 150, Training loss = 0.20533798782577836
Iteration 160, Training loss = 0.2055489253162762
Iteration 170, Training loss = 0.20514912111459724
Iteration 180, Training loss = 0.20387924167413066
Iteration 190, Training loss = 0.20287405508727843
Iteration 200, Training loss = 0.2031456608199267
Iteration 210, Training loss = 0.20086738985517752
Iteration 220, Training loss = 0.20073789858443725
Iteration 230, Training loss = 0.19903264380091631
Iteration 240, Training loss = 0.19965497231569843
Iteration 250, Training loss = 0.19922254637675585
Iteration 260, Training loss = 0.19932875537497985
Iteration 270, Training loss = 0.19805335782576297
Iteration 280, Training loss = 0.19834841679835666
Iteration 290, Training loss = 0.1963116099725023
Model training time: 89.63315510749817
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0340604332910068
Iteration 10, Training loss = 0.9195582181359259
Iteration 20, Training loss = 0.5336675993774248
Iteration 30, Training loss = 0.3811248227737952
Iteration 40, Training loss = 0.34802390318273924
Iteration 50, Training loss = 0.33013434841293066
Iteration 60, Training loss = 0.31720907467862836
Iteration 70, Training loss = 0.3096950290643651
Iteration 80, Training loss = 0.30507655074630957
Iteration 90, Training loss = 0.30118966059408325
Iteration 100, Training loss = 0.29859637299885494
Iteration 110, Training loss = 0.29579744757949444
Iteration 120, Training loss = 0.29263480727511326
Iteration 130, Training loss = 0.29006370772486145
Iteration 140, Training loss = 0.2881220625938425
Iteration 150, Training loss = 0.28528604699649673
Iteration 160, Training loss = 0.2829915846603504
Iteration 170, Training loss = 0.2802992239234528
Iteration 180, Training loss = 0.2785780579571563
Iteration 190, Training loss = 0.2767228670598228
Iteration 200, Training loss = 0.2749106357782935
Iteration 210, Training loss = 0.27359349449763554
Iteration 220, Training loss = 0.27293374659358594
Iteration 230, Training loss = 0.2707117769309288
Iteration 240, Training loss = 0.2698161763629476
Iteration 250, Training loss = 0.2687596301165756
Iteration 260, Training loss = 0.2678854389179156
Iteration 270, Training loss = 0.2670616739326053
Iteration 280, Training loss = 0.2654843584134959
Iteration 290, Training loss = 0.26511741220807106
Model training time: 78.35266089439392
Device: cuda
Iteration 0, Training loss = 0.9870560025247399
Iteration 10, Training loss = 0.8346748470108291
Iteration 20, Training loss = 0.44130663868885683
Iteration 30, Training loss = 0.3584972587616547
Iteration 40, Training loss = 0.3341910198042934
Iteration 50, Training loss = 0.3184100854511998
Iteration 60, Training loss = 0.30885182681003054
Iteration 70, Training loss = 0.30130202463571576
Iteration 80, Training loss = 0.2960829923406315
Iteration 90, Training loss = 0.2922535655291184
Iteration 100, Training loss = 0.2891894168611886
Iteration 110, Training loss = 0.2864634450075131
Iteration 120, Training loss = 0.28446361627699673
Iteration 130, Training loss = 0.28216910369442283
Iteration 140, Training loss = 0.2803344663934431
Iteration 150, Training loss = 0.2783626106698156
Iteration 160, Training loss = 0.2770334122621495
Iteration 170, Training loss = 0.275191983954918
Iteration 180, Training loss = 0.27400680721382015
Iteration 190, Training loss = 0.27285155402001554
Iteration 200, Training loss = 0.2711241648392977
Iteration 210, Training loss = 0.27158907890895717
Iteration 220, Training loss = 0.26849921220454614
Iteration 230, Training loss = 0.26712292256418635
Iteration 240, Training loss = 0.26601394852578353
Iteration 250, Training loss = 0.2654867619276047
Iteration 260, Training loss = 0.26476764067071645
Iteration 270, Training loss = 0.26335710020744857
Iteration 280, Training loss = 0.2627560265974146
Iteration 290, Training loss = 0.2626421133029288
Model training time: 76.20422172546387
Device: cuda
Iteration 0, Training loss = 1.0805710667861257
Iteration 10, Training loss = 0.834869896300173
Iteration 20, Training loss = 0.41310285643678935
Iteration 30, Training loss = 0.34907297890831307
Iteration 40, Training loss = 0.32704267956784383
Iteration 50, Training loss = 0.31570415119618034
Iteration 60, Training loss = 0.307424191115559
Iteration 70, Training loss = 0.30107000585339494
Iteration 80, Training loss = 0.2957841047414259
Iteration 90, Training loss = 0.2921486801427344
Iteration 100, Training loss = 0.2879926886247552
Iteration 110, Training loss = 0.2860032420443452
Iteration 120, Training loss = 0.2822017129420658
Iteration 130, Training loss = 0.280509066970452
Iteration 140, Training loss = 0.27826362257994314
Iteration 150, Training loss = 0.2762272084752719
Iteration 160, Training loss = 0.27621502440044843
Iteration 170, Training loss = 0.27403686692317325
Iteration 180, Training loss = 0.2730575776906405
Iteration 190, Training loss = 0.27161569371459565
Iteration 200, Training loss = 0.27074001463139113
Iteration 210, Training loss = 0.26941943434989396
Iteration 220, Training loss = 0.26859184890841514
Iteration 230, Training loss = 0.26819804944277964
Iteration 240, Training loss = 0.2673348847074785
Iteration 250, Training loss = 0.2669498363986683
Iteration 260, Training loss = 0.2663886196875342
Iteration 270, Training loss = 0.2654992007618941
Iteration 280, Training loss = 0.26498061684882585
Iteration 290, Training loss = 0.26434056943165507
Model training time: 76.388023853302
Device: cuda
Iteration 0, Training loss = 1.1346760873057415
Iteration 10, Training loss = 0.9735188645441175
Iteration 20, Training loss = 0.7968258395575095
Iteration 30, Training loss = 0.40533557267868575
Iteration 40, Training loss = 0.34788836733154627
Iteration 50, Training loss = 0.32651732660434096
Iteration 60, Training loss = 0.31465567050924625
Iteration 70, Training loss = 0.308218149099373
Iteration 80, Training loss = 0.30340378681098784
Iteration 90, Training loss = 0.2996989646251651
Iteration 100, Training loss = 0.2956279592525556
Iteration 110, Training loss = 0.2929689468824921
Iteration 120, Training loss = 0.29016220688387967
Iteration 130, Training loss = 0.2879666525528627
Iteration 140, Training loss = 0.2859889066593658
Iteration 150, Training loss = 0.2839934323432941
Iteration 160, Training loss = 0.28206110288555497
Iteration 170, Training loss = 0.2803217426014407
Iteration 180, Training loss = 0.2785828965873534
Iteration 190, Training loss = 0.2765870795445742
Iteration 200, Training loss = 0.27636957780462534
Iteration 210, Training loss = 0.2735958495289807
Iteration 220, Training loss = 0.27217480601463917
Iteration 230, Training loss = 0.2707551296638406
Iteration 240, Training loss = 0.2700999973044879
Iteration 250, Training loss = 0.26900499562422436
Iteration 260, Training loss = 0.26828788746382304
Iteration 270, Training loss = 0.2667986641543499
Iteration 280, Training loss = 0.2661472534859814
Iteration 290, Training loss = 0.2659531024415136
Model training time: 77.83367896080017
Device: cuda
Iteration 0, Training loss = 1.0564823648779864
Iteration 10, Training loss = 0.8098562250390721
Iteration 20, Training loss = 0.4149042796253582
Iteration 30, Training loss = 0.35142255923598287
Iteration 40, Training loss = 0.32714216432709625
Iteration 50, Training loss = 0.3130894245012947
Iteration 60, Training loss = 0.3055244232145485
Iteration 70, Training loss = 0.29837894320919894
Iteration 80, Training loss = 0.2940196623116876
Iteration 90, Training loss = 0.291546696662039
Iteration 100, Training loss = 0.2873590559055264
Iteration 110, Training loss = 0.2855054219275857
Iteration 120, Training loss = 0.28272398226503015
Iteration 130, Training loss = 0.28042919145546097
Iteration 140, Training loss = 0.278619651057294
Iteration 150, Training loss = 0.2762719937352743
Iteration 160, Training loss = 0.27556295426571425
Iteration 170, Training loss = 0.2734823822615227
Iteration 180, Training loss = 0.2715424984549555
Iteration 190, Training loss = 0.2703437802656261
Iteration 200, Training loss = 0.2692095409484877
Iteration 210, Training loss = 0.26773493157492745
Iteration 220, Training loss = 0.26733516678574004
Iteration 230, Training loss = 0.26550502751184546
Iteration 240, Training loss = 0.26496482327364496
Iteration 250, Training loss = 0.2649198212629355
Iteration 260, Training loss = 0.2631955803304479
Iteration 270, Training loss = 0.26292014917457734
Iteration 280, Training loss = 0.2617476568489835
Iteration 290, Training loss = 0.26090118641726634
Model training time: 76.8630542755127
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.998027391731739
Iteration 10, Training loss = 0.2866679010196374
Iteration 20, Training loss = 0.2622918222959225
Iteration 30, Training loss = 0.2570304059638427
Iteration 40, Training loss = 0.2503869706191696
Iteration 50, Training loss = 0.24829189178462213
Iteration 60, Training loss = 0.24595649898625338
Iteration 70, Training loss = 0.24316643465023774
Iteration 80, Training loss = 0.24267242705592743
Iteration 90, Training loss = 0.23986038723244116
Model training time: 18.80796194076538
Device: cuda
Iteration 0, Training loss = 0.9305338360942327
Iteration 10, Training loss = 0.27556117495092064
Iteration 20, Training loss = 0.2561101340330564
Iteration 30, Training loss = 0.25074858075151074
Iteration 40, Training loss = 0.2466424580376882
Iteration 50, Training loss = 0.24280576107020563
Iteration 60, Training loss = 0.24090226682332846
Iteration 70, Training loss = 0.23787127535503644
Iteration 80, Training loss = 0.23742804389733535
Iteration 90, Training loss = 0.23395156846023524
Model training time: 19.153728485107422
Device: cuda
Iteration 0, Training loss = 0.9584672347857401
Iteration 10, Training loss = 0.2799077625744618
Iteration 20, Training loss = 0.2601075768470764
Iteration 30, Training loss = 0.2523286428589087
Iteration 40, Training loss = 0.25003961316094947
Iteration 50, Training loss = 0.24468971437846238
Iteration 60, Training loss = 0.24059912275809509
Iteration 70, Training loss = 0.23981026803644803
Iteration 80, Training loss = 0.23424080114525098
Iteration 90, Training loss = 0.23375350895982522
Model training time: 20.04104781150818
Device: cuda
Iteration 0, Training loss = 0.9277450018204175
Iteration 10, Training loss = 0.26535789403491294
Iteration 20, Training loss = 0.2557393682117646
Iteration 30, Training loss = 0.24735458195209503
Iteration 40, Training loss = 0.24243914421934348
Iteration 50, Training loss = 0.2392732585565402
Iteration 60, Training loss = 0.2358875064036021
Iteration 70, Training loss = 0.2347935141565708
Iteration 80, Training loss = 0.22905253826712185
Iteration 90, Training loss = 0.22627236744245657
Model training time: 20.600144624710083
Device: cuda
Iteration 0, Training loss = 0.9574802540815793
Iteration 10, Training loss = 0.2673144874951014
Iteration 20, Training loss = 0.25030304978673273
Iteration 30, Training loss = 0.24486588271191487
Iteration 40, Training loss = 0.2407142105870522
Iteration 50, Training loss = 0.23745618688945586
Iteration 60, Training loss = 0.23419817522741282
Iteration 70, Training loss = 0.2323554435458321
Iteration 80, Training loss = 0.23133002357700697
Iteration 90, Training loss = 0.23182145971804857
Model training time: 20.870931386947632
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0040178436499376
Iteration 10, Training loss = 0.9986755269078108
Iteration 20, Training loss = 0.9763652349893863
Iteration 30, Training loss = 0.9461391975100224
Iteration 40, Training loss = 0.8982597950559396
Iteration 50, Training loss = 0.8193654710283647
Iteration 60, Training loss = 0.6874329803081659
Iteration 70, Training loss = 0.5392332968230431
Iteration 80, Training loss = 0.4330127560175382
Iteration 90, Training loss = 0.3864604313499652
Model training time: 17.869129419326782
Device: cuda
Iteration 0, Training loss = 1.017928453019032
Iteration 10, Training loss = 0.9995900467038155
Iteration 20, Training loss = 0.9876934610880338
Iteration 30, Training loss = 0.973896498290392
Iteration 40, Training loss = 0.9445786275542699
Iteration 50, Training loss = 0.9064715584883323
Iteration 60, Training loss = 0.8363544602806752
Iteration 70, Training loss = 0.7305072104701629
Iteration 80, Training loss = 0.5942840384176145
Iteration 90, Training loss = 0.4854466321949775
Model training time: 17.789219856262207
Device: cuda
Iteration 0, Training loss = 0.9906316049970113
Iteration 10, Training loss = 0.9688486766356689
Iteration 20, Training loss = 0.9405957070680765
Iteration 30, Training loss = 0.8900910908213029
Iteration 40, Training loss = 0.8047487867566255
Iteration 50, Training loss = 0.6773073117320354
Iteration 60, Training loss = 0.5298075704620435
Iteration 70, Training loss = 0.42604678267469775
Iteration 80, Training loss = 0.3832667994384582
Iteration 90, Training loss = 0.3699760644768293
Model training time: 17.112680912017822
Device: cuda
Iteration 0, Training loss = 1.0095156179024622
Iteration 10, Training loss = 0.9779201986698004
Iteration 20, Training loss = 0.9448770691569035
Iteration 30, Training loss = 0.8960410625888751
Iteration 40, Training loss = 0.8099527765925114
Iteration 50, Training loss = 0.6774816576104897
Iteration 60, Training loss = 0.5345700598106935
Iteration 70, Training loss = 0.4393495195187055
Iteration 80, Training loss = 0.39856685196550995
Iteration 90, Training loss = 0.3785199272231414
Model training time: 17.041344165802002
Device: cuda
Iteration 0, Training loss = 1.0331603363156319
Iteration 10, Training loss = 1.0052028814187417
Iteration 20, Training loss = 1.0030798705724568
Iteration 30, Training loss = 0.9958046107338026
Iteration 40, Training loss = 0.9876814215229108
Iteration 50, Training loss = 0.9826043970309771
Iteration 60, Training loss = 0.9709693021499194
Iteration 70, Training loss = 0.9533179665987308
Iteration 80, Training loss = 0.9243367067896403
Iteration 90, Training loss = 0.8754347717532744
Model training time: 16.729329109191895
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1392127493253121
Iteration 10, Training loss = 0.31144493402769935
Iteration 20, Training loss = 0.2714620867314247
Iteration 30, Training loss = 0.2625289367368588
Iteration 40, Training loss = 0.2573954736670622
Iteration 50, Training loss = 0.25387653708457947
Iteration 60, Training loss = 0.25143592346173066
Iteration 70, Training loss = 0.25036073920245355
Iteration 80, Training loss = 0.24835934934134668
Iteration 90, Training loss = 0.2468742084904359
Model training time: 18.568071365356445
Device: cuda
Iteration 0, Training loss = 0.9743983992017232
Iteration 10, Training loss = 0.28888918158526605
Iteration 20, Training loss = 0.2629731483757496
Iteration 30, Training loss = 0.25897287032925165
Iteration 40, Training loss = 0.2537039751903369
Iteration 50, Training loss = 0.2505586352199316
Iteration 60, Training loss = 0.2473387554861032
Iteration 70, Training loss = 0.24484362926047581
Iteration 80, Training loss = 0.2424843622228274
Iteration 90, Training loss = 0.24094455703519857
Model training time: 18.49616527557373
Device: cuda
Iteration 0, Training loss = 1.2408171794735467
Iteration 10, Training loss = 0.36340086878492284
Iteration 20, Training loss = 0.28030624168996626
Iteration 30, Training loss = 0.26014042344803995
Iteration 40, Training loss = 0.2532042580155226
Iteration 50, Training loss = 0.2503446807655004
Iteration 60, Training loss = 0.24805244201650986
Iteration 70, Training loss = 0.24545300322083327
Iteration 80, Training loss = 0.2440054681725227
Iteration 90, Training loss = 0.24370234531278795
Model training time: 18.961043119430542
Device: cuda
Iteration 0, Training loss = 0.9744323004896824
Iteration 10, Training loss = 0.29823318439034313
Iteration 20, Training loss = 0.2632724316074298
Iteration 30, Training loss = 0.25560658518224955
Iteration 40, Training loss = 0.2555464387226563
Iteration 50, Training loss = 0.2501739895401093
Iteration 60, Training loss = 0.2478785511965935
Iteration 70, Training loss = 0.24691811089332288
Iteration 80, Training loss = 0.24119128616383442
Iteration 90, Training loss = 0.2390403847138469
Model training time: 18.866350173950195
Device: cuda
Iteration 0, Training loss = 0.9377283167380553
Iteration 10, Training loss = 0.2776006074765554
Iteration 20, Training loss = 0.2553528919816017
Iteration 30, Training loss = 0.2505455958442046
Iteration 40, Training loss = 0.2469154026072759
Iteration 50, Training loss = 0.24450476567905682
Iteration 60, Training loss = 0.24252514770397773
Iteration 70, Training loss = 0.24072217325178477
Iteration 80, Training loss = 0.23900146023012125
Iteration 90, Training loss = 0.23671319593603796
Model training time: 19.2412371635437
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9947283004338925
Iteration 10, Training loss = 0.979920700765573
Iteration 20, Training loss = 0.9710869743273809
Iteration 30, Training loss = 0.9550019101454661
Iteration 40, Training loss = 0.9189526060452828
Iteration 50, Training loss = 0.8720548072686563
Iteration 60, Training loss = 0.8013054780088938
Iteration 70, Training loss = 0.7034554163423868
Iteration 80, Training loss = 0.6016253215762285
Iteration 90, Training loss = 0.5128931646736768
Model training time: 16.985363245010376
Device: cuda
Iteration 0, Training loss = 1.1996384005133922
Iteration 10, Training loss = 1.005225946696905
Iteration 20, Training loss = 1.0015249057457998
Iteration 30, Training loss = 0.9996757719379205
Iteration 40, Training loss = 1.0006693401015723
Iteration 50, Training loss = 0.9953273013234138
Iteration 60, Training loss = 0.9951591887153112
Iteration 70, Training loss = 0.99138926084225
Iteration 80, Training loss = 0.9897664493093123
Iteration 90, Training loss = 0.9821624801709101
Model training time: 16.73709726333618
Device: cuda
Iteration 0, Training loss = 1.227423298244293
Iteration 10, Training loss = 0.9871627682676682
Iteration 20, Training loss = 0.9771787845171415
Iteration 30, Training loss = 0.973430620936247
Iteration 40, Training loss = 0.9584737666524373
Iteration 50, Training loss = 0.9400629768004785
Iteration 60, Training loss = 0.8970966195830932
Iteration 70, Training loss = 0.8340582601152934
Iteration 80, Training loss = 0.7430949532068692
Iteration 90, Training loss = 0.6276646680556811
Model training time: 16.63754677772522
Device: cuda
Iteration 0, Training loss = 1.1408723988212073
Iteration 10, Training loss = 1.0139893218874931
Iteration 20, Training loss = 1.0064300923393323
Iteration 30, Training loss = 1.001799780015762
Iteration 40, Training loss = 0.9999186711815687
Iteration 50, Training loss = 0.9868867351458623
Iteration 60, Training loss = 0.9769192332258592
Iteration 70, Training loss = 0.961536807509569
Iteration 80, Training loss = 0.9365950444569955
Iteration 90, Training loss = 0.8952635360451845
Model training time: 17.290152311325073
Device: cuda
Iteration 0, Training loss = 1.1662102513588393
Iteration 10, Training loss = 0.9923021180125383
Iteration 20, Training loss = 0.9932509878507028
Iteration 30, Training loss = 0.9831728958166562
Iteration 40, Training loss = 0.9714575237952746
Iteration 50, Training loss = 0.9674803161850343
Iteration 60, Training loss = 0.9439393425217042
Iteration 70, Training loss = 0.9278465285897255
Iteration 80, Training loss = 0.880557504697488
Iteration 90, Training loss = 0.8334520046527569
Model training time: 16.79919123649597
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9678336697129103
Iteration 10, Training loss = 0.2867967264296917
Iteration 20, Training loss = 0.26179970886844856
Iteration 30, Training loss = 0.2551617364470775
Iteration 40, Training loss = 0.25183070751910025
Iteration 50, Training loss = 0.24963810122930086
Iteration 60, Training loss = 0.24689151232059187
Iteration 70, Training loss = 0.24292279357233873
Iteration 80, Training loss = 0.24301075405226305
Iteration 90, Training loss = 0.23905989327109778
Model training time: 18.978639841079712
Device: cuda
Iteration 0, Training loss = 0.9153141539830428
Iteration 10, Training loss = 0.2793819153538117
Iteration 20, Training loss = 0.26073026040998787
Iteration 30, Training loss = 0.25569130222384745
Iteration 40, Training loss = 0.2541544226786265
Iteration 50, Training loss = 0.25190878774111086
Iteration 60, Training loss = 0.2506017994422179
Iteration 70, Training loss = 0.24914506564919764
Iteration 80, Training loss = 0.2471330277621746
Iteration 90, Training loss = 0.24502229403990966
Model training time: 18.736297130584717
Device: cuda
Iteration 0, Training loss = 0.9477803707122803
Iteration 10, Training loss = 0.2780765723437071
Iteration 20, Training loss = 0.26318959719859636
Iteration 30, Training loss = 0.257083468998854
Iteration 40, Training loss = 0.25292155891656876
Iteration 50, Training loss = 0.24908762740401122
Iteration 60, Training loss = 0.24814611501418626
Iteration 70, Training loss = 0.2455130573362112
Iteration 80, Training loss = 0.24591219504005635
Iteration 90, Training loss = 0.2436965417403441
Model training time: 19.32441759109497
Device: cuda
Iteration 0, Training loss = 0.9583650002112756
Iteration 10, Training loss = 0.27993433764920783
Iteration 20, Training loss = 0.26210085961681145
Iteration 30, Training loss = 0.2569287915069323
Iteration 40, Training loss = 0.2505240412428975
Iteration 50, Training loss = 0.24777484907267186
Iteration 60, Training loss = 0.24777417099819735
Iteration 70, Training loss = 0.24445597669826105
Iteration 80, Training loss = 0.24551289471296164
Iteration 90, Training loss = 0.2427574831705827
Model training time: 19.190701246261597
Device: cuda
Iteration 0, Training loss = 1.0127427394573505
Iteration 10, Training loss = 0.278962863322634
Iteration 20, Training loss = 0.259042853059677
Iteration 30, Training loss = 0.25472595943854404
Iteration 40, Training loss = 0.25432948166361224
Iteration 50, Training loss = 0.24914087634533644
Iteration 60, Training loss = 0.247899760850347
Iteration 70, Training loss = 0.24740328424825117
Iteration 80, Training loss = 0.24538780533923552
Iteration 90, Training loss = 0.24312402967077035
Model training time: 19.191112995147705
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0066926141197865
Iteration 10, Training loss = 0.9896752931750737
Iteration 20, Training loss = 0.9734527096152306
Iteration 30, Training loss = 0.9451277210162237
Iteration 40, Training loss = 0.9170149375612919
Iteration 50, Training loss = 0.8546662898017809
Iteration 60, Training loss = 0.7760556867489448
Iteration 70, Training loss = 0.6770585236641077
Iteration 80, Training loss = 0.5800512163684919
Iteration 90, Training loss = 0.5013377571908327
Model training time: 16.782482385635376
Device: cuda
Iteration 0, Training loss = 1.0314371302150762
Iteration 10, Training loss = 0.9885650798678398
Iteration 20, Training loss = 0.9730596972199587
Iteration 30, Training loss = 0.9500424340367317
Iteration 40, Training loss = 0.9119898550785505
Iteration 50, Training loss = 0.8478371099783824
Iteration 60, Training loss = 0.7445424004242971
Iteration 70, Training loss = 0.6115321863729221
Iteration 80, Training loss = 0.4849199755833699
Iteration 90, Training loss = 0.4105012408242776
Model training time: 16.98170804977417
Device: cuda
Iteration 0, Training loss = 1.0059752349670117
Iteration 10, Training loss = 0.9919669994941125
Iteration 20, Training loss = 0.9862848571859874
Iteration 30, Training loss = 0.9802082759829668
Iteration 40, Training loss = 0.9679744077416567
Iteration 50, Training loss = 0.9523942006322054
Iteration 60, Training loss = 0.9269304430255523
Iteration 70, Training loss = 0.8721708024923618
Iteration 80, Training loss = 0.7955059019418863
Iteration 90, Training loss = 0.6938427546276495
Model training time: 17.10527753829956
Device: cuda
Iteration 0, Training loss = 1.042263836814807
Iteration 10, Training loss = 1.0057812258601189
Iteration 20, Training loss = 0.9908429986009231
Iteration 30, Training loss = 0.9658051820901724
Iteration 40, Training loss = 0.9344038814306259
Iteration 50, Training loss = 0.8902694903887235
Iteration 60, Training loss = 0.809858141037134
Iteration 70, Training loss = 0.6999559425390683
Iteration 80, Training loss = 0.5821525417268276
Iteration 90, Training loss = 0.4848081326255432
Model training time: 17.05273199081421
Device: cuda
Iteration 0, Training loss = 1.108902776470551
Iteration 10, Training loss = 0.9901925990214715
Iteration 20, Training loss = 0.9778598535519379
Iteration 30, Training loss = 0.9595079897687986
Iteration 40, Training loss = 0.9287740450638992
Iteration 50, Training loss = 0.882769701572565
Iteration 60, Training loss = 0.8144994372358689
Iteration 70, Training loss = 0.7257018937514379
Iteration 80, Training loss = 0.6260504911725338
Iteration 90, Training loss = 0.5367427419584531
Model training time: 16.626388788223267
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1235619749014194
Iteration 10, Training loss = 0.28767320012243897
Iteration 20, Training loss = 0.2655863097080818
Iteration 30, Training loss = 0.25777848809957504
Iteration 40, Training loss = 0.25112144792309177
Iteration 50, Training loss = 0.24686738182432377
Iteration 60, Training loss = 0.24400957544835714
Iteration 70, Training loss = 0.24318843005368343
Iteration 80, Training loss = 0.24097532535401675
Iteration 90, Training loss = 0.23834956086312348
Iteration 100, Training loss = 0.23833698091598657
Iteration 110, Training loss = 0.23469954155958617
Iteration 120, Training loss = 0.23381519503891468
Iteration 130, Training loss = 0.23284362815320492
Iteration 140, Training loss = 0.2336098076059268
Iteration 150, Training loss = 0.22984495465285504
Iteration 160, Training loss = 0.23038693861319468
Iteration 170, Training loss = 0.22829788099401271
Iteration 180, Training loss = 0.2255503724400814
Iteration 190, Training loss = 0.22388648012509713
Model training time: 37.82673406600952
Device: cuda
Iteration 0, Training loss = 0.9357741033801665
Iteration 10, Training loss = 0.2769694014524038
Iteration 20, Training loss = 0.25872176169202876
Iteration 30, Training loss = 0.25056377783990824
Iteration 40, Training loss = 0.24579986521544364
Iteration 50, Training loss = 0.24436678307560775
Iteration 60, Training loss = 0.2395949625911621
Iteration 70, Training loss = 0.23797562360190427
Iteration 80, Training loss = 0.2343393428107867
Iteration 90, Training loss = 0.23086636821524456
Iteration 100, Training loss = 0.22716220862303788
Iteration 110, Training loss = 0.2259225411197314
Iteration 120, Training loss = 0.22397410583037597
Iteration 130, Training loss = 0.22137821308122232
Iteration 140, Training loss = 0.21988803836015555
Iteration 150, Training loss = 0.21896424474051365
Iteration 160, Training loss = 0.21933322242246225
Iteration 170, Training loss = 0.2165679858567623
Iteration 180, Training loss = 0.21682355567239797
Iteration 190, Training loss = 0.21452705175257647
Model training time: 37.76354503631592
Device: cuda
Iteration 0, Training loss = 1.120115347779714
Iteration 10, Training loss = 0.2894571013748646
Iteration 20, Training loss = 0.26439948193728924
Iteration 30, Training loss = 0.2575258081062482
Iteration 40, Training loss = 0.2519547614054038
Iteration 50, Training loss = 0.24843023086969668
Iteration 60, Training loss = 0.24723447730334905
Iteration 70, Training loss = 0.2433128819729273
Iteration 80, Training loss = 0.24097320150870544
Iteration 90, Training loss = 0.2415484690035765
Iteration 100, Training loss = 0.2381663965777709
Iteration 110, Training loss = 0.2358505754516675
Iteration 120, Training loss = 0.23270713423307127
Iteration 130, Training loss = 0.2310372648330835
Iteration 140, Training loss = 0.23153291184168595
Iteration 150, Training loss = 0.22866468274822602
Iteration 160, Training loss = 0.22755960675959402
Iteration 170, Training loss = 0.22659298741760162
Iteration 180, Training loss = 0.2255289089651062
Iteration 190, Training loss = 0.22421846811014873
Model training time: 38.13072204589844
Device: cuda
Iteration 0, Training loss = 0.9913169202896265
Iteration 10, Training loss = 0.27097505445663744
Iteration 20, Training loss = 0.25584625939910227
Iteration 30, Training loss = 0.24856328348127696
Iteration 40, Training loss = 0.2440897159708234
Iteration 50, Training loss = 0.241646409751131
Iteration 60, Training loss = 0.23918084542338663
Iteration 70, Training loss = 0.23530030580094227
Iteration 80, Training loss = 0.23464487550350335
Iteration 90, Training loss = 0.23217946477234364
Iteration 100, Training loss = 0.23093119550209779
Iteration 110, Training loss = 0.22772468554858977
Iteration 120, Training loss = 0.22515229587084973
Iteration 130, Training loss = 0.22442070079537538
Iteration 140, Training loss = 0.22199889100514925
Iteration 150, Training loss = 0.21981675593325725
Iteration 160, Training loss = 0.21908882188682371
Iteration 170, Training loss = 0.21803075003509337
Iteration 180, Training loss = 0.21676534407127362
Iteration 190, Training loss = 0.21780648827552795
Model training time: 37.67491698265076
Device: cuda
Iteration 0, Training loss = 0.9516601745898907
Iteration 10, Training loss = 0.2723279907726325
Iteration 20, Training loss = 0.25721659840872657
Iteration 30, Training loss = 0.24858377730617157
Iteration 40, Training loss = 0.24513810586470824
Iteration 50, Training loss = 0.24425070417615083
Iteration 60, Training loss = 0.24200091081169936
Iteration 70, Training loss = 0.23849454070799625
Iteration 80, Training loss = 0.23593362220204794
Iteration 90, Training loss = 0.23537523972873503
Iteration 100, Training loss = 0.23146896594418928
Iteration 110, Training loss = 0.22794237446326476
Iteration 120, Training loss = 0.22716117621614382
Iteration 130, Training loss = 0.22466806666209146
Iteration 140, Training loss = 0.22592150763823435
Iteration 150, Training loss = 0.2207082434055897
Iteration 160, Training loss = 0.2184769415224974
Iteration 170, Training loss = 0.21829096127588016
Iteration 180, Training loss = 0.21688005034453595
Iteration 190, Training loss = 0.21403853172579637
Model training time: 37.82240271568298
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0918263546549356
Iteration 10, Training loss = 0.9751078543754724
Iteration 20, Training loss = 0.9583735351379101
Iteration 30, Training loss = 0.9306171232691178
Iteration 40, Training loss = 0.8958490886367284
Iteration 50, Training loss = 0.8284031576835192
Iteration 60, Training loss = 0.7225566471998508
Iteration 70, Training loss = 0.6045090401401887
Iteration 80, Training loss = 0.48849230259656906
Iteration 90, Training loss = 0.4265164773051555
Iteration 100, Training loss = 0.3969735543315227
Iteration 110, Training loss = 0.3810836389087714
Iteration 120, Training loss = 0.3697547090168183
Iteration 130, Training loss = 0.36340036128575987
Iteration 140, Training loss = 0.35840972111775327
Iteration 150, Training loss = 0.3488159738481045
Iteration 160, Training loss = 0.3419878830989966
Iteration 170, Training loss = 0.3366325023369147
Iteration 180, Training loss = 0.3316892096056388
Iteration 190, Training loss = 0.32898987428500104
Model training time: 34.052656412124634
Device: cuda
Iteration 0, Training loss = 1.0218696748981109
Iteration 10, Training loss = 0.9875713300246459
Iteration 20, Training loss = 0.9806168348743365
Iteration 30, Training loss = 0.9658362349638572
Iteration 40, Training loss = 0.9464359902418576
Iteration 50, Training loss = 0.9079090843980129
Iteration 60, Training loss = 0.8569470105262903
Iteration 70, Training loss = 0.7677424441163356
Iteration 80, Training loss = 0.6510373164828007
Iteration 90, Training loss = 0.5284220367096938
Iteration 100, Training loss = 0.44514402317313045
Iteration 110, Training loss = 0.4034413560651816
Iteration 120, Training loss = 0.3843891624934398
Iteration 130, Training loss = 0.372142962538279
Iteration 140, Training loss = 0.36637885066179127
Iteration 150, Training loss = 0.35565826612023205
Iteration 160, Training loss = 0.3527632367152434
Iteration 170, Training loss = 0.3442688792084272
Iteration 180, Training loss = 0.33819517521904063
Iteration 190, Training loss = 0.33288596541835713
Model training time: 34.81188082695007
Device: cuda
Iteration 0, Training loss = 1.1767108698303883
Iteration 10, Training loss = 0.9909414030038394
Iteration 20, Training loss = 0.9713990888916529
Iteration 30, Training loss = 0.9560615145243131
Iteration 40, Training loss = 0.9291294595369926
Iteration 50, Training loss = 0.8795557039288374
Iteration 60, Training loss = 0.7971924245357513
Iteration 70, Training loss = 0.6832295140394797
Iteration 80, Training loss = 0.5536853066430643
Iteration 90, Training loss = 0.45908123168807763
Iteration 100, Training loss = 0.40938138503294724
Iteration 110, Training loss = 0.3837113602516743
Iteration 120, Training loss = 0.3711643721908331
Iteration 130, Training loss = 0.35986938786048156
Iteration 140, Training loss = 0.3520414734689089
Iteration 150, Training loss = 0.3440077637250607
Iteration 160, Training loss = 0.3356916672335221
Iteration 170, Training loss = 0.3304806725623516
Iteration 180, Training loss = 0.3260362231387542
Iteration 190, Training loss = 0.3200044361158059
Model training time: 33.43986940383911
Device: cuda
Iteration 0, Training loss = 1.0803268351233923
Iteration 10, Training loss = 0.9919266998767853
Iteration 20, Training loss = 0.9817284523294523
Iteration 30, Training loss = 0.9697216978439918
Iteration 40, Training loss = 0.9538423986389086
Iteration 50, Training loss = 0.9128817279751484
Iteration 60, Training loss = 0.8563905323927219
Iteration 70, Training loss = 0.7539503683264439
Iteration 80, Training loss = 0.6101959952368186
Iteration 90, Training loss = 0.48169843308054483
Iteration 100, Training loss = 0.4106429557387645
Iteration 110, Training loss = 0.3805139081982466
Iteration 120, Training loss = 0.36844472472484296
Iteration 130, Training loss = 0.3581342546699139
Iteration 140, Training loss = 0.3496894161575116
Iteration 150, Training loss = 0.3443174910946534
Iteration 160, Training loss = 0.3364148072611827
Iteration 170, Training loss = 0.33098325763757414
Iteration 180, Training loss = 0.3271150366904644
Iteration 190, Training loss = 0.32269932558903325
Model training time: 33.4453501701355
Device: cuda
Iteration 0, Training loss = 1.0618842089405427
Iteration 10, Training loss = 0.9899349238436955
Iteration 20, Training loss = 0.9840355091370069
Iteration 30, Training loss = 0.977243008521887
Iteration 40, Training loss = 0.9550945850519034
Iteration 50, Training loss = 0.9318630729730313
Iteration 60, Training loss = 0.8914371069807273
Iteration 70, Training loss = 0.8217658583934491
Iteration 80, Training loss = 0.7258492433107816
Iteration 90, Training loss = 0.6150208685833675
Iteration 100, Training loss = 0.5130632591362183
Iteration 110, Training loss = 0.4498173946944567
Iteration 120, Training loss = 0.4134285980119155
Iteration 130, Training loss = 0.39330546815808004
Iteration 140, Training loss = 0.38077528230272806
Iteration 150, Training loss = 0.3704832438379526
Iteration 160, Training loss = 0.36143742902920795
Iteration 170, Training loss = 0.35356105548831135
Iteration 180, Training loss = 0.3477540627981608
Iteration 190, Training loss = 0.341638336244684
Model training time: 33.68235969543457
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9614689355859389
Iteration 10, Training loss = 0.2920662264984388
Iteration 20, Training loss = 0.26777295023202896
Iteration 30, Training loss = 0.2595691251067015
Iteration 40, Training loss = 0.2554927674623636
Iteration 50, Training loss = 0.2509264612140564
Iteration 60, Training loss = 0.24905655346810818
Iteration 70, Training loss = 0.2467763633825458
Iteration 80, Training loss = 0.24492428881617692
Iteration 90, Training loss = 0.24734529967491442
Iteration 100, Training loss = 0.24236984866169783
Iteration 110, Training loss = 0.24165185942099646
Iteration 120, Training loss = 0.2402544071754584
Iteration 130, Training loss = 0.238532823152267
Iteration 140, Training loss = 0.2384534667317684
Iteration 150, Training loss = 0.2364094745940887
Iteration 160, Training loss = 0.23687766349086395
Iteration 170, Training loss = 0.2347709395421239
Iteration 180, Training loss = 0.2318892107846645
Iteration 190, Training loss = 0.23141073507185167
Model training time: 37.66166377067566
Device: cuda
Iteration 0, Training loss = 1.2845755156416159
Iteration 10, Training loss = 0.32858744435585463
Iteration 20, Training loss = 0.2720457739554919
Iteration 30, Training loss = 0.2586821321254739
Iteration 40, Training loss = 0.25587007446357835
Iteration 50, Training loss = 0.25106412888719487
Iteration 60, Training loss = 0.24828520503181678
Iteration 70, Training loss = 0.24649718704705054
Iteration 80, Training loss = 0.2442523752554105
Iteration 90, Training loss = 0.24332104465709284
Iteration 100, Training loss = 0.2427596073024548
Iteration 110, Training loss = 0.23957283293398526
Iteration 120, Training loss = 0.24141433462500572
Iteration 130, Training loss = 0.23671232321514532
Iteration 140, Training loss = 0.2360776993804253
Iteration 150, Training loss = 0.23304378470549217
Iteration 160, Training loss = 0.23267143148069197
Iteration 170, Training loss = 0.2317931168497755
Iteration 180, Training loss = 0.2322017110597629
Iteration 190, Training loss = 0.2297942050231191
Model training time: 37.42539429664612
Device: cuda
Iteration 0, Training loss = 1.1834149286150932
Iteration 10, Training loss = 0.3274790057196067
Iteration 20, Training loss = 0.2680372535609282
Iteration 30, Training loss = 0.2568825652393011
Iteration 40, Training loss = 0.25072001622846496
Iteration 50, Training loss = 0.24956949126835054
Iteration 60, Training loss = 0.24868767952116635
Iteration 70, Training loss = 0.24270449213397044
Iteration 80, Training loss = 0.24104063814649215
Iteration 90, Training loss = 0.24211556973079076
Iteration 100, Training loss = 0.23752542377377933
Iteration 110, Training loss = 0.235427669607676
Iteration 120, Training loss = 0.23466274335693854
Iteration 130, Training loss = 0.23155458171207172
Iteration 140, Training loss = 0.23007058459692276
Iteration 150, Training loss = 0.22912383953539225
Iteration 160, Training loss = 0.22568461327598646
Iteration 170, Training loss = 0.22514021317832744
Iteration 180, Training loss = 0.2253903111156363
Iteration 190, Training loss = 0.2226902821029608
Model training time: 38.105287313461304
Device: cuda
Iteration 0, Training loss = 1.0119780451059341
Iteration 10, Training loss = 0.32147031420698535
Iteration 20, Training loss = 0.2678979365871503
Iteration 30, Training loss = 0.25490517197893214
Iteration 40, Training loss = 0.2528940155529059
Iteration 50, Training loss = 0.24917416073955023
Iteration 60, Training loss = 0.24626590490627748
Iteration 70, Training loss = 0.2457596559364062
Iteration 80, Training loss = 0.24168118290030038
Iteration 90, Training loss = 0.24024465257445207
Iteration 100, Training loss = 0.2394072457861442
Iteration 110, Training loss = 0.23766541086996978
Iteration 120, Training loss = 0.23855689110664222
Iteration 130, Training loss = 0.2354258970142557
Iteration 140, Training loss = 0.23658052932184476
Iteration 150, Training loss = 0.23366859125403258
Iteration 160, Training loss = 0.23290331575732964
Iteration 170, Training loss = 0.23208575853361532
Iteration 180, Training loss = 0.2319336264895705
Iteration 190, Training loss = 0.2313039739831136
Model training time: 38.60125541687012
Device: cuda
Iteration 0, Training loss = 1.2027415421146612
Iteration 10, Training loss = 0.3186694013957794
Iteration 20, Training loss = 0.2615661473514942
Iteration 30, Training loss = 0.2511707948377499
Iteration 40, Training loss = 0.24800777012625566
Iteration 50, Training loss = 0.24600665006213462
Iteration 60, Training loss = 0.24643505479280764
Iteration 70, Training loss = 0.24355006862718326
Iteration 80, Training loss = 0.2450014245338165
Iteration 90, Training loss = 0.24273009820339772
Iteration 100, Training loss = 0.23982925268893057
Iteration 110, Training loss = 0.23888448267602003
Iteration 120, Training loss = 0.23800281521219474
Iteration 130, Training loss = 0.2400785701779219
Iteration 140, Training loss = 0.23600348106657082
Iteration 150, Training loss = 0.23640474662757838
Iteration 160, Training loss = 0.2351770902482363
Iteration 170, Training loss = 0.23739757890311572
Iteration 180, Training loss = 0.23313621610689622
Iteration 190, Training loss = 0.23394440644635603
Model training time: 37.40621733665466
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.106669106735633
Iteration 10, Training loss = 0.9931757352673091
Iteration 20, Training loss = 0.9889605085437114
Iteration 30, Training loss = 0.9840990041310971
Iteration 40, Training loss = 0.9831792133358809
Iteration 50, Training loss = 0.9704823786249528
Iteration 60, Training loss = 0.9618591184799488
Iteration 70, Training loss = 0.9393783148664695
Iteration 80, Training loss = 0.9153414597878089
Iteration 90, Training loss = 0.8785721464799001
Iteration 100, Training loss = 0.829511505479996
Iteration 110, Training loss = 0.7615669538768438
Iteration 120, Training loss = 0.6862578440744144
Iteration 130, Training loss = 0.604109106442103
Iteration 140, Training loss = 0.5281214212568907
Iteration 150, Training loss = 0.4714239233961472
Iteration 160, Training loss = 0.4333346855755036
Iteration 170, Training loss = 0.4082963995348949
Iteration 180, Training loss = 0.391082404467922
Iteration 190, Training loss = 0.3840892481116148
Model training time: 33.98880934715271
Device: cuda
Iteration 0, Training loss = 1.229248315669023
Iteration 10, Training loss = 1.0016750114468427
Iteration 20, Training loss = 0.9929062810081702
Iteration 30, Training loss = 0.9850756589036721
Iteration 40, Training loss = 0.9775001767736214
Iteration 50, Training loss = 0.9633179966073769
Iteration 60, Training loss = 0.9430680389587696
Iteration 70, Training loss = 0.9128858441343675
Iteration 80, Training loss = 0.8485425180540636
Iteration 90, Training loss = 0.767253197156466
Iteration 100, Training loss = 0.6645141246800239
Iteration 110, Training loss = 0.5674270232422993
Iteration 120, Training loss = 0.4980823939236311
Iteration 130, Training loss = 0.44794582260342747
Iteration 140, Training loss = 0.4190415811653321
Iteration 150, Training loss = 0.4002935393498494
Iteration 160, Training loss = 0.38673234243805593
Iteration 170, Training loss = 0.37856340924134624
Iteration 180, Training loss = 0.3682289060491782
Iteration 190, Training loss = 0.3625747966938294
Model training time: 33.63738822937012
Device: cuda
Iteration 0, Training loss = 1.0269592020374079
Iteration 10, Training loss = 0.9916243077470706
Iteration 20, Training loss = 0.9868365067702073
Iteration 30, Training loss = 0.9844366827836404
Iteration 40, Training loss = 0.9784366190433502
Iteration 50, Training loss = 0.9630824046639296
Iteration 60, Training loss = 0.9472775063835658
Iteration 70, Training loss = 0.9207604046051319
Iteration 80, Training loss = 0.8789591915332354
Iteration 90, Training loss = 0.8103861166880681
Iteration 100, Training loss = 0.7168747673814113
Iteration 110, Training loss = 0.607754399570135
Iteration 120, Training loss = 0.5109741226411783
Iteration 130, Training loss = 0.44593168107362896
Iteration 140, Training loss = 0.4067548029124737
Iteration 150, Training loss = 0.3843391328476943
Iteration 160, Training loss = 0.37158250636779344
Iteration 170, Training loss = 0.35933575163093895
Iteration 180, Training loss = 0.3502095196969234
Iteration 190, Training loss = 0.34148897947027135
Model training time: 33.47824215888977
Device: cuda
Iteration 0, Training loss = 1.0091566655498285
Iteration 10, Training loss = 0.9796427694650797
Iteration 20, Training loss = 0.9585629334816566
Iteration 30, Training loss = 0.9185363294986578
Iteration 40, Training loss = 0.8607902079820633
Iteration 50, Training loss = 0.7710805953695223
Iteration 60, Training loss = 0.6491006621374533
Iteration 70, Training loss = 0.5296431152293315
Iteration 80, Training loss = 0.45032688746085536
Iteration 90, Training loss = 0.40651860632575476
Iteration 100, Training loss = 0.38661236144029176
Iteration 110, Training loss = 0.3744181070763331
Iteration 120, Training loss = 0.3666253640101506
Iteration 130, Training loss = 0.3583991160759559
Iteration 140, Training loss = 0.35049209070320314
Iteration 150, Training loss = 0.3465410713106394
Iteration 160, Training loss = 0.33930187127911127
Iteration 170, Training loss = 0.3346841431294496
Iteration 180, Training loss = 0.33010988940413183
Iteration 190, Training loss = 0.32648204181056756
Model training time: 33.709057569503784
Device: cuda
Iteration 0, Training loss = 1.0380785734607623
Iteration 10, Training loss = 0.9745337289686387
Iteration 20, Training loss = 0.9547567883363137
Iteration 30, Training loss = 0.9183430439577653
Iteration 40, Training loss = 0.8689737738325045
Iteration 50, Training loss = 0.7994222251268533
Iteration 60, Training loss = 0.7061358618621643
Iteration 70, Training loss = 0.6039671032474592
Iteration 80, Training loss = 0.5173172741555251
Iteration 90, Training loss = 0.4499427014245437
Iteration 100, Training loss = 0.40981443813787055
Iteration 110, Training loss = 0.3854184156427017
Iteration 120, Training loss = 0.37201664525155836
Iteration 130, Training loss = 0.3602382219754733
Iteration 140, Training loss = 0.3543220405968336
Iteration 150, Training loss = 0.3452598297825226
Iteration 160, Training loss = 0.3402098322717043
Iteration 170, Training loss = 0.3321553270977277
Iteration 180, Training loss = 0.3300847796579966
Iteration 190, Training loss = 0.32186285062478137
Model training time: 33.47602319717407
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9230264809269172
Iteration 10, Training loss = 0.28096554643259597
Iteration 20, Training loss = 0.2629131699124208
Iteration 30, Training loss = 0.2577484083863405
Iteration 40, Training loss = 0.25552700803830075
Iteration 50, Training loss = 0.25212934527259606
Iteration 60, Training loss = 0.25163180151811015
Iteration 70, Training loss = 0.2500001312448428
Iteration 80, Training loss = 0.24676549420333826
Iteration 90, Training loss = 0.24452464354152864
Iteration 100, Training loss = 0.24364180905887714
Iteration 110, Training loss = 0.24604052061644885
Iteration 120, Training loss = 0.2409868551274905
Iteration 130, Training loss = 0.23770713304670957
Iteration 140, Training loss = 0.2359083051291796
Iteration 150, Training loss = 0.2341664616161814
Iteration 160, Training loss = 0.23231328235795864
Iteration 170, Training loss = 0.23165159522054288
Iteration 180, Training loss = 0.23000978571004593
Iteration 190, Training loss = 0.2289738067640708
Model training time: 37.84787845611572
Device: cuda
Iteration 0, Training loss = 0.9473603763259374
Iteration 10, Training loss = 0.2796742577965443
Iteration 20, Training loss = 0.261312593634312
Iteration 30, Training loss = 0.2559882848022076
Iteration 40, Training loss = 0.2515861288859294
Iteration 50, Training loss = 0.24869097425387457
Iteration 60, Training loss = 0.2474657964821045
Iteration 70, Training loss = 0.2455247908544082
Iteration 80, Training loss = 0.24268613913311407
Iteration 90, Training loss = 0.2411316534361014
Iteration 100, Training loss = 0.23884819705898946
Iteration 110, Training loss = 0.2374039342483649
Iteration 120, Training loss = 0.23665902520028445
Iteration 130, Training loss = 0.2350445776604689
Iteration 140, Training loss = 0.23261770677681154
Iteration 150, Training loss = 0.23163037632520384
Iteration 160, Training loss = 0.23121468966396955
Iteration 170, Training loss = 0.22870781301305845
Iteration 180, Training loss = 0.22774846149751773
Iteration 190, Training loss = 0.22638754403361908
Model training time: 37.28837513923645
Device: cuda
Iteration 0, Training loss = 1.1501302822278097
Iteration 10, Training loss = 0.28607044454950553
Iteration 20, Training loss = 0.26983093097805977
Iteration 30, Training loss = 0.2596782255344666
Iteration 40, Training loss = 0.25612899080778545
Iteration 50, Training loss = 0.24996368773281574
Iteration 60, Training loss = 0.247827645104665
Iteration 70, Training loss = 0.2466286472403086
Iteration 80, Training loss = 0.24312383072594038
Iteration 90, Training loss = 0.24223036404985648
Iteration 100, Training loss = 0.24026551298224008
Iteration 110, Training loss = 0.23750141370468414
Iteration 120, Training loss = 0.2361439455014009
Iteration 130, Training loss = 0.23348271782295063
Iteration 140, Training loss = 0.23354615851377064
Iteration 150, Training loss = 0.22957768950324792
Iteration 160, Training loss = 0.22706225270835254
Iteration 170, Training loss = 0.22554421997987306
Iteration 180, Training loss = 0.2237029983303868
Iteration 190, Training loss = 0.2220391704199406
Model training time: 37.832590103149414
Device: cuda
Iteration 0, Training loss = 0.9984447726836572
Iteration 10, Training loss = 0.28666995064570355
Iteration 20, Training loss = 0.2579101315484597
Iteration 30, Training loss = 0.2538271713285492
Iteration 40, Training loss = 0.25099697078649813
Iteration 50, Training loss = 0.24770462283721337
Iteration 60, Training loss = 0.243120600684331
Iteration 70, Training loss = 0.24041138374461576
Iteration 80, Training loss = 0.23702285008934829
Iteration 90, Training loss = 0.2341265593870328
Iteration 100, Training loss = 0.2316618962929799
Iteration 110, Training loss = 0.22999441938904616
Iteration 120, Training loss = 0.22757145260962156
Iteration 130, Training loss = 0.22407143548704112
Iteration 140, Training loss = 0.22342795692384243
Iteration 150, Training loss = 0.2209200652746054
Iteration 160, Training loss = 0.21982812845649627
Iteration 170, Training loss = 0.21601905716726413
Iteration 180, Training loss = 0.21585200841610247
Iteration 190, Training loss = 0.2161196995383272
Model training time: 37.41594052314758
Device: cuda
Iteration 0, Training loss = 1.1084298394047296
Iteration 10, Training loss = 0.2956214648886369
Iteration 20, Training loss = 0.26671444295117486
Iteration 30, Training loss = 0.2579179818813617
Iteration 40, Training loss = 0.2539650102934012
Iteration 50, Training loss = 0.2500150336955602
Iteration 60, Training loss = 0.247890360868321
Iteration 70, Training loss = 0.24768740139328516
Iteration 80, Training loss = 0.2454759071652706
Iteration 90, Training loss = 0.24438438149025807
Iteration 100, Training loss = 0.24328198026006037
Iteration 110, Training loss = 0.24154524877667427
Iteration 120, Training loss = 0.24015678837895393
Iteration 130, Training loss = 0.2391474090086726
Iteration 140, Training loss = 0.23894774870803723
Iteration 150, Training loss = 0.23825887676614982
Iteration 160, Training loss = 0.23661156700780758
Iteration 170, Training loss = 0.2395359600106111
Iteration 180, Training loss = 0.2353795155023153
Iteration 190, Training loss = 0.23626260874936214
Model training time: 37.0681209564209
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0097455491240208
Iteration 10, Training loss = 0.9930392561050562
Iteration 20, Training loss = 0.9856971508035293
Iteration 30, Training loss = 0.9699887725023123
Iteration 40, Training loss = 0.9450286793021055
Iteration 50, Training loss = 0.9093200157468135
Iteration 60, Training loss = 0.8457888513803482
Iteration 70, Training loss = 0.7470322165351647
Iteration 80, Training loss = 0.6263705146713898
Iteration 90, Training loss = 0.5176660415644829
Iteration 100, Training loss = 0.44288937002420425
Iteration 110, Training loss = 0.40195950177999645
Iteration 120, Training loss = 0.38273917926618684
Iteration 130, Training loss = 0.3708732108084055
Iteration 140, Training loss = 0.3622017723436539
Iteration 150, Training loss = 0.3576653611201506
Iteration 160, Training loss = 0.35015750131928003
Iteration 170, Training loss = 0.3456894222360391
Iteration 180, Training loss = 0.34346511931373525
Iteration 190, Training loss = 0.3407819510365908
Model training time: 33.44452261924744
Device: cuda
Iteration 0, Training loss = 1.0931780349749785
Iteration 10, Training loss = 1.002853573514865
Iteration 20, Training loss = 0.999776867719797
Iteration 30, Training loss = 0.9926003813743591
Iteration 40, Training loss = 0.9880007975376569
Iteration 50, Training loss = 0.9916731377060597
Iteration 60, Training loss = 0.975944480070701
Iteration 70, Training loss = 0.966756940461122
Iteration 80, Training loss = 0.946088449886212
Iteration 90, Training loss = 0.9166148259089544
Iteration 100, Training loss = 0.8633367063907477
Iteration 110, Training loss = 0.7753563127838649
Iteration 120, Training loss = 0.6548477981526118
Iteration 130, Training loss = 0.5302492122237499
Iteration 140, Training loss = 0.44875158111636454
Iteration 150, Training loss = 0.4124781366151113
Iteration 160, Training loss = 0.39211899099441677
Iteration 170, Training loss = 0.38031909041679823
Iteration 180, Training loss = 0.36971949313122493
Iteration 190, Training loss = 0.3618637542598523
Model training time: 33.486547231674194
Device: cuda
Iteration 0, Training loss = 1.026132586483772
Iteration 10, Training loss = 1.0012301195126314
Iteration 20, Training loss = 0.9818839992468174
Iteration 30, Training loss = 0.9742408710030409
Iteration 40, Training loss = 0.9492900686768385
Iteration 50, Training loss = 0.9220936837104651
Iteration 60, Training loss = 0.87645673006773
Iteration 70, Training loss = 0.8080956597740834
Iteration 80, Training loss = 0.7149165129432311
Iteration 90, Training loss = 0.6114376920920152
Iteration 100, Training loss = 0.525315694224376
Iteration 110, Training loss = 0.45950369193003726
Iteration 120, Training loss = 0.42428969577527964
Iteration 130, Training loss = 0.40375084716540116
Iteration 140, Training loss = 0.39124705341572946
Iteration 150, Training loss = 0.38103727480539906
Iteration 160, Training loss = 0.3720008316808022
Iteration 170, Training loss = 0.3634834564649142
Iteration 180, Training loss = 0.35966086330322117
Iteration 190, Training loss = 0.3503404173713464
Model training time: 33.47904372215271
Device: cuda
Iteration 0, Training loss = 1.03530645943605
Iteration 10, Training loss = 0.9917746025782365
Iteration 20, Training loss = 0.9790561474286593
Iteration 30, Training loss = 0.959887700012097
Iteration 40, Training loss = 0.931354168515939
Iteration 50, Training loss = 0.8806015803263738
Iteration 60, Training loss = 0.79671367085897
Iteration 70, Training loss = 0.6716909643549186
Iteration 80, Training loss = 0.5337675841381917
Iteration 90, Training loss = 0.43585402260606104
Iteration 100, Training loss = 0.39049859817784566
Iteration 110, Training loss = 0.37138234164852363
Iteration 120, Training loss = 0.3578403745419704
Iteration 130, Training loss = 0.35076337995437473
Iteration 140, Training loss = 0.34114652685821056
Iteration 150, Training loss = 0.33644719851704746
Iteration 160, Training loss = 0.32913415950651354
Iteration 170, Training loss = 0.3268396139431458
Iteration 180, Training loss = 0.3206912482587191
Iteration 190, Training loss = 0.31727875052736354
Model training time: 33.40901517868042
Device: cuda
Iteration 0, Training loss = 1.0094742797888243
Iteration 10, Training loss = 0.9681643161636132
Iteration 20, Training loss = 0.9357276639113059
Iteration 30, Training loss = 0.884711582500201
Iteration 40, Training loss = 0.802195793734147
Iteration 50, Training loss = 0.6803273071463292
Iteration 60, Training loss = 0.5477633390289086
Iteration 70, Training loss = 0.4517105877972566
Iteration 80, Training loss = 0.4106162666128232
Iteration 90, Training loss = 0.3875514535376659
Iteration 100, Training loss = 0.3748516133771493
Iteration 110, Training loss = 0.3646077926342304
Iteration 120, Training loss = 0.358840403935084
Iteration 130, Training loss = 0.34906054918582624
Iteration 140, Training loss = 0.3420574560474891
Iteration 150, Training loss = 0.3359249778665029
Iteration 160, Training loss = 0.33271361557910073
Iteration 170, Training loss = 0.32667798462968606
Iteration 180, Training loss = 0.3232010104335271
Iteration 190, Training loss = 0.32117762301976865
Model training time: 34.21798300743103
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9773305803537369
Iteration 10, Training loss = 0.27090865044066537
Iteration 20, Training loss = 0.2555647778969545
Iteration 30, Training loss = 0.2516807892288153
Iteration 40, Training loss = 0.24782052363913792
Iteration 50, Training loss = 0.2461851638956712
Iteration 60, Training loss = 0.24530475515012556
Iteration 70, Training loss = 0.24275509726542693
Iteration 80, Training loss = 0.24373998750860876
Iteration 90, Training loss = 0.24057184002147272
Iteration 100, Training loss = 0.23877107280378157
Iteration 110, Training loss = 0.23702533829670686
Iteration 120, Training loss = 0.23604894973910773
Iteration 130, Training loss = 0.23426316979412848
Iteration 140, Training loss = 0.2319718706779755
Iteration 150, Training loss = 0.23025500831695703
Iteration 160, Training loss = 0.22861870406911924
Iteration 170, Training loss = 0.22729508793697908
Iteration 180, Training loss = 0.22430001106113195
Iteration 190, Training loss = 0.22329188847484496
Iteration 200, Training loss = 0.2220728863747074
Iteration 210, Training loss = 0.21937057118003184
Iteration 220, Training loss = 0.21769722651403683
Iteration 230, Training loss = 0.21600049261290294
Iteration 240, Training loss = 0.21525176585866854
Iteration 250, Training loss = 0.21434208449835962
Iteration 260, Training loss = 0.21257600209747368
Iteration 270, Training loss = 0.21208151451383644
Iteration 280, Training loss = 0.2103613203104872
Iteration 290, Training loss = 0.2121169539884879
Model training time: 55.63330149650574
Device: cuda
Iteration 0, Training loss = 0.9319826633884356
Iteration 10, Training loss = 0.28572529912568057
Iteration 20, Training loss = 0.2758551798760891
Iteration 30, Training loss = 0.26742080154900366
Iteration 40, Training loss = 0.26329992109766376
Iteration 50, Training loss = 0.2620135343705232
Iteration 60, Training loss = 0.2567898234209189
Iteration 70, Training loss = 0.25463468266221195
Iteration 80, Training loss = 0.2511759088016473
Iteration 90, Training loss = 0.2491459085677679
Iteration 100, Training loss = 0.24705177216002575
Iteration 110, Training loss = 0.2449494574505549
Iteration 120, Training loss = 0.24289500168882883
Iteration 130, Training loss = 0.24120381531807092
Iteration 140, Training loss = 0.23876006055909854
Iteration 150, Training loss = 0.23729137381395468
Iteration 160, Training loss = 0.23551744709794337
Iteration 170, Training loss = 0.23426665365695953
Iteration 180, Training loss = 0.23374940096758878
Iteration 190, Training loss = 0.23083465589353672
Iteration 200, Training loss = 0.23051850855923617
Iteration 210, Training loss = 0.22834219941152975
Iteration 220, Training loss = 0.22782895613748294
Iteration 230, Training loss = 0.2288573245302989
Iteration 240, Training loss = 0.2251439571667176
Iteration 250, Training loss = 0.22506097302987024
Iteration 260, Training loss = 0.2226503173319193
Iteration 270, Training loss = 0.22258075477125552
Iteration 280, Training loss = 0.2219639585998196
Iteration 290, Training loss = 0.22095154089709887
Model training time: 55.88983678817749
Device: cuda
Iteration 0, Training loss = 0.9425490584511024
Iteration 10, Training loss = 0.2733607951265115
Iteration 20, Training loss = 0.25988630678218144
Iteration 30, Training loss = 0.2508119263041478
Iteration 40, Training loss = 0.24364893783170444
Iteration 50, Training loss = 0.23969213635875627
Iteration 60, Training loss = 0.2367856607127648
Iteration 70, Training loss = 0.23444914344984752
Iteration 80, Training loss = 0.23061582775643238
Iteration 90, Training loss = 0.22861482847768527
Iteration 100, Training loss = 0.2254550990720208
Iteration 110, Training loss = 0.22358730120154527
Iteration 120, Training loss = 0.22460625686038
Iteration 130, Training loss = 0.22049774372806916
Iteration 140, Training loss = 0.21954405759103024
Iteration 150, Training loss = 0.21696201604432785
Iteration 160, Training loss = 0.2180106952929726
Iteration 170, Training loss = 0.21538419641840917
Iteration 180, Training loss = 0.21616569319023535
Iteration 190, Training loss = 0.21437574400065038
Iteration 200, Training loss = 0.21125733243444791
Iteration 210, Training loss = 0.2115006850889096
Iteration 220, Training loss = 0.21009538731036279
Iteration 230, Training loss = 0.20788771293770808
Iteration 240, Training loss = 0.20787572882209832
Iteration 250, Training loss = 0.20640899355594927
Iteration 260, Training loss = 0.20518801354158384
Iteration 270, Training loss = 0.20492365984962538
Iteration 280, Training loss = 0.20450028805778578
Iteration 290, Training loss = 0.20354787374918276
Model training time: 56.636871099472046
Device: cuda
Iteration 0, Training loss = 0.9693351863668516
Iteration 10, Training loss = 0.2718683752016379
Iteration 20, Training loss = 0.25687152032668775
Iteration 30, Training loss = 0.2511300669553188
Iteration 40, Training loss = 0.2471036987665754
Iteration 50, Training loss = 0.2457249125895592
Iteration 60, Training loss = 0.24279700549176106
Iteration 70, Training loss = 0.2393874839807932
Iteration 80, Training loss = 0.23677188544892347
Iteration 90, Training loss = 0.23464294780905431
Iteration 100, Training loss = 0.2317031011558496
Iteration 110, Training loss = 0.22896085450282463
Iteration 120, Training loss = 0.22701238833654386
Iteration 130, Training loss = 0.22584414281524146
Iteration 140, Training loss = 0.22299097564357978
Iteration 150, Training loss = 0.22175826898847634
Iteration 160, Training loss = 0.22023635056729501
Iteration 170, Training loss = 0.2193308876684079
Iteration 180, Training loss = 0.21785870803376803
Iteration 190, Training loss = 0.21678500708479148
Iteration 200, Training loss = 0.21468694157038742
Iteration 210, Training loss = 0.2139101826514189
Iteration 220, Training loss = 0.21434027766092464
Iteration 230, Training loss = 0.2124196202136003
Iteration 240, Training loss = 0.21169029068774903
Iteration 250, Training loss = 0.21060519572347403
Iteration 260, Training loss = 0.21030769712076738
Iteration 270, Training loss = 0.2084788718045904
Iteration 280, Training loss = 0.2079162412824539
Iteration 290, Training loss = 0.20734144217119768
Model training time: 56.91152286529541
Device: cuda
Iteration 0, Training loss = 0.955491973230472
Iteration 10, Training loss = 0.27829179038795143
Iteration 20, Training loss = 0.2565011026767584
Iteration 30, Training loss = 0.24819486459287313
Iteration 40, Training loss = 0.24335561828831068
Iteration 50, Training loss = 0.24278764226115668
Iteration 60, Training loss = 0.23980759771970603
Iteration 70, Training loss = 0.23684072566147035
Iteration 80, Training loss = 0.23508286891648403
Iteration 90, Training loss = 0.23312925977202562
Iteration 100, Training loss = 0.23189248101642498
Iteration 110, Training loss = 0.22955181564276034
Iteration 120, Training loss = 0.22737233648793057
Iteration 130, Training loss = 0.22718148549588826
Iteration 140, Training loss = 0.2252142349114785
Iteration 150, Training loss = 0.22278551062425742
Iteration 160, Training loss = 0.2229121417666857
Iteration 170, Training loss = 0.21914258527641112
Iteration 180, Training loss = 0.21815129197560823
Iteration 190, Training loss = 0.2157348802026648
Iteration 200, Training loss = 0.213504982395814
Iteration 210, Training loss = 0.21277235040011314
Iteration 220, Training loss = 0.21121463630921566
Iteration 230, Training loss = 0.21192639629141644
Iteration 240, Training loss = 0.20984460614048517
Iteration 250, Training loss = 0.2077243308035227
Iteration 260, Training loss = 0.20651844000587097
Iteration 270, Training loss = 0.20814162872445124
Iteration 280, Training loss = 0.20633395632299092
Iteration 290, Training loss = 0.2055401998357131
Model training time: 61.76429796218872
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.019897502202254
Iteration 10, Training loss = 0.9960253061010287
Iteration 20, Training loss = 0.9863198617329965
Iteration 30, Training loss = 0.9783772356235064
Iteration 40, Training loss = 0.965010734131703
Iteration 50, Training loss = 0.9473514012419261
Iteration 60, Training loss = 0.9188709694605607
Iteration 70, Training loss = 0.8688483513318576
Iteration 80, Training loss = 0.7965334963340026
Iteration 90, Training loss = 0.6746264535647172
Iteration 100, Training loss = 0.5490095082383889
Iteration 110, Training loss = 0.45383064964642894
Iteration 120, Training loss = 0.4088384047723733
Iteration 130, Training loss = 0.38512491492124706
Iteration 140, Training loss = 0.37530729891015935
Iteration 150, Training loss = 0.363462748292547
Iteration 160, Training loss = 0.354671793918197
Iteration 170, Training loss = 0.34738455211313873
Iteration 180, Training loss = 0.3428513831817187
Iteration 190, Training loss = 0.3359277881681919
Iteration 200, Training loss = 0.3310997302715595
Iteration 210, Training loss = 0.3291155207329072
Iteration 220, Training loss = 0.32779622020629734
Iteration 230, Training loss = 0.32309025440078515
Iteration 240, Training loss = 0.32089868703713786
Iteration 250, Training loss = 0.3191106654703617
Iteration 260, Training loss = 0.31707938703206867
Iteration 270, Training loss = 0.31574264269035596
Iteration 280, Training loss = 0.3130494814652663
Iteration 290, Training loss = 0.31185787830215234
Model training time: 52.0046112537384
Device: cuda
Iteration 0, Training loss = 1.004784165093532
Iteration 10, Training loss = 0.9629573340599353
Iteration 20, Training loss = 0.9290654670733672
Iteration 30, Training loss = 0.8849411090979209
Iteration 40, Training loss = 0.8093022681199588
Iteration 50, Training loss = 0.7121419127170856
Iteration 60, Training loss = 0.598782118409872
Iteration 70, Training loss = 0.4997805772492519
Iteration 80, Training loss = 0.43504027420511615
Iteration 90, Training loss = 0.3984863998798224
Iteration 100, Training loss = 0.3797600090217132
Iteration 110, Training loss = 0.36579833795818
Iteration 120, Training loss = 0.3540816173817103
Iteration 130, Training loss = 0.350262064486742
Iteration 140, Training loss = 0.3406259983491439
Iteration 150, Training loss = 0.3388642743229866
Iteration 160, Training loss = 0.3292682385788514
Iteration 170, Training loss = 0.3258112299327667
Iteration 180, Training loss = 0.3226559812632891
Iteration 190, Training loss = 0.3186976028463015
Iteration 200, Training loss = 0.31695804859583193
Iteration 210, Training loss = 0.3146503114929566
Iteration 220, Training loss = 0.3148959279060364
Iteration 230, Training loss = 0.31254967846549475
Iteration 240, Training loss = 0.30983690530634844
Iteration 250, Training loss = 0.3105012012215761
Iteration 260, Training loss = 0.30640052316280514
Iteration 270, Training loss = 0.30620818427548957
Iteration 280, Training loss = 0.306269299525481
Iteration 290, Training loss = 0.3028660217443338
Model training time: 50.36204648017883
Device: cuda
Iteration 0, Training loss = 1.0155195427628665
Iteration 10, Training loss = 0.997152216732502
Iteration 20, Training loss = 0.9919496992459664
Iteration 30, Training loss = 0.9783678925954379
Iteration 40, Training loss = 0.9522721389165292
Iteration 50, Training loss = 0.912039628968789
Iteration 60, Training loss = 0.8410436574083108
Iteration 70, Training loss = 0.7210826057081039
Iteration 80, Training loss = 0.5748976675363687
Iteration 90, Training loss = 0.4618412505548734
Iteration 100, Training loss = 0.40541836562064976
Iteration 110, Training loss = 0.38234417226452094
Iteration 120, Training loss = 0.37028014416304916
Iteration 130, Training loss = 0.3618106569808263
Iteration 140, Training loss = 0.3542615340019648
Iteration 150, Training loss = 0.3480168543756008
Iteration 160, Training loss = 0.34491352536357367
Iteration 170, Training loss = 0.339153452991293
Iteration 180, Training loss = 0.3347735612724836
Iteration 190, Training loss = 0.3321929222975786
Iteration 200, Training loss = 0.32730443961918354
Iteration 210, Training loss = 0.32505447331529397
Iteration 220, Training loss = 0.3211657181382179
Iteration 230, Training loss = 0.3190521721083384
Iteration 240, Training loss = 0.316865533303756
Iteration 250, Training loss = 0.3161239232867956
Iteration 260, Training loss = 0.3129719692067458
Iteration 270, Training loss = 0.3113922675928244
Iteration 280, Training loss = 0.3103688989694302
Iteration 290, Training loss = 0.30979392720529664
Model training time: 51.231653451919556
Device: cuda
Iteration 0, Training loss = 1.019900450339684
Iteration 10, Training loss = 1.0074420726070037
Iteration 20, Training loss = 0.9979716871793454
Iteration 30, Training loss = 0.9920922076472869
Iteration 40, Training loss = 0.9932012712726226
Iteration 50, Training loss = 0.9816299906143775
Iteration 60, Training loss = 0.9702598120157535
Iteration 70, Training loss = 0.9481358264501278
Iteration 80, Training loss = 0.9098381388645905
Iteration 90, Training loss = 0.8515662682744173
Iteration 100, Training loss = 0.7626561015271224
Iteration 110, Training loss = 0.6567634762479708
Iteration 120, Training loss = 0.5512534004564469
Iteration 130, Training loss = 0.472353789382256
Iteration 140, Training loss = 0.42294117020299804
Iteration 150, Training loss = 0.39511990561508215
Iteration 160, Training loss = 0.37923900740077865
Iteration 170, Training loss = 0.36842693187869513
Iteration 180, Training loss = 0.3587192189521514
Iteration 190, Training loss = 0.35120715100604755
Iteration 200, Training loss = 0.34546678527616537
Iteration 210, Training loss = 0.3398024207697465
Iteration 220, Training loss = 0.3333774054279694
Iteration 230, Training loss = 0.32954220244517696
Iteration 240, Training loss = 0.32616578243099725
Iteration 250, Training loss = 0.322724108512585
Iteration 260, Training loss = 0.3186915773325242
Iteration 270, Training loss = 0.3179060279463346
Iteration 280, Training loss = 0.31275762230730975
Iteration 290, Training loss = 0.31110263085709167
Model training time: 51.580604553222656
Device: cuda
Iteration 0, Training loss = 1.0557620840576978
Iteration 10, Training loss = 0.970048073392648
Iteration 20, Training loss = 0.9511692764667364
Iteration 30, Training loss = 0.9186039372132375
Iteration 40, Training loss = 0.8677257786576564
Iteration 50, Training loss = 0.7791711584879801
Iteration 60, Training loss = 0.6569200547841879
Iteration 70, Training loss = 0.5212850269789879
Iteration 80, Training loss = 0.42626136570022655
Iteration 90, Training loss = 0.3808377170218871
Iteration 100, Training loss = 0.3621096805884288
Iteration 110, Training loss = 0.35346258474657166
Iteration 120, Training loss = 0.3476216619691023
Iteration 130, Training loss = 0.33977821197074193
Iteration 140, Training loss = 0.33701334635798746
Iteration 150, Training loss = 0.3348882420418354
Iteration 160, Training loss = 0.326809678083429
Iteration 170, Training loss = 0.32385840997673
Iteration 180, Training loss = 0.32033007649274975
Iteration 190, Training loss = 0.31895182109796083
Iteration 200, Training loss = 0.3169144349029431
Iteration 210, Training loss = 0.31380811921105933
Iteration 220, Training loss = 0.31178124621510506
Iteration 230, Training loss = 0.31042776715296966
Iteration 240, Training loss = 0.30848108504254085
Iteration 250, Training loss = 0.30733867548406124
Iteration 260, Training loss = 0.30627656556092775
Iteration 270, Training loss = 0.3060053950892045
Iteration 280, Training loss = 0.3043685665783974
Iteration 290, Training loss = 0.30337732141980756
Model training time: 50.30481505393982
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9852588847279549
Iteration 10, Training loss = 0.30084621562407565
Iteration 20, Training loss = 0.26566189627807874
Iteration 30, Training loss = 0.2575157266110182
Iteration 40, Training loss = 0.2530693728476763
Iteration 50, Training loss = 0.2538348937836977
Iteration 60, Training loss = 0.24896992012285268
Iteration 70, Training loss = 0.24471209914638445
Iteration 80, Training loss = 0.24335574351537687
Iteration 90, Training loss = 0.24083485232236293
Iteration 100, Training loss = 0.23797253168259674
Iteration 110, Training loss = 0.24124911742714736
Iteration 120, Training loss = 0.2354973484403812
Iteration 130, Training loss = 0.2354374209848734
Iteration 140, Training loss = 0.2344880447937892
Iteration 150, Training loss = 0.2336699807873139
Iteration 160, Training loss = 0.23232019520722902
Iteration 170, Training loss = 0.23361795581877232
Iteration 180, Training loss = 0.2303855186328292
Iteration 190, Training loss = 0.23113370436028793
Iteration 200, Training loss = 0.22970119269134906
Iteration 210, Training loss = 0.2319186904396002
Iteration 220, Training loss = 0.2308619310362981
Iteration 230, Training loss = 0.2279830670270782
Iteration 240, Training loss = 0.22776148196023244
Iteration 250, Training loss = 0.22665043633717757
Iteration 260, Training loss = 0.22552151555338731
Iteration 270, Training loss = 0.23018746722776157
Iteration 280, Training loss = 0.22644500369922474
Iteration 290, Training loss = 0.22385862111472166
Model training time: 57.32298183441162
Device: cuda
Iteration 0, Training loss = 1.1713658657211523
Iteration 10, Training loss = 0.3244713102109157
Iteration 20, Training loss = 0.26829852908849716
Iteration 30, Training loss = 0.25825098515130007
Iteration 40, Training loss = 0.25254551213807785
Iteration 50, Training loss = 0.24969039069345364
Iteration 60, Training loss = 0.24703716744597143
Iteration 70, Training loss = 0.2450609509474956
Iteration 80, Training loss = 0.24359853665989178
Iteration 90, Training loss = 0.2431000232553253
Iteration 100, Training loss = 0.24132174353760022
Iteration 110, Training loss = 0.24145953708256668
Iteration 120, Training loss = 0.23922352105952227
Iteration 130, Training loss = 0.2428088835798777
Iteration 140, Training loss = 0.23810512486558694
Iteration 150, Training loss = 0.24086975420896822
Iteration 160, Training loss = 0.23693392884272796
Iteration 170, Training loss = 0.2347885282137073
Iteration 180, Training loss = 0.23571211730058378
Iteration 190, Training loss = 0.23555130425554055
Iteration 200, Training loss = 0.234049242706253
Iteration 210, Training loss = 0.23340029627657854
Iteration 220, Training loss = 0.23388715718801206
Iteration 230, Training loss = 0.23237716657324478
Iteration 240, Training loss = 0.23061363470668977
Iteration 250, Training loss = 0.23105740289275462
Iteration 260, Training loss = 0.22897435001169258
Iteration 270, Training loss = 0.22900597765468633
Iteration 280, Training loss = 0.22689136733802465
Iteration 290, Training loss = 0.22556373849511147
Model training time: 56.076690673828125
Device: cuda
Iteration 0, Training loss = 0.973316982961618
Iteration 10, Training loss = 0.29420750903395504
Iteration 20, Training loss = 0.263749450015334
Iteration 30, Training loss = 0.25556567626503796
Iteration 40, Training loss = 0.25228227546008736
Iteration 50, Training loss = 0.24732601671264723
Iteration 60, Training loss = 0.2449170328103579
Iteration 70, Training loss = 0.24309223474791417
Iteration 80, Training loss = 0.24118328008514184
Iteration 90, Training loss = 0.24062982430824867
Iteration 100, Training loss = 0.23674484428304893
Iteration 110, Training loss = 0.23528848445186248
Iteration 120, Training loss = 0.23339733080222055
Iteration 130, Training loss = 0.2318465245457796
Iteration 140, Training loss = 0.22993417771962973
Iteration 150, Training loss = 0.22938032161730987
Iteration 160, Training loss = 0.22750057752888936
Iteration 170, Training loss = 0.2283588185046728
Iteration 180, Training loss = 0.22538484102831438
Iteration 190, Training loss = 0.2252926557109906
Iteration 200, Training loss = 0.22327734451167858
Iteration 210, Training loss = 0.22512858719206774
Iteration 220, Training loss = 0.2228528749770843
Iteration 230, Training loss = 0.22137542699392027
Iteration 240, Training loss = 0.2211802081706432
Iteration 250, Training loss = 0.220284023680366
Iteration 260, Training loss = 0.22000611731066153
Iteration 270, Training loss = 0.2190155998732035
Iteration 280, Training loss = 0.21831420407845423
Iteration 290, Training loss = 0.21739006192924884
Model training time: 55.88823175430298
Device: cuda
Iteration 0, Training loss = 0.9461497395084455
Iteration 10, Training loss = 0.29749656511613953
Iteration 20, Training loss = 0.2617862797700442
Iteration 30, Training loss = 0.2522951037121507
Iteration 40, Training loss = 0.25065250064317995
Iteration 50, Training loss = 0.2492279247022592
Iteration 60, Training loss = 0.2467537002208141
Iteration 70, Training loss = 0.24655005436104077
Iteration 80, Training loss = 0.24653185488512883
Iteration 90, Training loss = 0.24321015322437653
Iteration 100, Training loss = 0.24278251454234123
Iteration 110, Training loss = 0.24015463344179666
Iteration 120, Training loss = 0.2385936054186179
Iteration 130, Training loss = 0.23922066318874174
Iteration 140, Training loss = 0.23617325981075948
Iteration 150, Training loss = 0.23466999649714965
Iteration 160, Training loss = 0.23220800515264273
Iteration 170, Training loss = 0.2311081845456591
Iteration 180, Training loss = 0.2298205656787524
Iteration 190, Training loss = 0.22781907516316727
Iteration 200, Training loss = 0.23320182470174936
Iteration 210, Training loss = 0.22521074961584348
Iteration 220, Training loss = 0.22449874720321253
Iteration 230, Training loss = 0.2233922560341083
Iteration 240, Training loss = 0.22255354059430268
Iteration 250, Training loss = 0.22195784776256636
Iteration 260, Training loss = 0.22007179002349192
Iteration 270, Training loss = 0.22058139311579558
Iteration 280, Training loss = 0.21875807958153579
Iteration 290, Training loss = 0.2176939110056712
Model training time: 56.3424232006073
Device: cuda
Iteration 0, Training loss = 0.9399762566273029
Iteration 10, Training loss = 0.28299994107622367
Iteration 20, Training loss = 0.2623773871992643
Iteration 30, Training loss = 0.25559192122175145
Iteration 40, Training loss = 0.2521393463875239
Iteration 50, Training loss = 0.25060929057116693
Iteration 60, Training loss = 0.24788804868092904
Iteration 70, Training loss = 0.24463620867866737
Iteration 80, Training loss = 0.2422216351215656
Iteration 90, Training loss = 0.24065187458808607
Iteration 100, Training loss = 0.24203228592299497
Iteration 110, Training loss = 0.2366647790544308
Iteration 120, Training loss = 0.23808408299317727
Iteration 130, Training loss = 0.233982490375638
Iteration 140, Training loss = 0.2319501844736246
Iteration 150, Training loss = 0.23027292975726035
Iteration 160, Training loss = 0.22832848984175003
Iteration 170, Training loss = 0.2303846744963756
Iteration 180, Training loss = 0.22582449943113786
Iteration 190, Training loss = 0.22504345327615738
Iteration 200, Training loss = 0.22390653823430723
Iteration 210, Training loss = 0.22234797964875513
Iteration 220, Training loss = 0.2195498042811568
Iteration 230, Training loss = 0.22205415955529764
Iteration 240, Training loss = 0.21807775288247144
Iteration 250, Training loss = 0.21792297788824028
Iteration 260, Training loss = 0.216999168937596
Iteration 270, Training loss = 0.21557247273337382
Iteration 280, Training loss = 0.21635721652553633
Iteration 290, Training loss = 0.21408464637799904
Model training time: 56.17223596572876
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.2299851700663567
Iteration 10, Training loss = 1.0018771611727202
Iteration 20, Training loss = 0.9867742422681588
Iteration 30, Training loss = 0.9753095547740276
Iteration 40, Training loss = 0.9527070636932666
Iteration 50, Training loss = 0.9273894681380346
Iteration 60, Training loss = 0.8831258542262591
Iteration 70, Training loss = 0.8008889831029452
Iteration 80, Training loss = 0.692982517182827
Iteration 90, Training loss = 0.5785372383319415
Iteration 100, Training loss = 0.4814331414034733
Iteration 110, Training loss = 0.42272856172460777
Iteration 120, Training loss = 0.39773534510571223
Iteration 130, Training loss = 0.37787935281029117
Iteration 140, Training loss = 0.3707416265343244
Iteration 150, Training loss = 0.36018111284535664
Iteration 160, Training loss = 0.3537776226607653
Iteration 170, Training loss = 0.34934639214323115
Iteration 180, Training loss = 0.3411455544141623
Iteration 190, Training loss = 0.33493307189872634
Iteration 200, Training loss = 0.3286117141923079
Iteration 210, Training loss = 0.3251868228499706
Iteration 220, Training loss = 0.3222107021854474
Iteration 230, Training loss = 0.3174052449086538
Iteration 240, Training loss = 0.3174620785105687
Iteration 250, Training loss = 0.31159223000017494
Iteration 260, Training loss = 0.3105686427308963
Iteration 270, Training loss = 0.30744238140491337
Iteration 280, Training loss = 0.3046624303723757
Iteration 290, Training loss = 0.30216333069480383
Model training time: 50.90634870529175
Device: cuda
Iteration 0, Training loss = 1.1143982742841427
Iteration 10, Training loss = 0.9777773836484323
Iteration 20, Training loss = 0.9603490135990657
Iteration 30, Training loss = 0.9325185085718448
Iteration 40, Training loss = 0.8978582047499143
Iteration 50, Training loss = 0.8286133236609973
Iteration 60, Training loss = 0.7383869433632264
Iteration 70, Training loss = 0.6197001681878016
Iteration 80, Training loss = 0.5150373016412442
Iteration 90, Training loss = 0.4478986541239115
Iteration 100, Training loss = 0.4123324150076279
Iteration 110, Training loss = 0.39483341431388486
Iteration 120, Training loss = 0.38207777446279156
Iteration 130, Training loss = 0.3712118252252157
Iteration 140, Training loss = 0.362504983607393
Iteration 150, Training loss = 0.35755776413358176
Iteration 160, Training loss = 0.34992381500510067
Iteration 170, Training loss = 0.3423606198854171
Iteration 180, Training loss = 0.33808356208296925
Iteration 190, Training loss = 0.33307687966869426
Iteration 200, Training loss = 0.32964246748731685
Iteration 210, Training loss = 0.32571654723813903
Iteration 220, Training loss = 0.3226389637073645
Iteration 230, Training loss = 0.3197115585207939
Iteration 240, Training loss = 0.3191447894160564
Iteration 250, Training loss = 0.31514955827823055
Iteration 260, Training loss = 0.3131206151671134
Iteration 270, Training loss = 0.31307431553991943
Iteration 280, Training loss = 0.3105500370550614
Iteration 290, Training loss = 0.30862461087795406
Model training time: 50.69080996513367
Device: cuda
Iteration 0, Training loss = 1.1703933517520244
Iteration 10, Training loss = 1.0000280170486524
Iteration 20, Training loss = 0.9895712291965117
Iteration 30, Training loss = 0.9783655800498449
Iteration 40, Training loss = 0.9669819256434073
Iteration 50, Training loss = 0.9395769897561806
Iteration 60, Training loss = 0.908733416062135
Iteration 70, Training loss = 0.8428342222021177
Iteration 80, Training loss = 0.7422590015026239
Iteration 90, Training loss = 0.5972952097654343
Iteration 100, Training loss = 0.46660798458525765
Iteration 110, Training loss = 0.39552617273651636
Iteration 120, Training loss = 0.36393850368375963
Iteration 130, Training loss = 0.35011113764574897
Iteration 140, Training loss = 0.3430646931609282
Iteration 150, Training loss = 0.3363145293238071
Iteration 160, Training loss = 0.3290201906974499
Iteration 170, Training loss = 0.32320021078563654
Iteration 180, Training loss = 0.31946574939558137
Iteration 190, Training loss = 0.31699459383694023
Iteration 200, Training loss = 0.3127022599085019
Iteration 210, Training loss = 0.3129686926706479
Iteration 220, Training loss = 0.3080313148406836
Iteration 230, Training loss = 0.30588432702307516
Iteration 240, Training loss = 0.3045624971676331
Iteration 250, Training loss = 0.30131880953334844
Iteration 260, Training loss = 0.30106522085574955
Iteration 270, Training loss = 0.29910145098200214
Iteration 280, Training loss = 0.3009353429079056
Iteration 290, Training loss = 0.2988521945304595
Model training time: 50.36614441871643
Device: cuda
Iteration 0, Training loss = 1.0133735571916287
Iteration 10, Training loss = 0.9981093314977793
Iteration 20, Training loss = 0.9862739764727079
Iteration 30, Training loss = 0.9720690548419952
Iteration 40, Training loss = 0.952740086385837
Iteration 50, Training loss = 0.9237452338521297
Iteration 60, Training loss = 0.8658745099718754
Iteration 70, Training loss = 0.7772376818152574
Iteration 80, Training loss = 0.6630536501224225
Iteration 90, Training loss = 0.5430319340756307
Iteration 100, Training loss = 0.4557793272229341
Iteration 110, Training loss = 0.409609317063139
Iteration 120, Training loss = 0.3854386262022532
Iteration 130, Training loss = 0.37208752663662803
Iteration 140, Training loss = 0.36189816161417043
Iteration 150, Training loss = 0.35301135738308614
Iteration 160, Training loss = 0.34638135607999104
Iteration 170, Training loss = 0.3415390894963191
Iteration 180, Training loss = 0.3342329498666983
Iteration 190, Training loss = 0.32944557543557423
Iteration 200, Training loss = 0.3262668067159561
Iteration 210, Training loss = 0.32689162300756347
Iteration 220, Training loss = 0.3191838324643098
Iteration 230, Training loss = 0.31645196131788766
Iteration 240, Training loss = 0.3154296814822234
Iteration 250, Training loss = 0.3132895453331562
Iteration 260, Training loss = 0.3100316354001944
Iteration 270, Training loss = 0.3102836861060216
Iteration 280, Training loss = 0.3083720657114799
Iteration 290, Training loss = 0.3050827571692375
Model training time: 50.24783778190613
Device: cuda
Iteration 0, Training loss = 1.0950654653402476
Iteration 10, Training loss = 0.9754441455006599
Iteration 20, Training loss = 0.9608631753004514
Iteration 30, Training loss = 0.9433890609787061
Iteration 40, Training loss = 0.9099692736680691
Iteration 50, Training loss = 0.8599694010156852
Iteration 60, Training loss = 0.7818479016423225
Iteration 70, Training loss = 0.6829819008708
Iteration 80, Training loss = 0.5790145124953526
Iteration 90, Training loss = 0.49924309723652327
Iteration 100, Training loss = 0.4499301967712549
Iteration 110, Training loss = 0.4234086320950435
Iteration 120, Training loss = 0.4065394318447663
Iteration 130, Training loss = 0.39306846318336636
Iteration 140, Training loss = 0.3812084292563108
Iteration 150, Training loss = 0.37139509632610357
Iteration 160, Training loss = 0.3613442310060446
Iteration 170, Training loss = 0.35475694173230576
Iteration 180, Training loss = 0.34903457947075367
Iteration 190, Training loss = 0.34325254880464995
Iteration 200, Training loss = 0.3356300957787495
Iteration 210, Training loss = 0.3313496678781051
Iteration 220, Training loss = 0.32715673157228875
Iteration 230, Training loss = 0.3243495754611034
Iteration 240, Training loss = 0.3212690717325761
Iteration 250, Training loss = 0.3171411510556936
Iteration 260, Training loss = 0.31517661921679974
Iteration 270, Training loss = 0.3115949620707677
Iteration 280, Training loss = 0.3096039584622933
Iteration 290, Training loss = 0.30819224265332407
Model training time: 49.791842460632324
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0417005545817888
Iteration 10, Training loss = 0.28657588391349864
Iteration 20, Training loss = 0.2645111515258367
Iteration 30, Training loss = 0.25682722075054276
Iteration 40, Training loss = 0.2541440690939243
Iteration 50, Training loss = 0.25121648151140946
Iteration 60, Training loss = 0.2477243856455271
Iteration 70, Training loss = 0.2453905210758631
Iteration 80, Training loss = 0.24555723641354305
Iteration 90, Training loss = 0.2425615290323129
Iteration 100, Training loss = 0.24050356376056486
Iteration 110, Training loss = 0.23732244674689496
Iteration 120, Training loss = 0.23697439968012846
Iteration 130, Training loss = 0.23518637023293054
Iteration 140, Training loss = 0.23410301469266415
Iteration 150, Training loss = 0.2328731450610436
Iteration 160, Training loss = 0.2301396019756794
Iteration 170, Training loss = 0.23058544371563655
Iteration 180, Training loss = 0.2266929676899543
Iteration 190, Training loss = 0.22506262834828633
Iteration 200, Training loss = 0.22575271946306413
Iteration 210, Training loss = 0.22585850946891767
Iteration 220, Training loss = 0.2227275030544171
Iteration 230, Training loss = 0.22187231280482733
Iteration 240, Training loss = 0.22249255838015905
Iteration 250, Training loss = 0.22072932990984276
Iteration 260, Training loss = 0.22022725813663924
Iteration 270, Training loss = 0.21766108081031305
Iteration 280, Training loss = 0.21871409873263195
Iteration 290, Training loss = 0.21709903279462686
Model training time: 56.765334129333496
Device: cuda
Iteration 0, Training loss = 0.9770125878544954
Iteration 10, Training loss = 0.27883324385262453
Iteration 20, Training loss = 0.2614375533392796
Iteration 30, Training loss = 0.2564058414158913
Iteration 40, Training loss = 0.25620947849865144
Iteration 50, Training loss = 0.2526794872604884
Iteration 60, Training loss = 0.24962894016733536
Iteration 70, Training loss = 0.24864979532475656
Iteration 80, Training loss = 0.24965706310020044
Iteration 90, Training loss = 0.2457021983483663
Iteration 100, Training loss = 0.245430281242499
Iteration 110, Training loss = 0.24295668877088106
Iteration 120, Training loss = 0.24145166690532976
Iteration 130, Training loss = 0.24060875077087146
Iteration 140, Training loss = 0.23948951781942293
Iteration 150, Training loss = 0.2365115899592638
Iteration 160, Training loss = 0.23589433170855045
Iteration 170, Training loss = 0.23589998827530786
Iteration 180, Training loss = 0.23387719203646368
Iteration 190, Training loss = 0.23214188984666878
Iteration 200, Training loss = 0.23113334648167858
Iteration 210, Training loss = 0.23367141745984554
Iteration 220, Training loss = 0.23385130270169333
Iteration 230, Training loss = 0.23215017166848367
Iteration 240, Training loss = 0.22885472886264324
Iteration 250, Training loss = 0.22682679580667844
Iteration 260, Training loss = 0.2262701582736694
Iteration 270, Training loss = 0.22533770364064437
Iteration 280, Training loss = 0.22414036446179336
Iteration 290, Training loss = 0.2228750390215562
Model training time: 58.00904607772827
Device: cuda
Iteration 0, Training loss = 0.9664211760346706
Iteration 10, Training loss = 0.2719321520282672
Iteration 20, Training loss = 0.2576942898046512
Iteration 30, Training loss = 0.25007987079712063
Iteration 40, Training loss = 0.24933923437045172
Iteration 50, Training loss = 0.24564507682449543
Iteration 60, Training loss = 0.24360639587617838
Iteration 70, Training loss = 0.24198014131532267
Iteration 80, Training loss = 0.23998028699022073
Iteration 90, Training loss = 0.23778984939249662
Iteration 100, Training loss = 0.23659487211933503
Iteration 110, Training loss = 0.2346798601345374
Iteration 120, Training loss = 0.23263958010535973
Iteration 130, Training loss = 0.23183454372561896
Iteration 140, Training loss = 0.22968874169656864
Iteration 150, Training loss = 0.2288486145150203
Iteration 160, Training loss = 0.2285141686980541
Iteration 170, Training loss = 0.22836189072292584
Iteration 180, Training loss = 0.23004266397597697
Iteration 190, Training loss = 0.22569347058351225
Iteration 200, Training loss = 0.22429418205641782
Iteration 210, Training loss = 0.22394407798464483
Iteration 220, Training loss = 0.22232188692746255
Iteration 230, Training loss = 0.22155057466947115
Iteration 240, Training loss = 0.22189909320038098
Iteration 250, Training loss = 0.22084342502057552
Iteration 260, Training loss = 0.22032795631541655
Iteration 270, Training loss = 0.22119429449622446
Iteration 280, Training loss = 0.2184949151168649
Iteration 290, Training loss = 0.21849958383693144
Model training time: 57.47733545303345
Device: cuda
Iteration 0, Training loss = 0.9400504552401029
Iteration 10, Training loss = 0.2746462650023974
Iteration 20, Training loss = 0.262700184320028
Iteration 30, Training loss = 0.25724797194393784
Iteration 40, Training loss = 0.25489702290640426
Iteration 50, Training loss = 0.25271890226465005
Iteration 60, Training loss = 0.25050052642249143
Iteration 70, Training loss = 0.24850526122519603
Iteration 80, Training loss = 0.24663478078750464
Iteration 90, Training loss = 0.24491504766047
Iteration 100, Training loss = 0.2447169589308592
Iteration 110, Training loss = 0.24291282519698143
Iteration 120, Training loss = 0.2421259037577189
Iteration 130, Training loss = 0.23982683526208767
Iteration 140, Training loss = 0.23937220083406338
Iteration 150, Training loss = 0.2380963545292616
Iteration 160, Training loss = 0.23651432095525357
Iteration 170, Training loss = 0.23525343233576188
Iteration 180, Training loss = 0.23416487586039764
Iteration 190, Training loss = 0.23457658075942442
Iteration 200, Training loss = 0.23455344068889433
Iteration 210, Training loss = 0.23027471080422401
Iteration 220, Training loss = 0.2292869444936514
Iteration 230, Training loss = 0.22836643951730087
Iteration 240, Training loss = 0.22734182156049287
Iteration 250, Training loss = 0.22518460323604253
Iteration 260, Training loss = 0.2223177500642263
Iteration 270, Training loss = 0.2218097919741502
Iteration 280, Training loss = 0.2207201887638523
Iteration 290, Training loss = 0.21894777917231506
Model training time: 56.768141746520996
Device: cuda
Iteration 0, Training loss = 0.9546434833453252
Iteration 10, Training loss = 0.2826948294845911
Iteration 20, Training loss = 0.26366047733105147
Iteration 30, Training loss = 0.2572668186173989
Iteration 40, Training loss = 0.2544304347382142
Iteration 50, Training loss = 0.25181669877985347
Iteration 60, Training loss = 0.24808922968804836
Iteration 70, Training loss = 0.24522366403387144
Iteration 80, Training loss = 0.2441116197464558
Iteration 90, Training loss = 0.24145175846150288
Iteration 100, Training loss = 0.2392007765861658
Iteration 110, Training loss = 0.23719306397609985
Iteration 120, Training loss = 0.23355079657183245
Iteration 130, Training loss = 0.2313280627131462
Iteration 140, Training loss = 0.22918232229466623
Iteration 150, Training loss = 0.22771938818578535
Iteration 160, Training loss = 0.2273288446550186
Iteration 170, Training loss = 0.22700207594495553
Iteration 180, Training loss = 0.22494852299300525
Iteration 190, Training loss = 0.22547842390262163
Iteration 200, Training loss = 0.2223228637415629
Iteration 210, Training loss = 0.2221731047074382
Iteration 220, Training loss = 0.2220010609151079
Iteration 230, Training loss = 0.2216111423017887
Iteration 240, Training loss = 0.22009404920614684
Iteration 250, Training loss = 0.21842661285056517
Iteration 260, Training loss = 0.21818187073446238
Iteration 270, Training loss = 0.21831554016814783
Iteration 280, Training loss = 0.21676054761673397
Iteration 290, Training loss = 0.21627119505921236
Model training time: 56.96433472633362
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1003480123785825
Iteration 10, Training loss = 0.9985996479025254
Iteration 20, Training loss = 0.9882380687273465
Iteration 30, Training loss = 0.9783504696992728
Iteration 40, Training loss = 0.9687217926749816
Iteration 50, Training loss = 0.9531246595657789
Iteration 60, Training loss = 0.9310956368079553
Iteration 70, Training loss = 0.8851355062081263
Iteration 80, Training loss = 0.8198370767327455
Iteration 90, Training loss = 0.7204094196741397
Iteration 100, Training loss = 0.5972481986077932
Iteration 110, Training loss = 0.4867063620342658
Iteration 120, Training loss = 0.4197864500949016
Iteration 130, Training loss = 0.38109346049336285
Iteration 140, Training loss = 0.3629974860411424
Iteration 150, Training loss = 0.3485615740601833
Iteration 160, Training loss = 0.34293700425097573
Iteration 170, Training loss = 0.3333081017033412
Iteration 180, Training loss = 0.3279464817964114
Iteration 190, Training loss = 0.3253556482780438
Iteration 200, Training loss = 0.3217744566500187
Iteration 210, Training loss = 0.31782916474800843
Iteration 220, Training loss = 0.31586425049373734
Iteration 230, Training loss = 0.3138647231344993
Iteration 240, Training loss = 0.31369197755478895
Iteration 250, Training loss = 0.31345781683921814
Iteration 260, Training loss = 0.3117582484220083
Iteration 270, Training loss = 0.3111819811165333
Iteration 280, Training loss = 0.3079956599439566
Iteration 290, Training loss = 0.30702095865630186
Model training time: 51.73368215560913
Device: cuda
Iteration 0, Training loss = 0.9846096571821433
Iteration 10, Training loss = 0.966110324057249
Iteration 20, Training loss = 0.9464950922590035
Iteration 30, Training loss = 0.8994145020842552
Iteration 40, Training loss = 0.8383346647024155
Iteration 50, Training loss = 0.7334242497499173
Iteration 60, Training loss = 0.6006815556723338
Iteration 70, Training loss = 0.48666374012827873
Iteration 80, Training loss = 0.42243265589842427
Iteration 90, Training loss = 0.39642942954714483
Iteration 100, Training loss = 0.3780709424844155
Iteration 110, Training loss = 0.3658134607741466
Iteration 120, Training loss = 0.35753427546184796
Iteration 130, Training loss = 0.3510668995575263
Iteration 140, Training loss = 0.34720576296632105
Iteration 150, Training loss = 0.3389759691288838
Iteration 160, Training loss = 0.3361237792728039
Iteration 170, Training loss = 0.33254111228654015
Iteration 180, Training loss = 0.32717897986563355
Iteration 190, Training loss = 0.32414470684642976
Iteration 200, Training loss = 0.3195748512561505
Iteration 210, Training loss = 0.31794805280291116
Iteration 220, Training loss = 0.31436592440765637
Iteration 230, Training loss = 0.31135430186986923
Iteration 240, Training loss = 0.31447449111594605
Iteration 250, Training loss = 0.30857562460005283
Iteration 260, Training loss = 0.3059994622778434
Iteration 270, Training loss = 0.3051135931164026
Iteration 280, Training loss = 0.30621183792559
Iteration 290, Training loss = 0.301500434629046
Model training time: 50.631852865219116
Device: cuda
Iteration 0, Training loss = 1.0033526518023932
Iteration 10, Training loss = 0.9944569336680266
Iteration 20, Training loss = 0.9864813822966355
Iteration 30, Training loss = 0.9733416959643364
Iteration 40, Training loss = 0.952128780575899
Iteration 50, Training loss = 0.9189737651210564
Iteration 60, Training loss = 0.8565649361564562
Iteration 70, Training loss = 0.7604202140982335
Iteration 80, Training loss = 0.6274202832808862
Iteration 90, Training loss = 0.5042809101824577
Iteration 100, Training loss = 0.42761961485330874
Iteration 110, Training loss = 0.39470755208570224
Iteration 120, Training loss = 0.3768337798806337
Iteration 130, Training loss = 0.3667851998828925
Iteration 140, Training loss = 0.35746531546689
Iteration 150, Training loss = 0.34940717541254485
Iteration 160, Training loss = 0.3449665757899101
Iteration 170, Training loss = 0.3378190934084929
Iteration 180, Training loss = 0.3348898320243909
Iteration 190, Training loss = 0.32829810148821426
Iteration 200, Training loss = 0.324297886628371
Iteration 210, Training loss = 0.3215223267101325
Iteration 220, Training loss = 0.3174634209045997
Iteration 230, Training loss = 0.31477726050294363
Iteration 240, Training loss = 0.3122804690725528
Iteration 250, Training loss = 0.3122336831516944
Iteration 260, Training loss = 0.3113492008012075
Iteration 270, Training loss = 0.30619619213617766
Iteration 280, Training loss = 0.3089113507706385
Iteration 290, Training loss = 0.3033187472476409
Model training time: 51.52092528343201
Device: cuda
Iteration 0, Training loss = 1.0060430593215501
Iteration 10, Training loss = 0.9812508156666389
Iteration 20, Training loss = 0.9642843449345002
Iteration 30, Training loss = 0.9394878063064355
Iteration 40, Training loss = 0.8978348311323386
Iteration 50, Training loss = 0.8387301535560534
Iteration 60, Training loss = 0.7473149173534833
Iteration 70, Training loss = 0.6393876385230285
Iteration 80, Training loss = 0.5368976931159313
Iteration 90, Training loss = 0.46263211134534615
Iteration 100, Training loss = 0.41835106393465626
Iteration 110, Training loss = 0.3944993203935715
Iteration 120, Training loss = 0.37962203472852707
Iteration 130, Training loss = 0.36937946711595243
Iteration 140, Training loss = 0.35836656463260835
Iteration 150, Training loss = 0.3518499772136028
Iteration 160, Training loss = 0.3437610983562011
Iteration 170, Training loss = 0.338451638387946
Iteration 180, Training loss = 0.3369207943861301
Iteration 190, Training loss = 0.33002595087656605
Iteration 200, Training loss = 0.3292803100955028
Iteration 210, Training loss = 0.3229720859000316
Iteration 220, Training loss = 0.3202854122679967
Iteration 230, Training loss = 0.3170361715154006
Iteration 240, Training loss = 0.3143278632599574
Iteration 250, Training loss = 0.3114703529729293
Iteration 260, Training loss = 0.3107474274360217
Iteration 270, Training loss = 0.3082801322810925
Iteration 280, Training loss = 0.3085286792081136
Iteration 290, Training loss = 0.30474844130759055
Model training time: 51.004486322402954
Device: cuda
Iteration 0, Training loss = 1.0199462421811545
Iteration 10, Training loss = 0.9779980515058224
Iteration 20, Training loss = 0.9600879326462746
Iteration 30, Training loss = 0.9270632926088113
Iteration 40, Training loss = 0.8756921818623176
Iteration 50, Training loss = 0.7938262252853467
Iteration 60, Training loss = 0.6676524739999038
Iteration 70, Training loss = 0.5342491879486121
Iteration 80, Training loss = 0.4439781749477753
Iteration 90, Training loss = 0.39628606404249483
Iteration 100, Training loss = 0.3740735955249805
Iteration 110, Training loss = 0.3650436197909025
Iteration 120, Training loss = 0.35568237347671616
Iteration 130, Training loss = 0.3463904336094856
Iteration 140, Training loss = 0.34111652422982913
Iteration 150, Training loss = 0.3367147520184517
Iteration 160, Training loss = 0.3302768881504352
Iteration 170, Training loss = 0.3272523802633469
Iteration 180, Training loss = 0.322681687055872
Iteration 190, Training loss = 0.3229447906980148
Iteration 200, Training loss = 0.31724437727377963
Iteration 210, Training loss = 0.3162802065221163
Iteration 220, Training loss = 0.31621990051980203
Iteration 230, Training loss = 0.3100936341171081
Iteration 240, Training loss = 0.30890732989288294
Iteration 250, Training loss = 0.3070732100078693
Iteration 260, Training loss = 0.3072846503211902
Iteration 270, Training loss = 0.3067943915151633
Iteration 280, Training loss = 0.30312494632716364
Iteration 290, Training loss = 0.30098873004317284
Model training time: 51.45502758026123
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8680873298301146
Iteration 10, Training loss = 0.2603154117957904
Iteration 20, Training loss = 0.24762913613365248
Iteration 30, Training loss = 0.24030723000088564
Iteration 40, Training loss = 0.23333532305864188
Iteration 50, Training loss = 0.22866349022548932
Iteration 60, Training loss = 0.22459446595838437
Iteration 70, Training loss = 0.22214221596144712
Iteration 80, Training loss = 0.22067415764412054
Iteration 90, Training loss = 0.21787449316336557
Model training time: 19.053317070007324
Device: cuda
Iteration 0, Training loss = 0.8464592918753624
Iteration 10, Training loss = 0.25913101649628234
Iteration 20, Training loss = 0.2484122092047563
Iteration 30, Training loss = 0.24365652209291092
Iteration 40, Training loss = 0.2382374690988889
Iteration 50, Training loss = 0.23527503901949295
Iteration 60, Training loss = 0.23127363514728272
Iteration 70, Training loss = 0.2282359404489398
Iteration 80, Training loss = 0.22643857076764107
Iteration 90, Training loss = 0.2210713064727875
Model training time: 18.591856002807617
Device: cuda
Iteration 0, Training loss = 0.828312260313676
Iteration 10, Training loss = 0.2580678685066792
Iteration 20, Training loss = 0.25175353999321276
Iteration 30, Training loss = 0.24479464656458452
Iteration 40, Training loss = 0.2387063315568062
Iteration 50, Training loss = 0.23269048118247435
Iteration 60, Training loss = 0.23005277269448227
Iteration 70, Training loss = 0.22821587309814417
Iteration 80, Training loss = 0.22478095563844994
Iteration 90, Training loss = 0.22275224722062165
Model training time: 18.847615718841553
Device: cuda
Iteration 0, Training loss = 0.8638069025025918
Iteration 10, Training loss = 0.25962734179427993
Iteration 20, Training loss = 0.2517382503988651
Iteration 30, Training loss = 0.24595664613522017
Iteration 40, Training loss = 0.23918931007098693
Iteration 50, Training loss = 0.23523515212134674
Iteration 60, Training loss = 0.23034843730811888
Iteration 70, Training loss = 0.22603895393415138
Iteration 80, Training loss = 0.22325762955901715
Iteration 90, Training loss = 0.2183710025766721
Model training time: 18.80499815940857
Device: cuda
Iteration 0, Training loss = 0.8598738309855645
Iteration 10, Training loss = 0.25984972443145055
Iteration 20, Training loss = 0.2492978498339653
Iteration 30, Training loss = 0.24298769923356864
Iteration 40, Training loss = 0.23743792308064607
Iteration 50, Training loss = 0.230889912837973
Iteration 60, Training loss = 0.22635368085824525
Iteration 70, Training loss = 0.2199116490351466
Iteration 80, Training loss = 0.21728200221864077
Iteration 90, Training loss = 0.21339370539555183
Model training time: 19.12148642539978
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0466839940502093
Iteration 10, Training loss = 0.9651638670609548
Iteration 20, Training loss = 0.9171801278224359
Iteration 30, Training loss = 0.783190267590376
Iteration 40, Training loss = 0.5804613126585116
Iteration 50, Training loss = 0.4392729584987347
Iteration 60, Training loss = 0.38710289190594965
Iteration 70, Training loss = 0.36335293705073685
Iteration 80, Training loss = 0.34720222967175335
Iteration 90, Training loss = 0.3354260167823388
Model training time: 16.824253797531128
Device: cuda
Iteration 0, Training loss = 1.0140631279120078
Iteration 10, Training loss = 0.9576379771416004
Iteration 20, Training loss = 0.868098293359463
Iteration 30, Training loss = 0.6578592423063058
Iteration 40, Training loss = 0.4489020209472913
Iteration 50, Training loss = 0.3828115543493858
Iteration 60, Training loss = 0.36053747368546635
Iteration 70, Training loss = 0.3482396308905803
Iteration 80, Training loss = 0.3353898452165035
Iteration 90, Training loss = 0.3273947867923058
Model training time: 16.561856031417847
Device: cuda
Iteration 0, Training loss = 1.1152217032817693
Iteration 10, Training loss = 0.9832359712857467
Iteration 20, Training loss = 0.939303037065726
Iteration 30, Training loss = 0.8385018878258191
Iteration 40, Training loss = 0.6180992923103846
Iteration 50, Training loss = 0.42651266561677825
Iteration 60, Training loss = 0.3739601909541167
Iteration 70, Training loss = 0.35372369320919883
Iteration 80, Training loss = 0.34265770190037215
Iteration 90, Training loss = 0.33125971687527805
Model training time: 16.91901421546936
Device: cuda
Iteration 0, Training loss = 1.045477835031656
Iteration 10, Training loss = 0.9421566243355091
Iteration 20, Training loss = 0.8205955051458799
Iteration 30, Training loss = 0.581788931041956
Iteration 40, Training loss = 0.4250260852277279
Iteration 50, Training loss = 0.3835369503268829
Iteration 60, Training loss = 0.35815060410935146
Iteration 70, Training loss = 0.34349368398006147
Iteration 80, Training loss = 0.33262986288620877
Iteration 90, Training loss = 0.3238024770353849
Model training time: 17.173420429229736
Device: cuda
Iteration 0, Training loss = 1.046389873784322
Iteration 10, Training loss = 0.973851699095506
Iteration 20, Training loss = 0.9151427906293136
Iteration 30, Training loss = 0.7592532697778481
Iteration 40, Training loss = 0.4877174854851686
Iteration 50, Training loss = 0.37932934440099275
Iteration 60, Training loss = 0.35564684280409264
Iteration 70, Training loss = 0.34504510863469195
Iteration 80, Training loss = 0.3322119452059269
Iteration 90, Training loss = 0.32782673033384174
Model training time: 16.506194591522217
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8766664627652901
Iteration 10, Training loss = 0.26463502480720097
Iteration 20, Training loss = 0.2545116712840704
Iteration 30, Training loss = 0.24933563015208796
Iteration 40, Training loss = 0.24653977141357386
Iteration 50, Training loss = 0.24237986109577692
Iteration 60, Training loss = 0.2399062030017376
Iteration 70, Training loss = 0.23636028012977198
Iteration 80, Training loss = 0.23627652414143085
Iteration 90, Training loss = 0.23108732506919366
Model training time: 18.672594785690308
Device: cuda
Iteration 0, Training loss = 0.9703846943493073
Iteration 10, Training loss = 0.2769739762521707
Iteration 20, Training loss = 0.26003669359936166
Iteration 30, Training loss = 0.2574327655423146
Iteration 40, Training loss = 0.2534445650302447
Iteration 50, Training loss = 0.25100456407436955
Iteration 60, Training loss = 0.24934540115869963
Iteration 70, Training loss = 0.24857378435822633
Iteration 80, Training loss = 0.24611639245771444
Iteration 90, Training loss = 0.24552747521262902
Model training time: 18.636672973632812
Device: cuda
Iteration 0, Training loss = 0.9381516535694783
Iteration 10, Training loss = 0.27311163381315195
Iteration 20, Training loss = 0.2533892997755454
Iteration 30, Training loss = 0.2479683728172229
Iteration 40, Training loss = 0.2450034778851729
Iteration 50, Training loss = 0.24113766433527836
Iteration 60, Training loss = 0.2384439496896588
Iteration 70, Training loss = 0.23676388682081148
Iteration 80, Training loss = 0.23539147719454306
Iteration 90, Training loss = 0.23382565909280226
Model training time: 18.81265950202942
Device: cuda
Iteration 0, Training loss = 1.345028562614551
Iteration 10, Training loss = 0.4049570650721972
Iteration 20, Training loss = 0.29914097487926483
Iteration 30, Training loss = 0.2739062627347616
Iteration 40, Training loss = 0.26626458606467795
Iteration 50, Training loss = 0.2638298864834584
Iteration 60, Training loss = 0.2587924940654865
Iteration 70, Training loss = 0.2561653070151806
Iteration 80, Training loss = 0.253289852434626
Iteration 90, Training loss = 0.24961864919616625
Model training time: 19.238465547561646
Device: cuda
Iteration 0, Training loss = 0.9160563034506944
Iteration 10, Training loss = 0.2696925106530006
Iteration 20, Training loss = 0.25496844560481036
Iteration 30, Training loss = 0.2482508193128384
Iteration 40, Training loss = 0.24555697535666135
Iteration 50, Training loss = 0.24326110846148088
Iteration 60, Training loss = 0.24127653749802938
Iteration 70, Training loss = 0.24058309913827822
Iteration 80, Training loss = 0.23806398347593272
Iteration 90, Training loss = 0.23661192821768615
Model training time: 19.10383701324463
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0331875034249747
Iteration 10, Training loss = 0.9786405357030722
Iteration 20, Training loss = 0.9272958527390773
Iteration 30, Training loss = 0.7990867111545342
Iteration 40, Training loss = 0.5722202182962344
Iteration 50, Training loss = 0.4217050066934182
Iteration 60, Training loss = 0.38241763470264584
Iteration 70, Training loss = 0.36049254052340984
Iteration 80, Training loss = 0.3464729176977506
Iteration 90, Training loss = 0.33702223962889266
Model training time: 17.161203622817993
Device: cuda
Iteration 0, Training loss = 1.0086857940141971
Iteration 10, Training loss = 0.9879299797690831
Iteration 20, Training loss = 0.9530984856761419
Iteration 30, Training loss = 0.872844846202777
Iteration 40, Training loss = 0.6859357609198644
Iteration 50, Training loss = 0.48036771840774095
Iteration 60, Training loss = 0.40058839665009427
Iteration 70, Training loss = 0.37473252415657043
Iteration 80, Training loss = 0.3537781430551639
Iteration 90, Training loss = 0.34122912843640035
Model training time: 16.91279172897339
Device: cuda
Iteration 0, Training loss = 1.5155147755375276
Iteration 10, Training loss = 0.9475461210195835
Iteration 20, Training loss = 0.8416652272527034
Iteration 30, Training loss = 0.6266940574233348
Iteration 40, Training loss = 0.45423263196761793
Iteration 50, Training loss = 0.391846827016427
Iteration 60, Training loss = 0.3697258951858832
Iteration 70, Training loss = 0.35424509157355016
Iteration 80, Training loss = 0.3430115736734409
Iteration 90, Training loss = 0.3341670830089312
Model training time: 16.88517141342163
Device: cuda
Iteration 0, Training loss = 0.9952426936763984
Iteration 10, Training loss = 0.9708412464421529
Iteration 20, Training loss = 0.9309074999048159
Iteration 30, Training loss = 0.8151561359946544
Iteration 40, Training loss = 0.6107556579204706
Iteration 50, Training loss = 0.45752083940001637
Iteration 60, Training loss = 0.40125521931510705
Iteration 70, Training loss = 0.3759894190499416
Iteration 80, Training loss = 0.3579127500550105
Iteration 90, Training loss = 0.348726201372651
Model training time: 16.911973476409912
Device: cuda
Iteration 0, Training loss = 1.0667069439704602
Iteration 10, Training loss = 0.9360599196874179
Iteration 20, Training loss = 0.8347013048254527
Iteration 30, Training loss = 0.6064740920869204
Iteration 40, Training loss = 0.43268374239022916
Iteration 50, Training loss = 0.38010109029710293
Iteration 60, Training loss = 0.36001035069616943
Iteration 70, Training loss = 0.344432350104818
Iteration 80, Training loss = 0.33373295788008434
Iteration 90, Training loss = 0.32253694319381165
Model training time: 16.895155906677246
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9121118182173142
Iteration 10, Training loss = 0.26375691750301766
Iteration 20, Training loss = 0.2506342652038886
Iteration 30, Training loss = 0.24546533421828196
Iteration 40, Training loss = 0.24079637516003388
Iteration 50, Training loss = 0.2364720584681401
Iteration 60, Training loss = 0.2316543782273164
Iteration 70, Training loss = 0.22749005888517088
Iteration 80, Training loss = 0.22406155744997355
Iteration 90, Training loss = 0.2225734263371963
Model training time: 18.904818773269653
Device: cuda
Iteration 0, Training loss = 0.8588888897345617
Iteration 10, Training loss = 0.26445329726602024
Iteration 20, Training loss = 0.25209070627505964
Iteration 30, Training loss = 0.2516650462952944
Iteration 40, Training loss = 0.244630968484741
Iteration 50, Training loss = 0.24147725004989368
Iteration 60, Training loss = 0.2382167037576437
Iteration 70, Training loss = 0.2334029688141667
Iteration 80, Training loss = 0.2291920016018244
Iteration 90, Training loss = 0.2257109357473942
Model training time: 19.605867862701416
Device: cuda
Iteration 0, Training loss = 0.7986180713543525
Iteration 10, Training loss = 0.25928866103864634
Iteration 20, Training loss = 0.24982250911685136
Iteration 30, Training loss = 0.24751157198960966
Iteration 40, Training loss = 0.24202789423557428
Iteration 50, Training loss = 0.23793068728767908
Iteration 60, Training loss = 0.23525392937545592
Iteration 70, Training loss = 0.23225044530744737
Iteration 80, Training loss = 0.22953555446404678
Iteration 90, Training loss = 0.22528194736402768
Model training time: 19.697108507156372
Device: cuda
Iteration 0, Training loss = 0.8211618696267788
Iteration 10, Training loss = 0.2598279440918794
Iteration 20, Training loss = 0.25162217665750247
Iteration 30, Training loss = 0.2452819887548685
Iteration 40, Training loss = 0.24051853527243322
Iteration 50, Training loss = 0.2378514168354181
Iteration 60, Training loss = 0.23661908965844375
Iteration 70, Training loss = 0.23861852990320095
Iteration 80, Training loss = 0.2340886270484099
Iteration 90, Training loss = 0.23131478004730666
Model training time: 20.181830406188965
Device: cuda
Iteration 0, Training loss = 0.878936322262654
Iteration 10, Training loss = 0.26399342314555097
Iteration 20, Training loss = 0.2536142160399602
Iteration 30, Training loss = 0.24840188040756261
Iteration 40, Training loss = 0.24253184907138348
Iteration 50, Training loss = 0.23936443403363228
Iteration 60, Training loss = 0.23728299685395682
Iteration 70, Training loss = 0.2334317987641463
Iteration 80, Training loss = 0.2308592673820945
Iteration 90, Training loss = 0.22863213030191568
Model training time: 20.913125038146973
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9952139092179445
Iteration 10, Training loss = 0.952313074698815
Iteration 20, Training loss = 0.8581157630452743
Iteration 30, Training loss = 0.6484923058977494
Iteration 40, Training loss = 0.44405218271108776
Iteration 50, Training loss = 0.3826222958473059
Iteration 60, Training loss = 0.36247948456842166
Iteration 70, Training loss = 0.3476672153919935
Iteration 80, Training loss = 0.3381351666668287
Iteration 90, Training loss = 0.3289195615798235
Model training time: 18.116999864578247
Device: cuda
Iteration 0, Training loss = 1.3607103371849427
Iteration 10, Training loss = 0.9927504395063107
Iteration 20, Training loss = 0.9816458127819575
Iteration 30, Training loss = 0.9484358831093862
Iteration 40, Training loss = 0.8697698609187052
Iteration 50, Training loss = 0.6807332428602072
Iteration 60, Training loss = 0.4703210990589399
Iteration 70, Training loss = 0.390875694127037
Iteration 80, Training loss = 0.3650468358626732
Iteration 90, Training loss = 0.3526205825977601
Model training time: 17.717003107070923
Device: cuda
Iteration 0, Training loss = 1.0240237454955394
Iteration 10, Training loss = 0.9875788929370733
Iteration 20, Training loss = 0.967502616919004
Iteration 30, Training loss = 0.9209291929235826
Iteration 40, Training loss = 0.7824641368710078
Iteration 50, Training loss = 0.5374118857658826
Iteration 60, Training loss = 0.3883222284225317
Iteration 70, Training loss = 0.3590543826039021
Iteration 80, Training loss = 0.3445365299972204
Iteration 90, Training loss = 0.3338125166125022
Model training time: 17.033004999160767
Device: cuda
Iteration 0, Training loss = 1.012089110337771
Iteration 10, Training loss = 0.9761471966138253
Iteration 20, Training loss = 0.9322247086809232
Iteration 30, Training loss = 0.7956277160690381
Iteration 40, Training loss = 0.525328861692777
Iteration 50, Training loss = 0.3941040239655055
Iteration 60, Training loss = 0.3693020133158335
Iteration 70, Training loss = 0.3545736320889913
Iteration 80, Training loss = 0.34301267521312606
Iteration 90, Training loss = 0.33356910600112033
Model training time: 16.820484161376953
Device: cuda
Iteration 0, Training loss = 1.0583945836585302
Iteration 10, Training loss = 0.9738941467725314
Iteration 20, Training loss = 0.9269903691915365
Iteration 30, Training loss = 0.7908164641031852
Iteration 40, Training loss = 0.5582069963789903
Iteration 50, Training loss = 0.42048582162421483
Iteration 60, Training loss = 0.3811380841697638
Iteration 70, Training loss = 0.3598433794597021
Iteration 80, Training loss = 0.34404091938183856
Iteration 90, Training loss = 0.3338765143775023
Model training time: 16.72469997406006
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.92452800904329
Iteration 10, Training loss = 0.2608432891563727
Iteration 20, Training loss = 0.2517573016767318
Iteration 30, Training loss = 0.24647491009762654
Iteration 40, Training loss = 0.2423256724499739
Iteration 50, Training loss = 0.23998604227717107
Iteration 60, Training loss = 0.23511736897321847
Iteration 70, Training loss = 0.2313227501626198
Iteration 80, Training loss = 0.22887236008850428
Iteration 90, Training loss = 0.2254634745992147
Iteration 100, Training loss = 0.22413778290725672
Iteration 110, Training loss = 0.21745157148689032
Iteration 120, Training loss = 0.2166354888333724
Iteration 130, Training loss = 0.2140743194434505
Iteration 140, Training loss = 0.2109298691726648
Iteration 150, Training loss = 0.2091676375040641
Iteration 160, Training loss = 0.2095925987769778
Iteration 170, Training loss = 0.20821999486249226
Iteration 180, Training loss = 0.20461997368301338
Iteration 190, Training loss = 0.20421525012128627
Model training time: 37.46194934844971
Device: cuda
Iteration 0, Training loss = 0.7897989907516882
Iteration 10, Training loss = 0.26034847432031083
Iteration 20, Training loss = 0.24900366853062922
Iteration 30, Training loss = 0.24279136511568838
Iteration 40, Training loss = 0.23988263831975368
Iteration 50, Training loss = 0.23504463855463725
Iteration 60, Training loss = 0.2304767039962686
Iteration 70, Training loss = 0.22668193732030117
Iteration 80, Training loss = 0.22528086108370468
Iteration 90, Training loss = 0.2271298452352102
Iteration 100, Training loss = 0.22138225760024327
Iteration 110, Training loss = 0.22170921730307433
Iteration 120, Training loss = 0.21822253041542494
Iteration 130, Training loss = 0.21847132693689603
Iteration 140, Training loss = 0.21340767935348245
Iteration 150, Training loss = 0.21461412095679686
Iteration 160, Training loss = 0.21452265798759002
Iteration 170, Training loss = 0.21140051991320574
Iteration 180, Training loss = 0.20798746410470742
Iteration 190, Training loss = 0.2079323300232108
Model training time: 38.17388153076172
Device: cuda
Iteration 0, Training loss = 0.7385615187768753
Iteration 10, Training loss = 0.2571833635178896
Iteration 20, Training loss = 0.24675029473235974
Iteration 30, Training loss = 0.2421816116055617
Iteration 40, Training loss = 0.2359595182709969
Iteration 50, Training loss = 0.23570048895019752
Iteration 60, Training loss = 0.22994904076823822
Iteration 70, Training loss = 0.22682285910615554
Iteration 80, Training loss = 0.22383341078574842
Iteration 90, Training loss = 0.2191094635054469
Iteration 100, Training loss = 0.21676242050643152
Iteration 110, Training loss = 0.21532275728308237
Iteration 120, Training loss = 0.2156205541955737
Iteration 130, Training loss = 0.21258365305570456
Iteration 140, Training loss = 0.2109415137137358
Iteration 150, Training loss = 0.20639686573010224
Iteration 160, Training loss = 0.20637856882352096
Iteration 170, Training loss = 0.2056612717704131
Iteration 180, Training loss = 0.2026645032545695
Iteration 190, Training loss = 0.20553043541999963
Model training time: 38.115700006484985
Device: cuda
Iteration 0, Training loss = 0.8112887869087549
Iteration 10, Training loss = 0.25293493113265586
Iteration 20, Training loss = 0.24307788650576884
Iteration 30, Training loss = 0.23607800600047296
Iteration 40, Training loss = 0.22968021257279012
Iteration 50, Training loss = 0.225340542741693
Iteration 60, Training loss = 0.22216381490803683
Iteration 70, Training loss = 0.2198638441041112
Iteration 80, Training loss = 0.21801522015952146
Iteration 90, Training loss = 0.2147210382928069
Iteration 100, Training loss = 0.21129763720986935
Iteration 110, Training loss = 0.2106009008219609
Iteration 120, Training loss = 0.2071570876555947
Iteration 130, Training loss = 0.20713250224406904
Iteration 140, Training loss = 0.2056157297383134
Iteration 150, Training loss = 0.2049022945933617
Iteration 160, Training loss = 0.2033763541481816
Iteration 170, Training loss = 0.20227469518207586
Iteration 180, Training loss = 0.20187060893155062
Iteration 190, Training loss = 0.20054652785452512
Model training time: 37.72172236442566
Device: cuda
Iteration 0, Training loss = 1.1033691889964616
Iteration 10, Training loss = 0.2623088802569188
Iteration 20, Training loss = 0.2549705079828317
Iteration 30, Training loss = 0.24692953750491142
Iteration 40, Training loss = 0.2431763940705703
Iteration 50, Training loss = 0.24064403686385888
Iteration 60, Training loss = 0.24017613137570712
Iteration 70, Training loss = 0.23907700266975623
Iteration 80, Training loss = 0.23528332793368742
Iteration 90, Training loss = 0.2374236615231404
Iteration 100, Training loss = 0.2339155443299275
Iteration 110, Training loss = 0.23093873434341872
Iteration 120, Training loss = 0.229519513507302
Iteration 130, Training loss = 0.2270902432501316
Iteration 140, Training loss = 0.22444453119085386
Iteration 150, Training loss = 0.22141106842229
Iteration 160, Training loss = 0.21829907667751497
Iteration 170, Training loss = 0.2206451387789387
Iteration 180, Training loss = 0.21509008023601311
Iteration 190, Training loss = 0.21455995824474555
Model training time: 37.75187134742737
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1202421457721636
Iteration 10, Training loss = 0.9874505274570905
Iteration 20, Training loss = 0.9581604714577014
Iteration 30, Training loss = 0.8802924603223801
Iteration 40, Training loss = 0.6830697122674722
Iteration 50, Training loss = 0.45847148476884914
Iteration 60, Training loss = 0.3849217746979915
Iteration 70, Training loss = 0.36445405721091306
Iteration 80, Training loss = 0.3483275369955943
Iteration 90, Training loss = 0.3335411607359464
Iteration 100, Training loss = 0.32575681519049865
Iteration 110, Training loss = 0.31869429708100283
Iteration 120, Training loss = 0.31466738808040434
Iteration 130, Training loss = 0.31248583644628525
Iteration 140, Training loss = 0.30722346543692625
Iteration 150, Training loss = 0.30670499400450635
Iteration 160, Training loss = 0.3032510452545606
Iteration 170, Training loss = 0.3007744656732449
Iteration 180, Training loss = 0.29907900080657923
Iteration 190, Training loss = 0.29783801619823164
Model training time: 34.48808979988098
Device: cuda
Iteration 0, Training loss = 0.9926861238021117
Iteration 10, Training loss = 0.9671488484511008
Iteration 20, Training loss = 0.9021205844787451
Iteration 30, Training loss = 0.7392844242545275
Iteration 40, Training loss = 0.522263447395884
Iteration 50, Training loss = 0.4209685159417299
Iteration 60, Training loss = 0.39173533122699994
Iteration 70, Training loss = 0.37045644137721795
Iteration 80, Training loss = 0.35197155077296954
Iteration 90, Training loss = 0.3402943015098572
Iteration 100, Training loss = 0.3294811164243863
Iteration 110, Training loss = 0.32312159369198173
Iteration 120, Training loss = 0.31708406972197384
Iteration 130, Training loss = 0.31230919851133454
Iteration 140, Training loss = 0.3089833315461874
Iteration 150, Training loss = 0.30545114568219733
Iteration 160, Training loss = 0.3021071371980585
Iteration 170, Training loss = 0.3043558783829212
Iteration 180, Training loss = 0.29889723443641114
Iteration 190, Training loss = 0.29765927576674867
Model training time: 34.167447566986084
Device: cuda
Iteration 0, Training loss = 1.015082754767858
Iteration 10, Training loss = 0.959605909883976
Iteration 20, Training loss = 0.8566384453039902
Iteration 30, Training loss = 0.6191344106426606
Iteration 40, Training loss = 0.419413385196374
Iteration 50, Training loss = 0.3673898493154691
Iteration 60, Training loss = 0.34941868476856214
Iteration 70, Training loss = 0.33792084297881675
Iteration 80, Training loss = 0.33007434855860013
Iteration 90, Training loss = 0.32569454853924423
Iteration 100, Training loss = 0.3162014621954698
Iteration 110, Training loss = 0.3110865798707192
Iteration 120, Training loss = 0.3073560970907028
Iteration 130, Training loss = 0.30466517748741
Iteration 140, Training loss = 0.3011700569723661
Iteration 150, Training loss = 0.2994849062882937
Iteration 160, Training loss = 0.29592201801446766
Iteration 170, Training loss = 0.29490640487235326
Iteration 180, Training loss = 0.29171299662154454
Iteration 190, Training loss = 0.290250675752759
Model training time: 33.735223054885864
Device: cuda
Iteration 0, Training loss = 1.0256364322625673
Iteration 10, Training loss = 0.9594349924188393
Iteration 20, Training loss = 0.8813128964259074
Iteration 30, Training loss = 0.6813429003724685
Iteration 40, Training loss = 0.4618055998132779
Iteration 50, Training loss = 0.3824966624379158
Iteration 60, Training loss = 0.3576925164088607
Iteration 70, Training loss = 0.34357120669805086
Iteration 80, Training loss = 0.33349293799927604
Iteration 90, Training loss = 0.32377130509569096
Iteration 100, Training loss = 0.31969415511076266
Iteration 110, Training loss = 0.3122147424862935
Iteration 120, Training loss = 0.30593043694702476
Iteration 130, Training loss = 0.3041845088681349
Iteration 140, Training loss = 0.30131041229917455
Iteration 150, Training loss = 0.2967653369101194
Iteration 160, Training loss = 0.2952670374741921
Iteration 170, Training loss = 0.2921090987152778
Iteration 180, Training loss = 0.2899170613919313
Iteration 190, Training loss = 0.28862202095870787
Model training time: 34.60581994056702
Device: cuda
Iteration 0, Training loss = 1.0104554054828792
Iteration 10, Training loss = 0.9556643257920558
Iteration 20, Training loss = 0.8659559855094323
Iteration 30, Training loss = 0.6547590405322038
Iteration 40, Training loss = 0.4516992603357022
Iteration 50, Training loss = 0.38765889391876185
Iteration 60, Training loss = 0.3661015646962019
Iteration 70, Training loss = 0.34628048137976575
Iteration 80, Training loss = 0.3331749794574884
Iteration 90, Training loss = 0.32369973539159846
Iteration 100, Training loss = 0.31718360632658005
Iteration 110, Training loss = 0.30832186713814735
Iteration 120, Training loss = 0.30588920772648776
Iteration 130, Training loss = 0.30174103579842126
Iteration 140, Training loss = 0.2977545807281366
Iteration 150, Training loss = 0.2941184431457749
Iteration 160, Training loss = 0.2940091976466087
Iteration 170, Training loss = 0.29387138769603693
Iteration 180, Training loss = 0.28912302063634765
Iteration 190, Training loss = 0.2866292813649544
Model training time: 34.019564151763916
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.074820255430845
Iteration 10, Training loss = 0.26930661728748906
Iteration 20, Training loss = 0.2520200660308966
Iteration 30, Training loss = 0.24803313297721055
Iteration 40, Training loss = 0.24224761185737756
Iteration 50, Training loss = 0.23612407981776273
Iteration 60, Training loss = 0.23291687824978277
Iteration 70, Training loss = 0.2290170368953393
Iteration 80, Training loss = 0.2278607122313518
Iteration 90, Training loss = 0.22502835367161494
Iteration 100, Training loss = 0.22483458541906798
Iteration 110, Training loss = 0.22376491251186684
Iteration 120, Training loss = 0.22192132630600378
Iteration 130, Training loss = 0.2212198722677735
Iteration 140, Training loss = 0.2210936350747943
Iteration 150, Training loss = 0.22001327058443657
Iteration 160, Training loss = 0.22191135599636114
Iteration 170, Training loss = 0.22008376532735732
Iteration 180, Training loss = 0.21810241007747558
Iteration 190, Training loss = 0.21755671673096144
Model training time: 37.83422875404358
Device: cuda
Iteration 0, Training loss = 0.9024166837334633
Iteration 10, Training loss = 0.2654911977453874
Iteration 20, Training loss = 0.25583952020567197
Iteration 30, Training loss = 0.25051400960924536
Iteration 40, Training loss = 0.24940719011311346
Iteration 50, Training loss = 0.24671830695409042
Iteration 60, Training loss = 0.24569813047464079
Iteration 70, Training loss = 0.24379867386932558
Iteration 80, Training loss = 0.24100966250094083
Iteration 90, Training loss = 0.24169792220569575
Iteration 100, Training loss = 0.235078071244061
Iteration 110, Training loss = 0.2344877698387091
Iteration 120, Training loss = 0.23196251489795172
Iteration 130, Training loss = 0.23041549110068724
Iteration 140, Training loss = 0.22780635150579306
Iteration 150, Training loss = 0.22815958281549123
Iteration 160, Training loss = 0.22681763023138046
Iteration 170, Training loss = 0.22480103409347627
Iteration 180, Training loss = 0.2243004343830622
Iteration 190, Training loss = 0.22403697132204586
Model training time: 37.90289616584778
Device: cuda
Iteration 0, Training loss = 0.9423693945774665
Iteration 10, Training loss = 0.27166282600508285
Iteration 20, Training loss = 0.25248197093605995
Iteration 30, Training loss = 0.24859727575228766
Iteration 40, Training loss = 0.24642867953158343
Iteration 50, Training loss = 0.23933373771321315
Iteration 60, Training loss = 0.23708571106768572
Iteration 70, Training loss = 0.23490200034127787
Iteration 80, Training loss = 0.2303317693563608
Iteration 90, Training loss = 0.22968900998911032
Iteration 100, Training loss = 0.22648161162550634
Iteration 110, Training loss = 0.22425961680710316
Iteration 120, Training loss = 0.22275870207410592
Iteration 130, Training loss = 0.22077585577678221
Iteration 140, Training loss = 0.21952519974169823
Iteration 150, Training loss = 0.21942779722695166
Iteration 160, Training loss = 0.2181335614564327
Iteration 170, Training loss = 0.21841403648543817
Iteration 180, Training loss = 0.2152384722318787
Iteration 190, Training loss = 0.21382852450299722
Model training time: 37.873626708984375
Device: cuda
Iteration 0, Training loss = 0.8735776853102905
Iteration 10, Training loss = 0.2745733473163385
Iteration 20, Training loss = 0.25957696397717184
Iteration 30, Training loss = 0.2508598705037282
Iteration 40, Training loss = 0.24552952770430309
Iteration 50, Training loss = 0.24083061330020428
Iteration 60, Training loss = 0.2370078698373758
Iteration 70, Training loss = 0.23249472491443157
Iteration 80, Training loss = 0.2281505294287434
Iteration 90, Training loss = 0.22829135600477457
Iteration 100, Training loss = 0.22454027702602056
Iteration 110, Training loss = 0.22203522075254184
Iteration 120, Training loss = 0.22179476673213336
Iteration 130, Training loss = 0.21974553382740572
Iteration 140, Training loss = 0.21987956002927744
Iteration 150, Training loss = 0.22074440537163845
Iteration 160, Training loss = 0.21925366540940908
Iteration 170, Training loss = 0.21644460409879684
Iteration 180, Training loss = 0.2184951508131165
Iteration 190, Training loss = 0.2148662040439936
Model training time: 37.7785484790802
Device: cuda
Iteration 0, Training loss = 0.909654510135834
Iteration 10, Training loss = 0.2923205183962217
Iteration 20, Training loss = 0.25505805760622025
Iteration 30, Training loss = 0.24597333901776716
Iteration 40, Training loss = 0.23912960376877052
Iteration 50, Training loss = 0.23440927567963415
Iteration 60, Training loss = 0.23141816373054797
Iteration 70, Training loss = 0.22937482967972755
Iteration 80, Training loss = 0.22731160487119967
Iteration 90, Training loss = 0.22469816299585196
Iteration 100, Training loss = 0.2238525997560758
Iteration 110, Training loss = 0.2238820819900586
Iteration 120, Training loss = 0.22115941875829145
Iteration 130, Training loss = 0.21776399560845816
Iteration 140, Training loss = 0.21672751589749867
Iteration 150, Training loss = 0.21824629024530834
Iteration 160, Training loss = 0.2148645413466371
Iteration 170, Training loss = 0.21363937124036825
Iteration 180, Training loss = 0.2126069748057769
Iteration 190, Training loss = 0.21147505509165618
Model training time: 37.6232328414917
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1261874557687686
Iteration 10, Training loss = 0.979138715335956
Iteration 20, Training loss = 0.9423303157091141
Iteration 30, Training loss = 0.8576329826162412
Iteration 40, Training loss = 0.6503573787900118
Iteration 50, Training loss = 0.43847890427479375
Iteration 60, Training loss = 0.37396724722706354
Iteration 70, Training loss = 0.35437620517153007
Iteration 80, Training loss = 0.3407973856306993
Iteration 90, Training loss = 0.33218050920046294
Iteration 100, Training loss = 0.3247234708128067
Iteration 110, Training loss = 0.32112846724115884
Iteration 120, Training loss = 0.3156772300314445
Iteration 130, Training loss = 0.3134442219653955
Iteration 140, Training loss = 0.3096234401544699
Iteration 150, Training loss = 0.30727861262857914
Iteration 160, Training loss = 0.30587815693937814
Iteration 170, Training loss = 0.3050517402589321
Iteration 180, Training loss = 0.3018546858086036
Iteration 190, Training loss = 0.30542340960640174
Model training time: 34.419212341308594
Device: cuda
Iteration 0, Training loss = 1.1895194145349355
Iteration 10, Training loss = 0.902496110361356
Iteration 20, Training loss = 0.7101927901116701
Iteration 30, Training loss = 0.4677362535148859
Iteration 40, Training loss = 0.38884052304694283
Iteration 50, Training loss = 0.3657065009554991
Iteration 60, Training loss = 0.3513617478311062
Iteration 70, Training loss = 0.34100568724366337
Iteration 80, Training loss = 0.3296513654864751
Iteration 90, Training loss = 0.32163153015650237
Iteration 100, Training loss = 0.31898538725307357
Iteration 110, Training loss = 0.3129465989768505
Iteration 120, Training loss = 0.3129589038972671
Iteration 130, Training loss = 0.3082869534309094
Iteration 140, Training loss = 0.3058094817858476
Iteration 150, Training loss = 0.30907506295121634
Iteration 160, Training loss = 0.30294485504810625
Iteration 170, Training loss = 0.30267744554350007
Iteration 180, Training loss = 0.3022267044736789
Iteration 190, Training loss = 0.2987245882932956
Model training time: 33.66503381729126
Device: cuda
Iteration 0, Training loss = 1.006102602642316
Iteration 10, Training loss = 0.9995158790395811
Iteration 20, Training loss = 0.9935359364518752
Iteration 30, Training loss = 0.9815708530636934
Iteration 40, Training loss = 0.9597258155162518
Iteration 50, Training loss = 0.9030932119259467
Iteration 60, Training loss = 0.7676907731936529
Iteration 70, Training loss = 0.543891432766731
Iteration 80, Training loss = 0.4118458568476714
Iteration 90, Training loss = 0.36939926646076715
Iteration 100, Training loss = 0.34850392247048706
Iteration 110, Training loss = 0.33269939915491986
Iteration 120, Training loss = 0.3210292232151215
Iteration 130, Training loss = 0.3147810224730235
Iteration 140, Training loss = 0.30886765856009263
Iteration 150, Training loss = 0.30536471035044926
Iteration 160, Training loss = 0.30180216623613465
Iteration 170, Training loss = 0.2991710752248764
Iteration 180, Training loss = 0.29655686744417137
Iteration 190, Training loss = 0.29610883344251376
Model training time: 33.97417879104614
Device: cuda
Iteration 0, Training loss = 0.9914751614515598
Iteration 10, Training loss = 0.9586274400353432
Iteration 20, Training loss = 0.892073077078049
Iteration 30, Training loss = 0.72164570998687
Iteration 40, Training loss = 0.49241676253194994
Iteration 50, Training loss = 0.4002749641927389
Iteration 60, Training loss = 0.37154273230295914
Iteration 70, Training loss = 0.3540731038038547
Iteration 80, Training loss = 0.3419119601066296
Iteration 90, Training loss = 0.3305006441302024
Iteration 100, Training loss = 0.32274106846979034
Iteration 110, Training loss = 0.3140979442172326
Iteration 120, Training loss = 0.30809270003094125
Iteration 130, Training loss = 0.3029915750599824
Iteration 140, Training loss = 0.30207105124225986
Iteration 150, Training loss = 0.29626269394961685
Iteration 160, Training loss = 0.29428621381521225
Iteration 170, Training loss = 0.2919527617498086
Iteration 180, Training loss = 0.29036291144215143
Iteration 190, Training loss = 0.29206334312374777
Model training time: 33.629570960998535
Device: cuda
Iteration 0, Training loss = 0.9893573774741247
Iteration 10, Training loss = 0.9296567571850923
Iteration 20, Training loss = 0.812677895793548
Iteration 30, Training loss = 0.5977909287007955
Iteration 40, Training loss = 0.4510598893348987
Iteration 50, Training loss = 0.404717877220649
Iteration 60, Training loss = 0.38146939716086936
Iteration 70, Training loss = 0.36140223707144076
Iteration 80, Training loss = 0.3491289707330557
Iteration 90, Training loss = 0.3373767753633169
Iteration 100, Training loss = 0.3305022345425991
Iteration 110, Training loss = 0.3219265765868701
Iteration 120, Training loss = 0.3150859965154758
Iteration 130, Training loss = 0.31141246984211296
Iteration 140, Training loss = 0.3079112719457883
Iteration 150, Training loss = 0.3033784796985296
Iteration 160, Training loss = 0.30107495933771133
Iteration 170, Training loss = 0.29820775083051276
Iteration 180, Training loss = 0.29595033924740094
Iteration 190, Training loss = 0.2955508024360125
Model training time: 33.78339886665344
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.730376089994724
Iteration 10, Training loss = 0.2654476101295306
Iteration 20, Training loss = 0.25534721478246725
Iteration 30, Training loss = 0.25230904682897604
Iteration 40, Training loss = 0.24495796142862394
Iteration 50, Training loss = 0.24332340735082442
Iteration 60, Training loss = 0.23842079579257047
Iteration 70, Training loss = 0.23459284050533405
Iteration 80, Training loss = 0.2321226314569895
Iteration 90, Training loss = 0.22950394867131343
Iteration 100, Training loss = 0.2246631310822872
Iteration 110, Training loss = 0.2223432811263662
Iteration 120, Training loss = 0.2196977870682111
Iteration 130, Training loss = 0.2191961181278412
Iteration 140, Training loss = 0.21588416003550476
Iteration 150, Training loss = 0.21403962230453125
Iteration 160, Training loss = 0.2139383972120973
Iteration 170, Training loss = 0.21639119704755452
Iteration 180, Training loss = 0.21304386316870266
Iteration 190, Training loss = 0.21187335539322633
Model training time: 37.76307392120361
Device: cuda
Iteration 0, Training loss = 0.8839034031216915
Iteration 10, Training loss = 0.2628253767123589
Iteration 20, Training loss = 0.25381442732535875
Iteration 30, Training loss = 0.2478072905483154
Iteration 40, Training loss = 0.2438834854043447
Iteration 50, Training loss = 0.2403929942072584
Iteration 60, Training loss = 0.2346569117731773
Iteration 70, Training loss = 0.23341957852244377
Iteration 80, Training loss = 0.22718730396949327
Iteration 90, Training loss = 0.22594850639311168
Iteration 100, Training loss = 0.2217065948419846
Iteration 110, Training loss = 0.22060639841052201
Iteration 120, Training loss = 0.21893308526621416
Iteration 130, Training loss = 0.217498566955328
Iteration 140, Training loss = 0.21423959961304298
Iteration 150, Training loss = 0.2122260150141441
Iteration 160, Training loss = 0.21243467389677578
Iteration 170, Training loss = 0.21029262251865405
Iteration 180, Training loss = 0.2097918909902756
Iteration 190, Training loss = 0.2074708529533102
Model training time: 38.002565145492554
Device: cuda
Iteration 0, Training loss = 0.8818618878722191
Iteration 10, Training loss = 0.2678392738676988
Iteration 20, Training loss = 0.2537701684408463
Iteration 30, Training loss = 0.24887919798493385
Iteration 40, Training loss = 0.24036944628908083
Iteration 50, Training loss = 0.23905478604137897
Iteration 60, Training loss = 0.2299152803965486
Iteration 70, Training loss = 0.22914948500692844
Iteration 80, Training loss = 0.22327346641283768
Iteration 90, Training loss = 0.22048131046959987
Iteration 100, Training loss = 0.21752189472317696
Iteration 110, Training loss = 0.21771106969278592
Iteration 120, Training loss = 0.21505377169411916
Iteration 130, Training loss = 0.21374038356141403
Iteration 140, Training loss = 0.21338160758694777
Iteration 150, Training loss = 0.21216601061706358
Iteration 160, Training loss = 0.20995580691557664
Iteration 170, Training loss = 0.20783648124107948
Iteration 180, Training loss = 0.20732665513283932
Iteration 190, Training loss = 0.20528110245672557
Model training time: 37.58774137496948
Device: cuda
Iteration 0, Training loss = 0.8168680882797792
Iteration 10, Training loss = 0.26209425281446713
Iteration 20, Training loss = 0.25258848687204033
Iteration 30, Training loss = 0.24654306686268404
Iteration 40, Training loss = 0.24340114169395888
Iteration 50, Training loss = 0.24121143274868911
Iteration 60, Training loss = 0.23675752703387004
Iteration 70, Training loss = 0.23319449075139487
Iteration 80, Training loss = 0.2317346974920768
Iteration 90, Training loss = 0.22835951241163108
Iteration 100, Training loss = 0.2250695858294001
Iteration 110, Training loss = 0.2244681084098724
Iteration 120, Training loss = 0.222900595730887
Iteration 130, Training loss = 0.22075957174484545
Iteration 140, Training loss = 0.2192428675121986
Iteration 150, Training loss = 0.22176524251699448
Iteration 160, Training loss = 0.21806892774139458
Iteration 170, Training loss = 0.21668645066137499
Iteration 180, Training loss = 0.21525953308894083
Iteration 190, Training loss = 0.21429694100068167
Model training time: 38.21973252296448
Device: cuda
Iteration 0, Training loss = 0.845417495816946
Iteration 10, Training loss = 0.25852512740171874
Iteration 20, Training loss = 0.25402901980739373
Iteration 30, Training loss = 0.24727617161205181
Iteration 40, Training loss = 0.24471407340696225
Iteration 50, Training loss = 0.24167750145380312
Iteration 60, Training loss = 0.24073460461715093
Iteration 70, Training loss = 0.23885023751511023
Iteration 80, Training loss = 0.23797760373697832
Iteration 90, Training loss = 0.234866062250848
Iteration 100, Training loss = 0.23363799458512893
Iteration 110, Training loss = 0.23423615757089394
Iteration 120, Training loss = 0.23076243899189508
Iteration 130, Training loss = 0.22813467208582622
Iteration 140, Training loss = 0.22841729309696418
Iteration 150, Training loss = 0.2264396743132518
Iteration 160, Training loss = 0.22317575025730407
Iteration 170, Training loss = 0.22334244037763432
Iteration 180, Training loss = 0.22136443423537108
Iteration 190, Training loss = 0.2198393319088679
Model training time: 37.790982484817505
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0209366564567273
Iteration 10, Training loss = 0.9886353359772608
Iteration 20, Training loss = 0.9421317462737744
Iteration 30, Training loss = 0.8195423045410559
Iteration 40, Training loss = 0.6259099832521036
Iteration 50, Training loss = 0.46945519344164777
Iteration 60, Training loss = 0.4068301199720456
Iteration 70, Training loss = 0.378247315207353
Iteration 80, Training loss = 0.36102314479649067
Iteration 90, Training loss = 0.34861588205855626
Iteration 100, Training loss = 0.3394018610796103
Iteration 110, Training loss = 0.32907752076593727
Iteration 120, Training loss = 0.3228835092427639
Iteration 130, Training loss = 0.3170881387419425
Iteration 140, Training loss = 0.3129772383433122
Iteration 150, Training loss = 0.3101997897028923
Iteration 160, Training loss = 0.30572970397770405
Iteration 170, Training loss = 0.3022892721570455
Iteration 180, Training loss = 0.30069765305289853
Iteration 190, Training loss = 0.29714002474569357
Model training time: 33.34966230392456
Device: cuda
Iteration 0, Training loss = 1.355038308753417
Iteration 10, Training loss = 0.9945478118382968
Iteration 20, Training loss = 0.977596582701573
Iteration 30, Training loss = 0.9436466046250783
Iteration 40, Training loss = 0.8312751312668507
Iteration 50, Training loss = 0.6042763547828565
Iteration 60, Training loss = 0.434364753560378
Iteration 70, Training loss = 0.38761452298897964
Iteration 80, Training loss = 0.3700912005912799
Iteration 90, Training loss = 0.3587108704333122
Iteration 100, Training loss = 0.3418720608147291
Iteration 110, Training loss = 0.3329659002618148
Iteration 120, Training loss = 0.32679044698866516
Iteration 130, Training loss = 0.3207305282927476
Iteration 140, Training loss = 0.3161419425159693
Iteration 150, Training loss = 0.3132779323137723
Iteration 160, Training loss = 0.3110019647731231
Iteration 170, Training loss = 0.307672455906868
Iteration 180, Training loss = 0.3060670823145371
Iteration 190, Training loss = 0.30244260653853416
Model training time: 33.180747747421265
Device: cuda
Iteration 0, Training loss = 1.009368318777818
Iteration 10, Training loss = 0.9721287741110876
Iteration 20, Training loss = 0.9158964695838782
Iteration 30, Training loss = 0.7612306091647881
Iteration 40, Training loss = 0.5335334232793405
Iteration 50, Training loss = 0.4105556085705757
Iteration 60, Training loss = 0.37774282980423707
Iteration 70, Training loss = 0.3599112221541313
Iteration 80, Training loss = 0.3475070889466084
Iteration 90, Training loss = 0.335653544905094
Iteration 100, Training loss = 0.3312034502338905
Iteration 110, Training loss = 0.32170736918655723
Iteration 120, Training loss = 0.3152557324904662
Iteration 130, Training loss = 0.31145056050557357
Iteration 140, Training loss = 0.3101572584933959
Iteration 150, Training loss = 0.30439679998044783
Iteration 160, Training loss = 0.30110399797558784
Iteration 170, Training loss = 0.29798339765805465
Iteration 180, Training loss = 0.2963237471591968
Iteration 190, Training loss = 0.2936592521862342
Model training time: 33.38348841667175
Device: cuda
Iteration 0, Training loss = 1.0269924969627306
Iteration 10, Training loss = 0.9628486507214032
Iteration 20, Training loss = 0.8924565011492143
Iteration 30, Training loss = 0.6920272312485255
Iteration 40, Training loss = 0.4519728637085511
Iteration 50, Training loss = 0.38268588941830856
Iteration 60, Training loss = 0.36454280666433847
Iteration 70, Training loss = 0.3501639820348758
Iteration 80, Training loss = 0.3403057228200711
Iteration 90, Training loss = 0.33005787202945125
Iteration 100, Training loss = 0.3233439445209045
Iteration 110, Training loss = 0.31944468058645725
Iteration 120, Training loss = 0.31503925730402654
Iteration 130, Training loss = 0.3125926024065568
Iteration 140, Training loss = 0.307825945604306
Iteration 150, Training loss = 0.3048504995038876
Iteration 160, Training loss = 0.30511182045134216
Iteration 170, Training loss = 0.2994612380862236
Iteration 180, Training loss = 0.29723424335511833
Iteration 190, Training loss = 0.2959900866620816
Model training time: 33.534467458724976
Device: cuda
Iteration 0, Training loss = 1.0416136618990164
Iteration 10, Training loss = 0.9382875063098394
Iteration 20, Training loss = 0.8279582955516301
Iteration 30, Training loss = 0.606104893466601
Iteration 40, Training loss = 0.43117310479283333
Iteration 50, Training loss = 0.38069193495007664
Iteration 60, Training loss = 0.3593553862032982
Iteration 70, Training loss = 0.3443154807274158
Iteration 80, Training loss = 0.33438521466003013
Iteration 90, Training loss = 0.3232835756184963
Iteration 100, Training loss = 0.31714734277473045
Iteration 110, Training loss = 0.3142681127557388
Iteration 120, Training loss = 0.3047180288972763
Iteration 130, Training loss = 0.3017625560840735
Iteration 140, Training loss = 0.2988001456340918
Iteration 150, Training loss = 0.2961826960627849
Iteration 160, Training loss = 0.2938300732236642
Iteration 170, Training loss = 0.29259828478097916
Iteration 180, Training loss = 0.2903189563120787
Iteration 190, Training loss = 0.2866043478537064
Model training time: 33.45878720283508
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.771978607831093
Iteration 10, Training loss = 0.26012244261801243
Iteration 20, Training loss = 0.25231791430941
Iteration 30, Training loss = 0.2446076633551946
Iteration 40, Training loss = 0.24354068820293134
Iteration 50, Training loss = 0.23875298164784908
Iteration 60, Training loss = 0.23553462326526642
Iteration 70, Training loss = 0.23191964569000098
Iteration 80, Training loss = 0.2261825601498668
Iteration 90, Training loss = 0.22263410300589526
Iteration 100, Training loss = 0.21954533722824776
Iteration 110, Training loss = 0.21790803825626007
Iteration 120, Training loss = 0.2176056197629525
Iteration 130, Training loss = 0.21388388181535098
Iteration 140, Training loss = 0.21276164004722467
Iteration 150, Training loss = 0.2118759390253287
Iteration 160, Training loss = 0.20988139108969614
Iteration 170, Training loss = 0.20716543610279375
Iteration 180, Training loss = 0.20816130448992437
Iteration 190, Training loss = 0.2034777868539095
Iteration 200, Training loss = 0.20357099956331345
Iteration 210, Training loss = 0.2029620110272215
Iteration 220, Training loss = 0.200590551759188
Iteration 230, Training loss = 0.20045923011807296
Iteration 240, Training loss = 0.200051801565748
Iteration 250, Training loss = 0.20044198503287938
Iteration 260, Training loss = 0.198188130528881
Iteration 270, Training loss = 0.19783525570080832
Iteration 280, Training loss = 0.2000401631140938
Iteration 290, Training loss = 0.1976383074115102
Model training time: 56.36516046524048
Device: cuda
Iteration 0, Training loss = 0.889272638811515
Iteration 10, Training loss = 0.25953646577321565
Iteration 20, Training loss = 0.24400512114740336
Iteration 30, Training loss = 0.2379449137414877
Iteration 40, Training loss = 0.2330944540982063
Iteration 50, Training loss = 0.22880278520572644
Iteration 60, Training loss = 0.22547468543052673
Iteration 70, Training loss = 0.2231025522431502
Iteration 80, Training loss = 0.2215281596693855
Iteration 90, Training loss = 0.2177248286943023
Iteration 100, Training loss = 0.2176770709025172
Iteration 110, Training loss = 0.21517343253183824
Iteration 120, Training loss = 0.21219813264906406
Iteration 130, Training loss = 0.2129784174120197
Iteration 140, Training loss = 0.2120399083942175
Iteration 150, Training loss = 0.20978635076719981
Iteration 160, Training loss = 0.20939604228792283
Iteration 170, Training loss = 0.20636941564197725
Iteration 180, Training loss = 0.20734935382810923
Iteration 190, Training loss = 0.20490241881746513
Iteration 200, Training loss = 0.20356911055457133
Iteration 210, Training loss = 0.20359989828788316
Iteration 220, Training loss = 0.20166629352248633
Iteration 230, Training loss = 0.2016166109018601
Iteration 240, Training loss = 0.20215621812698933
Iteration 250, Training loss = 0.20051943689871293
Iteration 260, Training loss = 0.20104744872794703
Iteration 270, Training loss = 0.19916402706159994
Iteration 280, Training loss = 0.2002202868461609
Iteration 290, Training loss = 0.19846632310117668
Model training time: 57.003145694732666
Device: cuda
Iteration 0, Training loss = 0.8053983300924301
Iteration 10, Training loss = 0.25984382915955323
Iteration 20, Training loss = 0.2505117690620514
Iteration 30, Training loss = 0.24376467830286577
Iteration 40, Training loss = 0.23788320258832896
Iteration 50, Training loss = 0.23370611338088146
Iteration 60, Training loss = 0.22977336997596118
Iteration 70, Training loss = 0.2273596552415536
Iteration 80, Training loss = 0.22339756428622282
Iteration 90, Training loss = 0.2206208878554977
Iteration 100, Training loss = 0.21733001784349865
Iteration 110, Training loss = 0.21287329509281194
Iteration 120, Training loss = 0.21065984143374059
Iteration 130, Training loss = 0.20844129365510666
Iteration 140, Training loss = 0.20834395275093043
Iteration 150, Training loss = 0.20697886038285035
Iteration 160, Training loss = 0.20709886401891708
Iteration 170, Training loss = 0.20259772398724005
Iteration 180, Training loss = 0.203090666506726
Iteration 190, Training loss = 0.20376366441353008
Iteration 200, Training loss = 0.20153225693278587
Iteration 210, Training loss = 0.20071381158553636
Iteration 220, Training loss = 0.1999447141559078
Iteration 230, Training loss = 0.19854544239261976
Iteration 240, Training loss = 0.20046588864464027
Iteration 250, Training loss = 0.19898047090436405
Iteration 260, Training loss = 0.19801512833398122
Iteration 270, Training loss = 0.19709042359430057
Iteration 280, Training loss = 0.19614677439228848
Iteration 290, Training loss = 0.19511796763310066
Model training time: 57.19050860404968
Device: cuda
Iteration 0, Training loss = 0.8867938730579156
Iteration 10, Training loss = 0.25932592167877233
Iteration 20, Training loss = 0.25137647499258703
Iteration 30, Training loss = 0.24358743636940533
Iteration 40, Training loss = 0.24187811062886164
Iteration 50, Training loss = 0.239491585498819
Iteration 60, Training loss = 0.2353725302964449
Iteration 70, Training loss = 0.23302090125015149
Iteration 80, Training loss = 0.23035886138677597
Iteration 90, Training loss = 0.2309174291932812
Iteration 100, Training loss = 0.22897716511327487
Iteration 110, Training loss = 0.22967564959365588
Iteration 120, Training loss = 0.2264228667347477
Iteration 130, Training loss = 0.22585874838897815
Iteration 140, Training loss = 0.22329364545070207
Iteration 150, Training loss = 0.22313834390101525
Iteration 160, Training loss = 0.21934157817696148
Iteration 170, Training loss = 0.2175547627445597
Iteration 180, Training loss = 0.21480401361790988
Iteration 190, Training loss = 0.21287331450730562
Iteration 200, Training loss = 0.21182736869041735
Iteration 210, Training loss = 0.2097794089036492
Iteration 220, Training loss = 0.2128374625283938
Iteration 230, Training loss = 0.20648117592701545
Iteration 240, Training loss = 0.2057724455371499
Iteration 250, Training loss = 0.20570328717048353
Iteration 260, Training loss = 0.20563587689628968
Iteration 270, Training loss = 0.20286204809179673
Iteration 280, Training loss = 0.2028085748450114
Iteration 290, Training loss = 0.20516430436132047
Model training time: 56.13229560852051
Device: cuda
Iteration 0, Training loss = 1.0663935393095016
Iteration 10, Training loss = 0.25863438500807834
Iteration 20, Training loss = 0.24818523443089083
Iteration 30, Training loss = 0.24326192100460714
Iteration 40, Training loss = 0.2406814947294501
Iteration 50, Training loss = 0.2373254978312896
Iteration 60, Training loss = 0.23278479056003
Iteration 70, Training loss = 0.229136734126279
Iteration 80, Training loss = 0.22669491744958437
Iteration 90, Training loss = 0.22299907485452983
Iteration 100, Training loss = 0.22110817982600287
Iteration 110, Training loss = 0.2168707479364597
Iteration 120, Training loss = 0.21577060394562209
Iteration 130, Training loss = 0.21414773896909678
Iteration 140, Training loss = 0.21552696594825158
Iteration 150, Training loss = 0.21266505723962417
Iteration 160, Training loss = 0.21198988168572003
Iteration 170, Training loss = 0.21239704645883578
Iteration 180, Training loss = 0.20857490004541782
Iteration 190, Training loss = 0.20856514637573406
Iteration 200, Training loss = 0.20829953597142145
Iteration 210, Training loss = 0.20761540044958776
Iteration 220, Training loss = 0.20618726083865532
Iteration 230, Training loss = 0.2054680259898305
Iteration 240, Training loss = 0.20439779536368755
Iteration 250, Training loss = 0.20395010981995326
Iteration 260, Training loss = 0.20604334528056475
Iteration 270, Training loss = 0.20419394955612147
Iteration 280, Training loss = 0.20343919148525366
Iteration 290, Training loss = 0.20255860714958265
Model training time: 57.13937520980835
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0375137449457095
Iteration 10, Training loss = 0.9676569253206253
Iteration 20, Training loss = 0.9049467794024028
Iteration 30, Training loss = 0.7355597603779572
Iteration 40, Training loss = 0.48189786133857876
Iteration 50, Training loss = 0.38339834574323434
Iteration 60, Training loss = 0.359362965879532
Iteration 70, Training loss = 0.3429411682658471
Iteration 80, Training loss = 0.3307275496996366
Iteration 90, Training loss = 0.32158624036954
Iteration 100, Training loss = 0.31532521388278556
Iteration 110, Training loss = 0.30962222957840335
Iteration 120, Training loss = 0.30693965261945355
Iteration 130, Training loss = 0.30149280767028147
Iteration 140, Training loss = 0.2995260130040921
Iteration 150, Training loss = 0.296686031210881
Iteration 160, Training loss = 0.2945917287411598
Iteration 170, Training loss = 0.2933032860836157
Iteration 180, Training loss = 0.2914962044988687
Iteration 190, Training loss = 0.2904843924423823
Iteration 200, Training loss = 0.2891998587606045
Iteration 210, Training loss = 0.28699066332326484
Iteration 220, Training loss = 0.2866100753442599
Iteration 230, Training loss = 0.2859724947752861
Iteration 240, Training loss = 0.28411145579929536
Iteration 250, Training loss = 0.2823298152249593
Iteration 260, Training loss = 0.2836229077611978
Iteration 270, Training loss = 0.2802339018537448
Iteration 280, Training loss = 0.2815693744386618
Iteration 290, Training loss = 0.2814151965654813
Model training time: 49.896430253982544
Device: cuda
Iteration 0, Training loss = 1.0155531557706685
Iteration 10, Training loss = 0.9843025164535413
Iteration 20, Training loss = 0.9589558811141894
Iteration 30, Training loss = 0.8672583401203156
Iteration 40, Training loss = 0.6471124939047374
Iteration 50, Training loss = 0.42954891089063424
Iteration 60, Training loss = 0.36348194600297856
Iteration 70, Training loss = 0.3464779191865371
Iteration 80, Training loss = 0.3366318058509093
Iteration 90, Training loss = 0.3290603245393588
Iteration 100, Training loss = 0.31822639918671203
Iteration 110, Training loss = 0.3133225075613994
Iteration 120, Training loss = 0.3094221127147858
Iteration 130, Training loss = 0.3087330004916741
Iteration 140, Training loss = 0.30367742536159664
Iteration 150, Training loss = 0.2996344742580102
Iteration 160, Training loss = 0.29781031207396436
Iteration 170, Training loss = 0.2961296343172972
Iteration 180, Training loss = 0.29539251499451125
Iteration 190, Training loss = 0.2935755467758729
Iteration 200, Training loss = 0.2944557120880255
Iteration 210, Training loss = 0.2895501149961582
Iteration 220, Training loss = 0.2888296620490459
Iteration 230, Training loss = 0.28900810703635216
Iteration 240, Training loss = 0.2873366532417444
Iteration 250, Training loss = 0.285470004265125
Iteration 260, Training loss = 0.2849492102574844
Iteration 270, Training loss = 0.28394762311990446
Iteration 280, Training loss = 0.2825728545968349
Iteration 290, Training loss = 0.28075312085163134
Model training time: 50.20497751235962
Device: cuda
Iteration 0, Training loss = 1.0659453645348549
Iteration 10, Training loss = 0.9533399796256652
Iteration 20, Training loss = 0.8563624047316037
Iteration 30, Training loss = 0.6567336469888687
Iteration 40, Training loss = 0.4625582326776706
Iteration 50, Training loss = 0.39064013642760426
Iteration 60, Training loss = 0.3668589470191644
Iteration 70, Training loss = 0.35008884937717366
Iteration 80, Training loss = 0.34020188307532895
Iteration 90, Training loss = 0.32997058074061686
Iteration 100, Training loss = 0.3227336078595657
Iteration 110, Training loss = 0.31583781425769514
Iteration 120, Training loss = 0.31310319184110713
Iteration 130, Training loss = 0.3085093779059557
Iteration 140, Training loss = 0.30646568808991176
Iteration 150, Training loss = 0.3030440972114985
Iteration 160, Training loss = 0.2980267836783941
Iteration 170, Training loss = 0.2954674675487555
Iteration 180, Training loss = 0.29195389710366726
Iteration 190, Training loss = 0.29077659962842095
Iteration 200, Training loss = 0.2884469194194445
Iteration 210, Training loss = 0.28493689688352436
Iteration 220, Training loss = 0.28316339979378075
Iteration 230, Training loss = 0.2810620821725864
Iteration 240, Training loss = 0.28053829389122814
Iteration 250, Training loss = 0.2776876696600364
Iteration 260, Training loss = 0.27672555956702966
Iteration 270, Training loss = 0.27420692876554453
Iteration 280, Training loss = 0.27453384032616246
Iteration 290, Training loss = 0.2729844352087149
Model training time: 51.42973494529724
Device: cuda
Iteration 0, Training loss = 1.1319876095423331
Iteration 10, Training loss = 0.9720931557508615
Iteration 20, Training loss = 0.929535123018118
Iteration 30, Training loss = 0.8118582522639861
Iteration 40, Training loss = 0.580702181905508
Iteration 50, Training loss = 0.4281725347615205
Iteration 60, Training loss = 0.3840004222897383
Iteration 70, Training loss = 0.3601709632919385
Iteration 80, Training loss = 0.3471905326900574
Iteration 90, Training loss = 0.3323005404896461
Iteration 100, Training loss = 0.32538360887422013
Iteration 110, Training loss = 0.31701764621986794
Iteration 120, Training loss = 0.3123987979040696
Iteration 130, Training loss = 0.30696414253459525
Iteration 140, Training loss = 0.3025616188175403
Iteration 150, Training loss = 0.30010104996080583
Iteration 160, Training loss = 0.29724490513595253
Iteration 170, Training loss = 0.2953041340582646
Iteration 180, Training loss = 0.29279259520654494
Iteration 190, Training loss = 0.2898723382789355
Iteration 200, Training loss = 0.2870070193536006
Iteration 210, Training loss = 0.28668756439135623
Iteration 220, Training loss = 0.2836051746629752
Iteration 230, Training loss = 0.2823622706704415
Iteration 240, Training loss = 0.28141651302576065
Iteration 250, Training loss = 0.27960130386054516
Iteration 260, Training loss = 0.28061104809435516
Iteration 270, Training loss = 0.27696513599501205
Iteration 280, Training loss = 0.277123801696759
Iteration 290, Training loss = 0.2754623885624684
Model training time: 50.212607860565186
Device: cuda
Iteration 0, Training loss = 1.0331396088004112
Iteration 10, Training loss = 0.9998541434223835
Iteration 20, Training loss = 0.9811385560494202
Iteration 30, Training loss = 0.9527650635976058
Iteration 40, Training loss = 0.8872718828228804
Iteration 50, Training loss = 0.7222504526949846
Iteration 60, Training loss = 0.4803350539161609
Iteration 70, Training loss = 0.38506113866773933
Iteration 80, Training loss = 0.35753759225973714
Iteration 90, Training loss = 0.34510987524229747
Iteration 100, Training loss = 0.32955310636988056
Iteration 110, Training loss = 0.32738535965864474
Iteration 120, Training loss = 0.3153667790958515
Iteration 130, Training loss = 0.311425772567208
Iteration 140, Training loss = 0.3067899076984479
Iteration 150, Training loss = 0.30762738247330373
Iteration 160, Training loss = 0.30091361271647304
Iteration 170, Training loss = 0.30487344399667704
Iteration 180, Training loss = 0.29902482663209623
Iteration 190, Training loss = 0.2968304853599805
Iteration 200, Training loss = 0.29415380395948887
Iteration 210, Training loss = 0.29508864936920315
Iteration 220, Training loss = 0.29174791734952193
Iteration 230, Training loss = 0.29167109097425753
Iteration 240, Training loss = 0.28969395619172317
Iteration 250, Training loss = 0.29091102715868217
Iteration 260, Training loss = 0.28626764368695706
Iteration 270, Training loss = 0.2866827124873033
Iteration 280, Training loss = 0.2872422499438891
Iteration 290, Training loss = 0.2852950507345108
Model training time: 49.84246253967285
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8349700770699061
Iteration 10, Training loss = 0.27229610099815404
Iteration 20, Training loss = 0.2592082484983481
Iteration 30, Training loss = 0.25255919333833915
Iteration 40, Training loss = 0.24830917727488738
Iteration 50, Training loss = 0.24799047926297554
Iteration 60, Training loss = 0.2452052253513382
Iteration 70, Training loss = 0.24196320199049437
Iteration 80, Training loss = 0.23952766550848117
Iteration 90, Training loss = 0.23823904260419881
Iteration 100, Training loss = 0.2374063913638775
Iteration 110, Training loss = 0.23382828456278032
Iteration 120, Training loss = 0.23380406910123733
Iteration 130, Training loss = 0.23264551162719727
Iteration 140, Training loss = 0.23058826476335526
Iteration 150, Training loss = 0.23291976363039935
Iteration 160, Training loss = 0.22919491563852018
Iteration 170, Training loss = 0.22745567302291209
Iteration 180, Training loss = 0.226327450802693
Iteration 190, Training loss = 0.22428737279887384
Iteration 200, Training loss = 0.2223454167923102
Iteration 210, Training loss = 0.21960366567453513
Iteration 220, Training loss = 0.21901297533454803
Iteration 230, Training loss = 0.21969727555719706
Iteration 240, Training loss = 0.2186186880303117
Iteration 250, Training loss = 0.21579798497259617
Iteration 260, Training loss = 0.21589189187551922
Iteration 270, Training loss = 0.21513133044712818
Iteration 280, Training loss = 0.21252723421471623
Iteration 290, Training loss = 0.21193895682405967
Model training time: 56.33577609062195
Device: cuda
Iteration 0, Training loss = 0.8380021068912286
Iteration 10, Training loss = 0.25877650927465695
Iteration 20, Training loss = 0.24946595599445012
Iteration 30, Training loss = 0.24555021467117163
Iteration 40, Training loss = 0.24164859792934015
Iteration 50, Training loss = 0.2415197086162292
Iteration 60, Training loss = 0.23831236491409633
Iteration 70, Training loss = 0.23461206171375054
Iteration 80, Training loss = 0.23141379597095343
Iteration 90, Training loss = 0.23037392560106057
Iteration 100, Training loss = 0.2282679042277428
Iteration 110, Training loss = 0.2262191133430371
Iteration 120, Training loss = 0.2248596020329457
Iteration 130, Training loss = 0.22333410790619942
Iteration 140, Training loss = 0.22105189061795288
Iteration 150, Training loss = 0.2204180142054191
Iteration 160, Training loss = 0.2206928301602602
Iteration 170, Training loss = 0.21873842208431318
Iteration 180, Training loss = 0.21841848011200243
Iteration 190, Training loss = 0.2170877498932756
Iteration 200, Training loss = 0.2152692497922824
Iteration 210, Training loss = 0.21638852479652718
Iteration 220, Training loss = 0.21737279979368815
Iteration 230, Training loss = 0.2148324682448919
Iteration 240, Training loss = 0.21334468798998457
Iteration 250, Training loss = 0.21363777896532646
Iteration 260, Training loss = 0.21726375087522543
Iteration 270, Training loss = 0.21211835720504707
Iteration 280, Training loss = 0.2140044033384094
Iteration 290, Training loss = 0.21255725457404667
Model training time: 60.38957238197327
Device: cuda
Iteration 0, Training loss = 0.8808252180998142
Iteration 10, Training loss = 0.2708033142754665
Iteration 20, Training loss = 0.26057856587263256
Iteration 30, Training loss = 0.2518061400892643
Iteration 40, Training loss = 0.24665426276624203
Iteration 50, Training loss = 0.24527814626120603
Iteration 60, Training loss = 0.2431923599483875
Iteration 70, Training loss = 0.24014885171961325
Iteration 80, Training loss = 0.2375866612419486
Iteration 90, Training loss = 0.23725480385697806
Iteration 100, Training loss = 0.23516767257108137
Iteration 110, Training loss = 0.23306935015492714
Iteration 120, Training loss = 0.231981795711013
Iteration 130, Training loss = 0.23376019270374224
Iteration 140, Training loss = 0.23165690540694273
Iteration 150, Training loss = 0.2313862144947052
Iteration 160, Training loss = 0.22996901004360273
Iteration 170, Training loss = 0.22780845113671744
Iteration 180, Training loss = 0.2286471533947266
Iteration 190, Training loss = 0.22622779656488162
Iteration 200, Training loss = 0.22837980263508284
Iteration 210, Training loss = 0.22580488198078597
Iteration 220, Training loss = 0.2255461887241556
Iteration 230, Training loss = 0.22427341936585996
Iteration 240, Training loss = 0.22490980461812937
Iteration 250, Training loss = 0.22310239384667233
Iteration 260, Training loss = 0.22306342609226704
Iteration 270, Training loss = 0.22283363471237513
Iteration 280, Training loss = 0.22217114742558736
Iteration 290, Training loss = 0.22112915607599112
Model training time: 58.618019580841064
Device: cuda
Iteration 0, Training loss = 0.875909221286957
Iteration 10, Training loss = 0.2643500328637086
Iteration 20, Training loss = 0.2551017234531733
Iteration 30, Training loss = 0.2480975600102773
Iteration 40, Training loss = 0.24433610316079396
Iteration 50, Training loss = 0.24018195500740638
Iteration 60, Training loss = 0.23750458242228398
Iteration 70, Training loss = 0.23442402109503746
Iteration 80, Training loss = 0.23295252540936837
Iteration 90, Training loss = 0.23160075023770332
Iteration 100, Training loss = 0.22903735684947327
Iteration 110, Training loss = 0.22842866791268954
Iteration 120, Training loss = 0.225262380634936
Iteration 130, Training loss = 0.2234186174777838
Iteration 140, Training loss = 0.2224988964601205
Iteration 150, Training loss = 0.22058930025937465
Iteration 160, Training loss = 0.22027611044737008
Iteration 170, Training loss = 0.21864954862170494
Iteration 180, Training loss = 0.21776389187345138
Iteration 190, Training loss = 0.21772029272352272
Iteration 200, Training loss = 0.21571981469885662
Iteration 210, Training loss = 0.2159942126331421
Iteration 220, Training loss = 0.21296599478675768
Iteration 230, Training loss = 0.21209061797708273
Iteration 240, Training loss = 0.21216374556892192
Iteration 250, Training loss = 0.21068537428688544
Iteration 260, Training loss = 0.21095134210414612
Iteration 270, Training loss = 0.2119826776190446
Iteration 280, Training loss = 0.20935655213319337
Iteration 290, Training loss = 0.2097924014983269
Model training time: 56.656861543655396
Device: cuda
Iteration 0, Training loss = 0.8582640794607309
Iteration 10, Training loss = 0.2631844482026421
Iteration 20, Training loss = 0.25150690886836785
Iteration 30, Training loss = 0.2465436073163381
Iteration 40, Training loss = 0.2433430369083698
Iteration 50, Training loss = 0.24030479050886172
Iteration 60, Training loss = 0.23710881359875202
Iteration 70, Training loss = 0.23565660789608955
Iteration 80, Training loss = 0.23100962002690023
Iteration 90, Training loss = 0.2290965234144376
Iteration 100, Training loss = 0.2276994467784579
Iteration 110, Training loss = 0.2266867306943123
Iteration 120, Training loss = 0.22424499332331693
Iteration 130, Training loss = 0.22258350253105164
Iteration 140, Training loss = 0.21873384026380685
Iteration 150, Training loss = 0.21676227264106274
Iteration 160, Training loss = 0.21807802898379472
Iteration 170, Training loss = 0.21478791950413814
Iteration 180, Training loss = 0.2145456327841832
Iteration 190, Training loss = 0.21230027093910253
Iteration 200, Training loss = 0.21147670618330056
Iteration 210, Training loss = 0.21204244760939708
Iteration 220, Training loss = 0.21071532748353022
Iteration 230, Training loss = 0.21130694126567015
Iteration 240, Training loss = 0.20927414219253337
Iteration 250, Training loss = 0.21068322307501847
Iteration 260, Training loss = 0.20983494760898444
Iteration 270, Training loss = 0.20920070805228674
Iteration 280, Training loss = 0.20699142858099479
Iteration 290, Training loss = 0.20760287738476807
Model training time: 57.072918176651
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0856964353185434
Iteration 10, Training loss = 0.992643049130073
Iteration 20, Training loss = 0.9800822740563979
Iteration 30, Training loss = 0.9535657574351017
Iteration 40, Training loss = 0.8959235852727523
Iteration 50, Training loss = 0.7662712361377019
Iteration 60, Training loss = 0.5952310906006739
Iteration 70, Training loss = 0.4634047659257284
Iteration 80, Training loss = 0.40828867715138656
Iteration 90, Training loss = 0.37675929427720034
Iteration 100, Training loss = 0.3590232340189127
Iteration 110, Training loss = 0.34364602671792877
Iteration 120, Training loss = 0.3332225519877214
Iteration 130, Training loss = 0.32264543131280404
Iteration 140, Training loss = 0.32115015831704324
Iteration 150, Training loss = 0.31289726911255944
Iteration 160, Training loss = 0.30776823899493766
Iteration 170, Training loss = 0.30589323304593563
Iteration 180, Training loss = 0.3010776454152969
Iteration 190, Training loss = 0.2986928946696795
Iteration 200, Training loss = 0.29553982343238133
Iteration 210, Training loss = 0.2962912255181716
Iteration 220, Training loss = 0.2915664462802502
Iteration 230, Training loss = 0.29047943422427547
Iteration 240, Training loss = 0.28825359533612543
Iteration 250, Training loss = 0.2870560367233478
Iteration 260, Training loss = 0.2865653166977259
Iteration 270, Training loss = 0.2862765366354814
Iteration 280, Training loss = 0.2839590028788035
Iteration 290, Training loss = 0.2829134097466102
Model training time: 51.798731565475464
Device: cuda
Iteration 0, Training loss = 1.0197481748003225
Iteration 10, Training loss = 0.9881487471552995
Iteration 20, Training loss = 0.9639278134474387
Iteration 30, Training loss = 0.8904369278595998
Iteration 40, Training loss = 0.7236772374464915
Iteration 50, Training loss = 0.5115632419116222
Iteration 60, Training loss = 0.4130275137722492
Iteration 70, Training loss = 0.38100249735781777
Iteration 80, Training loss = 0.35952999700720495
Iteration 90, Training loss = 0.3447072637768892
Iteration 100, Training loss = 0.33265117928385735
Iteration 110, Training loss = 0.3237034807411524
Iteration 120, Training loss = 0.31803843622597366
Iteration 130, Training loss = 0.3113823116112214
Iteration 140, Training loss = 0.3086710566511521
Iteration 150, Training loss = 0.3041445506879917
Iteration 160, Training loss = 0.3023532872589735
Iteration 170, Training loss = 0.3008773789669459
Iteration 180, Training loss = 0.29643366313897646
Iteration 190, Training loss = 0.2960087496500749
Iteration 200, Training loss = 0.2913866550303422
Iteration 210, Training loss = 0.2904570148541377
Iteration 220, Training loss = 0.2885275309762129
Iteration 230, Training loss = 0.28821906132193714
Iteration 240, Training loss = 0.28548201675025314
Iteration 250, Training loss = 0.28352757822722197
Iteration 260, Training loss = 0.28412902197585654
Iteration 270, Training loss = 0.2840751310667166
Iteration 280, Training loss = 0.2818392612613164
Iteration 290, Training loss = 0.2787628692503159
Model training time: 53.21034336090088
Device: cuda
Iteration 0, Training loss = 1.0194181094949062
Iteration 10, Training loss = 0.9552485659145392
Iteration 20, Training loss = 0.8803293739373867
Iteration 30, Training loss = 0.6867799982428551
Iteration 40, Training loss = 0.46418560238984913
Iteration 50, Training loss = 0.3858026693073603
Iteration 60, Training loss = 0.3556654484799275
Iteration 70, Training loss = 0.3390525265668447
Iteration 80, Training loss = 0.3274499033219539
Iteration 90, Training loss = 0.3164610941536151
Iteration 100, Training loss = 0.3095836339948269
Iteration 110, Training loss = 0.303454322167314
Iteration 120, Training loss = 0.29943216348496765
Iteration 130, Training loss = 0.2950869192584203
Iteration 140, Training loss = 0.2919589511763591
Iteration 150, Training loss = 0.28936470156678784
Iteration 160, Training loss = 0.28662688471376896
Iteration 170, Training loss = 0.2861999472459921
Iteration 180, Training loss = 0.28219483663829475
Iteration 190, Training loss = 0.28227198109603846
Iteration 200, Training loss = 0.27836928638414693
Iteration 210, Training loss = 0.2785619729413436
Iteration 220, Training loss = 0.2771646692775763
Iteration 230, Training loss = 0.2756483435917359
Iteration 240, Training loss = 0.27556289460223454
Iteration 250, Training loss = 0.27259675026513064
Iteration 260, Training loss = 0.2761149307569632
Iteration 270, Training loss = 0.2708642856719402
Iteration 280, Training loss = 0.27070199583585447
Iteration 290, Training loss = 0.26926857319015723
Model training time: 51.585034132003784
Device: cuda
Iteration 0, Training loss = 1.016710185087644
Iteration 10, Training loss = 1.0001066544881234
Iteration 20, Training loss = 0.9708144779388721
Iteration 30, Training loss = 0.92423115670681
Iteration 40, Training loss = 0.7912709816144063
Iteration 50, Training loss = 0.5790426888718054
Iteration 60, Training loss = 0.43317953726420033
Iteration 70, Training loss = 0.3856068459840921
Iteration 80, Training loss = 0.364162187020366
Iteration 90, Training loss = 0.34833059875437844
Iteration 100, Training loss = 0.3380135277716013
Iteration 110, Training loss = 0.32756258361041546
Iteration 120, Training loss = 0.3207307072499624
Iteration 130, Training loss = 0.31484470029289907
Iteration 140, Training loss = 0.3086363203250445
Iteration 150, Training loss = 0.3062380575216733
Iteration 160, Training loss = 0.2993091330505334
Iteration 170, Training loss = 0.29631189428842986
Iteration 180, Training loss = 0.2954436193865079
Iteration 190, Training loss = 0.29351546386113536
Iteration 200, Training loss = 0.2892225318803237
Iteration 210, Training loss = 0.286841195793106
Iteration 220, Training loss = 0.28575974760147244
Iteration 230, Training loss = 0.2840222249237391
Iteration 240, Training loss = 0.28228478133678436
Iteration 250, Training loss = 0.2848290422788033
Iteration 260, Training loss = 0.2783574936195062
Iteration 270, Training loss = 0.2776805894592634
Iteration 280, Training loss = 0.2758521933395129
Iteration 290, Training loss = 0.2743220893809429
Model training time: 50.2558114528656
Device: cuda
Iteration 0, Training loss = 1.159634672678434
Iteration 10, Training loss = 1.0012715309858322
Iteration 20, Training loss = 0.9880596685868043
Iteration 30, Training loss = 0.9736915110395505
Iteration 40, Training loss = 0.9301451026247098
Iteration 50, Training loss = 0.808528485493018
Iteration 60, Training loss = 0.5886506985586423
Iteration 70, Training loss = 0.4346142310935717
Iteration 80, Training loss = 0.39173149919280636
Iteration 90, Training loss = 0.36626946080762607
Iteration 100, Training loss = 0.3487337903621105
Iteration 110, Training loss = 0.3371550612724744
Iteration 120, Training loss = 0.3292164129133408
Iteration 130, Training loss = 0.3206965812983421
Iteration 140, Training loss = 0.31472154577764183
Iteration 150, Training loss = 0.3107061047966664
Iteration 160, Training loss = 0.3108923045488504
Iteration 170, Training loss = 0.3057285318007836
Iteration 180, Training loss = 0.3034700875958571
Iteration 190, Training loss = 0.30095177802902
Iteration 200, Training loss = 0.3000597107009246
Iteration 210, Training loss = 0.29863312009435433
Iteration 220, Training loss = 0.29502182024029583
Iteration 230, Training loss = 0.2932106452779128
Iteration 240, Training loss = 0.2925320971184052
Iteration 250, Training loss = 0.29084961087657857
Iteration 260, Training loss = 0.2886669940959949
Iteration 270, Training loss = 0.28675288119568276
Iteration 280, Training loss = 0.28562815129183805
Iteration 290, Training loss = 0.28387374416566813
Model training time: 49.698049783706665
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8638306182737534
Iteration 10, Training loss = 0.2658532148656937
Iteration 20, Training loss = 0.2538742163720039
Iteration 30, Training loss = 0.24712437806794277
Iteration 40, Training loss = 0.24116861089490926
Iteration 50, Training loss = 0.23601505962701944
Iteration 60, Training loss = 0.23271996191201302
Iteration 70, Training loss = 0.22733324536910424
Iteration 80, Training loss = 0.22350546201834312
Iteration 90, Training loss = 0.22072225314779922
Iteration 100, Training loss = 0.21853789773124915
Iteration 110, Training loss = 0.21644621714949608
Iteration 120, Training loss = 0.21441042537872607
Iteration 130, Training loss = 0.21209864827016225
Iteration 140, Training loss = 0.21229170900411332
Iteration 150, Training loss = 0.21008849717103517
Iteration 160, Training loss = 0.20896997632315525
Iteration 170, Training loss = 0.20892455299886373
Iteration 180, Training loss = 0.2073297186109882
Iteration 190, Training loss = 0.20689197923414981
Iteration 200, Training loss = 0.20685221799291098
Iteration 210, Training loss = 0.20578485985214895
Iteration 220, Training loss = 0.2060234176997955
Iteration 230, Training loss = 0.2061476533372815
Iteration 240, Training loss = 0.20413902731469044
Iteration 250, Training loss = 0.20496459434238765
Iteration 260, Training loss = 0.20332717100301614
Iteration 270, Training loss = 0.20450969878584146
Iteration 280, Training loss = 0.2036426731456931
Iteration 290, Training loss = 0.20260509017568368
Model training time: 56.18269109725952
Device: cuda
Iteration 0, Training loss = 0.9778812332795217
Iteration 10, Training loss = 0.26431403481043303
Iteration 20, Training loss = 0.2518099110860091
Iteration 30, Training loss = 0.24727863617814505
Iteration 40, Training loss = 0.2440375811778582
Iteration 50, Training loss = 0.24440680315288213
Iteration 60, Training loss = 0.24158250268262166
Iteration 70, Training loss = 0.24079622156344926
Iteration 80, Training loss = 0.2385018298115868
Iteration 90, Training loss = 0.2364951021109636
Iteration 100, Training loss = 0.2342755778764303
Iteration 110, Training loss = 0.23300734678140053
Iteration 120, Training loss = 0.2317834009344761
Iteration 130, Training loss = 0.2303532145630855
Iteration 140, Training loss = 0.2268547907901498
Iteration 150, Training loss = 0.22554850614128205
Iteration 160, Training loss = 0.2211635447322176
Iteration 170, Training loss = 0.21903076101667607
Iteration 180, Training loss = 0.21843974177653974
Iteration 190, Training loss = 0.21681382385297462
Iteration 200, Training loss = 0.2143082316391743
Iteration 210, Training loss = 0.21267539735596913
Iteration 220, Training loss = 0.21344879248107856
Iteration 230, Training loss = 0.21183724156939066
Iteration 240, Training loss = 0.20989283847694212
Iteration 250, Training loss = 0.21065759243300328
Iteration 260, Training loss = 0.20865532297354478
Iteration 270, Training loss = 0.21035106623402008
Iteration 280, Training loss = 0.20741089361791426
Iteration 290, Training loss = 0.20731642541403955
Model training time: 56.10177779197693
Device: cuda
Iteration 0, Training loss = 0.9801144875012912
Iteration 10, Training loss = 0.2673139934642957
Iteration 20, Training loss = 0.25432402139099747
Iteration 30, Training loss = 0.24827493297365996
Iteration 40, Training loss = 0.24226997814212853
Iteration 50, Training loss = 0.23558306608062524
Iteration 60, Training loss = 0.22887151416104573
Iteration 70, Training loss = 0.2271235166833951
Iteration 80, Training loss = 0.2189907621449003
Iteration 90, Training loss = 0.2175067484092254
Iteration 100, Training loss = 0.21491623970751578
Iteration 110, Training loss = 0.21300922670903114
Iteration 120, Training loss = 0.21120175948509803
Iteration 130, Training loss = 0.21255363824848944
Iteration 140, Training loss = 0.21166433823796418
Iteration 150, Training loss = 0.20820474696273988
Iteration 160, Training loss = 0.20779419382317707
Iteration 170, Training loss = 0.20647143063923487
Iteration 180, Training loss = 0.20503907636381113
Iteration 190, Training loss = 0.20538914511696651
Iteration 200, Training loss = 0.20563825886123455
Iteration 210, Training loss = 0.20532837266532275
Iteration 220, Training loss = 0.20272249792917416
Iteration 230, Training loss = 0.2030745496113713
Iteration 240, Training loss = 0.20282414469581383
Iteration 250, Training loss = 0.20457882111748824
Iteration 260, Training loss = 0.2020631403877185
Iteration 270, Training loss = 0.20206526165398267
Iteration 280, Training loss = 0.20334583356116825
Iteration 290, Training loss = 0.20002928376197815
Model training time: 57.09854459762573
Device: cuda
Iteration 0, Training loss = 0.9027694710172139
Iteration 10, Training loss = 0.2663087263130225
Iteration 20, Training loss = 0.2537332067552667
Iteration 30, Training loss = 0.24705180819504535
Iteration 40, Training loss = 0.24324203096330166
Iteration 50, Training loss = 0.24079994871639288
Iteration 60, Training loss = 0.23667004489554808
Iteration 70, Training loss = 0.23353196651889727
Iteration 80, Training loss = 0.23176772491290018
Iteration 90, Training loss = 0.22835064206558925
Iteration 100, Training loss = 0.22594960864919883
Iteration 110, Training loss = 0.22690657834307507
Iteration 120, Training loss = 0.22139155943519795
Iteration 130, Training loss = 0.22066500489241803
Iteration 140, Training loss = 0.21906262292311743
Iteration 150, Training loss = 0.21705131805860078
Iteration 160, Training loss = 0.21620174934371159
Iteration 170, Training loss = 0.21373153836108172
Iteration 180, Training loss = 0.21246247702779678
Iteration 190, Training loss = 0.21130016245521033
Iteration 200, Training loss = 0.20999288537467903
Iteration 210, Training loss = 0.20991641175574982
Iteration 220, Training loss = 0.20574038690672472
Iteration 230, Training loss = 0.20477492448229057
Iteration 240, Training loss = 0.2061564730337033
Iteration 250, Training loss = 0.20467540206244358
Iteration 260, Training loss = 0.20416062110318586
Iteration 270, Training loss = 0.2029135826115425
Iteration 280, Training loss = 0.2046920504564276
Iteration 290, Training loss = 0.20175424504738587
Model training time: 56.94742012023926
Device: cuda
Iteration 0, Training loss = 0.8854127205335177
Iteration 10, Training loss = 0.26580974316367734
Iteration 20, Training loss = 0.24864305928349495
Iteration 30, Training loss = 0.24223477171304134
Iteration 40, Training loss = 0.23728221547431672
Iteration 50, Training loss = 0.23127595578821805
Iteration 60, Training loss = 0.22885905120235223
Iteration 70, Training loss = 0.2260431321337819
Iteration 80, Training loss = 0.22247499635872933
Iteration 90, Training loss = 0.2212038769458349
Iteration 100, Training loss = 0.21873342145520908
Iteration 110, Training loss = 0.2168832138324013
Iteration 120, Training loss = 0.21652510590278184
Iteration 130, Training loss = 0.21434037652439797
Iteration 140, Training loss = 0.21487020106556323
Iteration 150, Training loss = 0.21253943643890894
Iteration 160, Training loss = 0.2114117992683672
Iteration 170, Training loss = 0.2116240137614883
Iteration 180, Training loss = 0.2125221653483235
Iteration 190, Training loss = 0.20997275147013938
Iteration 200, Training loss = 0.2086649269152146
Iteration 210, Training loss = 0.2084730169377648
Iteration 220, Training loss = 0.20719718668036735
Iteration 230, Training loss = 0.21027050654475504
Iteration 240, Training loss = 0.20707359480170104
Iteration 250, Training loss = 0.20832887485336798
Iteration 260, Training loss = 0.20726577994915155
Iteration 270, Training loss = 0.20582539674181205
Iteration 280, Training loss = 0.20543054505609548
Iteration 290, Training loss = 0.20342802399626145
Model training time: 56.07613754272461
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 128, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9961931688281206
Iteration 10, Training loss = 0.9556716760763755
Iteration 20, Training loss = 0.8563162678709397
Iteration 30, Training loss = 0.6192380831791804
Iteration 40, Training loss = 0.42493161960290027
Iteration 50, Training loss = 0.3724918720813898
Iteration 60, Training loss = 0.3546227097797852
Iteration 70, Training loss = 0.33898261991830975
Iteration 80, Training loss = 0.32990588749257416
Iteration 90, Training loss = 0.3233206561551644
Iteration 100, Training loss = 0.3176528589370159
Iteration 110, Training loss = 0.3143778917594598
Iteration 120, Training loss = 0.31192719764434373
Iteration 130, Training loss = 0.3112671399632326
Iteration 140, Training loss = 0.3073174310131715
Iteration 150, Training loss = 0.3043196801669322
Iteration 160, Training loss = 0.30149210115464836
Iteration 170, Training loss = 0.3025622425171045
Iteration 180, Training loss = 0.29850831613517725
Iteration 190, Training loss = 0.29686866313792193
Iteration 200, Training loss = 0.29868895187973976
Iteration 210, Training loss = 0.2929164416228349
Iteration 220, Training loss = 0.2925915080480851
Iteration 230, Training loss = 0.29156038566277576
Iteration 240, Training loss = 0.29131740073745066
Iteration 250, Training loss = 0.28753685850936633
Iteration 260, Training loss = 0.28827875222151095
Iteration 270, Training loss = 0.2865014440165116
Iteration 280, Training loss = 0.28536777656811935
Iteration 290, Training loss = 0.28280207701027393
Model training time: 50.90194225311279
Device: cuda
Iteration 0, Training loss = 1.0076630791792502
Iteration 10, Training loss = 0.9691710919141769
Iteration 20, Training loss = 0.927258798136161
Iteration 30, Training loss = 0.8030122816562653
Iteration 40, Training loss = 0.5947669870578326
Iteration 50, Training loss = 0.4362327135526217
Iteration 60, Training loss = 0.3861859586949532
Iteration 70, Training loss = 0.3661608803444184
Iteration 80, Training loss = 0.34878626107596433
Iteration 90, Training loss = 0.34161941735790324
Iteration 100, Training loss = 0.3304004848289948
Iteration 110, Training loss = 0.3238949388838731
Iteration 120, Training loss = 0.3195041941049007
Iteration 130, Training loss = 0.31408125061828357
Iteration 140, Training loss = 0.31054968386888504
Iteration 150, Training loss = 0.3069420366619642
Iteration 160, Training loss = 0.303294596190636
Iteration 170, Training loss = 0.3011939577185191
Iteration 180, Training loss = 0.2982392942962738
Iteration 190, Training loss = 0.2947475566313817
Iteration 200, Training loss = 0.29246718608416045
Iteration 210, Training loss = 0.2898846586736349
Iteration 220, Training loss = 0.2900931816548109
Iteration 230, Training loss = 0.286423458216282
Iteration 240, Training loss = 0.28445558866056114
Iteration 250, Training loss = 0.28285528203615773
Iteration 260, Training loss = 0.2806792247753877
Iteration 270, Training loss = 0.2791414141941529
Iteration 280, Training loss = 0.27781856432557106
Iteration 290, Training loss = 0.2764085796303474
Model training time: 50.354262590408325
Device: cuda
Iteration 0, Training loss = 1.1002502086070867
Iteration 10, Training loss = 0.9690138709086639
Iteration 20, Training loss = 0.9153152234279193
Iteration 30, Training loss = 0.7580399490319766
Iteration 40, Training loss = 0.5055108368396759
Iteration 50, Training loss = 0.3965325094759464
Iteration 60, Training loss = 0.36661881093795484
Iteration 70, Training loss = 0.35229039263839906
Iteration 80, Training loss = 0.3373817673955972
Iteration 90, Training loss = 0.3289021183378421
Iteration 100, Training loss = 0.3231886411802127
Iteration 110, Training loss = 0.31799265240820557
Iteration 120, Training loss = 0.3127181621698233
Iteration 130, Training loss = 0.3073918464092108
Iteration 140, Training loss = 0.30501292559963006
Iteration 150, Training loss = 0.30247849526886755
Iteration 160, Training loss = 0.30371726562197393
Iteration 170, Training loss = 0.2983327916321846
Iteration 180, Training loss = 0.2954923132291207
Iteration 190, Training loss = 0.29344002458338553
Iteration 200, Training loss = 0.2917315706324119
Iteration 210, Training loss = 0.29238341032312465
Iteration 220, Training loss = 0.28861320505921656
Iteration 230, Training loss = 0.286950232890936
Iteration 240, Training loss = 0.2855104056115334
Iteration 250, Training loss = 0.28712950446284735
Iteration 260, Training loss = 0.2828403366013215
Iteration 270, Training loss = 0.28260957764891476
Iteration 280, Training loss = 0.28163364042456335
Iteration 290, Training loss = 0.28224215670846975
Model training time: 50.34598183631897
Device: cuda
Iteration 0, Training loss = 1.0818005949258804
Iteration 10, Training loss = 0.9734623306072675
Iteration 20, Training loss = 0.928411718744498
Iteration 30, Training loss = 0.795235429245692
Iteration 40, Training loss = 0.53320950957445
Iteration 50, Training loss = 0.3896578521682666
Iteration 60, Training loss = 0.3562524545078094
Iteration 70, Training loss = 0.3415115780841846
Iteration 80, Training loss = 0.3304885999801067
Iteration 90, Training loss = 0.3245659704105212
Iteration 100, Training loss = 0.32100116246594834
Iteration 110, Training loss = 0.3131371991565594
Iteration 120, Training loss = 0.31147704932552117
Iteration 130, Training loss = 0.30611014710022855
Iteration 140, Training loss = 0.3041379934606644
Iteration 150, Training loss = 0.3008426412080343
Iteration 160, Training loss = 0.2992812099938209
Iteration 170, Training loss = 0.29681137891916126
Iteration 180, Training loss = 0.29658697130015266
Iteration 190, Training loss = 0.2939979439744583
Iteration 200, Training loss = 0.29132232585778606
Iteration 210, Training loss = 0.28807985997543883
Iteration 220, Training loss = 0.2880032096917813
Iteration 230, Training loss = 0.28581721607882243
Iteration 240, Training loss = 0.28339719557418275
Iteration 250, Training loss = 0.28389575289419067
Iteration 260, Training loss = 0.2832445361866401
Iteration 270, Training loss = 0.27999213925347877
Iteration 280, Training loss = 0.2800034563988447
Iteration 290, Training loss = 0.2776334248483181
Model training time: 50.009671211242676
Device: cuda
Iteration 0, Training loss = 1.0552276458877783
Iteration 10, Training loss = 0.9743928112662755
Iteration 20, Training loss = 0.9431157175164956
Iteration 30, Training loss = 0.8492416109030063
Iteration 40, Training loss = 0.6181278592691972
Iteration 50, Training loss = 0.4230207697703288
Iteration 60, Training loss = 0.3732156160359199
Iteration 70, Training loss = 0.35474896445297277
Iteration 80, Training loss = 0.3397693937787643
Iteration 90, Training loss = 0.32953418590701544
Iteration 100, Training loss = 0.3230216968804598
Iteration 110, Training loss = 0.31745903022014177
Iteration 120, Training loss = 0.31314002736829794
Iteration 130, Training loss = 0.31011610850691795
Iteration 140, Training loss = 0.3073558152581637
Iteration 150, Training loss = 0.3061757736767714
Iteration 160, Training loss = 0.30478751487456834
Iteration 170, Training loss = 0.3027964639835633
Iteration 180, Training loss = 0.30038742764064896
Iteration 190, Training loss = 0.300672218346825
Iteration 200, Training loss = 0.2977005643053697
Iteration 210, Training loss = 0.2970383713165155
Iteration 220, Training loss = 0.29653855900351817
Iteration 230, Training loss = 0.2964500615803095
Iteration 240, Training loss = 0.2936349091334985
Iteration 250, Training loss = 0.2984128016978502
Iteration 260, Training loss = 0.29169802797528416
Iteration 270, Training loss = 0.29215098903156245
Iteration 280, Training loss = 0.2906763586573876
Iteration 290, Training loss = 0.28891673856056654
Model training time: 50.437705993652344
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0026024671701284
Iteration 10, Training loss = 0.3071316242791139
Iteration 20, Training loss = 0.26930496067954945
Iteration 30, Training loss = 0.26117207425145
Iteration 40, Training loss = 0.25767541791384035
Iteration 50, Training loss = 0.2552330671594693
Iteration 60, Training loss = 0.25382234958501965
Iteration 70, Training loss = 0.2524128286884381
Iteration 80, Training loss = 0.25097461904470736
Iteration 90, Training loss = 0.24960666837600562
Model training time: 14.27315616607666
Device: cuda
Iteration 0, Training loss = 1.0026346674332252
Iteration 10, Training loss = 0.3234323604175678
Iteration 20, Training loss = 0.27259887239107716
Iteration 30, Training loss = 0.2583248010621621
Iteration 40, Training loss = 0.2517593104678851
Iteration 50, Training loss = 0.24915855664473313
Iteration 60, Training loss = 0.24556738396103567
Iteration 70, Training loss = 0.24334261709680924
Iteration 80, Training loss = 0.24258830570257628
Iteration 90, Training loss = 0.24037606211809012
Model training time: 14.364628314971924
Device: cuda
Iteration 0, Training loss = 0.9956987316791828
Iteration 10, Training loss = 0.30101403479392713
Iteration 20, Training loss = 0.26687799480098945
Iteration 30, Training loss = 0.2593712918460369
Iteration 40, Training loss = 0.25521486739699656
Iteration 50, Training loss = 0.25264159074196446
Iteration 60, Training loss = 0.2504695691168308
Iteration 70, Training loss = 0.24780155890263045
Iteration 80, Training loss = 0.24524202197790146
Iteration 90, Training loss = 0.24332708349594703
Model training time: 14.00218653678894
Device: cuda
Iteration 0, Training loss = 0.9698790013790131
Iteration 10, Training loss = 0.2964224356871385
Iteration 20, Training loss = 0.26755008216087633
Iteration 30, Training loss = 0.25917512665574366
Iteration 40, Training loss = 0.2540305509017064
Iteration 50, Training loss = 0.2505550361596621
Iteration 60, Training loss = 0.24745612161663863
Iteration 70, Training loss = 0.24484206420870927
Iteration 80, Training loss = 0.243340937850567
Iteration 90, Training loss = 0.24123290066535658
Model training time: 14.3495352268219
Device: cuda
Iteration 0, Training loss = 1.0362694091521776
Iteration 10, Training loss = 0.31065135133954197
Iteration 20, Training loss = 0.2706674090944804
Iteration 30, Training loss = 0.2587605846615938
Iteration 40, Training loss = 0.253050085157156
Iteration 50, Training loss = 0.24986046867874953
Iteration 60, Training loss = 0.24769902917054984
Iteration 70, Training loss = 0.24604260491637084
Iteration 80, Training loss = 0.24434419004962996
Iteration 90, Training loss = 0.2420164653314994
Model training time: 14.410415410995483
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.2194016312177365
Iteration 10, Training loss = 0.9858015855917563
Iteration 20, Training loss = 0.9779831113723608
Iteration 30, Training loss = 0.9712261855602264
Iteration 40, Training loss = 0.9633982353485547
Iteration 50, Training loss = 0.9539300100161479
Iteration 60, Training loss = 0.9390064523770258
Iteration 70, Training loss = 0.921029942540022
Iteration 80, Training loss = 0.8958998505885785
Iteration 90, Training loss = 0.8670300371371783
Model training time: 13.211980819702148
Device: cuda
Iteration 0, Training loss = 0.9959719811494534
Iteration 10, Training loss = 0.9557110873552469
Iteration 20, Training loss = 0.9407842583381213
Iteration 30, Training loss = 0.9244088289829401
Iteration 40, Training loss = 0.9037458736162919
Iteration 50, Training loss = 0.8791697644270383
Iteration 60, Training loss = 0.8480796045981921
Iteration 70, Training loss = 0.8107570444162076
Iteration 80, Training loss = 0.7689207517183744
Iteration 90, Training loss = 0.719600362273363
Model training time: 13.02092170715332
Device: cuda
Iteration 0, Training loss = 1.3036550191732554
Iteration 10, Training loss = 0.9985934553238062
Iteration 20, Training loss = 0.9940987940017993
Iteration 30, Training loss = 0.9876872346951411
Iteration 40, Training loss = 0.9830700835356345
Iteration 50, Training loss = 0.9754055875998277
Iteration 60, Training loss = 0.9677658310303321
Iteration 70, Training loss = 0.9578597534161347
Iteration 80, Training loss = 0.9431459961029199
Iteration 90, Training loss = 0.9267079520684022
Model training time: 12.975841999053955
Device: cuda
Iteration 0, Training loss = 1.0105303995884383
Iteration 10, Training loss = 1.000854649222814
Iteration 20, Training loss = 0.9997220795888168
Iteration 30, Training loss = 0.9964332190843729
Iteration 40, Training loss = 0.9922553060146478
Iteration 50, Training loss = 0.9886667613799756
Iteration 60, Training loss = 0.9843031408695074
Iteration 70, Training loss = 0.9807091561647562
Iteration 80, Training loss = 0.9756778398385415
Iteration 90, Training loss = 0.9688726101930325
Model training time: 13.025362730026245
Device: cuda
Iteration 0, Training loss = 1.0198916964806044
Iteration 10, Training loss = 0.9776301487134054
Iteration 20, Training loss = 0.9630246792848294
Iteration 30, Training loss = 0.948092404466409
Iteration 40, Training loss = 0.9308820022986486
Iteration 50, Training loss = 0.907411550100033
Iteration 60, Training loss = 0.8777753836833514
Iteration 70, Training loss = 0.8388308527377936
Iteration 80, Training loss = 0.7942625501981149
Iteration 90, Training loss = 0.7422827837558893
Model training time: 12.95839524269104
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9524993231663337
Iteration 10, Training loss = 0.33490025997161865
Iteration 20, Training loss = 0.2812018179549621
Iteration 30, Training loss = 0.2633972314114754
Iteration 40, Training loss = 0.25738316459151417
Iteration 50, Training loss = 0.25286801961752087
Iteration 60, Training loss = 0.25027952801722747
Iteration 70, Training loss = 0.248293965195234
Iteration 80, Training loss = 0.24633121891663626
Iteration 90, Training loss = 0.24616065506751722
Model training time: 14.26684021949768
Device: cuda
Iteration 0, Training loss = 1.019566447688983
Iteration 10, Training loss = 0.40720890863583636
Iteration 20, Training loss = 0.3157869932743219
Iteration 30, Training loss = 0.2825296005377403
Iteration 40, Training loss = 0.2688819468021393
Iteration 50, Training loss = 0.2615454182601892
Iteration 60, Training loss = 0.2571113705635071
Iteration 70, Training loss = 0.2540396492068584
Iteration 80, Training loss = 0.25291050569369244
Iteration 90, Training loss = 0.2515027861182506
Model training time: 14.139507055282593
Device: cuda
Iteration 0, Training loss = 1.029880934036695
Iteration 10, Training loss = 0.3597864250724132
Iteration 20, Training loss = 0.2910589369443747
Iteration 30, Training loss = 0.2676586273771066
Iteration 40, Training loss = 0.2584642893992938
Iteration 50, Training loss = 0.25488859758927274
Iteration 60, Training loss = 0.2524173755485278
Iteration 70, Training loss = 0.25093615227020705
Iteration 80, Training loss = 0.2488334052837812
Iteration 90, Training loss = 0.248530284429972
Model training time: 14.27956247329712
Device: cuda
Iteration 0, Training loss = 1.1014691224465003
Iteration 10, Training loss = 0.4204560506802339
Iteration 20, Training loss = 0.31850154955799764
Iteration 30, Training loss = 0.28161174574723613
Iteration 40, Training loss = 0.2668238780819453
Iteration 50, Training loss = 0.25774521180070364
Iteration 60, Training loss = 0.25288084149360657
Iteration 70, Training loss = 0.2496292611154226
Iteration 80, Training loss = 0.245717228605197
Iteration 90, Training loss = 0.2437765231499305
Model training time: 14.016870975494385
Device: cuda
Iteration 0, Training loss = 1.0439459887834697
Iteration 10, Training loss = 0.3717748640248409
Iteration 20, Training loss = 0.28365913320046204
Iteration 30, Training loss = 0.2613990172170676
Iteration 40, Training loss = 0.2546657398343086
Iteration 50, Training loss = 0.25002004779302156
Iteration 60, Training loss = 0.24777049857836503
Iteration 70, Training loss = 0.24571513212644136
Iteration 80, Training loss = 0.24520506079380328
Iteration 90, Training loss = 0.2432422016102534
Model training time: 14.17322587966919
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0198794672122369
Iteration 10, Training loss = 1.0060948523191304
Iteration 20, Training loss = 1.0030062473737276
Iteration 30, Training loss = 1.0010602451287782
Iteration 40, Training loss = 1.0003880789646735
Iteration 50, Training loss = 0.9969120117334219
Iteration 60, Training loss = 0.9959701024568998
Iteration 70, Training loss = 0.9929341329978063
Iteration 80, Training loss = 0.9933552421056308
Iteration 90, Training loss = 0.9918600607376832
Model training time: 13.233884334564209
Device: cuda
Iteration 0, Training loss = 1.3030751909200962
Iteration 10, Training loss = 1.0099123280781965
Iteration 20, Training loss = 0.9962676901083726
Iteration 30, Training loss = 0.9832518731172268
Iteration 40, Training loss = 0.9722039802716329
Iteration 50, Training loss = 0.9579960027566323
Iteration 60, Training loss = 0.941794746197187
Iteration 70, Training loss = 0.9215904095998178
Iteration 80, Training loss = 0.8948811705295856
Iteration 90, Training loss = 0.8608713814845452
Model training time: 13.054260492324829
Device: cuda
Iteration 0, Training loss = 0.9853475884749339
Iteration 10, Training loss = 0.9778511684674484
Iteration 20, Training loss = 0.9704070423658078
Iteration 30, Training loss = 0.9617587201870405
Iteration 40, Training loss = 0.9499062735300797
Iteration 50, Training loss = 0.9365829699314557
Iteration 60, Training loss = 0.9181848397621741
Iteration 70, Training loss = 0.8969528147807488
Iteration 80, Training loss = 0.8703069721276944
Iteration 90, Training loss = 0.8374527727182095
Model training time: 13.293034076690674
Device: cuda
Iteration 0, Training loss = 1.124024809553073
Iteration 10, Training loss = 1.003788804778686
Iteration 20, Training loss = 0.9985052897379949
Iteration 30, Training loss = 0.9938632937578055
Iteration 40, Training loss = 0.9908810957120016
Iteration 50, Training loss = 0.9833909227297857
Iteration 60, Training loss = 0.978528340275471
Iteration 70, Training loss = 0.9724165258499292
Iteration 80, Training loss = 0.9631919184556375
Iteration 90, Training loss = 0.9535633165102738
Model training time: 12.975842952728271
Device: cuda
Iteration 0, Training loss = 1.122632669714781
Iteration 10, Training loss = 1.0067101063636632
Iteration 20, Training loss = 1.002419956601583
Iteration 30, Training loss = 1.000401299733382
Iteration 40, Training loss = 0.9988422118700467
Iteration 50, Training loss = 0.9973111336047833
Iteration 60, Training loss = 0.9971804676147608
Iteration 70, Training loss = 0.9936346331467996
Iteration 80, Training loss = 0.9921154735180048
Iteration 90, Training loss = 0.9881588461307379
Model training time: 13.037572622299194
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9789498528608909
Iteration 10, Training loss = 0.2963427992967459
Iteration 20, Training loss = 0.26730189472436905
Iteration 30, Training loss = 0.2598460835333054
Iteration 40, Training loss = 0.2548853170413237
Iteration 50, Training loss = 0.2538247572687956
Iteration 60, Training loss = 0.2509198314868487
Iteration 70, Training loss = 0.24872634301965052
Iteration 80, Training loss = 0.2471227399431742
Iteration 90, Training loss = 0.24580150470137596
Model training time: 14.124705791473389
Device: cuda
Iteration 0, Training loss = 0.986998852628928
Iteration 10, Training loss = 0.30281215705550635
Iteration 20, Training loss = 0.270160085020157
Iteration 30, Training loss = 0.26014935970306396
Iteration 40, Training loss = 0.25630399842674917
Iteration 50, Training loss = 0.25252368301153183
Iteration 60, Training loss = 0.25092459699282277
Iteration 70, Training loss = 0.24944529567773527
Iteration 80, Training loss = 0.24696491773311907
Iteration 90, Training loss = 0.24601382905474076
Model training time: 14.206642866134644
Device: cuda
Iteration 0, Training loss = 0.9914262844966009
Iteration 10, Training loss = 0.3024801081762864
Iteration 20, Training loss = 0.26679969321076685
Iteration 30, Training loss = 0.26006483745116454
Iteration 40, Training loss = 0.2545838559476229
Iteration 50, Training loss = 0.253314828643432
Iteration 60, Training loss = 0.2505735236291702
Iteration 70, Training loss = 0.24835887322059044
Iteration 80, Training loss = 0.24734947543877822
Iteration 90, Training loss = 0.2452172917815355
Model training time: 14.476948499679565
Device: cuda
Iteration 0, Training loss = 1.1884095726104884
Iteration 10, Training loss = 0.35519136717686284
Iteration 20, Training loss = 0.28417886019899297
Iteration 30, Training loss = 0.2657982065127446
Iteration 40, Training loss = 0.2592059201919116
Iteration 50, Training loss = 0.2553307769390253
Iteration 60, Training loss = 0.2524122383732062
Iteration 70, Training loss = 0.24988941131876066
Iteration 80, Training loss = 0.24688872809593493
Iteration 90, Training loss = 0.24512728131734407
Model training time: 14.301592350006104
Device: cuda
Iteration 0, Training loss = 1.077674919596085
Iteration 10, Training loss = 0.32672673062636304
Iteration 20, Training loss = 0.27729648609574026
Iteration 30, Training loss = 0.2626846939898454
Iteration 40, Training loss = 0.2554121587712031
Iteration 50, Training loss = 0.2510497120137398
Iteration 60, Training loss = 0.248913364055065
Iteration 70, Training loss = 0.24613096221135214
Iteration 80, Training loss = 0.24279503581615594
Iteration 90, Training loss = 0.2406048390727777
Model training time: 14.259817361831665
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.183076044687858
Iteration 10, Training loss = 0.9899665862321854
Iteration 20, Training loss = 0.9813479712376227
Iteration 30, Training loss = 0.9735964169869056
Iteration 40, Training loss = 0.9603872964015374
Iteration 50, Training loss = 0.9464220966284091
Iteration 60, Training loss = 0.9309258518310694
Iteration 70, Training loss = 0.9081221348964251
Iteration 80, Training loss = 0.8793714940547943
Iteration 90, Training loss = 0.8429612322495534
Model training time: 12.980950117111206
Device: cuda
Iteration 0, Training loss = 1.1584206303724875
Iteration 10, Training loss = 1.0062677963421895
Iteration 20, Training loss = 1.0044889484460537
Iteration 30, Training loss = 0.9986870873432893
Iteration 40, Training loss = 0.9968530833721161
Iteration 50, Training loss = 0.9942995286904849
Iteration 60, Training loss = 0.9902198234429727
Iteration 70, Training loss = 0.9858366824113406
Iteration 80, Training loss = 0.9803904398129537
Iteration 90, Training loss = 0.9739997386932373
Model training time: 13.344852209091187
Device: cuda
Iteration 0, Training loss = 1.026818016400704
Iteration 10, Training loss = 0.9998990962138543
Iteration 20, Training loss = 0.9942724509881093
Iteration 30, Training loss = 0.9856041348897494
Iteration 40, Training loss = 0.9788374453783035
Iteration 50, Training loss = 0.9724840109164898
Iteration 60, Training loss = 0.9622468501329422
Iteration 70, Training loss = 0.9507906987116888
Iteration 80, Training loss = 0.9364880953843777
Iteration 90, Training loss = 0.921141830774454
Model training time: 13.048975467681885
Device: cuda
Iteration 0, Training loss = 1.0085082833583539
Iteration 10, Training loss = 0.9993206537686862
Iteration 20, Training loss = 0.9946302656943982
Iteration 30, Training loss = 0.9935826808214188
Iteration 40, Training loss = 0.9880024469815768
Iteration 50, Training loss = 0.9824607223272324
Iteration 60, Training loss = 0.978856583054249
Iteration 70, Training loss = 0.9719773324636313
Iteration 80, Training loss = 0.9643539969737713
Iteration 90, Training loss = 0.9545110234847436
Model training time: 12.973360538482666
Device: cuda
Iteration 0, Training loss = 1.0073319249428236
Iteration 10, Training loss = 1.0007760261113827
Iteration 20, Training loss = 0.9981301083014562
Iteration 30, Training loss = 0.9964928672863886
Iteration 40, Training loss = 0.9940118927222031
Iteration 50, Training loss = 0.9912720563320013
Iteration 60, Training loss = 0.9890536539829694
Iteration 70, Training loss = 0.9850809975312307
Iteration 80, Training loss = 0.9794421322070636
Iteration 90, Training loss = 0.9749371065543249
Model training time: 12.983266830444336
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9769766789216262
Iteration 10, Training loss = 0.2948149608877989
Iteration 20, Training loss = 0.2656902943093043
Iteration 30, Training loss = 0.2572651198850228
Iteration 40, Training loss = 0.25359046201293284
Iteration 50, Training loss = 0.2519944201295192
Iteration 60, Training loss = 0.2492259253676121
Iteration 70, Training loss = 0.2478271069434973
Iteration 80, Training loss = 0.24688929290725634
Iteration 90, Training loss = 0.2455235178080889
Iteration 100, Training loss = 0.24464219378737304
Iteration 110, Training loss = 0.24285889302308744
Iteration 120, Training loss = 0.24157020506950524
Iteration 130, Training loss = 0.2405535879616554
Iteration 140, Training loss = 0.23967950848432687
Iteration 150, Training loss = 0.23891187946383768
Iteration 160, Training loss = 0.23831849831801194
Iteration 170, Training loss = 0.2369567622932104
Iteration 180, Training loss = 0.23551417170808867
Iteration 190, Training loss = 0.2350241785439161
Model training time: 28.572777032852173
Device: cuda
Iteration 0, Training loss = 0.9633502719494013
Iteration 10, Training loss = 0.29865484971266526
Iteration 20, Training loss = 0.266410173704991
Iteration 30, Training loss = 0.2576619965525774
Iteration 40, Training loss = 0.25314072892069817
Iteration 50, Training loss = 0.25003630782549197
Iteration 60, Training loss = 0.2484305205826576
Iteration 70, Training loss = 0.24658575195532578
Iteration 80, Training loss = 0.2447814938540642
Iteration 90, Training loss = 0.24319578993778962
Iteration 100, Training loss = 0.24186819505233031
Iteration 110, Training loss = 0.2404353842139244
Iteration 120, Training loss = 0.23910885476149046
Iteration 130, Training loss = 0.23703026656921095
Iteration 140, Training loss = 0.23579784339437118
Iteration 150, Training loss = 0.23625941316668803
Iteration 160, Training loss = 0.23376428708434105
Iteration 170, Training loss = 0.23252819736416525
Iteration 180, Training loss = 0.23127428556864077
Iteration 190, Training loss = 0.22969640676791853
Model training time: 28.492424726486206
Device: cuda
Iteration 0, Training loss = 0.9751113435396781
Iteration 10, Training loss = 0.2957790938134377
Iteration 20, Training loss = 0.2612578651079765
Iteration 30, Training loss = 0.2543045242245381
Iteration 40, Training loss = 0.2503812556656507
Iteration 50, Training loss = 0.24601698924715704
Iteration 60, Training loss = 0.24351119307371286
Iteration 70, Training loss = 0.24151971133855674
Iteration 80, Training loss = 0.23938181308599618
Iteration 90, Training loss = 0.23742245395596212
Iteration 100, Training loss = 0.2355104684829712
Iteration 110, Training loss = 0.23386670935612458
Iteration 120, Training loss = 0.231969592662958
Iteration 130, Training loss = 0.23024085708535635
Iteration 140, Training loss = 0.2289511919594728
Iteration 150, Training loss = 0.22797714001857317
Iteration 160, Training loss = 0.22674455579656821
Iteration 170, Training loss = 0.22642378766949361
Iteration 180, Training loss = 0.22553943355496114
Iteration 190, Training loss = 0.22414719599943894
Model training time: 28.367220401763916
Device: cuda
Iteration 0, Training loss = 1.0570776852277608
Iteration 10, Training loss = 0.2973925878222172
Iteration 20, Training loss = 0.2678019321308686
Iteration 30, Training loss = 0.2586960316850589
Iteration 40, Training loss = 0.2530073463343657
Iteration 50, Training loss = 0.2504105854492921
Iteration 60, Training loss = 0.24827972054481506
Iteration 70, Training loss = 0.24474321878873384
Iteration 80, Training loss = 0.24253731392897093
Iteration 90, Training loss = 0.24053762537928727
Iteration 100, Training loss = 0.23796838923142508
Iteration 110, Training loss = 0.2367603134077329
Iteration 120, Training loss = 0.23524386493059304
Iteration 130, Training loss = 0.23357369206272638
Iteration 140, Training loss = 0.2321003540777243
Iteration 150, Training loss = 0.23054740606592253
Iteration 160, Training loss = 0.22937594812649947
Iteration 170, Training loss = 0.22788754048255774
Iteration 180, Training loss = 0.22657010503686392
Iteration 190, Training loss = 0.22612306303702867
Model training time: 28.800991535186768
Device: cuda
Iteration 0, Training loss = 0.9848017394542694
Iteration 10, Training loss = 0.2914124821814207
Iteration 20, Training loss = 0.26291609526826787
Iteration 30, Training loss = 0.254195363762287
Iteration 40, Training loss = 0.2504663854264296
Iteration 50, Training loss = 0.2477670727440944
Iteration 60, Training loss = 0.2452350158530932
Iteration 70, Training loss = 0.24298428744077682
Iteration 80, Training loss = 0.24129596897042715
Iteration 90, Training loss = 0.23915808160717672
Iteration 100, Training loss = 0.23741906766708082
Iteration 110, Training loss = 0.23456729776584184
Iteration 120, Training loss = 0.23333926871418953
Iteration 130, Training loss = 0.2315209017923245
Iteration 140, Training loss = 0.22963427952848947
Iteration 150, Training loss = 0.2274943939768351
Iteration 160, Training loss = 0.22492102017769447
Iteration 170, Training loss = 0.2239969206544069
Iteration 180, Training loss = 0.22248548412552246
Iteration 190, Training loss = 0.2204965605185582
Model training time: 28.027371406555176
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.192070912856322
Iteration 10, Training loss = 1.0120194370930011
Iteration 20, Training loss = 1.0009802430868149
Iteration 30, Training loss = 0.9930451730122933
Iteration 40, Training loss = 0.980254067824437
Iteration 50, Training loss = 0.9703881488396571
Iteration 60, Training loss = 0.9594681641230216
Iteration 70, Training loss = 0.9451363384723663
Iteration 80, Training loss = 0.9274003620331104
Iteration 90, Training loss = 0.9040575887148197
Iteration 100, Training loss = 0.8737177195457312
Iteration 110, Training loss = 0.8345835036956347
Iteration 120, Training loss = 0.7840959464128201
Iteration 130, Training loss = 0.7224298566579819
Iteration 140, Training loss = 0.6542746665386053
Iteration 150, Training loss = 0.5833195780332272
Iteration 160, Training loss = 0.5205395731788415
Iteration 170, Training loss = 0.4712704901511853
Iteration 180, Training loss = 0.43886718039329237
Iteration 190, Training loss = 0.4168243689032701
Model training time: 26.23191237449646
Device: cuda
Iteration 0, Training loss = 1.08258762382544
Iteration 10, Training loss = 0.9823581541960056
Iteration 20, Training loss = 0.9720633786458236
Iteration 30, Training loss = 0.9641965593282993
Iteration 40, Training loss = 0.9512021690607071
Iteration 50, Training loss = 0.9391658329046689
Iteration 60, Training loss = 0.9177096646565658
Iteration 70, Training loss = 0.8940833944540757
Iteration 80, Training loss = 0.8608599408314779
Iteration 90, Training loss = 0.8160676681078397
Iteration 100, Training loss = 0.7640138474794534
Iteration 110, Training loss = 0.7016436835894218
Iteration 120, Training loss = 0.6375867667106482
Iteration 130, Training loss = 0.5742413252592087
Iteration 140, Training loss = 0.5185030217354114
Iteration 150, Training loss = 0.47398754151967853
Iteration 160, Training loss = 0.44002644889629805
Iteration 170, Training loss = 0.4167911341557136
Iteration 180, Training loss = 0.4014317634013983
Iteration 190, Training loss = 0.3904032919269342
Model training time: 26.01177954673767
Device: cuda
Iteration 0, Training loss = 1.0134099847995317
Iteration 10, Training loss = 0.9964001877949789
Iteration 20, Training loss = 0.9834337429358408
Iteration 30, Training loss = 0.9725044805269975
Iteration 40, Training loss = 0.9580385719354336
Iteration 50, Training loss = 0.9352348985580298
Iteration 60, Training loss = 0.9128100998126544
Iteration 70, Training loss = 0.8808572590351105
Iteration 80, Training loss = 0.8386281717282075
Iteration 90, Training loss = 0.7889429617386597
Iteration 100, Training loss = 0.7274343955975312
Iteration 110, Training loss = 0.660129916209441
Iteration 120, Training loss = 0.5893990695476532
Iteration 130, Training loss = 0.5248960359738424
Iteration 140, Training loss = 0.4711484100956183
Iteration 150, Training loss = 0.4321134789631917
Iteration 160, Training loss = 0.4064694969699933
Iteration 170, Training loss = 0.38960195504702055
Iteration 180, Training loss = 0.37893645751934785
Iteration 190, Training loss = 0.3732124257546205
Model training time: 25.967592477798462
Device: cuda
Iteration 0, Training loss = 1.0381303418141146
Iteration 10, Training loss = 0.9990905053340472
Iteration 20, Training loss = 0.9952352551313547
Iteration 30, Training loss = 0.9925399101697482
Iteration 40, Training loss = 0.990282647884809
Iteration 50, Training loss = 0.9848805448183646
Iteration 60, Training loss = 0.9828811677602621
Iteration 70, Training loss = 0.9762530968739436
Iteration 80, Training loss = 0.9700034512923315
Iteration 90, Training loss = 0.9602034802620227
Iteration 100, Training loss = 0.9500188048069294
Iteration 110, Training loss = 0.9353316036554483
Iteration 120, Training loss = 0.915991886303975
Iteration 130, Training loss = 0.8913850520665829
Iteration 140, Training loss = 0.854995322915224
Iteration 150, Training loss = 0.8117583015790353
Iteration 160, Training loss = 0.756791026546405
Iteration 170, Training loss = 0.6930836186959193
Iteration 180, Training loss = 0.6254890469404367
Iteration 190, Training loss = 0.5592804797566854
Model training time: 26.69491219520569
Device: cuda
Iteration 0, Training loss = 1.5831722892247713
Iteration 10, Training loss = 0.9954597697808192
Iteration 20, Training loss = 0.9905500767322687
Iteration 30, Training loss = 0.9839373150697122
Iteration 40, Training loss = 0.980817892230474
Iteration 50, Training loss = 0.9719224090759571
Iteration 60, Training loss = 0.9637523637368128
Iteration 70, Training loss = 0.9543652935669973
Iteration 80, Training loss = 0.938955701314486
Iteration 90, Training loss = 0.9227725244485415
Iteration 100, Training loss = 0.9004860612062308
Iteration 110, Training loss = 0.8686757110632383
Iteration 120, Training loss = 0.8283804827011548
Iteration 130, Training loss = 0.7814843929730929
Iteration 140, Training loss = 0.7221218336087006
Iteration 150, Training loss = 0.6575738283304068
Iteration 160, Training loss = 0.5911472004193526
Iteration 170, Training loss = 0.5278082627516526
Iteration 180, Training loss = 0.47673349254406416
Iteration 190, Training loss = 0.4360155491874768
Model training time: 26.433613061904907
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0119695571752696
Iteration 10, Training loss = 0.36865718032305056
Iteration 20, Training loss = 0.2973710069289574
Iteration 30, Training loss = 0.27358600153372836
Iteration 40, Training loss = 0.266020259891565
Iteration 50, Training loss = 0.26030650849525744
Iteration 60, Training loss = 0.25606334324066454
Iteration 70, Training loss = 0.2537055385227387
Iteration 80, Training loss = 0.2506580679462506
Iteration 90, Training loss = 0.2496630225617152
Iteration 100, Training loss = 0.24879500384514147
Iteration 110, Training loss = 0.24749101583774274
Iteration 120, Training loss = 0.2466552512576947
Iteration 130, Training loss = 0.24573312642482611
Iteration 140, Training loss = 0.2453676791718373
Iteration 150, Training loss = 0.24435571724405655
Iteration 160, Training loss = 0.24389960932043883
Iteration 170, Training loss = 0.24262533709406853
Iteration 180, Training loss = 0.24299369379878044
Iteration 190, Training loss = 0.24218372284219816
Model training time: 28.5121009349823
Device: cuda
Iteration 0, Training loss = 1.0188203855202749
Iteration 10, Training loss = 0.35649189009116244
Iteration 20, Training loss = 0.296584169452007
Iteration 30, Training loss = 0.27278468729211736
Iteration 40, Training loss = 0.2621551133119143
Iteration 50, Training loss = 0.258462834243591
Iteration 60, Training loss = 0.25512511684344363
Iteration 70, Training loss = 0.2537222791176576
Iteration 80, Training loss = 0.25203074228305083
Iteration 90, Training loss = 0.2510198552448016
Iteration 100, Training loss = 0.24924397783783767
Iteration 110, Training loss = 0.24812290559594447
Iteration 120, Training loss = 0.2471060472039076
Iteration 130, Training loss = 0.24599825590848923
Iteration 140, Training loss = 0.245505498865476
Iteration 150, Training loss = 0.24505326094535682
Iteration 160, Training loss = 0.24379996611521795
Iteration 170, Training loss = 0.24305027608688062
Iteration 180, Training loss = 0.24397868032638842
Iteration 190, Training loss = 0.2421651459657229
Model training time: 28.6611647605896
Device: cuda
Iteration 0, Training loss = 1.0023375336940472
Iteration 10, Training loss = 0.33775512358317006
Iteration 20, Training loss = 0.27735225970928484
Iteration 30, Training loss = 0.26101672935944337
Iteration 40, Training loss = 0.2554896743251727
Iteration 50, Training loss = 0.2532062392968398
Iteration 60, Training loss = 0.2511954167141364
Iteration 70, Training loss = 0.2485296923953753
Iteration 80, Training loss = 0.2467017056277165
Iteration 90, Training loss = 0.24539169239310119
Iteration 100, Training loss = 0.24329707800195768
Iteration 110, Training loss = 0.24255697457836226
Iteration 120, Training loss = 0.2410711869597435
Iteration 130, Training loss = 0.24061423683395752
Iteration 140, Training loss = 0.2383058569752253
Iteration 150, Training loss = 0.236348083959176
Iteration 160, Training loss = 0.2351943586881344
Iteration 170, Training loss = 0.23417179802289376
Iteration 180, Training loss = 0.2328973991366533
Iteration 190, Training loss = 0.23152476291243845
Model training time: 28.382874011993408
Device: cuda
Iteration 0, Training loss = 0.9970419796613547
Iteration 10, Training loss = 0.3529012088592236
Iteration 20, Training loss = 0.2847568140580104
Iteration 30, Training loss = 0.2649013288319111
Iteration 40, Training loss = 0.25860171449872166
Iteration 50, Training loss = 0.25448238075925755
Iteration 60, Training loss = 0.25166331919339985
Iteration 70, Training loss = 0.24953812876572976
Iteration 80, Training loss = 0.24768508082399002
Iteration 90, Training loss = 0.24578073936013076
Iteration 100, Training loss = 0.24384671478317335
Iteration 110, Training loss = 0.2415999323129654
Iteration 120, Training loss = 0.24086430153021446
Iteration 130, Training loss = 0.23953500476021033
Iteration 140, Training loss = 0.23828055594976133
Iteration 150, Training loss = 0.23778154815618807
Iteration 160, Training loss = 0.23674747777672914
Iteration 170, Training loss = 0.23735956102609634
Iteration 180, Training loss = 0.23565569777901357
Iteration 190, Training loss = 0.23563634804808176
Model training time: 28.96877956390381
Device: cuda
Iteration 0, Training loss = 1.697938247368886
Iteration 10, Training loss = 0.5661777527286456
Iteration 20, Training loss = 0.3518935052248148
Iteration 30, Training loss = 0.2904854551530801
Iteration 40, Training loss = 0.2687912978805028
Iteration 50, Training loss = 0.25859965584599054
Iteration 60, Training loss = 0.25158411780228984
Iteration 70, Training loss = 0.2491782459502037
Iteration 80, Training loss = 0.24644818586798814
Iteration 90, Training loss = 0.24521920113609388
Iteration 100, Training loss = 0.24337392329023436
Iteration 110, Training loss = 0.24174722140798202
Iteration 120, Training loss = 0.2409084516649063
Iteration 130, Training loss = 0.24004637077450752
Iteration 140, Training loss = 0.2392361175555449
Iteration 150, Training loss = 0.23861828646980798
Iteration 160, Training loss = 0.2373639032817804
Iteration 170, Training loss = 0.23640334319609863
Iteration 180, Training loss = 0.23613378978692567
Iteration 190, Training loss = 0.2348131348307316
Model training time: 30.773001670837402
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.992160650400015
Iteration 10, Training loss = 0.9812134538705533
Iteration 20, Training loss = 0.9736254467413976
Iteration 30, Training loss = 0.9631255406599778
Iteration 40, Training loss = 0.9486653896478506
Iteration 50, Training loss = 0.9322789437495745
Iteration 60, Training loss = 0.9139266082873712
Iteration 70, Training loss = 0.8838853652660663
Iteration 80, Training loss = 0.8509741952786078
Iteration 90, Training loss = 0.8051703056463828
Iteration 100, Training loss = 0.7521227586727876
Iteration 110, Training loss = 0.6867048304814559
Iteration 120, Training loss = 0.6199336877235999
Iteration 130, Training loss = 0.5540505447066747
Iteration 140, Training loss = 0.4968360278468866
Iteration 150, Training loss = 0.4522576452447818
Iteration 160, Training loss = 0.4204490322333116
Iteration 170, Training loss = 0.39869998223506486
Iteration 180, Training loss = 0.3835815414786339
Iteration 190, Training loss = 0.37506622534531814
Model training time: 28.61494755744934
Device: cuda
Iteration 0, Training loss = 1.0839437739207194
Iteration 10, Training loss = 1.00411767913745
Iteration 20, Training loss = 1.0001333138117423
Iteration 30, Training loss = 0.9928043713936439
Iteration 40, Training loss = 0.9878132079656308
Iteration 50, Training loss = 0.9811852952608695
Iteration 60, Training loss = 0.9749992913924731
Iteration 70, Training loss = 0.9653456600812765
Iteration 80, Training loss = 0.9539955533467807
Iteration 90, Training loss = 0.9412201952475768
Iteration 100, Training loss = 0.9254325341719848
Iteration 110, Training loss = 0.9023038229117026
Iteration 120, Training loss = 0.8751864960560432
Iteration 130, Training loss = 0.8438494010613515
Iteration 140, Training loss = 0.8066041950996106
Iteration 150, Training loss = 0.7648953474484957
Iteration 160, Training loss = 0.7192037403583527
Iteration 170, Training loss = 0.6730230977902045
Iteration 180, Training loss = 0.6288594231009483
Iteration 190, Training loss = 0.5857678456948354
Model training time: 27.016966581344604
Device: cuda
Iteration 0, Training loss = 1.4326959619155297
Iteration 10, Training loss = 1.0112114881093686
Iteration 20, Training loss = 1.000008273583192
Iteration 30, Training loss = 0.9980010688304901
Iteration 40, Training loss = 0.9955379928533847
Iteration 50, Training loss = 0.9943084223912313
Iteration 60, Training loss = 0.9937419146299362
Iteration 70, Training loss = 0.9924860745668411
Iteration 80, Training loss = 0.9890934228897095
Iteration 90, Training loss = 0.9872329911360374
Iteration 100, Training loss = 0.9839896937975516
Iteration 110, Training loss = 0.9786712584587244
Iteration 120, Training loss = 0.9755152257589194
Iteration 130, Training loss = 0.9689080336919198
Iteration 140, Training loss = 0.9600536135526804
Iteration 150, Training loss = 0.9539168786544067
Iteration 160, Training loss = 0.9406993652765567
Iteration 170, Training loss = 0.9266139532511051
Iteration 180, Training loss = 0.9059627583393683
Iteration 190, Training loss = 0.8813962730077597
Model training time: 26.487553596496582
Device: cuda
Iteration 0, Training loss = 0.9998697099777368
Iteration 10, Training loss = 0.9864818041141217
Iteration 20, Training loss = 0.9811426011415628
Iteration 30, Training loss = 0.9752422697269
Iteration 40, Training loss = 0.9664134474901053
Iteration 50, Training loss = 0.9564190988357251
Iteration 60, Training loss = 0.9445471350963299
Iteration 70, Training loss = 0.9274531006813049
Iteration 80, Training loss = 0.9045114287963281
Iteration 90, Training loss = 0.8780336643640811
Iteration 100, Training loss = 0.8417103691742971
Iteration 110, Training loss = 0.7966556571997129
Iteration 120, Training loss = 0.7419775930734781
Iteration 130, Training loss = 0.6810589868288773
Iteration 140, Training loss = 0.6151855123730806
Iteration 150, Training loss = 0.5538350853782433
Iteration 160, Training loss = 0.5005143989737217
Iteration 170, Training loss = 0.4587129606650426
Iteration 180, Training loss = 0.4272637716852702
Iteration 190, Training loss = 0.40569458729945695
Model training time: 26.117199420928955
Device: cuda
Iteration 0, Training loss = 1.2390317366673396
Iteration 10, Training loss = 0.9993640172940034
Iteration 20, Training loss = 0.989224754847013
Iteration 30, Training loss = 0.9820109284841098
Iteration 40, Training loss = 0.9754631851728146
Iteration 50, Training loss = 0.9679793256979722
Iteration 60, Training loss = 0.9574168794430219
Iteration 70, Training loss = 0.9441556540819315
Iteration 80, Training loss = 0.9273039675675906
Iteration 90, Training loss = 0.9045801839003196
Iteration 100, Training loss = 0.8731279464868399
Iteration 110, Training loss = 0.8339676375572498
Iteration 120, Training loss = 0.7853524272258465
Iteration 130, Training loss = 0.7274606445660958
Iteration 140, Training loss = 0.6605462259971179
Iteration 150, Training loss = 0.5950204197030801
Iteration 160, Training loss = 0.5336603459257346
Iteration 170, Training loss = 0.4859708971702136
Iteration 180, Training loss = 0.4497557511696449
Iteration 190, Training loss = 0.4237496010386027
Model training time: 26.522754192352295
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0056317563240345
Iteration 10, Training loss = 0.3117421384041126
Iteration 20, Training loss = 0.2753776300411958
Iteration 30, Training loss = 0.2626904211938381
Iteration 40, Training loss = 0.25779012189461636
Iteration 50, Training loss = 0.25487353108250177
Iteration 60, Training loss = 0.25303560065535396
Iteration 70, Training loss = 0.2513546860561921
Iteration 80, Training loss = 0.24852967290924147
Iteration 90, Training loss = 0.24738813048371902
Iteration 100, Training loss = 0.24656142896184555
Iteration 110, Training loss = 0.24484336806031373
Iteration 120, Training loss = 0.24448617519094393
Iteration 130, Training loss = 0.24335614649149087
Iteration 140, Training loss = 0.24273894469325358
Iteration 150, Training loss = 0.24190577148244932
Iteration 160, Training loss = 0.2416226170383967
Iteration 170, Training loss = 0.2399471178650856
Iteration 180, Training loss = 0.23920863752181715
Iteration 190, Training loss = 0.23891676160005423
Model training time: 28.62062954902649
Device: cuda
Iteration 0, Training loss = 1.148372205404135
Iteration 10, Training loss = 0.319572353305725
Iteration 20, Training loss = 0.2747281279701453
Iteration 30, Training loss = 0.26427775564102024
Iteration 40, Training loss = 0.25820171288572824
Iteration 50, Training loss = 0.25395369787628835
Iteration 60, Training loss = 0.2510095826135232
Iteration 70, Training loss = 0.24912265201027578
Iteration 80, Training loss = 0.2464394337282731
Iteration 90, Training loss = 0.24592262219924194
Iteration 100, Training loss = 0.24366161857660001
Iteration 110, Training loss = 0.24247886784947836
Iteration 120, Training loss = 0.2408069260418415
Iteration 130, Training loss = 0.23942246660590172
Iteration 140, Training loss = 0.23880412658819786
Iteration 150, Training loss = 0.23704771611552972
Iteration 160, Training loss = 0.23574288819844907
Iteration 170, Training loss = 0.2341202746790189
Iteration 180, Training loss = 0.2341405815229966
Iteration 190, Training loss = 0.23276707902550697
Model training time: 28.259799480438232
Device: cuda
Iteration 0, Training loss = 1.119436272061788
Iteration 10, Training loss = 0.32111434925061005
Iteration 20, Training loss = 0.27321747977000016
Iteration 30, Training loss = 0.2607057432715709
Iteration 40, Training loss = 0.255340980222592
Iteration 50, Training loss = 0.25195975744953525
Iteration 60, Training loss = 0.24980223465424317
Iteration 70, Training loss = 0.2465357780456543
Iteration 80, Training loss = 0.2442634326334183
Iteration 90, Training loss = 0.24193415905420596
Iteration 100, Training loss = 0.24104882003023073
Iteration 110, Training loss = 0.2392269725409838
Iteration 120, Training loss = 0.23799202419244325
Iteration 130, Training loss = 0.23661984675205672
Iteration 140, Training loss = 0.23570304937087572
Iteration 150, Training loss = 0.23524082366090554
Iteration 160, Training loss = 0.23450043654212585
Iteration 170, Training loss = 0.2333719183046084
Iteration 180, Training loss = 0.23209563384835535
Iteration 190, Training loss = 0.2317088062946613
Model training time: 28.83874011039734
Device: cuda
Iteration 0, Training loss = 1.0165695972167528
Iteration 10, Training loss = 0.3021967379519573
Iteration 20, Training loss = 0.26616058154748035
Iteration 30, Training loss = 0.2582939974963665
Iteration 40, Training loss = 0.25580887410503167
Iteration 50, Training loss = 0.2532792936723966
Iteration 60, Training loss = 0.2512797463971835
Iteration 70, Training loss = 0.24858680338813707
Iteration 80, Training loss = 0.24737994544781172
Iteration 90, Training loss = 0.2462230109824584
Iteration 100, Training loss = 0.24426753876300958
Iteration 110, Training loss = 0.24347126856446266
Iteration 120, Training loss = 0.24096626301224416
Iteration 130, Training loss = 0.23971194315415162
Iteration 140, Training loss = 0.23896285289755234
Iteration 150, Training loss = 0.23630493936630395
Iteration 160, Training loss = 0.23521223778908068
Iteration 170, Training loss = 0.23407622512716514
Iteration 180, Training loss = 0.23274508405190247
Iteration 190, Training loss = 0.23125683544920042
Model training time: 28.25674533843994
Device: cuda
Iteration 0, Training loss = 1.0587760794621248
Iteration 10, Training loss = 0.3250954827437034
Iteration 20, Training loss = 0.2706688745663716
Iteration 30, Training loss = 0.25941285490989685
Iteration 40, Training loss = 0.25553178099485546
Iteration 50, Training loss = 0.25186992064118385
Iteration 60, Training loss = 0.24945270442045653
Iteration 70, Training loss = 0.2480854887801867
Iteration 80, Training loss = 0.24645964314158147
Iteration 90, Training loss = 0.24514426663517952
Iteration 100, Training loss = 0.2434914203790518
Iteration 110, Training loss = 0.24231432464260322
Iteration 120, Training loss = 0.24062700254412797
Iteration 130, Training loss = 0.2400838520664435
Iteration 140, Training loss = 0.23922235088852736
Iteration 150, Training loss = 0.23853819645368135
Iteration 160, Training loss = 0.23806576602734053
Iteration 170, Training loss = 0.23758946903623068
Iteration 180, Training loss = 0.23567337800677007
Iteration 190, Training loss = 0.2350467240008024
Model training time: 28.164836168289185
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 0.9878260928850907
Iteration 10, Training loss = 0.9796511141153482
Iteration 20, Training loss = 0.9736985059884878
Iteration 30, Training loss = 0.9642377220667325
Iteration 40, Training loss = 0.9521853316288728
Iteration 50, Training loss = 0.9382395664086709
Iteration 60, Training loss = 0.9212777121708944
Iteration 70, Training loss = 0.8972207559989049
Iteration 80, Training loss = 0.8688439956078162
Iteration 90, Training loss = 0.8297831760003016
Iteration 100, Training loss = 0.7836269724827546
Iteration 110, Training loss = 0.726666672871663
Iteration 120, Training loss = 0.6647445227091129
Iteration 130, Training loss = 0.5994589615326661
Iteration 140, Training loss = 0.5400609345390246
Iteration 150, Training loss = 0.4896848356494537
Iteration 160, Training loss = 0.4516002770799857
Iteration 170, Training loss = 0.42594162088174087
Iteration 180, Training loss = 0.40602508932352066
Iteration 190, Training loss = 0.39463428465219647
Model training time: 25.999171018600464
Device: cuda
Iteration 0, Training loss = 1.1618272398526852
Iteration 10, Training loss = 0.9849253824123969
Iteration 20, Training loss = 0.9764868880693729
Iteration 30, Training loss = 0.9687049159636865
Iteration 40, Training loss = 0.95564726797434
Iteration 50, Training loss = 0.9440321314793366
Iteration 60, Training loss = 0.9266997266274232
Iteration 70, Training loss = 0.904014880840595
Iteration 80, Training loss = 0.8772120292370136
Iteration 90, Training loss = 0.8411361070779654
Iteration 100, Training loss = 0.7949379132344172
Iteration 110, Training loss = 0.7370482270534222
Iteration 120, Training loss = 0.6722205842916782
Iteration 130, Training loss = 0.6047408838684742
Iteration 140, Training loss = 0.5426339633189715
Iteration 150, Training loss = 0.4887627621109669
Iteration 160, Training loss = 0.45071903272317004
Iteration 170, Training loss = 0.4249138448100824
Iteration 180, Training loss = 0.40838425434552705
Iteration 190, Training loss = 0.3970848545432091
Model training time: 26.02126383781433
Device: cuda
Iteration 0, Training loss = 0.9966049503821593
Iteration 10, Training loss = 0.9955236120865896
Iteration 20, Training loss = 0.9880774812056468
Iteration 30, Training loss = 0.9831937883908932
Iteration 40, Training loss = 0.976143901164715
Iteration 50, Training loss = 0.9682485919732314
Iteration 60, Training loss = 0.9580053308835397
Iteration 70, Training loss = 0.9468613198170295
Iteration 80, Training loss = 0.9297342839149328
Iteration 90, Training loss = 0.9074424631320513
Iteration 100, Training loss = 0.8793700394722131
Iteration 110, Training loss = 0.8446932561122454
Iteration 120, Training loss = 0.8018722098607284
Iteration 130, Training loss = 0.7489947000375161
Iteration 140, Training loss = 0.6904876598945031
Iteration 150, Training loss = 0.6277178422762797
Iteration 160, Training loss = 0.5679106408586869
Iteration 170, Training loss = 0.5141244903206825
Iteration 180, Training loss = 0.47153352888730854
Iteration 190, Training loss = 0.4391544237732887
Model training time: 26.156137466430664
Device: cuda
Iteration 0, Training loss = 0.9945111194482217
Iteration 10, Training loss = 0.971659738283891
Iteration 20, Training loss = 0.9559988723351405
Iteration 30, Training loss = 0.9393544735816809
Iteration 40, Training loss = 0.9163143061674558
Iteration 50, Training loss = 0.8891162895239316
Iteration 60, Training loss = 0.8528149173809931
Iteration 70, Training loss = 0.8070540370849463
Iteration 80, Training loss = 0.7528958675953058
Iteration 90, Training loss = 0.6900374843524053
Iteration 100, Training loss = 0.6219906107737467
Iteration 110, Training loss = 0.5579352080821991
Iteration 120, Training loss = 0.5041319498649011
Iteration 130, Training loss = 0.4636704411644202
Iteration 140, Training loss = 0.4348200565347305
Iteration 150, Training loss = 0.41574463133628553
Iteration 160, Training loss = 0.40369191708473057
Iteration 170, Training loss = 0.39249653483812624
Iteration 180, Training loss = 0.3853181887131471
Iteration 190, Training loss = 0.37883448428832567
Model training time: 26.210445880889893
Device: cuda
Iteration 0, Training loss = 1.0032042070077016
Iteration 10, Training loss = 0.9953290327237203
Iteration 20, Training loss = 0.9907296277009524
Iteration 30, Training loss = 0.9842108740256383
Iteration 40, Training loss = 0.976887590610064
Iteration 50, Training loss = 0.9697535565266242
Iteration 60, Training loss = 0.9574797646357462
Iteration 70, Training loss = 0.9469830898138193
Iteration 80, Training loss = 0.9273131283429953
Iteration 90, Training loss = 0.9056601489965732
Iteration 100, Training loss = 0.8753606573893473
Iteration 110, Training loss = 0.8365306441600506
Iteration 120, Training loss = 0.7877925565609565
Iteration 130, Training loss = 0.7311359013502414
Iteration 140, Training loss = 0.6662789732217789
Iteration 150, Training loss = 0.6014782161666796
Iteration 160, Training loss = 0.5393566282895895
Iteration 170, Training loss = 0.48626115173101425
Iteration 180, Training loss = 0.4435315069097739
Iteration 190, Training loss = 0.4122190418151709
Model training time: 26.29129934310913
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.0911446041785753
Iteration 10, Training loss = 0.3151548490501367
Iteration 20, Training loss = 0.27004922038087475
Iteration 30, Training loss = 0.25754250901249737
Iteration 40, Training loss = 0.2526274790557531
Iteration 50, Training loss = 0.25056008650706363
Iteration 60, Training loss = 0.24914685550790566
Iteration 70, Training loss = 0.24631485457603747
Iteration 80, Training loss = 0.24539597160541093
Iteration 90, Training loss = 0.24282134209687894
Iteration 100, Training loss = 0.24155297732123962
Iteration 110, Training loss = 0.23958130954549864
Iteration 120, Training loss = 0.2382636726475679
Iteration 130, Training loss = 0.23802420067099425
Iteration 140, Training loss = 0.23692718320167983
Iteration 150, Training loss = 0.23517675439898783
Iteration 160, Training loss = 0.23386782722977492
Iteration 170, Training loss = 0.23291531004584753
Iteration 180, Training loss = 0.23176736699847075
Iteration 190, Training loss = 0.23079076323371667
Iteration 200, Training loss = 0.22972025578984848
Iteration 210, Training loss = 0.22797809885098383
Iteration 220, Training loss = 0.22761766325968963
Iteration 230, Training loss = 0.22571160386388117
Iteration 240, Training loss = 0.22465626723491228
Iteration 250, Training loss = 0.22457815907322443
Iteration 260, Training loss = 0.22268871361246476
Iteration 270, Training loss = 0.2219233916929135
Iteration 280, Training loss = 0.2202811622275756
Iteration 290, Training loss = 0.22038282167453033
Model training time: 42.15764784812927
Device: cuda
Iteration 0, Training loss = 0.9951858623669698
Iteration 10, Training loss = 0.30490444571926045
Iteration 20, Training loss = 0.2655545650766446
Iteration 30, Training loss = 0.2583061425158611
Iteration 40, Training loss = 0.25457744816174877
Iteration 50, Training loss = 0.25252656982495236
Iteration 60, Training loss = 0.2505016587674618
Iteration 70, Training loss = 0.2483949689910962
Iteration 80, Training loss = 0.24614490539981768
Iteration 90, Training loss = 0.2428821663443859
Iteration 100, Training loss = 0.24070301050176987
Iteration 110, Training loss = 0.2400067041699703
Iteration 120, Training loss = 0.238696183436192
Iteration 130, Training loss = 0.23749108440600908
Iteration 140, Training loss = 0.23568483126851228
Iteration 150, Training loss = 0.23518965078087953
Iteration 160, Training loss = 0.23381811609634987
Iteration 170, Training loss = 0.23341921086494738
Iteration 180, Training loss = 0.23115070393452278
Iteration 190, Training loss = 0.2300324677847899
Iteration 200, Training loss = 0.22856295137451246
Iteration 210, Training loss = 0.22786685795738146
Iteration 220, Training loss = 0.22591283057744688
Iteration 230, Training loss = 0.2247736296401574
Iteration 240, Training loss = 0.2226807394853005
Iteration 250, Training loss = 0.2221594129044276
Iteration 260, Training loss = 0.2207340867473529
Iteration 270, Training loss = 0.2198714605317666
Iteration 280, Training loss = 0.2190427499321791
Iteration 290, Training loss = 0.21794581814454153
Model training time: 42.75850701332092
Device: cuda
Iteration 0, Training loss = 0.9850825885167489
Iteration 10, Training loss = 0.2908217620391112
Iteration 20, Training loss = 0.2640100703216516
Iteration 30, Training loss = 0.25676463401088345
Iteration 40, Training loss = 0.2525964230298996
Iteration 50, Training loss = 0.2505352236330509
Iteration 60, Training loss = 0.24928254290268972
Iteration 70, Training loss = 0.24618317330112824
Iteration 80, Training loss = 0.24452289738334143
Iteration 90, Training loss = 0.24256613965217882
Iteration 100, Training loss = 0.2416074275970459
Iteration 110, Training loss = 0.23993825855163428
Iteration 120, Training loss = 0.2403371328344712
Iteration 130, Training loss = 0.23795144288585737
Iteration 140, Training loss = 0.23568344546052125
Iteration 150, Training loss = 0.23459499730513647
Iteration 160, Training loss = 0.2331551809150439
Iteration 170, Training loss = 0.2327827183673015
Iteration 180, Training loss = 0.2315413522032591
Iteration 190, Training loss = 0.23004428956371087
Iteration 200, Training loss = 0.22889843640419152
Iteration 210, Training loss = 0.22753954621461722
Iteration 220, Training loss = 0.22748133110312316
Iteration 230, Training loss = 0.22617068227667075
Iteration 240, Training loss = 0.225254779538283
Iteration 250, Training loss = 0.22444852986014807
Iteration 260, Training loss = 0.22281217918946192
Iteration 270, Training loss = 0.22238842455240396
Iteration 280, Training loss = 0.22205491020129278
Iteration 290, Training loss = 0.22093097407084245
Model training time: 43.100804805755615
Device: cuda
Iteration 0, Training loss = 0.9753799816736808
Iteration 10, Training loss = 0.28566803811834407
Iteration 20, Training loss = 0.26021102242744887
Iteration 30, Training loss = 0.25335948856977314
Iteration 40, Training loss = 0.24993366604814163
Iteration 50, Training loss = 0.24619635395132577
Iteration 60, Training loss = 0.24396935305916345
Iteration 70, Training loss = 0.24082040442870215
Iteration 80, Training loss = 0.23824783873099548
Iteration 90, Training loss = 0.23686599244291967
Iteration 100, Training loss = 0.2358072895843249
Iteration 110, Training loss = 0.23263190314173698
Iteration 120, Training loss = 0.23276828458675972
Iteration 130, Training loss = 0.23001008108258247
Iteration 140, Training loss = 0.2295943759381771
Iteration 150, Training loss = 0.228517796844244
Iteration 160, Training loss = 0.22733992968614286
Iteration 170, Training loss = 0.22681848475566277
Iteration 180, Training loss = 0.225180528484858
Iteration 190, Training loss = 0.22393893651091135
Iteration 200, Training loss = 0.22284287500839967
Iteration 210, Training loss = 0.22308471712928551
Iteration 220, Training loss = 0.22140895947813988
Iteration 230, Training loss = 0.22048700600862503
Iteration 240, Training loss = 0.2197775488289503
Iteration 250, Training loss = 0.2199205604310219
Iteration 260, Training loss = 0.21880449297336432
Iteration 270, Training loss = 0.21821017792591682
Iteration 280, Training loss = 0.21709619577114397
Iteration 290, Training loss = 0.21691515755194885
Model training time: 42.78057146072388
Device: cuda
Iteration 0, Training loss = 1.2056493839392295
Iteration 10, Training loss = 0.33231450617313385
Iteration 20, Training loss = 0.27129150554537773
Iteration 30, Training loss = 0.2574315280295335
Iteration 40, Training loss = 0.25079456946024525
Iteration 50, Training loss = 0.2472300394796408
Iteration 60, Training loss = 0.24421527752509484
Iteration 70, Training loss = 0.2422280623935736
Iteration 80, Training loss = 0.24074342703590026
Iteration 90, Training loss = 0.23790139962847417
Iteration 100, Training loss = 0.23753682523965836
Iteration 110, Training loss = 0.2345310733295404
Iteration 120, Training loss = 0.233841283676716
Iteration 130, Training loss = 0.23180460614653733
Iteration 140, Training loss = 0.22954522560422236
Iteration 150, Training loss = 0.2296055698624024
Iteration 160, Training loss = 0.2262217325086777
Iteration 170, Training loss = 0.22622374072670937
Iteration 180, Training loss = 0.2234072433068202
Iteration 190, Training loss = 0.22344472717780334
Iteration 200, Training loss = 0.22161286209638303
Iteration 210, Training loss = 0.22111714402070412
Iteration 220, Training loss = 0.21969485827363455
Iteration 230, Training loss = 0.21837985429626244
Iteration 240, Training loss = 0.2172850754398566
Iteration 250, Training loss = 0.21723984869626853
Iteration 260, Training loss = 0.21590009417671424
Iteration 270, Training loss = 0.21426359191536903
Iteration 280, Training loss = 0.2150734536922895
Iteration 290, Training loss = 0.21243092904870325
Model training time: 42.343034505844116
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0997454088467817
Iteration 10, Training loss = 0.9982505154151183
Iteration 20, Training loss = 0.9961298119563323
Iteration 30, Training loss = 0.9913187004052676
Iteration 40, Training loss = 0.9860155513653388
Iteration 50, Training loss = 0.9807221476848309
Iteration 60, Training loss = 0.9766627080165423
Iteration 70, Training loss = 0.9700721353292465
Iteration 80, Training loss = 0.9616260459789863
Iteration 90, Training loss = 0.9477589669135901
Iteration 100, Training loss = 0.9322561197555982
Iteration 110, Training loss = 0.9121590268153411
Iteration 120, Training loss = 0.8830939645950611
Iteration 130, Training loss = 0.844829818377128
Iteration 140, Training loss = 0.794992175239783
Iteration 150, Training loss = 0.7311157286167145
Iteration 160, Training loss = 0.6588351187797693
Iteration 170, Training loss = 0.5871022802132827
Iteration 180, Training loss = 0.5204655619767996
Iteration 190, Training loss = 0.4677662012668756
Iteration 200, Training loss = 0.43104265744869524
Iteration 210, Training loss = 0.40693508833646774
Iteration 220, Training loss = 0.3925472509402495
Iteration 230, Training loss = 0.3824721397115634
Iteration 240, Training loss = 0.3741316325389422
Iteration 250, Training loss = 0.3680389288526315
Iteration 260, Training loss = 0.3634334636422304
Iteration 270, Training loss = 0.3583970410892597
Iteration 280, Training loss = 0.35413697763131213
Iteration 290, Training loss = 0.3498259341487518
Model training time: 39.16736364364624
Device: cuda
Iteration 0, Training loss = 1.0120874402614741
Iteration 10, Training loss = 1.004308596253395
Iteration 20, Training loss = 0.9986672940162512
Iteration 30, Training loss = 0.9950590248291309
Iteration 40, Training loss = 0.9875019467793978
Iteration 50, Training loss = 0.9832036369121991
Iteration 60, Training loss = 0.9764471913759525
Iteration 70, Training loss = 0.9704630088347656
Iteration 80, Training loss = 0.9608937364358169
Iteration 90, Training loss = 0.9513577234286529
Iteration 100, Training loss = 0.9368282430447065
Iteration 110, Training loss = 0.9198849075115644
Iteration 120, Training loss = 0.9009858713700221
Iteration 130, Training loss = 0.8717973679304123
Iteration 140, Training loss = 0.8378723985873736
Iteration 150, Training loss = 0.7962698856225381
Iteration 160, Training loss = 0.7444089536483471
Iteration 170, Training loss = 0.6880330672630897
Iteration 180, Training loss = 0.6249926227789658
Iteration 190, Training loss = 0.5651515762393291
Iteration 200, Training loss = 0.5099952988899671
Iteration 210, Training loss = 0.4629816710948944
Iteration 220, Training loss = 0.4282143316589869
Iteration 230, Training loss = 0.40343771359095204
Iteration 240, Training loss = 0.3882496953010559
Iteration 250, Training loss = 0.37718140104642284
Iteration 260, Training loss = 0.37057138922122806
Iteration 270, Training loss = 0.36397549194785267
Iteration 280, Training loss = 0.36084717454818577
Iteration 290, Training loss = 0.3550672565515225
Model training time: 38.99811100959778
Device: cuda
Iteration 0, Training loss = 1.0349622254188244
Iteration 10, Training loss = 1.0098857444066267
Iteration 20, Training loss = 1.0021557464049413
Iteration 30, Training loss = 0.9953362678106015
Iteration 40, Training loss = 0.9886570974038198
Iteration 50, Training loss = 0.9806358069181442
Iteration 60, Training loss = 0.9710559936670157
Iteration 70, Training loss = 0.9594394140518628
Iteration 80, Training loss = 0.9470281176842176
Iteration 90, Training loss = 0.9314302217501861
Iteration 100, Training loss = 0.9090446245211822
Iteration 110, Training loss = 0.885787240587748
Iteration 120, Training loss = 0.8529496766053714
Iteration 130, Training loss = 0.813322711449403
Iteration 140, Training loss = 0.7653603221361454
Iteration 150, Training loss = 0.7111657938131919
Iteration 160, Training loss = 0.6520382538437843
Iteration 170, Training loss = 0.5917191883692374
Iteration 180, Training loss = 0.5371449595460525
Iteration 190, Training loss = 0.4912286624312401
Iteration 200, Training loss = 0.45373788991799724
Iteration 210, Training loss = 0.4265926382862605
Iteration 220, Training loss = 0.40681812969537884
Iteration 230, Training loss = 0.39345514602386034
Iteration 240, Training loss = 0.3836699884671431
Iteration 250, Training loss = 0.37671508066929305
Iteration 260, Training loss = 0.3695790068461345
Iteration 270, Training loss = 0.3651678373034184
Iteration 280, Training loss = 0.3602021330824265
Iteration 290, Training loss = 0.3568577388158211
Model training time: 39.056119441986084
Device: cuda
Iteration 0, Training loss = 1.1377318260761409
Iteration 10, Training loss = 1.0048425507086973
Iteration 20, Training loss = 1.003281662097344
Iteration 30, Training loss = 1.0014946896296282
Iteration 40, Training loss = 0.9977692228097182
Iteration 50, Training loss = 0.9963121929993997
Iteration 60, Training loss = 0.9949783396262389
Iteration 70, Training loss = 0.9939180417702749
Iteration 80, Training loss = 0.9896897776768758
Iteration 90, Training loss = 0.988335289634191
Iteration 100, Training loss = 0.983955748952352
Iteration 110, Training loss = 0.9811728711311634
Iteration 120, Training loss = 0.9759842306375504
Iteration 130, Training loss = 0.9734424444345328
Iteration 140, Training loss = 0.9669408546044276
Iteration 150, Training loss = 0.9577584518836095
Iteration 160, Training loss = 0.9480957320103278
Iteration 170, Training loss = 0.9359568609641149
Iteration 180, Training loss = 0.9194727287842677
Iteration 190, Training loss = 0.8981865621530093
Iteration 200, Training loss = 0.8724900885270193
Iteration 210, Training loss = 0.8354174586442801
Iteration 220, Training loss = 0.7920013436904321
Iteration 230, Training loss = 0.7374592664150091
Iteration 240, Training loss = 0.6784083396196365
Iteration 250, Training loss = 0.6149169487448839
Iteration 260, Training loss = 0.5556565391329619
Iteration 270, Training loss = 0.5049147795026119
Iteration 280, Training loss = 0.4661175064169444
Iteration 290, Training loss = 0.4381982030776831
Model training time: 39.07127857208252
Device: cuda
Iteration 0, Training loss = 1.0445161278431232
Iteration 10, Training loss = 0.9987857341766357
Iteration 20, Training loss = 0.9935833605436178
Iteration 30, Training loss = 0.9902061980504256
Iteration 40, Training loss = 0.9868883937597275
Iteration 50, Training loss = 0.980206528535256
Iteration 60, Training loss = 0.973875859609017
Iteration 70, Training loss = 0.96682686989124
Iteration 80, Training loss = 0.9576642341338671
Iteration 90, Training loss = 0.9461932010375537
Iteration 100, Training loss = 0.9330354481935501
Iteration 110, Training loss = 0.9115592929033133
Iteration 120, Training loss = 0.8863516140442628
Iteration 130, Training loss = 0.8540954165733777
Iteration 140, Training loss = 0.8130969290549939
Iteration 150, Training loss = 0.7607215069807493
Iteration 160, Training loss = 0.7002206914699994
Iteration 170, Training loss = 0.6309671321740518
Iteration 180, Training loss = 0.5613923892378807
Iteration 190, Training loss = 0.5004817190078589
Iteration 200, Training loss = 0.45089111305200136
Iteration 210, Training loss = 0.4150925100995944
Iteration 220, Training loss = 0.3922654321560493
Iteration 230, Training loss = 0.3771387120852104
Iteration 240, Training loss = 0.36778488812538296
Iteration 250, Training loss = 0.36249843354408556
Iteration 260, Training loss = 0.3572968943760945
Iteration 270, Training loss = 0.35315002787571687
Iteration 280, Training loss = 0.34871915212044347
Iteration 290, Training loss = 0.34539266331837726
Model training time: 38.869354486465454
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.352494443838413
Iteration 10, Training loss = 0.711792284479508
Iteration 20, Training loss = 0.5305925452938447
Iteration 30, Training loss = 0.4270223287435678
Iteration 40, Training loss = 0.364901996575869
Iteration 50, Training loss = 0.3231814360389343
Iteration 60, Training loss = 0.2943612357171682
Iteration 70, Training loss = 0.27676759087122405
Iteration 80, Training loss = 0.2653042178314466
Iteration 90, Training loss = 0.2581331947675118
Iteration 100, Training loss = 0.25319986131328803
Iteration 110, Training loss = 0.2492748722434044
Iteration 120, Training loss = 0.24689806940463874
Iteration 130, Training loss = 0.24502876143042857
Iteration 140, Training loss = 0.2433469410126026
Iteration 150, Training loss = 0.24218433235700315
Iteration 160, Training loss = 0.24137551050919753
Iteration 170, Training loss = 0.24004473393926254
Iteration 180, Training loss = 0.2389546429308561
Iteration 190, Training loss = 0.23854444519831583
Iteration 200, Training loss = 0.23772602728926218
Iteration 210, Training loss = 0.23666609021333548
Iteration 220, Training loss = 0.2353461247224074
Iteration 230, Training loss = 0.23474750237969252
Iteration 240, Training loss = 0.23437058667723948
Iteration 250, Training loss = 0.23316923758158317
Iteration 260, Training loss = 0.2322500070127157
Iteration 270, Training loss = 0.2309858572597687
Iteration 280, Training loss = 0.23042948114184233
Iteration 290, Training loss = 0.22954882538089386
Model training time: 42.66548728942871
Device: cuda
Iteration 0, Training loss = 0.9799611201653113
Iteration 10, Training loss = 0.34806396812200546
Iteration 20, Training loss = 0.28779128136543125
Iteration 30, Training loss = 0.2676357234326693
Iteration 40, Training loss = 0.25910741491959643
Iteration 50, Training loss = 0.2544628516412698
Iteration 60, Training loss = 0.251459258966721
Iteration 70, Training loss = 0.2500528423832013
Iteration 80, Training loss = 0.24781918468383643
Iteration 90, Training loss = 0.24658868490503386
Iteration 100, Training loss = 0.2466497257924997
Iteration 110, Training loss = 0.24488662097316521
Iteration 120, Training loss = 0.2446755703825217
Iteration 130, Training loss = 0.24326113382211098
Iteration 140, Training loss = 0.24136280927520531
Iteration 150, Training loss = 0.24053039430425718
Iteration 160, Training loss = 0.23987067892001226
Iteration 170, Training loss = 0.23854046544203392
Iteration 180, Training loss = 0.2372478965956431
Iteration 190, Training loss = 0.23563689480607325
Iteration 200, Training loss = 0.23518075708013314
Iteration 210, Training loss = 0.23403301720435804
Iteration 220, Training loss = 0.23303283579074419
Iteration 230, Training loss = 0.23326691068135774
Iteration 240, Training loss = 0.23310732956116015
Iteration 250, Training loss = 0.2328163803769992
Iteration 260, Training loss = 0.23047703676498854
Iteration 270, Training loss = 0.2298459900686374
Iteration 280, Training loss = 0.22912285219018275
Iteration 290, Training loss = 0.22857526919016471
Model training time: 42.46006202697754
Device: cuda
Iteration 0, Training loss = 1.0154412663899934
Iteration 10, Training loss = 0.3654184438861333
Iteration 20, Training loss = 0.28889892250299454
Iteration 30, Training loss = 0.26690854361424077
Iteration 40, Training loss = 0.259088228814877
Iteration 50, Training loss = 0.25453890000398344
Iteration 60, Training loss = 0.2531945106501763
Iteration 70, Training loss = 0.2507547615812375
Iteration 80, Training loss = 0.2496893322811677
Iteration 90, Training loss = 0.2477355097921995
Iteration 100, Training loss = 0.2463622216421824
Iteration 110, Training loss = 0.24505374695246035
Iteration 120, Training loss = 0.2433230449947027
Iteration 130, Training loss = 0.24216048820660666
Iteration 140, Training loss = 0.24151377838391525
Iteration 150, Training loss = 0.2410263751561825
Iteration 160, Training loss = 0.2405913667036937
Iteration 170, Training loss = 0.2384042562200473
Iteration 180, Training loss = 0.2376016375537102
Iteration 190, Training loss = 0.23642508026499015
Iteration 200, Training loss = 0.23662394715043214
Iteration 210, Training loss = 0.23555323424247596
Iteration 220, Training loss = 0.2345446995817698
Iteration 230, Training loss = 0.23358932997171694
Iteration 240, Training loss = 0.23432544045723402
Iteration 250, Training loss = 0.23305851564957544
Iteration 260, Training loss = 0.23233292968227312
Iteration 270, Training loss = 0.23197216884447977
Iteration 280, Training loss = 0.23120896403606123
Iteration 290, Training loss = 0.23026575692571127
Model training time: 42.99287271499634
Device: cuda
Iteration 0, Training loss = 1.0138805565925746
Iteration 10, Training loss = 0.35613690259364933
Iteration 20, Training loss = 0.2868558466434479
Iteration 30, Training loss = 0.2638411900171867
Iteration 40, Training loss = 0.25610951190957654
Iteration 50, Training loss = 0.25174247272885764
Iteration 60, Training loss = 0.2496903002835237
Iteration 70, Training loss = 0.24803675424594146
Iteration 80, Training loss = 0.24657053070572707
Iteration 90, Training loss = 0.2453560273234661
Iteration 100, Training loss = 0.24492447450757027
Iteration 110, Training loss = 0.24406071924246275
Iteration 120, Training loss = 0.24327157982266867
Iteration 130, Training loss = 0.2429652139544487
Iteration 140, Training loss = 0.2420071756037382
Iteration 150, Training loss = 0.24122253174965197
Iteration 160, Training loss = 0.241314509167121
Iteration 170, Training loss = 0.239369514469917
Iteration 180, Training loss = 0.23842630334771597
Iteration 190, Training loss = 0.2386882224908242
Iteration 200, Training loss = 0.23789540620950553
Iteration 210, Training loss = 0.23750649106044036
Iteration 220, Training loss = 0.23627089995604295
Iteration 230, Training loss = 0.2356476798080481
Iteration 240, Training loss = 0.23591679993730325
Iteration 250, Training loss = 0.2351023145020008
Iteration 260, Training loss = 0.23480532633570525
Iteration 270, Training loss = 0.2339772842824459
Iteration 280, Training loss = 0.2338122917482486
Iteration 290, Training loss = 0.2325758203290976
Model training time: 42.4936363697052
Device: cuda
Iteration 0, Training loss = 0.9746782275346609
Iteration 10, Training loss = 0.31731182967240995
Iteration 20, Training loss = 0.27295787689777523
Iteration 30, Training loss = 0.258713777248676
Iteration 40, Training loss = 0.25446488335728645
Iteration 50, Training loss = 0.25155959679530215
Iteration 60, Training loss = 0.24967671243044046
Iteration 70, Training loss = 0.24863598237817103
Iteration 80, Training loss = 0.24794749170541763
Iteration 90, Training loss = 0.24601782858371735
Iteration 100, Training loss = 0.24609808537822503
Iteration 110, Training loss = 0.24406240823177192
Iteration 120, Training loss = 0.24258233205630228
Iteration 130, Training loss = 0.24147979170084
Iteration 140, Training loss = 0.23946059867739677
Iteration 150, Training loss = 0.2382662144417946
Iteration 160, Training loss = 0.23668280731026942
Iteration 170, Training loss = 0.2356897727228128
Iteration 180, Training loss = 0.23380327224731445
Iteration 190, Training loss = 0.2328672996507241
Iteration 200, Training loss = 0.23268779195272005
Iteration 210, Training loss = 0.2290094755589962
Iteration 220, Training loss = 0.2289589296739835
Iteration 230, Training loss = 0.2284772671186007
Iteration 240, Training loss = 0.2276224009692669
Iteration 250, Training loss = 0.22725091244165713
Iteration 260, Training loss = 0.22545297673115364
Iteration 270, Training loss = 0.22523862954515678
Iteration 280, Training loss = 0.224232040059108
Iteration 290, Training loss = 0.2243459144463906
Model training time: 42.78294849395752
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0009127328029046
Iteration 10, Training loss = 0.9966309941731967
Iteration 20, Training loss = 0.9912624531067334
Iteration 30, Training loss = 0.9865219627435391
Iteration 40, Training loss = 0.982449397444725
Iteration 50, Training loss = 0.9781529192741101
Iteration 60, Training loss = 0.9683024138212204
Iteration 70, Training loss = 0.9590875242765133
Iteration 80, Training loss = 0.9478631672950891
Iteration 90, Training loss = 0.9327823794805087
Iteration 100, Training loss = 0.9166434693794984
Iteration 110, Training loss = 0.8943256735801697
Iteration 120, Training loss = 0.8663547577766272
Iteration 130, Training loss = 0.8321644216775894
Iteration 140, Training loss = 0.7918130491788571
Iteration 150, Training loss = 0.7452874103417764
Iteration 160, Training loss = 0.6949698328971863
Iteration 170, Training loss = 0.6410681846050116
Iteration 180, Training loss = 0.5898052362295297
Iteration 190, Training loss = 0.5433379417428603
Iteration 200, Training loss = 0.5039918032976297
Iteration 210, Training loss = 0.4711638574416821
Iteration 220, Training loss = 0.4484091498530828
Iteration 230, Training loss = 0.4274087611299295
Iteration 240, Training loss = 0.41502515513163346
Iteration 250, Training loss = 0.40341860915605837
Iteration 260, Training loss = 0.3956930282024237
Iteration 270, Training loss = 0.38813231312311613
Iteration 280, Training loss = 0.38193568415366685
Iteration 290, Training loss = 0.37659851576273257
Model training time: 39.14393877983093
Device: cuda
Iteration 0, Training loss = 0.9944436630377402
Iteration 10, Training loss = 0.9875549089450103
Iteration 20, Training loss = 0.983303597340217
Iteration 30, Training loss = 0.9756166510857068
Iteration 40, Training loss = 0.9707119373174814
Iteration 50, Training loss = 0.9617000268055842
Iteration 60, Training loss = 0.9507044198421332
Iteration 70, Training loss = 0.9362742751836777
Iteration 80, Training loss = 0.922260181261943
Iteration 90, Training loss = 0.9038449262197201
Iteration 100, Training loss = 0.8811452113665067
Iteration 110, Training loss = 0.8543347601707165
Iteration 120, Training loss = 0.8263230656202023
Iteration 130, Training loss = 0.7924155581455964
Iteration 140, Training loss = 0.7589915027985206
Iteration 150, Training loss = 0.7235629661725118
Iteration 160, Training loss = 0.6890280716694318
Iteration 170, Training loss = 0.6553598183851975
Iteration 180, Training loss = 0.6228878899262502
Iteration 190, Training loss = 0.5928620707530242
Iteration 200, Training loss = 0.5670829942593207
Iteration 210, Training loss = 0.542476508479852
Iteration 220, Training loss = 0.5209860005057775
Iteration 230, Training loss = 0.5031043090499364
Iteration 240, Training loss = 0.4874879333835382
Iteration 250, Training loss = 0.47225800958963543
Iteration 260, Training loss = 0.46165928875024503
Iteration 270, Training loss = 0.45065998515257466
Iteration 280, Training loss = 0.4400691051895802
Iteration 290, Training loss = 0.43147090765146107
Model training time: 39.19358682632446
Device: cuda
Iteration 0, Training loss = 1.0054998455139308
Iteration 10, Training loss = 1.002025843812869
Iteration 20, Training loss = 0.9991236581252172
Iteration 30, Training loss = 0.9970499173952982
Iteration 40, Training loss = 0.9938762004558856
Iteration 50, Training loss = 0.991467317709556
Iteration 60, Training loss = 0.985920053261977
Iteration 70, Training loss = 0.9844583891905271
Iteration 80, Training loss = 0.9765186986097922
Iteration 90, Training loss = 0.9697971940040588
Iteration 100, Training loss = 0.9626811593770981
Iteration 110, Training loss = 0.9495722135672202
Iteration 120, Training loss = 0.9370265889626282
Iteration 130, Training loss = 0.9195699519835986
Iteration 140, Training loss = 0.8960453168703959
Iteration 150, Training loss = 0.8667851514541186
Iteration 160, Training loss = 0.8297209808459649
Iteration 170, Training loss = 0.786777166219858
Iteration 180, Training loss = 0.7370588653362714
Iteration 190, Training loss = 0.6798098316559424
Iteration 200, Training loss = 0.6225360563168159
Iteration 210, Training loss = 0.5705935760186269
Iteration 220, Training loss = 0.5254300110615217
Iteration 230, Training loss = 0.4877438499377324
Iteration 240, Training loss = 0.4582218688267928
Iteration 250, Training loss = 0.4364007023664621
Iteration 260, Training loss = 0.4212230581503648
Iteration 270, Training loss = 0.40949855458277923
Iteration 280, Training loss = 0.4028916909144475
Iteration 290, Training loss = 0.39478988773547685
Model training time: 39.984058141708374
Device: cuda
Iteration 0, Training loss = 1.037443763934649
Iteration 10, Training loss = 1.0107285277201579
Iteration 20, Training loss = 1.0072459269028444
Iteration 30, Training loss = 1.0021103265193791
Iteration 40, Training loss = 0.9970700740814209
Iteration 50, Training loss = 0.9934661193535879
Iteration 60, Training loss = 0.9875863033991593
Iteration 70, Training loss = 0.9823509810062555
Iteration 80, Training loss = 0.9772658095909998
Iteration 90, Training loss = 0.9699394760223535
Iteration 100, Training loss = 0.9613540424750402
Iteration 110, Training loss = 0.9486123713163229
Iteration 120, Training loss = 0.9357809871435165
Iteration 130, Training loss = 0.9154454538455377
Iteration 140, Training loss = 0.8899824848541846
Iteration 150, Training loss = 0.8565872002106446
Iteration 160, Training loss = 0.8149000727213346
Iteration 170, Training loss = 0.7656454008359176
Iteration 180, Training loss = 0.7070937064977793
Iteration 190, Training loss = 0.6443481869422473
Iteration 200, Training loss = 0.5840594728405659
Iteration 210, Training loss = 0.5305742271817647
Iteration 220, Training loss = 0.487345485159984
Iteration 230, Training loss = 0.45524161137067354
Iteration 240, Training loss = 0.4318028814517535
Iteration 250, Training loss = 0.4164395240636972
Iteration 260, Training loss = 0.4045563265681267
Iteration 270, Training loss = 0.397075993510393
Iteration 280, Training loss = 0.38906657294585156
Iteration 290, Training loss = 0.3830232299291171
Model training time: 39.36129856109619
Device: cuda
Iteration 0, Training loss = 1.5291847907579863
Iteration 10, Training loss = 1.0014898627996445
Iteration 20, Training loss = 0.9925469813438562
Iteration 30, Training loss = 0.9888110275451953
Iteration 40, Training loss = 0.9833391824593911
Iteration 50, Training loss = 0.9771426331538421
Iteration 60, Training loss = 0.9730186657263682
Iteration 70, Training loss = 0.9657424722726529
Iteration 80, Training loss = 0.9574769586324692
Iteration 90, Training loss = 0.9452517812068646
Iteration 100, Training loss = 0.932073868238009
Iteration 110, Training loss = 0.9135859436713732
Iteration 120, Training loss = 0.890054844892942
Iteration 130, Training loss = 0.860406749523603
Iteration 140, Training loss = 0.8209786873597366
Iteration 150, Training loss = 0.7744451211049006
Iteration 160, Training loss = 0.7160850453835267
Iteration 170, Training loss = 0.6523271065491897
Iteration 180, Training loss = 0.5857846513390541
Iteration 190, Training loss = 0.5241575745435861
Iteration 200, Training loss = 0.47365011332126766
Iteration 210, Training loss = 0.43427694474275297
Iteration 220, Training loss = 0.40610500138539535
Iteration 230, Training loss = 0.38805563289385575
Iteration 240, Training loss = 0.3762438893318176
Iteration 250, Training loss = 0.3676756889774249
Iteration 260, Training loss = 0.36203667922661853
Iteration 270, Training loss = 0.3574390299618244
Iteration 280, Training loss = 0.3544173346689114
Iteration 290, Training loss = 0.3488782455141728
Model training time: 39.5192186832428
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.1592119680001185
Iteration 10, Training loss = 0.3430040536018518
Iteration 20, Training loss = 0.2781317795698459
Iteration 30, Training loss = 0.2655942142009735
Iteration 40, Training loss = 0.2608678163244174
Iteration 50, Training loss = 0.25702579577381796
Iteration 60, Training loss = 0.2549391363102656
Iteration 70, Training loss = 0.2528046125975939
Iteration 80, Training loss = 0.2514633092169578
Iteration 90, Training loss = 0.2496951139317109
Iteration 100, Training loss = 0.24896454209318528
Iteration 110, Training loss = 0.24748200550675392
Iteration 120, Training loss = 0.24609465352617776
Iteration 130, Training loss = 0.24518318130419806
Iteration 140, Training loss = 0.24455571346558058
Iteration 150, Training loss = 0.24356942136700338
Iteration 160, Training loss = 0.24386703767455542
Iteration 170, Training loss = 0.24197432914605507
Iteration 180, Training loss = 0.24180225741404754
Iteration 190, Training loss = 0.24091396652735197
Iteration 200, Training loss = 0.24080854654312134
Iteration 210, Training loss = 0.24030179874255106
Iteration 220, Training loss = 0.23967625057468048
Iteration 230, Training loss = 0.2389355725966967
Iteration 240, Training loss = 0.2379047839114299
Iteration 250, Training loss = 0.23792094880571732
Iteration 260, Training loss = 0.23713696375489235
Iteration 270, Training loss = 0.23691210351311243
Iteration 280, Training loss = 0.23630930196780425
Iteration 290, Training loss = 0.23637315115103355
Model training time: 43.09737992286682
Device: cuda
Iteration 0, Training loss = 0.982971205161168
Iteration 10, Training loss = 0.3080861046910286
Iteration 20, Training loss = 0.2759581689651196
Iteration 30, Training loss = 0.2674686410106145
Iteration 40, Training loss = 0.26151113097484296
Iteration 50, Training loss = 0.2574276425517522
Iteration 60, Training loss = 0.25586952383701617
Iteration 70, Training loss = 0.252891847720513
Iteration 80, Training loss = 0.25031673134519505
Iteration 90, Training loss = 0.24894941340272242
Iteration 100, Training loss = 0.2470828999693577
Iteration 110, Training loss = 0.24598841884961495
Iteration 120, Training loss = 0.24502274250754943
Iteration 130, Training loss = 0.24298096706087774
Iteration 140, Training loss = 0.24243241892411158
Iteration 150, Training loss = 0.2401586461525697
Iteration 160, Training loss = 0.23932097479701042
Iteration 170, Training loss = 0.2389510784011621
Iteration 180, Training loss = 0.23809014346737128
Iteration 190, Training loss = 0.23603755780137503
Iteration 200, Training loss = 0.23538580995339614
Iteration 210, Training loss = 0.23414031186929116
Iteration 220, Training loss = 0.23361195939091536
Iteration 230, Training loss = 0.2322009500975792
Iteration 240, Training loss = 0.23173588915513113
Iteration 250, Training loss = 0.22959383806357017
Iteration 260, Training loss = 0.2287436110469011
Iteration 270, Training loss = 0.22842233456098116
Iteration 280, Training loss = 0.22651879030924577
Iteration 290, Training loss = 0.22617205719535166
Model training time: 42.83486986160278
Device: cuda
Iteration 0, Training loss = 1.4095329665220702
Iteration 10, Training loss = 0.39856720028015286
Iteration 20, Training loss = 0.2915346711300887
Iteration 30, Training loss = 0.26706953604633993
Iteration 40, Training loss = 0.2595543465935267
Iteration 50, Training loss = 0.25649878268058485
Iteration 60, Training loss = 0.25338021465218985
Iteration 70, Training loss = 0.2506188414990902
Iteration 80, Training loss = 0.24967549110834414
Iteration 90, Training loss = 0.24934080214454576
Iteration 100, Training loss = 0.24747518306741348
Iteration 110, Training loss = 0.24600334350879377
Iteration 120, Training loss = 0.24452753135791191
Iteration 130, Training loss = 0.24400648818566248
Iteration 140, Training loss = 0.24410808774141166
Iteration 150, Training loss = 0.24345313069912103
Iteration 160, Training loss = 0.2420146900873918
Iteration 170, Training loss = 0.24140444541206726
Iteration 180, Training loss = 0.24083020222874787
Iteration 190, Training loss = 0.2411496455852802
Iteration 200, Training loss = 0.24038291063446265
Iteration 210, Training loss = 0.23940314467136675
Iteration 220, Training loss = 0.23978226803816283
Iteration 230, Training loss = 0.23885569606836027
Iteration 240, Training loss = 0.2394429717499476
Iteration 250, Training loss = 0.23841491914712465
Iteration 260, Training loss = 0.23800103079814178
Iteration 270, Training loss = 0.237982003734662
Iteration 280, Training loss = 0.23693352106672066
Iteration 290, Training loss = 0.23754658292119318
Model training time: 43.65135359764099
Device: cuda
Iteration 0, Training loss = 0.9949046545303785
Iteration 10, Training loss = 0.30556342320946545
Iteration 20, Training loss = 0.2686588414586507
Iteration 30, Training loss = 0.26073724776506424
Iteration 40, Training loss = 0.2572111389957942
Iteration 50, Training loss = 0.2543863184176959
Iteration 60, Training loss = 0.2530921777853599
Iteration 70, Training loss = 0.2511905153783468
Iteration 80, Training loss = 0.24823187205653924
Iteration 90, Training loss = 0.24699922221211287
Iteration 100, Training loss = 0.24535865193376175
Iteration 110, Training loss = 0.2439755995113116
Iteration 120, Training loss = 0.24372437137823838
Iteration 130, Training loss = 0.24153732078579757
Iteration 140, Training loss = 0.24065078594363654
Iteration 150, Training loss = 0.2404017333800976
Iteration 160, Training loss = 0.23869448928878859
Iteration 170, Training loss = 0.23796807057582414
Iteration 180, Training loss = 0.23709262162446976
Iteration 190, Training loss = 0.23671323347550172
Iteration 200, Training loss = 0.2363220201088832
Iteration 210, Training loss = 0.23556973269352546
Iteration 220, Training loss = 0.2345756427026712
Iteration 230, Training loss = 0.2347375054198962
Iteration 240, Training loss = 0.234057739090461
Iteration 250, Training loss = 0.23329491225572732
Iteration 260, Training loss = 0.23312389420775267
Iteration 270, Training loss = 0.23309299120536217
Iteration 280, Training loss = 0.23296379756468993
Iteration 290, Training loss = 0.23168091676556146
Model training time: 42.36480712890625
Device: cuda
Iteration 0, Training loss = 1.1326398047117086
Iteration 10, Training loss = 0.33901826521525014
Iteration 20, Training loss = 0.2850771044882444
Iteration 30, Training loss = 0.26565056064954173
Iteration 40, Training loss = 0.25811922951386523
Iteration 50, Training loss = 0.254903557495429
Iteration 60, Training loss = 0.2525460940714066
Iteration 70, Training loss = 0.25006239058879703
Iteration 80, Training loss = 0.24815568213279432
Iteration 90, Training loss = 0.24709533957334665
Iteration 100, Training loss = 0.2455113770870062
Iteration 110, Training loss = 0.24263463427241033
Iteration 120, Training loss = 0.24203417507501748
Iteration 130, Training loss = 0.240731718735053
Iteration 140, Training loss = 0.2382450341605223
Iteration 150, Training loss = 0.23636876018001482
Iteration 160, Training loss = 0.2350240759551525
Iteration 170, Training loss = 0.23430821786706263
Iteration 180, Training loss = 0.2320142800991352
Iteration 190, Training loss = 0.23065830595218217
Iteration 200, Training loss = 0.2293167578486296
Iteration 210, Training loss = 0.22666630979913932
Iteration 220, Training loss = 0.2248365993683155
Iteration 230, Training loss = 0.22339130450899786
Iteration 240, Training loss = 0.22170889062377122
Iteration 250, Training loss = 0.22074938393556154
Iteration 260, Training loss = 0.21957043185830116
Iteration 270, Training loss = 0.21784750945292986
Iteration 280, Training loss = 0.2174062728881836
Iteration 290, Training loss = 0.2160041627402489
Model training time: 42.96892023086548
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.001, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.136779336975171
Iteration 10, Training loss = 0.9658866272522852
Iteration 20, Training loss = 0.9473806149684466
Iteration 30, Training loss = 0.9260763296714196
Iteration 40, Training loss = 0.8980127676175191
Iteration 50, Training loss = 0.8626714979226773
Iteration 60, Training loss = 0.8192038501684482
Iteration 70, Training loss = 0.7686110597390395
Iteration 80, Training loss = 0.7081936849997594
Iteration 90, Training loss = 0.6461307899310038
Iteration 100, Training loss = 0.5841835490786113
Iteration 110, Training loss = 0.5298363251181749
Iteration 120, Training loss = 0.48498632128422076
Iteration 130, Training loss = 0.4510105103254318
Iteration 140, Training loss = 0.4262245652767328
Iteration 150, Training loss = 0.40922722965478897
Iteration 160, Training loss = 0.3967976257778131
Iteration 170, Training loss = 0.38929443577161205
Iteration 180, Training loss = 0.38227721189077085
Iteration 190, Training loss = 0.3764530810026022
Iteration 200, Training loss = 0.3717489941762044
Iteration 210, Training loss = 0.36784013589987385
Iteration 220, Training loss = 0.36216478107067257
Iteration 230, Training loss = 0.36002887040376663
Iteration 240, Training loss = 0.355050104455306
Iteration 250, Training loss = 0.3516990373340937
Iteration 260, Training loss = 0.3476683193674454
Iteration 270, Training loss = 0.3447249887081293
Iteration 280, Training loss = 0.3412211465720947
Iteration 290, Training loss = 0.33861630925765407
Model training time: 39.73284435272217
Device: cuda
Iteration 0, Training loss = 1.2086062064537635
Iteration 10, Training loss = 1.0075563857188592
Iteration 20, Training loss = 1.002283588051796
Iteration 30, Training loss = 0.9979476034641266
Iteration 40, Training loss = 0.9939733434181947
Iteration 50, Training loss = 0.9880041617613572
Iteration 60, Training loss = 0.9818757703671088
Iteration 70, Training loss = 0.975582650074592
Iteration 80, Training loss = 0.9631408338363354
Iteration 90, Training loss = 0.9511882937871493
Iteration 100, Training loss = 0.9377057197002264
Iteration 110, Training loss = 0.9131415165387667
Iteration 120, Training loss = 0.8880555985065607
Iteration 130, Training loss = 0.8524005034795175
Iteration 140, Training loss = 0.808653283577699
Iteration 150, Training loss = 0.7570542383652467
Iteration 160, Training loss = 0.6999461341362733
Iteration 170, Training loss = 0.6443758853352987
Iteration 180, Training loss = 0.5894596295861098
Iteration 190, Training loss = 0.542494762975436
Iteration 200, Training loss = 0.5039555189701227
Iteration 210, Training loss = 0.47339195070358425
Iteration 220, Training loss = 0.44963828646219695
Iteration 230, Training loss = 0.4322579944363007
Iteration 240, Training loss = 0.41801014657203966
Iteration 250, Training loss = 0.40864604596908277
Iteration 260, Training loss = 0.39900073409080505
Iteration 270, Training loss = 0.3910282844534287
Iteration 280, Training loss = 0.38542394913159883
Iteration 290, Training loss = 0.3785817875311925
Model training time: 39.38722515106201
Device: cuda
Iteration 0, Training loss = 1.1988242795834174
Iteration 10, Training loss = 0.9789795107566394
Iteration 20, Training loss = 0.971248678289927
Iteration 30, Training loss = 0.9628267964491477
Iteration 40, Training loss = 0.9522258501786453
Iteration 50, Training loss = 0.9356294675515249
Iteration 60, Training loss = 0.9193405978954755
Iteration 70, Training loss = 0.8935919541579026
Iteration 80, Training loss = 0.8601889335192167
Iteration 90, Training loss = 0.8226405301919351
Iteration 100, Training loss = 0.7729862148945148
Iteration 110, Training loss = 0.7165324263847791
Iteration 120, Training loss = 0.6548137441277504
Iteration 130, Training loss = 0.5924206301569939
Iteration 140, Training loss = 0.535613066301896
Iteration 150, Training loss = 0.4885732422654445
Iteration 160, Training loss = 0.44981033412309795
Iteration 170, Training loss = 0.4227719226708779
Iteration 180, Training loss = 0.40326495239367854
Iteration 190, Training loss = 0.3897156658080908
Iteration 200, Training loss = 0.3788951090895213
Iteration 210, Training loss = 0.37156659536636794
Iteration 220, Training loss = 0.36561527848243713
Iteration 230, Training loss = 0.35908885271503377
Iteration 240, Training loss = 0.35401776891488296
Iteration 250, Training loss = 0.3499683497043756
Iteration 260, Training loss = 0.34610076821767366
Iteration 270, Training loss = 0.34375380781980663
Iteration 280, Training loss = 0.33937291227854216
Iteration 290, Training loss = 0.3364784511236044
Model training time: 39.5405170917511
Device: cuda
Iteration 0, Training loss = 1.019130832873858
Iteration 10, Training loss = 0.9839285486019574
Iteration 20, Training loss = 0.9776889120156949
Iteration 30, Training loss = 0.969216912984848
Iteration 40, Training loss = 0.9595902665303304
Iteration 50, Training loss = 0.9452867347460526
Iteration 60, Training loss = 0.9285270823882177
Iteration 70, Training loss = 0.9046311332629278
Iteration 80, Training loss = 0.8739931216606727
Iteration 90, Training loss = 0.8329552205709311
Iteration 100, Training loss = 0.7819781062694696
Iteration 110, Training loss = 0.7194080249621317
Iteration 120, Training loss = 0.6485134546573346
Iteration 130, Training loss = 0.5748909809268438
Iteration 140, Training loss = 0.5107586200420673
Iteration 150, Training loss = 0.4584052018248118
Iteration 160, Training loss = 0.42078923892516357
Iteration 170, Training loss = 0.39667071058199954
Iteration 180, Training loss = 0.3824199644418863
Iteration 190, Training loss = 0.3722115181959592
Iteration 200, Training loss = 0.36545079717269313
Iteration 210, Training loss = 0.35983192519499707
Iteration 220, Training loss = 0.3552762863154595
Iteration 230, Training loss = 0.35163079087550825
Iteration 240, Training loss = 0.3478201882770428
Iteration 250, Training loss = 0.34430255053135067
Iteration 260, Training loss = 0.3419850649168858
Iteration 270, Training loss = 0.3397083511719337
Iteration 280, Training loss = 0.3357493295692481
Iteration 290, Training loss = 0.3328313139768747
Model training time: 40.155057191848755
Device: cuda
Iteration 0, Training loss = 1.0872026509963548
Iteration 10, Training loss = 0.9973578063341287
Iteration 20, Training loss = 0.9927315574425918
Iteration 30, Training loss = 0.9852192975007571
Iteration 40, Training loss = 0.980404801093615
Iteration 50, Training loss = 0.9729581968142436
Iteration 60, Training loss = 0.9642792584804388
Iteration 70, Training loss = 0.9545498501795989
Iteration 80, Training loss = 0.9383019208908081
Iteration 90, Training loss = 0.9210648765930762
Iteration 100, Training loss = 0.8950355454133108
Iteration 110, Training loss = 0.8618371624212998
Iteration 120, Training loss = 0.8191387630425967
Iteration 130, Training loss = 0.7676557119076068
Iteration 140, Training loss = 0.7050226078583643
Iteration 150, Training loss = 0.6390065757127908
Iteration 160, Training loss = 0.5700645905274612
Iteration 170, Training loss = 0.509939754811617
Iteration 180, Training loss = 0.4602913804925405
Iteration 190, Training loss = 0.42372735188557553
Iteration 200, Training loss = 0.39919056686071247
Iteration 210, Training loss = 0.3825050638272212
Iteration 220, Training loss = 0.37224252693928206
Iteration 230, Training loss = 0.36408039182424545
Iteration 240, Training loss = 0.3581349001480983
Iteration 250, Training loss = 0.35421395989564747
Iteration 260, Training loss = 0.35002479472985637
Iteration 270, Training loss = 0.3452502050651954
Iteration 280, Training loss = 0.34304268256976056
Iteration 290, Training loss = 0.33985945181204724
Model training time: 39.747618198394775
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9324997548873608
Iteration 10, Training loss = 0.2704535178267039
Iteration 20, Training loss = 0.25704935364998305
Iteration 30, Training loss = 0.24951430410146713
Iteration 40, Training loss = 0.24604330183221743
Iteration 50, Training loss = 0.2413899629161908
Iteration 60, Training loss = 0.2377738508467491
Iteration 70, Training loss = 0.2355118070084315
Iteration 80, Training loss = 0.23210608099515623
Iteration 90, Training loss = 0.22905960621742102
Model training time: 14.168379068374634
Device: cuda
Iteration 0, Training loss = 0.9924004375934601
Iteration 10, Training loss = 0.2693241009345421
Iteration 20, Training loss = 0.2554012181667181
Iteration 30, Training loss = 0.24798125974260843
Iteration 40, Training loss = 0.24582780582400468
Iteration 50, Training loss = 0.23941961847818816
Iteration 60, Training loss = 0.2367604778936276
Iteration 70, Training loss = 0.23258708159510905
Iteration 80, Training loss = 0.23141029477119446
Iteration 90, Training loss = 0.22647802216502336
Model training time: 14.053405046463013
Device: cuda
Iteration 0, Training loss = 0.9215566515922546
Iteration 10, Training loss = 0.2618068608527
Iteration 20, Training loss = 0.24993804211799914
Iteration 30, Training loss = 0.2457551658153534
Iteration 40, Training loss = 0.24142268414680773
Iteration 50, Training loss = 0.23918596368569595
Iteration 60, Training loss = 0.23692855812036073
Iteration 70, Training loss = 0.2344636948635945
Iteration 80, Training loss = 0.2314730200629968
Iteration 90, Training loss = 0.23088707746221468
Model training time: 14.099438905715942
Device: cuda
Iteration 0, Training loss = 0.955198674247815
Iteration 10, Training loss = 0.2634747847914696
Iteration 20, Training loss = 0.2497473694384098
Iteration 30, Training loss = 0.24448891901052916
Iteration 40, Training loss = 0.24090778455138206
Iteration 50, Training loss = 0.2363705405822167
Iteration 60, Training loss = 0.23286479224379247
Iteration 70, Training loss = 0.2286857564288836
Iteration 80, Training loss = 0.22589082557421464
Iteration 90, Training loss = 0.22201097985872856
Model training time: 14.027830362319946
Device: cuda
Iteration 0, Training loss = 1.1646444339018602
Iteration 10, Training loss = 0.2757752723991871
Iteration 20, Training loss = 0.2529651304850212
Iteration 30, Training loss = 0.24497336865617678
Iteration 40, Training loss = 0.24137271396242654
Iteration 50, Training loss = 0.23926914425996634
Iteration 60, Training loss = 0.23660641478804442
Iteration 70, Training loss = 0.2342045817237634
Iteration 80, Training loss = 0.23264843311447364
Iteration 90, Training loss = 0.23080036645898452
Model training time: 14.139156103134155
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.2274181636480184
Iteration 10, Training loss = 0.9681388495060114
Iteration 20, Training loss = 0.9408995279899011
Iteration 30, Training loss = 0.8965281465878854
Iteration 40, Training loss = 0.82360695875608
Iteration 50, Training loss = 0.7179742936904614
Iteration 60, Training loss = 0.5949486224697187
Iteration 70, Training loss = 0.4947254875531563
Iteration 80, Training loss = 0.43277248854820544
Iteration 90, Training loss = 0.4013180864545015
Model training time: 12.981626033782959
Device: cuda
Iteration 0, Training loss = 1.1934150331295454
Iteration 10, Training loss = 0.9853601730786837
Iteration 20, Training loss = 0.969716916863735
Iteration 30, Training loss = 0.9468909696890757
Iteration 40, Training loss = 0.9145449021687875
Iteration 50, Training loss = 0.8571384090643662
Iteration 60, Training loss = 0.774186528646029
Iteration 70, Training loss = 0.665143527663671
Iteration 80, Training loss = 0.5564748564591775
Iteration 90, Training loss = 0.4729628866681686
Model training time: 13.088490009307861
Device: cuda
Iteration 0, Training loss = 1.0453992199439268
Iteration 10, Training loss = 0.9924545631958888
Iteration 20, Training loss = 0.9853763075975271
Iteration 30, Training loss = 0.972417678970557
Iteration 40, Training loss = 0.9571502953767776
Iteration 50, Training loss = 0.932305246591568
Iteration 60, Training loss = 0.8911943172033017
Iteration 70, Training loss = 0.8282585946413187
Iteration 80, Training loss = 0.7336948800545472
Iteration 90, Training loss = 0.620965070449389
Model training time: 13.320131540298462
Device: cuda
Iteration 0, Training loss = 1.0348587402930627
Iteration 10, Training loss = 0.9840086022248635
Iteration 20, Training loss = 0.9595638868900446
Iteration 30, Training loss = 0.9139213814185216
Iteration 40, Training loss = 0.8374703331635549
Iteration 50, Training loss = 0.7155822905210348
Iteration 60, Training loss = 0.5684573157475545
Iteration 70, Training loss = 0.45678630299293077
Iteration 80, Training loss = 0.4061299321743158
Iteration 90, Training loss = 0.38539672241761136
Model training time: 13.428799390792847
Device: cuda
Iteration 0, Training loss = 1.1070043788506434
Iteration 10, Training loss = 0.9661061740838565
Iteration 20, Training loss = 0.9354794220282481
Iteration 30, Training loss = 0.8864415906942807
Iteration 40, Training loss = 0.8082581116602972
Iteration 50, Training loss = 0.6989345390063065
Iteration 60, Training loss = 0.5798597513483121
Iteration 70, Training loss = 0.4877965221038231
Iteration 80, Training loss = 0.4310902190896181
Iteration 90, Training loss = 0.4010860312443513
Model training time: 13.5722815990448
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9783038966930829
Iteration 10, Training loss = 0.29172884558255857
Iteration 20, Training loss = 0.2626756028487132
Iteration 30, Training loss = 0.25604076425616557
Iteration 40, Training loss = 0.2518068930277458
Iteration 50, Training loss = 0.2495705631489937
Iteration 60, Training loss = 0.24888847424433783
Iteration 70, Training loss = 0.24512636776153857
Iteration 80, Training loss = 0.24416657909750938
Iteration 90, Training loss = 0.24191026876752192
Model training time: 15.353815793991089
Device: cuda
Iteration 0, Training loss = 1.01222075178073
Iteration 10, Training loss = 0.2918755615559908
Iteration 20, Training loss = 0.26349496468901634
Iteration 30, Training loss = 0.25566983165649265
Iteration 40, Training loss = 0.25010635302617
Iteration 50, Training loss = 0.2478228136897087
Iteration 60, Training loss = 0.24495491643364614
Iteration 70, Training loss = 0.24438262816805106
Iteration 80, Training loss = 0.24151466872829658
Iteration 90, Training loss = 0.24001769434947234
Model training time: 15.746273279190063
Device: cuda
Iteration 0, Training loss = 0.9519070283724711
Iteration 10, Training loss = 0.2743518768021694
Iteration 20, Training loss = 0.255584399574078
Iteration 30, Training loss = 0.24900544921939188
Iteration 40, Training loss = 0.24447481276897284
Iteration 50, Training loss = 0.24121905548068193
Iteration 60, Training loss = 0.23921770573808596
Iteration 70, Training loss = 0.2381268968948951
Iteration 80, Training loss = 0.23612991290596816
Iteration 90, Training loss = 0.2347343105536241
Model training time: 15.524091005325317
Device: cuda
Iteration 0, Training loss = 0.994876123391665
Iteration 10, Training loss = 0.29441362103590596
Iteration 20, Training loss = 0.2587963712330048
Iteration 30, Training loss = 0.25046172508826625
Iteration 40, Training loss = 0.24613693872323403
Iteration 50, Training loss = 0.24152049307639784
Iteration 60, Training loss = 0.23868721608932203
Iteration 70, Training loss = 0.2363803515640589
Iteration 80, Training loss = 0.2357535955424492
Iteration 90, Training loss = 0.2341671305207106
Model training time: 15.210918664932251
Device: cuda
Iteration 0, Training loss = 0.9824967143627313
Iteration 10, Training loss = 0.2897869709592599
Iteration 20, Training loss = 0.26163564737026507
Iteration 30, Training loss = 0.254684356542734
Iteration 40, Training loss = 0.2526057018683507
Iteration 50, Training loss = 0.2503800575549786
Iteration 60, Training loss = 0.24875578570824403
Iteration 70, Training loss = 0.24723805429843757
Iteration 80, Training loss = 0.24522040096613076
Iteration 90, Training loss = 0.24446826428174973
Model training time: 14.799957990646362
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0031528908472795
Iteration 10, Training loss = 0.9729289343723884
Iteration 20, Training loss = 0.9508110307730161
Iteration 30, Training loss = 0.9175187337857026
Iteration 40, Training loss = 0.8637185876186078
Iteration 50, Training loss = 0.7879800223387204
Iteration 60, Training loss = 0.6899162622598501
Iteration 70, Training loss = 0.590208293726811
Iteration 80, Training loss = 0.5077137540166194
Iteration 90, Training loss = 0.45230327718533003
Model training time: 13.328222513198853
Device: cuda
Iteration 0, Training loss = 1.3346834251513848
Iteration 10, Training loss = 0.9787615973215836
Iteration 20, Training loss = 0.9631843910767481
Iteration 30, Training loss = 0.9388697903889877
Iteration 40, Training loss = 0.9025655824404496
Iteration 50, Training loss = 0.851025219147022
Iteration 60, Training loss = 0.7765720177155274
Iteration 70, Training loss = 0.6815049189787644
Iteration 80, Training loss = 0.5826663059683946
Iteration 90, Training loss = 0.5018056608163394
Model training time: 13.419872999191284
Device: cuda
Iteration 0, Training loss = 1.0511625179877648
Iteration 10, Training loss = 0.9947602359148172
Iteration 20, Training loss = 0.9856726068716782
Iteration 30, Training loss = 0.9755505999693503
Iteration 40, Training loss = 0.95828965650155
Iteration 50, Training loss = 0.9278233830745404
Iteration 60, Training loss = 0.8760928970116836
Iteration 70, Training loss = 0.7907409484569843
Iteration 80, Training loss = 0.6720253584476618
Iteration 90, Training loss = 0.5411924026333369
Model training time: 13.281938552856445
Device: cuda
Iteration 0, Training loss = 1.3643737572890062
Iteration 10, Training loss = 0.9923806396814493
Iteration 20, Training loss = 0.9800818940767875
Iteration 30, Training loss = 0.968468977854802
Iteration 40, Training loss = 0.9459079309151723
Iteration 50, Training loss = 0.9061918636927238
Iteration 60, Training loss = 0.8414464443922043
Iteration 70, Training loss = 0.745198945586498
Iteration 80, Training loss = 0.6274412641158471
Iteration 90, Training loss = 0.5205636431391423
Model training time: 13.418274402618408
Device: cuda
Iteration 0, Training loss = 1.4163945088019738
Iteration 10, Training loss = 1.000904190998811
Iteration 20, Training loss = 0.9939295752690389
Iteration 30, Training loss = 0.9871584394803414
Iteration 40, Training loss = 0.9783045122256646
Iteration 50, Training loss = 0.9633710258282148
Iteration 60, Training loss = 0.9438804686069489
Iteration 70, Training loss = 0.907839725796993
Iteration 80, Training loss = 0.8503123372793198
Iteration 90, Training loss = 0.7629260799059501
Model training time: 13.19731330871582
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9478652465801972
Iteration 10, Training loss = 0.2746807921391267
Iteration 20, Training loss = 0.2619752949820115
Iteration 30, Training loss = 0.2559628701553895
Iteration 40, Training loss = 0.2506567368713709
Iteration 50, Training loss = 0.24838253253927597
Iteration 60, Training loss = 0.24399657920002937
Iteration 70, Training loss = 0.24057699625308698
Iteration 80, Training loss = 0.23760545540314454
Iteration 90, Training loss = 0.23469314294365737
Model training time: 14.153885841369629
Device: cuda
Iteration 0, Training loss = 1.1005429728673055
Iteration 10, Training loss = 0.31085720973519176
Iteration 20, Training loss = 0.26617374185186166
Iteration 30, Training loss = 0.2569281349961574
Iteration 40, Training loss = 0.25228030349199587
Iteration 50, Training loss = 0.24783547222614288
Iteration 60, Training loss = 0.24658484269793218
Iteration 70, Training loss = 0.24299085713349855
Iteration 80, Training loss = 0.24280513565127665
Iteration 90, Training loss = 0.2404039717064454
Model training time: 14.083925008773804
Device: cuda
Iteration 0, Training loss = 1.2169119406204958
Iteration 10, Training loss = 0.29851563188892144
Iteration 20, Training loss = 0.2647316988844138
Iteration 30, Training loss = 0.25719370578344053
Iteration 40, Training loss = 0.2537617560189504
Iteration 50, Training loss = 0.24905617133929178
Iteration 60, Training loss = 0.24387171520636633
Iteration 70, Training loss = 0.24092041242581147
Iteration 80, Training loss = 0.23817281052470207
Iteration 90, Training loss = 0.23576606466219976
Model training time: 14.141908884048462
Device: cuda
Iteration 0, Training loss = 0.9583997416954774
Iteration 10, Training loss = 0.26624925463245463
Iteration 20, Training loss = 0.2533568267065745
Iteration 30, Training loss = 0.24894355753293404
Iteration 40, Training loss = 0.24552192739569223
Iteration 50, Training loss = 0.242951466773565
Iteration 60, Training loss = 0.2409725905610965
Iteration 70, Training loss = 0.23906100971194413
Iteration 80, Training loss = 0.23697605929695642
Iteration 90, Training loss = 0.2353582218862497
Model training time: 14.165820360183716
Device: cuda
Iteration 0, Training loss = 0.9350050584628031
Iteration 10, Training loss = 0.2718591013780007
Iteration 20, Training loss = 0.25840622702470195
Iteration 30, Training loss = 0.2530771726025985
Iteration 40, Training loss = 0.24928661636435068
Iteration 50, Training loss = 0.24695084501917547
Iteration 60, Training loss = 0.24352013377042916
Iteration 70, Training loss = 0.24024005578114435
Iteration 80, Training loss = 0.23622892023279116
Iteration 90, Training loss = 0.23394513359436622
Model training time: 14.038997650146484
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 100, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0602169162951982
Iteration 10, Training loss = 0.9923544617799612
Iteration 20, Training loss = 0.9780615533773716
Iteration 30, Training loss = 0.960857018828392
Iteration 40, Training loss = 0.9304025413898321
Iteration 50, Training loss = 0.8843818685183158
Iteration 60, Training loss = 0.8058857654149716
Iteration 70, Training loss = 0.6977069217425126
Iteration 80, Training loss = 0.5809075924066397
Iteration 90, Training loss = 0.4914885134651111
Model training time: 13.150045394897461
Device: cuda
Iteration 0, Training loss = 1.0372913972689555
Iteration 10, Training loss = 0.9855416359809729
Iteration 20, Training loss = 0.9702154042629095
Iteration 30, Training loss = 0.9414076060056686
Iteration 40, Training loss = 0.9010159178422048
Iteration 50, Training loss = 0.8363093997423465
Iteration 60, Training loss = 0.7458301060474836
Iteration 70, Training loss = 0.6406247157316941
Iteration 80, Training loss = 0.5461130554859455
Iteration 90, Training loss = 0.4768680620651979
Model training time: 12.98447299003601
Device: cuda
Iteration 0, Training loss = 1.0786922894991362
Iteration 10, Training loss = 0.9851261503421344
Iteration 20, Training loss = 0.9733226001262665
Iteration 30, Training loss = 0.9521303864625784
Iteration 40, Training loss = 0.9202297983261255
Iteration 50, Training loss = 0.8711973623587534
Iteration 60, Training loss = 0.7970813134541879
Iteration 70, Training loss = 0.7015700328808564
Iteration 80, Training loss = 0.5987151643404593
Iteration 90, Training loss = 0.5100057554932741
Model training time: 13.203946590423584
Device: cuda
Iteration 0, Training loss = 1.3003920912742615
Iteration 10, Training loss = 0.9882834920516381
Iteration 20, Training loss = 0.9784584629994172
Iteration 30, Training loss = 0.9673376713807766
Iteration 40, Training loss = 0.9500340899595847
Iteration 50, Training loss = 0.9185157349476447
Iteration 60, Training loss = 0.8665823111167321
Iteration 70, Training loss = 0.784208588875257
Iteration 80, Training loss = 0.6727989218555964
Iteration 90, Training loss = 0.551281492297466
Model training time: 13.564336776733398
Device: cuda
Iteration 0, Training loss = 1.013455597253946
Iteration 10, Training loss = 0.9625931531190872
Iteration 20, Training loss = 0.9346046837476584
Iteration 30, Training loss = 0.8896449265571741
Iteration 40, Training loss = 0.8189575167802664
Iteration 50, Training loss = 0.7142021048527497
Iteration 60, Training loss = 0.5947095998204671
Iteration 70, Training loss = 0.49424771620677066
Iteration 80, Training loss = 0.4348111404822423
Iteration 90, Training loss = 0.40376765452898467
Model training time: 13.625018119812012
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9335094312062631
Iteration 10, Training loss = 0.265605562294905
Iteration 20, Training loss = 0.25329258006352645
Iteration 30, Training loss = 0.2465220311513314
Iteration 40, Training loss = 0.24261429905891418
Iteration 50, Training loss = 0.23890950473455283
Iteration 60, Training loss = 0.23527737936148277
Iteration 70, Training loss = 0.2334145253094343
Iteration 80, Training loss = 0.2301004076233277
Iteration 90, Training loss = 0.22786461762510812
Iteration 100, Training loss = 0.22668487635942605
Iteration 110, Training loss = 0.22326862869354394
Iteration 120, Training loss = 0.22233155914224112
Iteration 130, Training loss = 0.21926133477917084
Iteration 140, Training loss = 0.21819347229141456
Iteration 150, Training loss = 0.21635458131249136
Iteration 160, Training loss = 0.21341158363681573
Iteration 170, Training loss = 0.2129070068208071
Iteration 180, Training loss = 0.21118958466328108
Iteration 190, Training loss = 0.21120413879935557
Model training time: 28.996154069900513
Device: cuda
Iteration 0, Training loss = 0.9400824709580495
Iteration 10, Training loss = 0.2682718660395879
Iteration 20, Training loss = 0.25433529684176814
Iteration 30, Training loss = 0.249637582554267
Iteration 40, Training loss = 0.24680774057140717
Iteration 50, Training loss = 0.2438295234281283
Iteration 60, Training loss = 0.2439894676208496
Iteration 70, Training loss = 0.24237352093824974
Iteration 80, Training loss = 0.23828228964255407
Iteration 90, Training loss = 0.2366337928061302
Iteration 100, Training loss = 0.2344184242762052
Iteration 110, Training loss = 0.23153599724173546
Iteration 120, Training loss = 0.2303103982256009
Iteration 130, Training loss = 0.22928935604599807
Iteration 140, Training loss = 0.22741870782696283
Iteration 150, Training loss = 0.22553539677308157
Iteration 160, Training loss = 0.2240209886087821
Iteration 170, Training loss = 0.22184008044692186
Iteration 180, Training loss = 0.22073550751576057
Iteration 190, Training loss = 0.2182934949031243
Model training time: 28.102503299713135
Device: cuda
Iteration 0, Training loss = 0.9397275379070869
Iteration 10, Training loss = 0.2625120491362535
Iteration 20, Training loss = 0.2516673490978204
Iteration 30, Training loss = 0.24699356206334555
Iteration 40, Training loss = 0.24036756100562903
Iteration 50, Training loss = 0.23795888830836004
Iteration 60, Training loss = 0.23396648208682352
Iteration 70, Training loss = 0.2322612817470844
Iteration 80, Training loss = 0.22833218798041344
Iteration 90, Training loss = 0.22609512106730387
Iteration 100, Training loss = 0.2233775707964714
Iteration 110, Training loss = 0.22155615114248717
Iteration 120, Training loss = 0.2198165122133035
Iteration 130, Training loss = 0.2194097225482647
Iteration 140, Training loss = 0.21653525760540596
Iteration 150, Training loss = 0.21421143097373155
Iteration 160, Training loss = 0.2132643274962902
Iteration 170, Training loss = 0.21218748619923225
Iteration 180, Training loss = 0.21173188978662857
Iteration 190, Training loss = 0.21042326064063951
Model training time: 28.07809591293335
Device: cuda
Iteration 0, Training loss = 0.9205489479578458
Iteration 10, Training loss = 0.260929945569772
Iteration 20, Training loss = 0.25012463933000195
Iteration 30, Training loss = 0.24704780916755015
Iteration 40, Training loss = 0.24304138811735007
Iteration 50, Training loss = 0.24056439302288568
Iteration 60, Training loss = 0.23855083607710326
Iteration 70, Training loss = 0.2376593600672025
Iteration 80, Training loss = 0.23404867230699614
Iteration 90, Training loss = 0.2326622040799031
Iteration 100, Training loss = 0.23126167030288622
Iteration 110, Training loss = 0.2289858554991392
Iteration 120, Training loss = 0.22727021660942298
Iteration 130, Training loss = 0.2256292333969703
Iteration 140, Training loss = 0.22502844064281538
Iteration 150, Training loss = 0.22295314216843018
Iteration 160, Training loss = 0.22187140985177115
Iteration 170, Training loss = 0.2203546089048569
Iteration 180, Training loss = 0.22042869948423827
Iteration 190, Training loss = 0.2185409401471798
Model training time: 28.251874685287476
Device: cuda
Iteration 0, Training loss = 0.9562653131209887
Iteration 10, Training loss = 0.2618077815725253
Iteration 20, Training loss = 0.24710928849302805
Iteration 30, Training loss = 0.24136148794339254
Iteration 40, Training loss = 0.23627471608611253
Iteration 50, Training loss = 0.23052232884443724
Iteration 60, Training loss = 0.22694377629802778
Iteration 70, Training loss = 0.22322479807413542
Iteration 80, Training loss = 0.21919269143388823
Iteration 90, Training loss = 0.21643458134852922
Iteration 100, Training loss = 0.21545069062939057
Iteration 110, Training loss = 0.21330320118711546
Iteration 120, Training loss = 0.21220857821978056
Iteration 130, Training loss = 0.21049993657148802
Iteration 140, Training loss = 0.20810303263939345
Iteration 150, Training loss = 0.20945397592507875
Iteration 160, Training loss = 0.20738240302755281
Iteration 170, Training loss = 0.20531784370541573
Iteration 180, Training loss = 0.20445684744761541
Iteration 190, Training loss = 0.20280870107504037
Model training time: 28.84186816215515
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.014599003470861
Iteration 10, Training loss = 1.0007049934222148
Iteration 20, Training loss = 0.9890011342672201
Iteration 30, Training loss = 0.9754006908490107
Iteration 40, Training loss = 0.9535498068882868
Iteration 50, Training loss = 0.9214612956230457
Iteration 60, Training loss = 0.8651172553117459
Iteration 70, Training loss = 0.7739841147111013
Iteration 80, Training loss = 0.6426405643041317
Iteration 90, Training loss = 0.5138838222393622
Iteration 100, Training loss = 0.4347548966224377
Iteration 110, Training loss = 0.40118034527851987
Iteration 120, Training loss = 0.3839465448489556
Iteration 130, Training loss = 0.3726496146275447
Iteration 140, Training loss = 0.3649841088515062
Iteration 150, Training loss = 0.35746646099365675
Iteration 160, Training loss = 0.35193780351143616
Iteration 170, Training loss = 0.3454467012332036
Iteration 180, Training loss = 0.3416434343044574
Iteration 190, Training loss = 0.3361739149460426
Model training time: 26.448039054870605
Device: cuda
Iteration 0, Training loss = 1.0307553593928998
Iteration 10, Training loss = 1.0011029575879757
Iteration 20, Training loss = 0.9826568193160571
Iteration 30, Training loss = 0.9595850729025327
Iteration 40, Training loss = 0.9249021949676367
Iteration 50, Training loss = 0.8705672988524804
Iteration 60, Training loss = 0.7847292090837772
Iteration 70, Training loss = 0.6719658775971487
Iteration 80, Training loss = 0.5521410961563771
Iteration 90, Training loss = 0.4629884178821857
Iteration 100, Training loss = 0.4158726752950595
Iteration 110, Training loss = 0.3927555691737395
Iteration 120, Training loss = 0.37853867274064285
Iteration 130, Training loss = 0.36741731143914735
Iteration 140, Training loss = 0.3584959226158949
Iteration 150, Training loss = 0.35133298944968444
Iteration 160, Training loss = 0.34632733578865343
Iteration 170, Training loss = 0.33836517798212856
Iteration 180, Training loss = 0.33325220472537553
Iteration 190, Training loss = 0.3300041009027224
Model training time: 26.656928539276123
Device: cuda
Iteration 0, Training loss = 1.4841284293394823
Iteration 10, Training loss = 1.0013703188070884
Iteration 20, Training loss = 0.9964533001184464
Iteration 30, Training loss = 0.992577075958252
Iteration 40, Training loss = 0.9874025514492621
Iteration 50, Training loss = 0.9800456280891712
Iteration 60, Training loss = 0.972276956989215
Iteration 70, Training loss = 0.9555376699337592
Iteration 80, Training loss = 0.9329984807051145
Iteration 90, Training loss = 0.897252245591237
Iteration 100, Training loss = 0.8433389354210633
Iteration 110, Training loss = 0.7643273174762726
Iteration 120, Training loss = 0.6652695227127808
Iteration 130, Training loss = 0.5613892307648292
Iteration 140, Training loss = 0.47837033008153623
Iteration 150, Training loss = 0.4236673563718796
Iteration 160, Training loss = 0.3969639682999024
Iteration 170, Training loss = 0.3805829882621765
Iteration 180, Training loss = 0.3712451005211243
Iteration 190, Training loss = 0.3622822265785474
Model training time: 26.12939739227295
Device: cuda
Iteration 0, Training loss = 1.009643791959836
Iteration 10, Training loss = 0.9924382280844909
Iteration 20, Training loss = 0.9833808449598459
Iteration 30, Training loss = 0.9724931705456513
Iteration 40, Training loss = 0.9523334915821369
Iteration 50, Training loss = 0.920623602775427
Iteration 60, Training loss = 0.8625634679427514
Iteration 70, Training loss = 0.7676785691426351
Iteration 80, Training loss = 0.6335414493313203
Iteration 90, Training loss = 0.5046261010261682
Iteration 100, Training loss = 0.4238086692415751
Iteration 110, Training loss = 0.38752426837499326
Iteration 120, Training loss = 0.3688691264161697
Iteration 130, Training loss = 0.3577112309061564
Iteration 140, Training loss = 0.3484761147544934
Iteration 150, Training loss = 0.3405468323482917
Iteration 160, Training loss = 0.33517071318167907
Iteration 170, Training loss = 0.32880322749798113
Iteration 180, Training loss = 0.3236627343755502
Iteration 190, Training loss = 0.32122537608330065
Model training time: 26.584209203720093
Device: cuda
Iteration 0, Training loss = 1.0530136640255268
Iteration 10, Training loss = 0.9945234106137202
Iteration 20, Training loss = 0.9828314299766834
Iteration 30, Training loss = 0.9690603350217526
Iteration 40, Training loss = 0.9449282987759664
Iteration 50, Training loss = 0.9071070781120887
Iteration 60, Training loss = 0.8392428652598307
Iteration 70, Training loss = 0.731987150815817
Iteration 80, Training loss = 0.5913081535926232
Iteration 90, Training loss = 0.4701940159385021
Iteration 100, Training loss = 0.40243856264994693
Iteration 110, Training loss = 0.3735670384306174
Iteration 120, Training loss = 0.3601804475944776
Iteration 130, Training loss = 0.3509336985074557
Iteration 140, Training loss = 0.34154690916721636
Iteration 150, Training loss = 0.3343723639845848
Iteration 160, Training loss = 0.3277812571479724
Iteration 170, Training loss = 0.3244508241231625
Iteration 180, Training loss = 0.31860533442634803
Iteration 190, Training loss = 0.3153272712459931
Model training time: 26.167952060699463
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 1.350210196696795
Iteration 10, Training loss = 0.32273123069451404
Iteration 20, Training loss = 0.2727679730607913
Iteration 30, Training loss = 0.26073342017256296
Iteration 40, Training loss = 0.2557869963347912
Iteration 50, Training loss = 0.2519273216334673
Iteration 60, Training loss = 0.25080080416340095
Iteration 70, Training loss = 0.2482750003154461
Iteration 80, Training loss = 0.24576068153748146
Iteration 90, Training loss = 0.2432203616660375
Iteration 100, Training loss = 0.24171626854401368
Iteration 110, Training loss = 0.2393193906889512
Iteration 120, Training loss = 0.2384266111140068
Iteration 130, Training loss = 0.23740155307146218
Iteration 140, Training loss = 0.23528216693263787
Iteration 150, Training loss = 0.23714602280121583
Iteration 160, Training loss = 0.23406369554308745
Iteration 170, Training loss = 0.23351010078421006
Iteration 180, Training loss = 0.23165507442676103
Iteration 190, Training loss = 0.23083345792614496
Model training time: 28.104586124420166
Device: cuda
Iteration 0, Training loss = 1.041044560762552
Iteration 10, Training loss = 0.31230814955555475
Iteration 20, Training loss = 0.26149786836825883
Iteration 30, Training loss = 0.25150657244599783
Iteration 40, Training loss = 0.24769910768820688
Iteration 50, Training loss = 0.24373733911376733
Iteration 60, Training loss = 0.241394358758743
Iteration 70, Training loss = 0.24053394421935081
Iteration 80, Training loss = 0.23773995471688417
Iteration 90, Training loss = 0.2368877180493795
Iteration 100, Training loss = 0.2359402053631269
Iteration 110, Training loss = 0.2346769949564567
Iteration 120, Training loss = 0.23257326592619604
Iteration 130, Training loss = 0.23079092771961138
Iteration 140, Training loss = 0.22920748935295984
Iteration 150, Training loss = 0.22657361024847397
Iteration 160, Training loss = 0.22459304361389235
Iteration 170, Training loss = 0.22301759513524863
Iteration 180, Training loss = 0.22173730083383048
Iteration 190, Training loss = 0.2208379148863829
Model training time: 28.33550000190735
Device: cuda
Iteration 0, Training loss = 0.9928292941588622
Iteration 10, Training loss = 0.28489631328445214
Iteration 20, Training loss = 0.26253222616819233
Iteration 30, Training loss = 0.25450929598166394
Iteration 40, Training loss = 0.24814952480105254
Iteration 50, Training loss = 0.2442726120352745
Iteration 60, Training loss = 0.24130062157144913
Iteration 70, Training loss = 0.23745170808755434
Iteration 80, Training loss = 0.23523035645484924
Iteration 90, Training loss = 0.23168719760500467
Iteration 100, Training loss = 0.23023060623269814
Iteration 110, Training loss = 0.22806556293597588
Iteration 120, Training loss = 0.22896976683002251
Iteration 130, Training loss = 0.22672759454983932
Iteration 140, Training loss = 0.22470618039369583
Iteration 150, Training loss = 0.22326261464219826
Iteration 160, Training loss = 0.22222334490372583
Iteration 170, Training loss = 0.2207610701712278
Iteration 180, Training loss = 0.21927895683508652
Iteration 190, Training loss = 0.2191039871137876
Model training time: 28.73962426185608
Device: cuda
Iteration 0, Training loss = 0.9726373965923603
Iteration 10, Training loss = 0.2760303286978832
Iteration 20, Training loss = 0.256884152499529
Iteration 30, Training loss = 0.25311488600877613
Iteration 40, Training loss = 0.250092036735553
Iteration 50, Training loss = 0.24782304465770721
Iteration 60, Training loss = 0.24547101328006157
Iteration 70, Training loss = 0.24366318921630198
Iteration 80, Training loss = 0.24222194002224848
Iteration 90, Training loss = 0.23999554090774977
Iteration 100, Training loss = 0.2389563379379419
Iteration 110, Training loss = 0.2381410994208776
Iteration 120, Training loss = 0.2365132776590494
Iteration 130, Training loss = 0.2353572788146826
Iteration 140, Training loss = 0.23292891222697037
Iteration 150, Training loss = 0.23198689377078643
Iteration 160, Training loss = 0.22986460505769804
Iteration 170, Training loss = 0.22860258703048414
Iteration 180, Training loss = 0.22680392431525084
Iteration 190, Training loss = 0.22588225998557532
Model training time: 28.609755277633667
Device: cuda
Iteration 0, Training loss = 0.9718171025698001
Iteration 10, Training loss = 0.28492991339701873
Iteration 20, Training loss = 0.2542387439081302
Iteration 30, Training loss = 0.24554370601589864
Iteration 40, Training loss = 0.24259819119022444
Iteration 50, Training loss = 0.24087047089750951
Iteration 60, Training loss = 0.23857535479160455
Iteration 70, Training loss = 0.23715862545829552
Iteration 80, Training loss = 0.23528310713859704
Iteration 90, Training loss = 0.23418615271265691
Iteration 100, Training loss = 0.23277074184555274
Iteration 110, Training loss = 0.23109022814493912
Iteration 120, Training loss = 0.22975084300224596
Iteration 130, Training loss = 0.2299504681275441
Iteration 140, Training loss = 0.2285070333343286
Iteration 150, Training loss = 0.22802972220457518
Iteration 160, Training loss = 0.22881065452328095
Iteration 170, Training loss = 0.2267035351922879
Iteration 180, Training loss = 0.22615598428707856
Iteration 190, Training loss = 0.22509054553050262
Model training time: 28.4185311794281
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0077927250128527
Iteration 10, Training loss = 0.9929746125753109
Iteration 20, Training loss = 0.9812714801384852
Iteration 30, Training loss = 0.9672202685704598
Iteration 40, Training loss = 0.9488635544593518
Iteration 50, Training loss = 0.9134780913591385
Iteration 60, Training loss = 0.8593891331782708
Iteration 70, Training loss = 0.7725691371239148
Iteration 80, Training loss = 0.6587479269275298
Iteration 90, Training loss = 0.5428968398616865
Iteration 100, Training loss = 0.46379173776278126
Iteration 110, Training loss = 0.42125800595833707
Iteration 120, Training loss = 0.3986176000191615
Iteration 130, Training loss = 0.3853057468166718
Iteration 140, Training loss = 0.37528232828928876
Iteration 150, Training loss = 0.36797998960201556
Iteration 160, Training loss = 0.3603800953580783
Iteration 170, Training loss = 0.3533063198511417
Iteration 180, Training loss = 0.34724697957818323
Iteration 190, Training loss = 0.3423372326562038
Model training time: 26.636324644088745
Device: cuda
Iteration 0, Training loss = 1.150414457687965
Iteration 10, Training loss = 0.9844322067040664
Iteration 20, Training loss = 0.9602296054363251
Iteration 30, Training loss = 0.9257893459155009
Iteration 40, Training loss = 0.8699865054625732
Iteration 50, Training loss = 0.7841168699356226
Iteration 60, Training loss = 0.6695448687443366
Iteration 70, Training loss = 0.5544740752531931
Iteration 80, Training loss = 0.4732452923288712
Iteration 90, Training loss = 0.42889041338975614
Iteration 100, Training loss = 0.4040460219750038
Iteration 110, Training loss = 0.3887433232023166
Iteration 120, Training loss = 0.37691351714042515
Iteration 130, Training loss = 0.3684070769410867
Iteration 140, Training loss = 0.3570643491469897
Iteration 150, Training loss = 0.3498421970468301
Iteration 160, Training loss = 0.341792341035146
Iteration 170, Training loss = 0.3351439506961749
Iteration 180, Training loss = 0.3302012773660513
Iteration 190, Training loss = 0.32617338116352373
Model training time: 26.159643173217773
Device: cuda
Iteration 0, Training loss = 1.1673709945036814
Iteration 10, Training loss = 0.9977435905199784
Iteration 20, Training loss = 0.9959294601128652
Iteration 30, Training loss = 0.9967321180380307
Iteration 40, Training loss = 0.9920735703064845
Iteration 50, Training loss = 0.9885756201468981
Iteration 60, Training loss = 0.98479746396725
Iteration 70, Training loss = 0.97957620712427
Iteration 80, Training loss = 0.9706448580210025
Iteration 90, Training loss = 0.9566586877291019
Iteration 100, Training loss = 0.9376409902022436
Iteration 110, Training loss = 0.9058480148132031
Iteration 120, Training loss = 0.8569456281570288
Iteration 130, Training loss = 0.7814611918651141
Iteration 140, Training loss = 0.6764355852053716
Iteration 150, Training loss = 0.564525839801018
Iteration 160, Training loss = 0.4774390854514562
Iteration 170, Training loss = 0.42769529211979646
Iteration 180, Training loss = 0.40115030396443146
Iteration 190, Training loss = 0.3856403042490666
Model training time: 26.473712682724
Device: cuda
Iteration 0, Training loss = 1.036028959430181
Iteration 10, Training loss = 0.9966408243546119
Iteration 20, Training loss = 0.9794266074895859
Iteration 30, Training loss = 0.960384991306525
Iteration 40, Training loss = 0.9291063249111176
Iteration 50, Training loss = 0.8817989700115644
Iteration 60, Training loss = 0.8101061720114487
Iteration 70, Training loss = 0.7042838759147204
Iteration 80, Training loss = 0.5867493943526194
Iteration 90, Training loss = 0.4905429258942604
Iteration 100, Training loss = 0.4307918158861307
Iteration 110, Training loss = 0.40169318650777525
Iteration 120, Training loss = 0.3855909968797977
Iteration 130, Training loss = 0.37258709508639115
Iteration 140, Training loss = 0.36336110360347307
Iteration 150, Training loss = 0.35562143130944324
Iteration 160, Training loss = 0.3474652231312715
Iteration 170, Training loss = 0.3417362877382682
Iteration 180, Training loss = 0.3366376491120228
Iteration 190, Training loss = 0.33255902849710905
Model training time: 26.065033197402954
Device: cuda
Iteration 0, Training loss = 1.6533812169845288
Iteration 10, Training loss = 0.9912079251729525
Iteration 20, Training loss = 0.9859093072322699
Iteration 30, Training loss = 0.9799295686758481
Iteration 40, Training loss = 0.9686624212906911
Iteration 50, Training loss = 0.9519625306129456
Iteration 60, Training loss = 0.9256195586461288
Iteration 70, Training loss = 0.8848677025391505
Iteration 80, Training loss = 0.8195351568552164
Iteration 90, Training loss = 0.722616508603096
Iteration 100, Training loss = 0.6077078466232007
Iteration 110, Training loss = 0.5061797104202784
Iteration 120, Training loss = 0.4388196582977588
Iteration 130, Training loss = 0.40395343876802003
Iteration 140, Training loss = 0.38604346777384096
Iteration 150, Training loss = 0.3735655935911032
Iteration 160, Training loss = 0.3631254354348549
Iteration 170, Training loss = 0.35527124542456406
Iteration 180, Training loss = 0.3476528324759923
Iteration 190, Training loss = 0.34145815613178104
Model training time: 26.29849672317505
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9552392478172596
Iteration 10, Training loss = 0.2709279174988086
Iteration 20, Training loss = 0.2572385095632993
Iteration 30, Training loss = 0.2529665609964958
Iteration 40, Training loss = 0.24912279070569918
Iteration 50, Training loss = 0.24487318327793708
Iteration 60, Training loss = 0.24081042351631018
Iteration 70, Training loss = 0.238464971861014
Iteration 80, Training loss = 0.23658098595646712
Iteration 90, Training loss = 0.23444290430499956
Iteration 100, Training loss = 0.2330331693475063
Iteration 110, Training loss = 0.23038127559881943
Iteration 120, Training loss = 0.22809797038252538
Iteration 130, Training loss = 0.2276768131324878
Iteration 140, Training loss = 0.2264704042329238
Iteration 150, Training loss = 0.22449235503490156
Iteration 160, Training loss = 0.22370844334363937
Iteration 170, Training loss = 0.22143696162563103
Iteration 180, Training loss = 0.2223220673891214
Iteration 190, Training loss = 0.22064699691075546
Model training time: 28.829874992370605
Device: cuda
Iteration 0, Training loss = 1.080039008305623
Iteration 10, Training loss = 0.2753759717138914
Iteration 20, Training loss = 0.25440278534705824
Iteration 30, Training loss = 0.24984165309713438
Iteration 40, Training loss = 0.2472110468034561
Iteration 50, Training loss = 0.24326653176775345
Iteration 60, Training loss = 0.24039080509772667
Iteration 70, Training loss = 0.2397580063686921
Iteration 80, Training loss = 0.2376309959934308
Iteration 90, Training loss = 0.23587077426222655
Iteration 100, Training loss = 0.2339021463233691
Iteration 110, Training loss = 0.23276726357065713
Iteration 120, Training loss = 0.23279722636708847
Iteration 130, Training loss = 0.2313101412012027
Iteration 140, Training loss = 0.23055760218546942
Iteration 150, Training loss = 0.22969692601607397
Iteration 160, Training loss = 0.2274515932569137
Iteration 170, Training loss = 0.22778876985494906
Iteration 180, Training loss = 0.2272768312921891
Iteration 190, Training loss = 0.2259964418525879
Model training time: 28.875671863555908
Device: cuda
Iteration 0, Training loss = 0.8552869879282438
Iteration 10, Training loss = 0.2675436930014537
Iteration 20, Training loss = 0.2576368393806311
Iteration 30, Training loss = 0.25195011095358777
Iteration 40, Training loss = 0.2468786583496974
Iteration 50, Training loss = 0.24477569013834
Iteration 60, Training loss = 0.23963165856324709
Iteration 70, Training loss = 0.23898321275527662
Iteration 80, Training loss = 0.236063358015739
Iteration 90, Training loss = 0.23368657007813454
Iteration 100, Training loss = 0.2314879596233368
Iteration 110, Training loss = 0.22958457928437453
Iteration 120, Training loss = 0.22800075062192404
Iteration 130, Training loss = 0.22521489428786132
Iteration 140, Training loss = 0.22397277704798257
Iteration 150, Training loss = 0.22303958867604917
Iteration 160, Training loss = 0.22244470939040184
Iteration 170, Training loss = 0.22026842116163328
Iteration 180, Training loss = 0.22030484934265798
Iteration 190, Training loss = 0.21716089603992608
Model training time: 28.653950929641724
Device: cuda
Iteration 0, Training loss = 1.1722897451657515
Iteration 10, Training loss = 0.3062417767941952
Iteration 20, Training loss = 0.2631765856192662
Iteration 30, Training loss = 0.25365035637066913
Iteration 40, Training loss = 0.24958605921039215
Iteration 50, Training loss = 0.24744872404978827
Iteration 60, Training loss = 0.2453581326856063
Iteration 70, Training loss = 0.24355567934421393
Iteration 80, Training loss = 0.24159842614944166
Iteration 90, Training loss = 0.2404971864934151
Iteration 100, Training loss = 0.23817313720400518
Iteration 110, Training loss = 0.23732517000574332
Iteration 120, Training loss = 0.23643827753571364
Iteration 130, Training loss = 0.23390160300410712
Iteration 140, Training loss = 0.2317918177980643
Iteration 150, Training loss = 0.23029998174080482
Iteration 160, Training loss = 0.2303275580589588
Iteration 170, Training loss = 0.22814300369757873
Iteration 180, Training loss = 0.22833118243859366
Iteration 190, Training loss = 0.22648226469755173
Model training time: 28.676305055618286
Device: cuda
Iteration 0, Training loss = 1.0373189942194865
Iteration 10, Training loss = 0.284797985966389
Iteration 20, Training loss = 0.25619187062749493
Iteration 30, Training loss = 0.24980303960350844
Iteration 40, Training loss = 0.24760740221692965
Iteration 50, Training loss = 0.24459440862903228
Iteration 60, Training loss = 0.24266205326868936
Iteration 70, Training loss = 0.2411488535312506
Iteration 80, Training loss = 0.23953678487585142
Iteration 90, Training loss = 0.2385977884897819
Iteration 100, Training loss = 0.2368792278262285
Iteration 110, Training loss = 0.2361760139465332
Iteration 120, Training loss = 0.2353543215073072
Iteration 130, Training loss = 0.23262489472444242
Iteration 140, Training loss = 0.2307712922875698
Iteration 150, Training loss = 0.23002594594772047
Iteration 160, Training loss = 0.22837601630733564
Iteration 170, Training loss = 0.22761862065929633
Iteration 180, Training loss = 0.22571506781073716
Iteration 190, Training loss = 0.22530073615220877
Model training time: 28.627694845199585
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 200, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.025566734946691
Iteration 10, Training loss = 0.995090126991272
Iteration 20, Training loss = 0.9729787844877976
Iteration 30, Training loss = 0.943466257590514
Iteration 40, Training loss = 0.8951339251719989
Iteration 50, Training loss = 0.830608029778187
Iteration 60, Training loss = 0.7402399159394778
Iteration 70, Training loss = 0.6420347197697713
Iteration 80, Training loss = 0.5567086998086709
Iteration 90, Training loss = 0.49551302595780444
Iteration 100, Training loss = 0.4571248304385405
Iteration 110, Training loss = 0.43206483125686646
Iteration 120, Training loss = 0.4145166501402855
Iteration 130, Training loss = 0.4011935998614018
Iteration 140, Training loss = 0.3875327196258765
Iteration 150, Training loss = 0.3756926174347217
Iteration 160, Training loss = 0.3655049247810474
Iteration 170, Training loss = 0.3573051799948399
Iteration 180, Training loss = 0.34968971518369824
Iteration 190, Training loss = 0.3426454279285211
Model training time: 26.558851718902588
Device: cuda
Iteration 0, Training loss = 1.0790728147213275
Iteration 10, Training loss = 0.9911867116506283
Iteration 20, Training loss = 0.9858430956418698
Iteration 30, Training loss = 0.9772888433474761
Iteration 40, Training loss = 0.9650298769657428
Iteration 50, Training loss = 0.948980334859628
Iteration 60, Training loss = 0.9248084322764323
Iteration 70, Training loss = 0.8897906278188412
Iteration 80, Training loss = 0.8387646686572295
Iteration 90, Training loss = 0.7680990099906921
Iteration 100, Training loss = 0.6832528824989612
Iteration 110, Training loss = 0.5949295214735545
Iteration 120, Training loss = 0.5214504863207157
Iteration 130, Training loss = 0.46674599383886045
Iteration 140, Training loss = 0.4295753561533414
Iteration 150, Training loss = 0.40411177564125794
Iteration 160, Training loss = 0.38696678613240904
Iteration 170, Training loss = 0.37255340757278294
Iteration 180, Training loss = 0.36029571524033177
Iteration 190, Training loss = 0.3506786502324618
Model training time: 26.8327054977417
Device: cuda
Iteration 0, Training loss = 0.9986807188162437
Iteration 10, Training loss = 0.9945954118783658
Iteration 20, Training loss = 0.9917677148030355
Iteration 30, Training loss = 0.9867003651765677
Iteration 40, Training loss = 0.9788093956617209
Iteration 50, Training loss = 0.9669519880643258
Iteration 60, Training loss = 0.9478552341461182
Iteration 70, Training loss = 0.911807507276535
Iteration 80, Training loss = 0.8506863506940695
Iteration 90, Training loss = 0.7508076864939469
Iteration 100, Training loss = 0.6253680391953542
Iteration 110, Training loss = 0.509644613816188
Iteration 120, Training loss = 0.4343926150065202
Iteration 130, Training loss = 0.3983480746929462
Iteration 140, Training loss = 0.3796561990792935
Iteration 150, Training loss = 0.3700618755358916
Iteration 160, Training loss = 0.36231840029358864
Iteration 170, Training loss = 0.35550345824315
Iteration 180, Training loss = 0.3498234341924007
Iteration 190, Training loss = 0.34423940915327805
Model training time: 26.259841203689575
Device: cuda
Iteration 0, Training loss = 1.0658747519438083
Iteration 10, Training loss = 1.006483107805252
Iteration 20, Training loss = 0.9975765232856457
Iteration 30, Training loss = 0.9894590446582208
Iteration 40, Training loss = 0.9801416763892541
Iteration 50, Training loss = 0.964799878688959
Iteration 60, Training loss = 0.937635050370143
Iteration 70, Training loss = 0.892896233842923
Iteration 80, Training loss = 0.8172993935071505
Iteration 90, Training loss = 0.7178995792682354
Iteration 100, Training loss = 0.6114610204329858
Iteration 110, Training loss = 0.5197006613016129
Iteration 120, Training loss = 0.4588444135510005
Iteration 130, Training loss = 0.42103110081874406
Iteration 140, Training loss = 0.39932759736592954
Iteration 150, Training loss = 0.3855298597079057
Iteration 160, Training loss = 0.37584653296149695
Iteration 170, Training loss = 0.36613188970547456
Iteration 180, Training loss = 0.35922066408854264
Iteration 190, Training loss = 0.3521265866091618
Model training time: 26.15114450454712
Device: cuda
Iteration 0, Training loss = 0.998212157533719
Iteration 10, Training loss = 0.9739013772744399
Iteration 20, Training loss = 0.9517995291031324
Iteration 30, Training loss = 0.9209325577204044
Iteration 40, Training loss = 0.8667823076248169
Iteration 50, Training loss = 0.7874859869480133
Iteration 60, Training loss = 0.6815165040584711
Iteration 70, Training loss = 0.5673325743812782
Iteration 80, Training loss = 0.4734647125005722
Iteration 90, Training loss = 0.4145841816296944
Iteration 100, Training loss = 0.38452883752492756
Iteration 110, Training loss = 0.3671332930143063
Iteration 120, Training loss = 0.35623788604369533
Iteration 130, Training loss = 0.3489177312988501
Iteration 140, Training loss = 0.3423207462407075
Iteration 150, Training loss = 0.3378827302501752
Iteration 160, Training loss = 0.33262352301524234
Iteration 170, Training loss = 0.32768815469283324
Iteration 180, Training loss = 0.3243263971347075
Iteration 190, Training loss = 0.3220384120941162
Model training time: 26.267793893814087
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.8873313745627036
Iteration 10, Training loss = 0.2731226066557261
Iteration 20, Training loss = 0.25644584831136924
Iteration 30, Training loss = 0.25023458009729016
Iteration 40, Training loss = 0.2467668721309075
Iteration 50, Training loss = 0.24252457658831889
Iteration 60, Training loss = 0.23864258653842485
Iteration 70, Training loss = 0.2347190984739707
Iteration 80, Training loss = 0.23278460537011808
Iteration 90, Training loss = 0.22909690869542268
Iteration 100, Training loss = 0.22663023852958128
Iteration 110, Training loss = 0.22349099413706705
Iteration 120, Training loss = 0.2229135942000609
Iteration 130, Training loss = 0.22058718336316255
Iteration 140, Training loss = 0.21854952722787857
Iteration 150, Training loss = 0.21670687542511866
Iteration 160, Training loss = 0.2159841639491228
Iteration 170, Training loss = 0.2151137785269664
Iteration 180, Training loss = 0.21415299807603544
Iteration 190, Training loss = 0.21216955637702575
Iteration 200, Training loss = 0.21095443688906157
Iteration 210, Training loss = 0.2108960633094494
Iteration 220, Training loss = 0.20953869418456003
Iteration 230, Training loss = 0.20888806650271782
Iteration 240, Training loss = 0.20820970288836038
Iteration 250, Training loss = 0.20783411654142234
Iteration 260, Training loss = 0.20794586120889738
Iteration 270, Training loss = 0.20606335481772056
Iteration 280, Training loss = 0.2054420391527506
Iteration 290, Training loss = 0.20473668480721804
Model training time: 42.449345111846924
Device: cuda
Iteration 0, Training loss = 1.0686538425775676
Iteration 10, Training loss = 0.2824445263697551
Iteration 20, Training loss = 0.25635575216550094
Iteration 30, Training loss = 0.25050414210328686
Iteration 40, Training loss = 0.24574170318933633
Iteration 50, Training loss = 0.24339271021577027
Iteration 60, Training loss = 0.23951493432888618
Iteration 70, Training loss = 0.23676311769164526
Iteration 80, Training loss = 0.23222383169027475
Iteration 90, Training loss = 0.22946824649205574
Iteration 100, Training loss = 0.22788950743583533
Iteration 110, Training loss = 0.22518630612355012
Iteration 120, Training loss = 0.22329830865447337
Iteration 130, Training loss = 0.21980061783240393
Iteration 140, Training loss = 0.21770807900107825
Iteration 150, Training loss = 0.21677076701934522
Iteration 160, Training loss = 0.21459822729229927
Iteration 170, Training loss = 0.21294291833272347
Iteration 180, Training loss = 0.2106901232439738
Iteration 190, Training loss = 0.20936404111293647
Iteration 200, Training loss = 0.20747392624616623
Iteration 210, Training loss = 0.2077788641819587
Iteration 220, Training loss = 0.20687610283493996
Iteration 230, Training loss = 0.20678509284670538
Iteration 240, Training loss = 0.20494939042971685
Iteration 250, Training loss = 0.2047208329805961
Iteration 260, Training loss = 0.20344071548718673
Iteration 270, Training loss = 0.20269688562704966
Iteration 280, Training loss = 0.20198393125946706
Iteration 290, Training loss = 0.20167319963757807
Model training time: 42.73275971412659
Device: cuda
Iteration 0, Training loss = 0.9251471952750132
Iteration 10, Training loss = 0.265642624348402
Iteration 20, Training loss = 0.2532505848659919
Iteration 30, Training loss = 0.2466855410199899
Iteration 40, Training loss = 0.24313240211743575
Iteration 50, Training loss = 0.23983419973116654
Iteration 60, Training loss = 0.2367574260211908
Iteration 70, Training loss = 0.23381059559491965
Iteration 80, Training loss = 0.22999882325530052
Iteration 90, Training loss = 0.22858944936440542
Iteration 100, Training loss = 0.22509290363926154
Iteration 110, Training loss = 0.2210629740013526
Iteration 120, Training loss = 0.219333539215418
Iteration 130, Training loss = 0.216326504945755
Iteration 140, Training loss = 0.21444879873440817
Iteration 150, Training loss = 0.21280233619304803
Iteration 160, Training loss = 0.21169510598366076
Iteration 170, Training loss = 0.2099693099466654
Iteration 180, Training loss = 0.21007544318070778
Iteration 190, Training loss = 0.20705132501629683
Iteration 200, Training loss = 0.20734419559056944
Iteration 210, Training loss = 0.20428473975222844
Iteration 220, Training loss = 0.20522591357047743
Iteration 230, Training loss = 0.20327433170034334
Iteration 240, Training loss = 0.20238404606397337
Iteration 250, Training loss = 0.20230986464482087
Iteration 260, Training loss = 0.20121625495644715
Iteration 270, Training loss = 0.20086255211096543
Iteration 280, Training loss = 0.20042204169126657
Iteration 290, Training loss = 0.2001334001811651
Model training time: 42.613943338394165
Device: cuda
Iteration 0, Training loss = 0.9309112475468562
Iteration 10, Training loss = 0.2632787838005103
Iteration 20, Training loss = 0.2519054748117924
Iteration 30, Training loss = 0.24843803850504068
Iteration 40, Training loss = 0.24438294051931456
Iteration 50, Training loss = 0.24147276494365472
Iteration 60, Training loss = 0.23812397053608528
Iteration 70, Training loss = 0.2353395762351843
Iteration 80, Training loss = 0.2337328548041674
Iteration 90, Training loss = 0.23130467820626038
Iteration 100, Training loss = 0.22957708715246275
Iteration 110, Training loss = 0.22822013210791808
Iteration 120, Training loss = 0.22681619370212922
Iteration 130, Training loss = 0.2248381576859034
Iteration 140, Training loss = 0.22517705909334695
Iteration 150, Training loss = 0.2215170653966757
Iteration 160, Training loss = 0.22058964807253617
Iteration 170, Training loss = 0.21950806190188116
Iteration 180, Training loss = 0.21965596509667543
Iteration 190, Training loss = 0.21699225128843233
Iteration 200, Training loss = 0.2168658014673453
Iteration 210, Training loss = 0.2142749253947001
Iteration 220, Training loss = 0.21366467212255186
Iteration 230, Training loss = 0.21244117417014563
Iteration 240, Training loss = 0.21161390296541727
Iteration 250, Training loss = 0.21045842508857066
Iteration 260, Training loss = 0.2084327623821222
Iteration 270, Training loss = 0.20713925590881935
Iteration 280, Training loss = 0.20643196512873357
Iteration 290, Training loss = 0.205144186432545
Model training time: 42.33236336708069
Device: cuda
Iteration 0, Training loss = 0.8692457297673593
Iteration 10, Training loss = 0.2638942049099849
Iteration 20, Training loss = 0.2526571905383697
Iteration 30, Training loss = 0.24762159012831175
Iteration 40, Training loss = 0.24366902502683493
Iteration 50, Training loss = 0.24008936091111258
Iteration 60, Training loss = 0.2378380009188102
Iteration 70, Training loss = 0.23402175593834657
Iteration 80, Training loss = 0.23083414595860702
Iteration 90, Training loss = 0.22740306332707405
Iteration 100, Training loss = 0.22538434542142427
Iteration 110, Training loss = 0.2236882196022914
Iteration 120, Training loss = 0.22225775483709115
Iteration 130, Training loss = 0.22043777973606035
Iteration 140, Training loss = 0.21810821644388712
Iteration 150, Training loss = 0.21855937632230613
Iteration 160, Training loss = 0.215903412264127
Iteration 170, Training loss = 0.21546156933674446
Iteration 180, Training loss = 0.21255261078476906
Iteration 190, Training loss = 0.21228137153845567
Iteration 200, Training loss = 0.2108579851113833
Iteration 210, Training loss = 0.20908892555878714
Iteration 220, Training loss = 0.20927364006638527
Iteration 230, Training loss = 0.20720674670659578
Iteration 240, Training loss = 0.20615474392588323
Iteration 250, Training loss = 0.2053022304406533
Iteration 260, Training loss = 0.20432794380646485
Iteration 270, Training loss = 0.20362618364966834
Iteration 280, Training loss = 0.20265005614895087
Iteration 290, Training loss = 0.2030265716692576
Model training time: 42.78840947151184
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.1543194903777196
Iteration 10, Training loss = 0.990276249555441
Iteration 20, Training loss = 0.9802219306047146
Iteration 30, Training loss = 0.967137371118252
Iteration 40, Training loss = 0.945486151255094
Iteration 50, Training loss = 0.9082938650479684
Iteration 60, Training loss = 0.8479998077337558
Iteration 70, Training loss = 0.7577908910237826
Iteration 80, Training loss = 0.6396585634121528
Iteration 90, Training loss = 0.5268232621825658
Iteration 100, Training loss = 0.44879943762834257
Iteration 110, Training loss = 0.4069916674723992
Iteration 120, Training loss = 0.38620247749181896
Iteration 130, Training loss = 0.37470972537994385
Iteration 140, Training loss = 0.36524698539422107
Iteration 150, Training loss = 0.3578480367477124
Iteration 160, Training loss = 0.35245929199915665
Iteration 170, Training loss = 0.3458688264855972
Iteration 180, Training loss = 0.339821252398766
Iteration 190, Training loss = 0.33537567808077884
Iteration 200, Training loss = 0.33099926778903377
Iteration 210, Training loss = 0.3271933839871333
Iteration 220, Training loss = 0.32268185025224316
Iteration 230, Training loss = 0.31912434101104736
Iteration 240, Training loss = 0.3163292299096401
Iteration 250, Training loss = 0.31380231793110186
Iteration 260, Training loss = 0.31057703924866825
Iteration 270, Training loss = 0.3075917953482041
Iteration 280, Training loss = 0.3054277343818775
Iteration 290, Training loss = 0.30418654101399273
Model training time: 39.287562131881714
Device: cuda
Iteration 0, Training loss = 1.0166717641628706
Iteration 10, Training loss = 0.9645168620806474
Iteration 20, Training loss = 0.9227458490775182
Iteration 30, Training loss = 0.8602474629878998
Iteration 40, Training loss = 0.7595751319940274
Iteration 50, Training loss = 0.6252599243934338
Iteration 60, Training loss = 0.49627009951151335
Iteration 70, Training loss = 0.4208358021882864
Iteration 80, Training loss = 0.3877360155949226
Iteration 90, Training loss = 0.3712332449280299
Iteration 100, Training loss = 0.36129043996334076
Iteration 110, Training loss = 0.35206657447493994
Iteration 120, Training loss = 0.3456274262414529
Iteration 130, Training loss = 0.3409417982284839
Iteration 140, Training loss = 0.3340754938813356
Iteration 150, Training loss = 0.329987416187158
Iteration 160, Training loss = 0.3251271190551611
Iteration 170, Training loss = 0.3225087128006495
Iteration 180, Training loss = 0.31953034979792744
Iteration 190, Training loss = 0.3171683756204752
Iteration 200, Training loss = 0.31466287259872144
Iteration 210, Training loss = 0.3119947798550129
Iteration 220, Training loss = 0.3109105856945881
Iteration 230, Training loss = 0.3089830932708887
Iteration 240, Training loss = 0.30719106930952805
Iteration 250, Training loss = 0.30677305391201604
Iteration 260, Training loss = 0.30520946990985137
Iteration 270, Training loss = 0.3043900854312457
Iteration 280, Training loss = 0.3023736892411342
Iteration 290, Training loss = 0.30224423798230976
Model training time: 39.5826575756073
Device: cuda
Iteration 0, Training loss = 1.0917137070344045
Iteration 10, Training loss = 0.9853726350344144
Iteration 20, Training loss = 0.9756412311242177
Iteration 30, Training loss = 0.958677351474762
Iteration 40, Training loss = 0.9301023471813935
Iteration 50, Training loss = 0.8787501878463305
Iteration 60, Training loss = 0.7940087157946366
Iteration 70, Training loss = 0.6639036066257037
Iteration 80, Training loss = 0.525394197839957
Iteration 90, Training loss = 0.4314315880720432
Iteration 100, Training loss = 0.38941964163230014
Iteration 110, Training loss = 0.37075971296200383
Iteration 120, Training loss = 0.36144102823275787
Iteration 130, Training loss = 0.35313247717343843
Iteration 140, Training loss = 0.34862137872439164
Iteration 150, Training loss = 0.34227043161025417
Iteration 160, Training loss = 0.33638679408110106
Iteration 170, Training loss = 0.33301153779029846
Iteration 180, Training loss = 0.3288354908044522
Iteration 190, Training loss = 0.3256870579834168
Iteration 200, Training loss = 0.32249407126353336
Iteration 210, Training loss = 0.3194773919307269
Iteration 220, Training loss = 0.3165317957217877
Iteration 230, Training loss = 0.3139174534724309
Iteration 240, Training loss = 0.3115177464026671
Iteration 250, Training loss = 0.3093859344147719
Iteration 260, Training loss = 0.3074295543707334
Iteration 270, Training loss = 0.3054737875667902
Iteration 280, Training loss = 0.3041136760551196
Iteration 290, Training loss = 0.3019915068378815
Model training time: 39.55202102661133
Device: cuda
Iteration 0, Training loss = 0.994753938454848
Iteration 10, Training loss = 0.9631196455313609
Iteration 20, Training loss = 0.9324566974089696
Iteration 30, Training loss = 0.8804716616868973
Iteration 40, Training loss = 0.79682768537448
Iteration 50, Training loss = 0.6758040866026511
Iteration 60, Training loss = 0.5479911451156323
Iteration 70, Training loss = 0.4570563555910037
Iteration 80, Training loss = 0.4121932542094818
Iteration 90, Training loss = 0.3900277024278274
Iteration 100, Training loss = 0.37676286468139064
Iteration 110, Training loss = 0.3672242210461543
Iteration 120, Training loss = 0.35947640985250473
Iteration 130, Training loss = 0.3515324764526807
Iteration 140, Training loss = 0.34535717792235887
Iteration 150, Training loss = 0.33944226744083256
Iteration 160, Training loss = 0.3346542220276136
Iteration 170, Training loss = 0.32971751947815603
Iteration 180, Training loss = 0.32645876333117485
Iteration 190, Training loss = 0.32203400880098343
Iteration 200, Training loss = 0.31948690488934517
Iteration 210, Training loss = 0.3160763867199421
Iteration 220, Training loss = 0.31385750896655595
Iteration 230, Training loss = 0.3109539727178904
Iteration 240, Training loss = 0.309107368095563
Iteration 250, Training loss = 0.30737751779647976
Iteration 260, Training loss = 0.3054503660935622
Iteration 270, Training loss = 0.3036043096620303
Iteration 280, Training loss = 0.3016818885046702
Iteration 290, Training loss = 0.3009275696598567
Model training time: 39.21163868904114
Device: cuda
Iteration 0, Training loss = 0.994120981830817
Iteration 10, Training loss = 0.9648329122708394
Iteration 20, Training loss = 0.93238588479849
Iteration 30, Training loss = 0.8801195678802637
Iteration 40, Training loss = 0.7910560312179419
Iteration 50, Training loss = 0.6626617690691581
Iteration 60, Training loss = 0.5298325952429038
Iteration 70, Training loss = 0.44222253274459106
Iteration 80, Training loss = 0.40330816346865433
Iteration 90, Training loss = 0.38342932898264664
Iteration 100, Training loss = 0.37137105889045274
Iteration 110, Training loss = 0.3612132336084659
Iteration 120, Training loss = 0.3515102439201795
Iteration 130, Training loss = 0.34408008192594236
Iteration 140, Training loss = 0.33742256376605767
Iteration 150, Training loss = 0.33230718253896785
Iteration 160, Training loss = 0.32700459085977995
Iteration 170, Training loss = 0.3224127676624518
Iteration 180, Training loss = 0.31934313676678217
Iteration 190, Training loss = 0.3153976537287235
Iteration 200, Training loss = 0.3134125496905584
Iteration 210, Training loss = 0.31177866143675953
Iteration 220, Training loss = 0.30892921812259233
Iteration 230, Training loss = 0.30732926869621646
Iteration 240, Training loss = 0.3052936118955796
Iteration 250, Training loss = 0.3030110336840153
Iteration 260, Training loss = 0.3015824665243809
Iteration 270, Training loss = 0.30122932677085584
Iteration 280, Training loss = 0.29899362474679947
Iteration 290, Training loss = 0.29923992689985496
Model training time: 39.95342445373535
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9741615286240211
Iteration 10, Training loss = 0.2761738635599613
Iteration 20, Training loss = 0.26141743849103266
Iteration 30, Training loss = 0.25677724383198297
Iteration 40, Training loss = 0.25374930839125925
Iteration 50, Training loss = 0.2523283422566377
Iteration 60, Training loss = 0.2489081509411335
Iteration 70, Training loss = 0.24752946398579156
Iteration 80, Training loss = 0.24648869639405838
Iteration 90, Training loss = 0.24437947456653303
Iteration 100, Training loss = 0.24249206769924897
Iteration 110, Training loss = 0.24057205221973932
Iteration 120, Training loss = 0.2389898211337053
Iteration 130, Training loss = 0.2383144787297799
Iteration 140, Training loss = 0.23713603558448645
Iteration 150, Training loss = 0.2365621732404599
Iteration 160, Training loss = 0.23499367128197962
Iteration 170, Training loss = 0.23486282475865805
Iteration 180, Training loss = 0.23281396696200737
Iteration 190, Training loss = 0.23260577882711703
Iteration 200, Training loss = 0.23243210865901068
Iteration 210, Training loss = 0.23080420952576858
Iteration 220, Training loss = 0.2303034528516806
Iteration 230, Training loss = 0.22863870515273169
Iteration 240, Training loss = 0.2283144768040914
Iteration 250, Training loss = 0.22661458844175705
Iteration 260, Training loss = 0.2264928066959748
Iteration 270, Training loss = 0.22531175871308035
Iteration 280, Training loss = 0.22438820078969002
Iteration 290, Training loss = 0.22442993493034288
Model training time: 42.55178689956665
Device: cuda
Iteration 0, Training loss = 0.9867172676783341
Iteration 10, Training loss = 0.2836178165788834
Iteration 20, Training loss = 0.2637989317568449
Iteration 30, Training loss = 0.2553379928263334
Iteration 40, Training loss = 0.25137240525621635
Iteration 50, Training loss = 0.24832728533790663
Iteration 60, Training loss = 0.2457887023114241
Iteration 70, Training loss = 0.24341227687322176
Iteration 80, Training loss = 0.24124275978941184
Iteration 90, Training loss = 0.23944436529508004
Iteration 100, Training loss = 0.23858164480099311
Iteration 110, Training loss = 0.2362400975364905
Iteration 120, Training loss = 0.2342025159070125
Iteration 130, Training loss = 0.23235865854299986
Iteration 140, Training loss = 0.23141237434286338
Iteration 150, Training loss = 0.22936645551369741
Iteration 160, Training loss = 0.22773709721290147
Iteration 170, Training loss = 0.22540931718853804
Iteration 180, Training loss = 0.2243940572326
Iteration 190, Training loss = 0.22270043136981818
Iteration 200, Training loss = 0.2203786957722444
Iteration 210, Training loss = 0.21920480321233088
Iteration 220, Training loss = 0.21836750610516623
Iteration 230, Training loss = 0.2174390322313859
Iteration 240, Training loss = 0.21603695647074625
Iteration 250, Training loss = 0.21707193410167328
Iteration 260, Training loss = 0.2150701221365195
Iteration 270, Training loss = 0.21504460275173187
Iteration 280, Training loss = 0.21441401885106012
Iteration 290, Training loss = 0.21435347314064318
Model training time: 43.17224383354187
Device: cuda
Iteration 0, Training loss = 1.003020621263064
Iteration 10, Training loss = 0.29117637328230417
Iteration 20, Training loss = 0.26104480429337573
Iteration 30, Training loss = 0.25564250407310635
Iteration 40, Training loss = 0.25186323288541573
Iteration 50, Training loss = 0.24782738118217543
Iteration 60, Training loss = 0.24671966754473174
Iteration 70, Training loss = 0.24461530091670844
Iteration 80, Training loss = 0.24310364115696687
Iteration 90, Training loss = 0.2414752792280454
Iteration 100, Training loss = 0.23949595397481552
Iteration 110, Training loss = 0.2377167040338883
Iteration 120, Training loss = 0.23655129911807868
Iteration 130, Training loss = 0.23510796175553247
Iteration 140, Training loss = 0.2340703374491288
Iteration 150, Training loss = 0.23316767869087365
Iteration 160, Training loss = 0.23302513217696777
Iteration 170, Training loss = 0.2315638902095648
Iteration 180, Training loss = 0.23051205162818617
Iteration 190, Training loss = 0.2300180082137768
Iteration 200, Training loss = 0.22873738723305556
Iteration 210, Training loss = 0.22791705910976118
Iteration 220, Training loss = 0.22702929836053115
Iteration 230, Training loss = 0.22620441821905282
Iteration 240, Training loss = 0.22508545592427254
Iteration 250, Training loss = 0.2240136887591619
Iteration 260, Training loss = 0.2222690754211866
Iteration 270, Training loss = 0.22130261791440156
Iteration 280, Training loss = 0.22208196182663625
Iteration 290, Training loss = 0.21959293691011575
Model training time: 42.704155683517456
Device: cuda
Iteration 0, Training loss = 0.9652860302191514
Iteration 10, Training loss = 0.28082235673299205
Iteration 20, Training loss = 0.2579717169014307
Iteration 30, Training loss = 0.2507055602394618
Iteration 40, Training loss = 0.24765415518329695
Iteration 50, Training loss = 0.2455071580524628
Iteration 60, Training loss = 0.2412353097819365
Iteration 70, Training loss = 0.23719700884360534
Iteration 80, Training loss = 0.2342168098458877
Iteration 90, Training loss = 0.2310836684818451
Iteration 100, Training loss = 0.229126922499675
Iteration 110, Training loss = 0.22601872338698462
Iteration 120, Training loss = 0.2231618229013223
Iteration 130, Training loss = 0.22211333077687484
Iteration 140, Training loss = 0.22009784212479225
Iteration 150, Training loss = 0.21837697206781462
Iteration 160, Training loss = 0.21831079658407432
Iteration 170, Training loss = 0.21713355498818251
Iteration 180, Training loss = 0.2167286528990819
Iteration 190, Training loss = 0.2151881823172936
Iteration 200, Training loss = 0.21409593694485152
Iteration 210, Training loss = 0.2143740083735723
Iteration 220, Training loss = 0.21347819154079145
Iteration 230, Training loss = 0.2117942855335199
Iteration 240, Training loss = 0.21162570984317705
Iteration 250, Training loss = 0.21062766330746505
Iteration 260, Training loss = 0.21066221967339516
Iteration 270, Training loss = 0.21061250911309168
Iteration 280, Training loss = 0.20950491841022784
Iteration 290, Training loss = 0.20833700895309448
Model training time: 42.43606972694397
Device: cuda
Iteration 0, Training loss = 1.101775470834512
Iteration 10, Training loss = 0.3096284596965863
Iteration 20, Training loss = 0.26425806232369864
Iteration 30, Training loss = 0.25271047766392046
Iteration 40, Training loss = 0.24847902816075546
Iteration 50, Training loss = 0.2470889796431248
Iteration 60, Training loss = 0.24414761851613337
Iteration 70, Training loss = 0.24208674837763494
Iteration 80, Training loss = 0.2397670247233831
Iteration 90, Training loss = 0.23758668825030327
Iteration 100, Training loss = 0.2347817484002847
Iteration 110, Training loss = 0.2323228378708546
Iteration 120, Training loss = 0.23127304676633614
Iteration 130, Training loss = 0.22899750247597694
Iteration 140, Training loss = 0.22726166936067435
Iteration 150, Training loss = 0.22681271342130807
Iteration 160, Training loss = 0.22531682740037257
Iteration 170, Training loss = 0.22280712483020929
Iteration 180, Training loss = 0.22221075055690911
Iteration 190, Training loss = 0.21990480789771447
Iteration 200, Training loss = 0.21872857671517593
Iteration 210, Training loss = 0.21755216299341276
Iteration 220, Training loss = 0.21604170335026887
Iteration 230, Training loss = 0.21565625157493812
Iteration 240, Training loss = 0.21516700117633894
Iteration 250, Training loss = 0.2144883226316709
Iteration 260, Training loss = 0.21415622131182596
Iteration 270, Training loss = 0.21327185888703054
Iteration 280, Training loss = 0.21370939079385537
Iteration 290, Training loss = 0.2131265404705818
Model training time: 42.24786067008972
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 4], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.0136060233299549
Iteration 10, Training loss = 0.9995606220685519
Iteration 20, Training loss = 0.9847058275571237
Iteration 30, Training loss = 0.9695900082588196
Iteration 40, Training loss = 0.9473995176645426
Iteration 50, Training loss = 0.9123300176400405
Iteration 60, Training loss = 0.8598569872287604
Iteration 70, Training loss = 0.7827948182821274
Iteration 80, Training loss = 0.6862589224026754
Iteration 90, Training loss = 0.5852511026538335
Iteration 100, Training loss = 0.5017732364627031
Iteration 110, Training loss = 0.44847778861339277
Iteration 120, Training loss = 0.41941515986736005
Iteration 130, Training loss = 0.39993415772914886
Iteration 140, Training loss = 0.3869125057871525
Iteration 150, Training loss = 0.3758757240497149
Iteration 160, Training loss = 0.36592297370617205
Iteration 170, Training loss = 0.3559091245898834
Iteration 180, Training loss = 0.34785649237724453
Iteration 190, Training loss = 0.34243030272997343
Iteration 200, Training loss = 0.3356691071620354
Iteration 210, Training loss = 0.329639474932964
Iteration 220, Training loss = 0.3252905847934576
Iteration 230, Training loss = 0.31983619928359985
Iteration 240, Training loss = 0.31632058265117496
Iteration 250, Training loss = 0.31280992466669816
Iteration 260, Training loss = 0.3103587633142105
Iteration 270, Training loss = 0.3071442561653944
Iteration 280, Training loss = 0.30452287942171097
Iteration 290, Training loss = 0.30246615409851074
Model training time: 39.93882465362549
Device: cuda
Iteration 0, Training loss = 1.2303844506923969
Iteration 10, Training loss = 0.9900387949668444
Iteration 20, Training loss = 0.9875050622683305
Iteration 30, Training loss = 0.9802554960434253
Iteration 40, Training loss = 0.9715671860254728
Iteration 50, Training loss = 0.9571542774255459
Iteration 60, Training loss = 0.939300652879935
Iteration 70, Training loss = 0.9080412089824677
Iteration 80, Training loss = 0.8595313154734098
Iteration 90, Training loss = 0.7865362304907578
Iteration 100, Training loss = 0.6840688116275347
Iteration 110, Training loss = 0.567981379536482
Iteration 120, Training loss = 0.4750025856953401
Iteration 130, Training loss = 0.41963543456334335
Iteration 140, Training loss = 0.39338531631689805
Iteration 150, Training loss = 0.3788799586204382
Iteration 160, Training loss = 0.36831395213420576
Iteration 170, Training loss = 0.3609978585289075
Iteration 180, Training loss = 0.35281800707945454
Iteration 190, Training loss = 0.3468804514178863
Iteration 200, Training loss = 0.3426761466723222
Iteration 210, Training loss = 0.3361349283502652
Iteration 220, Training loss = 0.3325595583480138
Iteration 230, Training loss = 0.3286878400697158
Iteration 240, Training loss = 0.3253556492810066
Iteration 250, Training loss = 0.3227713050750586
Iteration 260, Training loss = 0.3199158732134562
Iteration 270, Training loss = 0.3174596125116715
Iteration 280, Training loss = 0.3161419343489867
Iteration 290, Training loss = 0.3127205752982543
Model training time: 40.88555884361267
Device: cuda
Iteration 0, Training loss = 1.37281473668722
Iteration 10, Training loss = 0.9952648408137835
Iteration 20, Training loss = 0.982379614160611
Iteration 30, Training loss = 0.9647769985290674
Iteration 40, Training loss = 0.9394726329124891
Iteration 50, Training loss = 0.8954196320130274
Iteration 60, Training loss = 0.8257495394119849
Iteration 70, Training loss = 0.7207543276823484
Iteration 80, Training loss = 0.5876302317931101
Iteration 90, Training loss = 0.4756156017000859
Iteration 100, Training loss = 0.4091794846149591
Iteration 110, Training loss = 0.3775327560993341
Iteration 120, Training loss = 0.36349081763854396
Iteration 130, Training loss = 0.35436847175543124
Iteration 140, Training loss = 0.3470422808940594
Iteration 150, Training loss = 0.34243926864403945
Iteration 160, Training loss = 0.3370766178346597
Iteration 170, Training loss = 0.3317913762651957
Iteration 180, Training loss = 0.32834081896222556
Iteration 190, Training loss = 0.3250602569717627
Iteration 200, Training loss = 0.32181503251194954
Iteration 210, Training loss = 0.3187218689574645
Iteration 220, Training loss = 0.3175451314220062
Iteration 230, Training loss = 0.31406834750221324
Iteration 240, Training loss = 0.31286581233143806
Iteration 250, Training loss = 0.3104135517317515
Iteration 260, Training loss = 0.30941458992086923
Iteration 270, Training loss = 0.30672373651311946
Iteration 280, Training loss = 0.30570122685569984
Iteration 290, Training loss = 0.3046412184261359
Model training time: 43.054123401641846
Device: cuda
Iteration 0, Training loss = 1.125759475506269
Iteration 10, Training loss = 0.9558558051402752
Iteration 20, Training loss = 0.9285543366120412
Iteration 30, Training loss = 0.8819599862282093
Iteration 40, Training loss = 0.8065147193578573
Iteration 50, Training loss = 0.6969845443964005
Iteration 60, Training loss = 0.5732430213919053
Iteration 70, Training loss = 0.47342246255049336
Iteration 80, Training loss = 0.4138683544901701
Iteration 90, Training loss = 0.3841277286410332
Iteration 100, Training loss = 0.36849391288482225
Iteration 110, Training loss = 0.356773962195103
Iteration 120, Training loss = 0.3491031802617587
Iteration 130, Training loss = 0.34167257295205045
Iteration 140, Training loss = 0.33434993773698807
Iteration 150, Training loss = 0.3292958283653626
Iteration 160, Training loss = 0.32342707451719505
Iteration 170, Training loss = 0.31983510204232657
Iteration 180, Training loss = 0.316632251899976
Iteration 190, Training loss = 0.31297464210253495
Iteration 200, Training loss = 0.3110741293774201
Iteration 210, Training loss = 0.3082018165061107
Iteration 220, Training loss = 0.3052823640979253
Iteration 230, Training loss = 0.3038015609177259
Iteration 240, Training loss = 0.302110080822156
Iteration 250, Training loss = 0.3006746946619107
Iteration 260, Training loss = 0.2986938337293955
Iteration 270, Training loss = 0.29812580748246265
Iteration 280, Training loss = 0.2957010234777744
Iteration 290, Training loss = 0.2950041394394178
Model training time: 42.633692502975464
Device: cuda
Iteration 0, Training loss = 1.0046544785682971
Iteration 10, Training loss = 0.9763450771570206
Iteration 20, Training loss = 0.9361944427857032
Iteration 30, Training loss = 0.8748166859149933
Iteration 40, Training loss = 0.7767141793782895
Iteration 50, Training loss = 0.6456643107991952
Iteration 60, Training loss = 0.5182725626688737
Iteration 70, Training loss = 0.4350919654736152
Iteration 80, Training loss = 0.3939624703847445
Iteration 90, Training loss = 0.37230237688009554
Iteration 100, Training loss = 0.3591705835782565
Iteration 110, Training loss = 0.3489859321942696
Iteration 120, Training loss = 0.3399018966234647
Iteration 130, Training loss = 0.3340990093465035
Iteration 140, Training loss = 0.3276345371626891
Iteration 150, Training loss = 0.3225906929717614
Iteration 160, Training loss = 0.31850994321016163
Iteration 170, Training loss = 0.3161388351940192
Iteration 180, Training loss = 0.3122190185464345
Iteration 190, Training loss = 0.3099824247451929
Iteration 200, Training loss = 0.3079229232210379
Iteration 210, Training loss = 0.30711531381194407
Iteration 220, Training loss = 0.3050661087036133
Iteration 230, Training loss = 0.3034963198006153
Iteration 240, Training loss = 0.3020748415818581
Iteration 250, Training loss = 0.3010029640908425
Iteration 260, Training loss = 0.2988980312186938
Iteration 270, Training loss = 0.29776188931786096
Iteration 280, Training loss = 0.2960694864965402
Iteration 290, Training loss = 0.29515323128837806
Model training time: 40.512075901031494
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
Device: cuda
Iteration 0, Training loss = 0.9697078730051334
Iteration 10, Training loss = 0.2738924969274264
Iteration 20, Training loss = 0.257689132140233
Iteration 30, Training loss = 0.251203966828493
Iteration 40, Training loss = 0.24594192951917648
Iteration 50, Training loss = 0.24195501638146547
Iteration 60, Training loss = 0.23865846200631216
Iteration 70, Training loss = 0.23630445421888277
Iteration 80, Training loss = 0.23362337368038985
Iteration 90, Training loss = 0.2310221613599704
Iteration 100, Training loss = 0.22942189528391913
Iteration 110, Training loss = 0.22775357589125633
Iteration 120, Training loss = 0.2252824532871063
Iteration 130, Training loss = 0.22401685926776665
Iteration 140, Training loss = 0.2234354420350148
Iteration 150, Training loss = 0.22161286868728125
Iteration 160, Training loss = 0.220985531520385
Iteration 170, Training loss = 0.22094474618251508
Iteration 180, Training loss = 0.2188389146557221
Iteration 190, Training loss = 0.21903889912825364
Iteration 200, Training loss = 0.21811651467130735
Iteration 210, Training loss = 0.21762130438135222
Iteration 220, Training loss = 0.21513456793931815
Iteration 230, Training loss = 0.21550511454160398
Iteration 240, Training loss = 0.21495381427498964
Iteration 250, Training loss = 0.2141895855848606
Iteration 260, Training loss = 0.21299914012734705
Iteration 270, Training loss = 0.21290399363407722
Iteration 280, Training loss = 0.21243490020816141
Iteration 290, Training loss = 0.21221697645691726
Model training time: 42.36209321022034
Device: cuda
Iteration 0, Training loss = 0.9547776683018758
Iteration 10, Training loss = 0.26899098289700657
Iteration 20, Training loss = 0.2561886777671484
Iteration 30, Training loss = 0.24934497103095055
Iteration 40, Training loss = 0.24630291416094854
Iteration 50, Training loss = 0.24390943778248933
Iteration 60, Training loss = 0.24288670890606368
Iteration 70, Training loss = 0.24039520523869073
Iteration 80, Training loss = 0.2393040622656162
Iteration 90, Training loss = 0.23769625849448717
Iteration 100, Training loss = 0.23617110802577093
Iteration 110, Training loss = 0.23459100207457176
Iteration 120, Training loss = 0.23306352301285818
Iteration 130, Training loss = 0.23091269886264434
Iteration 140, Training loss = 0.2301615089751207
Iteration 150, Training loss = 0.22809370664449838
Iteration 160, Training loss = 0.22643272320811564
Iteration 170, Training loss = 0.2256083577298201
Iteration 180, Training loss = 0.2238810987999806
Iteration 190, Training loss = 0.22437980627784362
Iteration 200, Training loss = 0.22245235139360794
Iteration 210, Training loss = 0.2210668852696052
Iteration 220, Training loss = 0.22001134203030512
Iteration 230, Training loss = 0.21940480831723946
Iteration 240, Training loss = 0.21925953718332145
Iteration 250, Training loss = 0.21745224841512167
Iteration 260, Training loss = 0.21749227866530418
Iteration 270, Training loss = 0.21724667858618957
Iteration 280, Training loss = 0.2165847965157949
Iteration 290, Training loss = 0.21523288981272623
Model training time: 42.48764753341675
Device: cuda
Iteration 0, Training loss = 0.9363781087673627
Iteration 10, Training loss = 0.2736373801644032
Iteration 20, Training loss = 0.2554055056892909
Iteration 30, Training loss = 0.25053538468021613
Iteration 40, Training loss = 0.24698735057161406
Iteration 50, Training loss = 0.24393692669960168
Iteration 60, Training loss = 0.24177201808645174
Iteration 70, Training loss = 0.24089731247379229
Iteration 80, Training loss = 0.23851958748239738
Iteration 90, Training loss = 0.23675487849574822
Iteration 100, Training loss = 0.23511683740294897
Iteration 110, Training loss = 0.23443435762937254
Iteration 120, Training loss = 0.2309037484228611
Iteration 130, Training loss = 0.22899180708023217
Iteration 140, Training loss = 0.22692735665119612
Iteration 150, Training loss = 0.22726719396618697
Iteration 160, Training loss = 0.22422609936732513
Iteration 170, Training loss = 0.22387123623719582
Iteration 180, Training loss = 0.22212130318467432
Iteration 190, Training loss = 0.22162805239741618
Iteration 200, Training loss = 0.22007072545014894
Iteration 210, Training loss = 0.2194505545955438
Iteration 220, Training loss = 0.21898971985165888
Iteration 230, Training loss = 0.2186952683214958
Iteration 240, Training loss = 0.2171400819833462
Iteration 250, Training loss = 0.21813202935915726
Iteration 260, Training loss = 0.21601257473230362
Iteration 270, Training loss = 0.21524421240274721
Iteration 280, Training loss = 0.21470062320048994
Iteration 290, Training loss = 0.21436630418667427
Model training time: 42.48310708999634
Device: cuda
Iteration 0, Training loss = 1.0954441794982324
Iteration 10, Training loss = 0.27582988945337444
Iteration 20, Training loss = 0.25876707383073294
Iteration 30, Training loss = 0.2517273383071789
Iteration 40, Training loss = 0.24804839549156335
Iteration 50, Training loss = 0.24364859811388528
Iteration 60, Training loss = 0.24156459191670784
Iteration 70, Training loss = 0.23781274516995138
Iteration 80, Training loss = 0.23482913409288114
Iteration 90, Training loss = 0.23329554354915252
Iteration 100, Training loss = 0.22974863447822058
Iteration 110, Training loss = 0.22874796877686793
Iteration 120, Training loss = 0.22508229630497786
Iteration 130, Training loss = 0.22396532961955437
Iteration 140, Training loss = 0.223180258503327
Iteration 150, Training loss = 0.22020657245929426
Iteration 160, Training loss = 0.2183006385771128
Iteration 170, Training loss = 0.2173026714187402
Iteration 180, Training loss = 0.21522707033615845
Iteration 190, Training loss = 0.21445525638186014
Iteration 200, Training loss = 0.2140128549474936
Iteration 210, Training loss = 0.21268326416611671
Iteration 220, Training loss = 0.21118440765600938
Iteration 230, Training loss = 0.2112932574863617
Iteration 240, Training loss = 0.21037613399899924
Iteration 250, Training loss = 0.20886302882662186
Iteration 260, Training loss = 0.20858248237233895
Iteration 270, Training loss = 0.20827941309947234
Iteration 280, Training loss = 0.2070291919203905
Iteration 290, Training loss = 0.20701823928035223
Model training time: 42.39269733428955
Device: cuda
Iteration 0, Training loss = 0.9838195729714173
Iteration 10, Training loss = 0.27644999669148373
Iteration 20, Training loss = 0.2579164370321311
Iteration 30, Training loss = 0.2490950129353083
Iteration 40, Training loss = 0.24340818679103485
Iteration 50, Training loss = 0.23902670150766006
Iteration 60, Training loss = 0.23594765909589255
Iteration 70, Training loss = 0.23120928985568193
Iteration 80, Training loss = 0.2279626351709549
Iteration 90, Training loss = 0.22462904309997192
Iteration 100, Training loss = 0.22316359843199068
Iteration 110, Training loss = 0.21995582890052062
Iteration 120, Training loss = 0.21892864916187066
Iteration 130, Training loss = 0.21687797686228386
Iteration 140, Training loss = 0.21489865605074626
Iteration 150, Training loss = 0.213143989443779
Iteration 160, Training loss = 0.2121658528653475
Iteration 170, Training loss = 0.21119892396605933
Iteration 180, Training loss = 0.2087627826974942
Iteration 190, Training loss = 0.20837518716087708
Iteration 200, Training loss = 0.20773390909800163
Iteration 210, Training loss = 0.20645372168375895
Iteration 220, Training loss = 0.20578462143356985
Iteration 230, Training loss = 0.20420320145785809
Iteration 240, Training loss = 0.2051395129125852
Iteration 250, Training loss = 0.20340609751068628
Iteration 260, Training loss = 0.20251250037780175
Iteration 270, Training loss = 0.20296573638916016
Iteration 280, Training loss = 0.20165288391021582
Iteration 290, Training loss = 0.20137872059757894
Model training time: 42.39516496658325
{'activation_functions': ['relu', 'sigmoid'], 'batch_size': 256, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [12, 8], 'optimizer_type': <class 'torch.optim.sgd.SGD'>}
Device: cuda
Iteration 0, Training loss = 1.3422581083499467
Iteration 10, Training loss = 0.9856522920039984
Iteration 20, Training loss = 0.969728912298496
Iteration 30, Training loss = 0.9417386685426419
Iteration 40, Training loss = 0.9010428568491569
Iteration 50, Training loss = 0.8315667521495086
Iteration 60, Training loss = 0.7383556835926496
Iteration 70, Training loss = 0.6335494896540275
Iteration 80, Training loss = 0.5396544658220731
Iteration 90, Training loss = 0.46939713106705594
Iteration 100, Training loss = 0.42544182733847546
Iteration 110, Training loss = 0.40036071378451127
Iteration 120, Training loss = 0.38439496835836995
Iteration 130, Training loss = 0.37268415036109775
Iteration 140, Training loss = 0.36357328926141447
Iteration 150, Training loss = 0.3537699391062443
Iteration 160, Training loss = 0.34758987764899546
Iteration 170, Training loss = 0.3407372339413716
Iteration 180, Training loss = 0.33592839968892246
Iteration 190, Training loss = 0.3314272216879405
Iteration 200, Training loss = 0.32752695221167344
Iteration 210, Training loss = 0.3258573261017983
Iteration 220, Training loss = 0.32207823114899486
Iteration 230, Training loss = 0.31865200887505823
Iteration 240, Training loss = 0.31592671544506
Iteration 250, Training loss = 0.3136772057757928
Iteration 260, Training loss = 0.3123860230239538
Iteration 270, Training loss = 0.30917436371629053
Iteration 280, Training loss = 0.3076223355646317
Iteration 290, Training loss = 0.3055948274066815
Model training time: 39.573999881744385
Device: cuda
Iteration 0, Training loss = 1.0117914871527598
Iteration 10, Training loss = 1.0035706483400786
Iteration 20, Training loss = 0.9946145438230954
Iteration 30, Training loss = 0.9848827639451394
Iteration 40, Training loss = 0.9730723465864475
Iteration 50, Training loss = 0.9554967284202576
Iteration 60, Training loss = 0.9291836630839568
Iteration 70, Training loss = 0.8847095335905368
Iteration 80, Training loss = 0.8099847860061206
Iteration 90, Training loss = 0.7024272164473166
Iteration 100, Training loss = 0.569391107903077
Iteration 110, Training loss = 0.4640955959375088
Iteration 120, Training loss = 0.40914792051682103
Iteration 130, Training loss = 0.38518287585331845
Iteration 140, Training loss = 0.3716223050768559
Iteration 150, Training loss = 0.3625787840439723
Iteration 160, Training loss = 0.3547686246725229
Iteration 170, Training loss = 0.34768414955872756
Iteration 180, Training loss = 0.34177026267235094
Iteration 190, Training loss = 0.3360794536196269
Iteration 200, Training loss = 0.3313018570725734
Iteration 210, Training loss = 0.3272919706427134
Iteration 220, Training loss = 0.32417303409713966
Iteration 230, Training loss = 0.32065107128941095
Iteration 240, Training loss = 0.31978683116344303
Iteration 250, Training loss = 0.3164568431675434
Iteration 260, Training loss = 0.31511681240338546
Iteration 270, Training loss = 0.31344574002119213
Iteration 280, Training loss = 0.3116394416070901
Iteration 290, Training loss = 0.30981772536268604
Model training time: 39.6601836681366
Device: cuda
Iteration 0, Training loss = 1.055432901932643
Iteration 10, Training loss = 0.9785473656195861
Iteration 20, Training loss = 0.9594954573191129
Iteration 30, Training loss = 0.927982394511883
Iteration 40, Training loss = 0.8795712338044093
Iteration 50, Training loss = 0.798208624124527
Iteration 60, Training loss = 0.6819340999309833
Iteration 70, Training loss = 0.5572283285168501
Iteration 80, Training loss = 0.4619686712439244
Iteration 90, Training loss = 0.4115888017874498
Iteration 100, Training loss = 0.38614169794779557
Iteration 110, Training loss = 0.37076776646650755
Iteration 120, Training loss = 0.3598206604902561
Iteration 130, Training loss = 0.3513195835627042
Iteration 140, Training loss = 0.3435485838697507
Iteration 150, Training loss = 0.33779876736494213
Iteration 160, Training loss = 0.3322663983473411
Iteration 170, Training loss = 0.3276578996043939
Iteration 180, Training loss = 0.3232025094330311
Iteration 190, Training loss = 0.31967457899680507
Iteration 200, Training loss = 0.31722481233569294
Iteration 210, Training loss = 0.31402504759339184
Iteration 220, Training loss = 0.31160677854831403
Iteration 230, Training loss = 0.30994920747784466
Iteration 240, Training loss = 0.30703141540288925
Iteration 250, Training loss = 0.3051332198083401
Iteration 260, Training loss = 0.3035151714888903
Iteration 270, Training loss = 0.3016474788578657
Iteration 280, Training loss = 0.3005563805882747
Iteration 290, Training loss = 0.3002964834181162
Model training time: 39.35151243209839
Device: cuda
Iteration 0, Training loss = 1.3277832395755327
Iteration 10, Training loss = 0.9970882122333233
Iteration 20, Training loss = 0.9898039916386971
Iteration 30, Training loss = 0.9836554263646786
Iteration 40, Training loss = 0.971919582440303
Iteration 50, Training loss = 0.9558146916902982
Iteration 60, Training loss = 0.9259316726372793
Iteration 70, Training loss = 0.8762152034502763
Iteration 80, Training loss = 0.7910662190272257
Iteration 90, Training loss = 0.66871589078353
Iteration 100, Training loss = 0.5412287333836923
Iteration 110, Training loss = 0.451093179675249
Iteration 120, Training loss = 0.4075262529345659
Iteration 130, Training loss = 0.3877027825667308
Iteration 140, Training loss = 0.3755080992212662
Iteration 150, Training loss = 0.36687997900522673
Iteration 160, Training loss = 0.3600232394842001
Iteration 170, Training loss = 0.353843598984755
Iteration 180, Training loss = 0.34707871824502945
Iteration 190, Training loss = 0.342014841735363
Iteration 200, Training loss = 0.33651072531938553
Iteration 210, Training loss = 0.33177464140149265
Iteration 220, Training loss = 0.3282503095956949
Iteration 230, Training loss = 0.32488259501182115
Iteration 240, Training loss = 0.3214250903290052
Iteration 250, Training loss = 0.31877995225099415
Iteration 260, Training loss = 0.3161612405226781
Iteration 270, Training loss = 0.31372339106523073
Iteration 280, Training loss = 0.3113797696737143
Iteration 290, Training loss = 0.3099154388675323
Model training time: 39.361713886260986
Device: cuda
Iteration 0, Training loss = 1.122341317626146
Iteration 10, Training loss = 0.9923796642285126
Iteration 20, Training loss = 0.9754019436927942
Iteration 30, Training loss = 0.9541575931585752
Iteration 40, Training loss = 0.9222658826754644
Iteration 50, Training loss = 0.8659745248464438
Iteration 60, Training loss = 0.7801567224355844
Iteration 70, Training loss = 0.6511533283270322
Iteration 80, Training loss = 0.5149086902921016
Iteration 90, Training loss = 0.4241178614588884
Iteration 100, Training loss = 0.38362274720118594
Iteration 110, Training loss = 0.36696279736665577
Iteration 120, Training loss = 0.35618388652801514
Iteration 130, Training loss = 0.34750335262371945
Iteration 140, Training loss = 0.340988962409588
Iteration 150, Training loss = 0.334179954460034
Iteration 160, Training loss = 0.32961295115259975
Iteration 170, Training loss = 0.3260287929039735
Iteration 180, Training loss = 0.321852107747243
Iteration 190, Training loss = 0.31902671175507397
Iteration 200, Training loss = 0.316047186175218
Iteration 210, Training loss = 0.31413605350714463
Iteration 220, Training loss = 0.3126058870783219
Iteration 230, Training loss = 0.31148172026643384
Iteration 240, Training loss = 0.30939153123360413
Iteration 250, Training loss = 0.3089288708109122
Iteration 260, Training loss = 0.30737963404793006
Iteration 270, Training loss = 0.30501509744387406
Iteration 280, Training loss = 0.3050097625416059
Iteration 290, Training loss = 0.3047487288713455
Model training time: 39.795843839645386
best score: 0.21020397543907166 
 best params: {'activation_functions': ['relu', 'sigmoid'], 'batch_size': 64, 'learning_rate': 0.002, 'nb_epoch': 300, 'neuron_layers': [16, 8], 'optimizer_type': <class 'torch.optim.adam.Adam'>}
/var/spool/slurm/d/job102257/slurm_script: line 10: /user/bin/nividia-smi: No such file or directory
 09:51:19 up 42 days, 15 min,  5 users,  load average: 1.98, 2.00, 1.98
